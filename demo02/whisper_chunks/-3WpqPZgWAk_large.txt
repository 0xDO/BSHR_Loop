{"text": " Morning everybody, David Shapiro here. I'm doing the rare recording two videos back to back because I woke up with inspiration. Woke up at 5 in the morning and was ready to go. Okay, so the reason that you're here, this dude, Ravi, he asked a good question and he put it so succinctly that I need to borrow his language. So thank you Ravi for asking this question. He's not the first person to ask this question, but he put it very succinctly. So basically he says he wants school kids, wants to help school kids with a conversational app that is friendly. So that, the chatbot we've got worked out. I have a previous video where I did my tutor chatbot, works perfectly fine. Here's an example. Fine-tuned model, it's got 600 samples. It can handle adversarial students. It can handle emotional issues. It's very kind. It will adapt to whatever level the person needs. If you ask it about quantum physics, it'll answer quantum physics. If you ask it about colors, it'll talk about colors. It can handle all age range, all needs. So I've got that part covered. But there are two primary issues. One is GPT-3 is static. It was trained, you know, I think the update was it was updated with information circa 2021 already aging. So how do you integrate new information? Big issue. And a similar issue has to do with memories. And I love this term goldfish memory. It doesn't remember anything long term. Goldfish have short memory. There is some science out there that goldfish actually have longer memories than you might think, but the point is that, you know, it's got a tiny little brain. It can't remember long-term things. So, how do we solve these two problems? That's what we're doing today. So, I have created a new repo. It's called Long-Term Chat with External Sources. So let me just show you that repo real quick. Sorry, I don't know why I'm so... I've got some ginger tea which will... Ow! That's hot. I just spilled hot tea on my leg. I'm struggling this morning. Okay, Long Term Chat External Sources. So we've got chat figured out. That's not a big deal, right? You know, you fine tune a model, you've got a chat bot, okay, whatever. But let's solve these other two problems where we've got long-term memory and external sources. So there's two primary ways that I'm going to tackle this. Oh, here's the repo for the chat bot if you need to see that. So this is how I created the fine tune for that. And then this is the multi-document answering. So this is how do you query information from an external source? That's this. What I'm going to do is instead of querying a document, I'm going to actually query Wikipedia. Because Wikipedia is a live source of external information and it's searchable. So that's what we'll do. And yes. So without further ado, let's jump in. I got started. I know that you guys usually like to see the whole process. These are just copy-pasted mostly from this one, from answerquestions.py, because I solved some problems here, like search indexing, that sort of thing. So I just copy-pasted the functions. So if you want to see how I did all that, go watch the other video. Let's get to the new stuff though. Okay, so first of all, if we're doing a chat, if name equals main, so we'll do conversation. So we'll do, make sure I'm not using that variable anywhere else. Okay, good. So we'll have one list to accumulate the entire conversation. That's probably the only global thing. While true, let's see, input is a function. Let's see, input is a function. I wanna say like user says input. We'll just say user. So that way it'll say like, okay, this is what you say. Okay, so line in equals user, and then we'll do percent user says. Okay, so this will be basically populating this. So user says, hey, Tim, can you teach me about ancient Rome? So I just, I brought this up. One thing I noticed, and this is kudos to OpenAI, this is something I've been complaining about for a while, is that it tends to unload your fine-tuned models. This has been sitting here for 10 minutes. And look, did matrons ever have sex with gladiators? This is a real thing that happened in ancient Rome. Let's see how it handles it. Oh yeah, so I trained it not to engage with sex. So let's see if we can get it to... yeah, he's not gonna talk about it. Okay, that's fine. Because this is meant to be PG or G-rated. Did gladiators ever fight lions? He should answer that. Yes, okay. They're called Venetiones. Oh, cool. So this has been sitting here for like 10 minutes and you saw it was ready to go instantly. So good job, OpenAI. Thank you for fixing that. That was the most annoying thing ever was that our fine-tuned models were not ready to go. Okay, so fine-tuned models ready to go at all times. Wonderful. My Spanish teacher used to say todo el mundo, so all the world. I guess that's a turn of phrase in Spain where it's just like everything is good. Let's go. So todo el mundo. Or maybe that was just something she said. I don't know. OK. Brain reset. What are we doing? OK. So we have this. And then what do we need to do? Well, it's not just that simple. Once we have a conversation going. So what we'll do is we'll say conversation.appendLineIn. OK, so we'll just start this. Conversation starts. We put something in. Excuse me. And then what do we do? Well, first, we need to search the rest of the conversation. So what we're going to do is search indexed. So we're going to search. Let's cut this down a little. Okay. So count five. That's going to error out the first time because it's going to be too short. But, oh, here's how we'll do this. Yeah, yeah, gotcha, gotcha, gotcha. Okay, so search index, the data is gonna be the conversation. Oh, I need to actually keep track of the whole thing. Hmm, hmm, okay. Okay. So the conversation will actually need to have the embeddings with it. Yeah. Vector equals GPT-3 embedding, and then we'll do line in. And so then we'll say, info equals, no, that needs to be a dictionary. So line of conversation equals line in, and then the vector equals the vector. Okay, and oh, in case it's not obvious, I prefer the term vector because that is mathematically what it returns. But in the context of a large language model, this vector is technically an embedding or a semantic vector. So an embedding is a vector that has semantic meaning, but a vector is just mathematically, this is what it is. Okay, so we will store. So the reason that we're doing this is what it is. Okay, so we will store. So the reason that we're doing this is that every line of conversation, so let me come back over here. Every line of conversation is going to have an associated embedding or vector, which means that this whole thing will be searchable. Okay, so conversation append.info, so we're just gonna accumulate the entire conversation right here. And then what we do is we, I guess we actually won't need this because it's going to, well, yeah, okay. So we'll search, we'll search for the most, for similar, so I'll need to modify the index. Also, I need to move my microphone because it's blocking a little bit of my field of vision. Hope you can still hear me. If not, deal with it. Okay. So we get our line in, and then what do we do? We search for... We search, we search, we search. old lines equals search index and so we'll do... we'll pass info so that's our most recent one. We'll search or pass conversation and we'll leave it at that. I'm not gonna pass a variable for how many to return. We'll just take that out. We'll hard code that, doesn't matter. So we'll do recent and then we'll do all lines. So what you're supposed to do is if you use a variable in a function, especially if it's a list, you're supposed to do is if you use a variable in a function, especially if it's a list, you're supposed to use a different name. Otherwise, because GPT-3 or not GPT-3, Python, Python automatically has lists as global variables. I'm probably saying this wrong, but you can modify the contents of a list from any function. But if it's local to a function, it won't be modified. So if I have a different name, even though it's the same conversation, like I'm not gonna be modifying and I'm just gonna be reading from it. But anyways, this is best practices, I think. Okay, so what we're gonna do is, where did I go? I lost my search index, there we go, okay. We don't need the vector cause we're already gonna have that. So for I in all lines, score is similarity. So we're gonna do this vector. So we're gonna do recent vector. And actually what we'll do is if yeah so if these are the same then we skip our current line so if I vector equals recent vector continue and that should be the last one technically Technically, you could probably just skip the last one, but I don't want to make that assumption. Because what if we're searching, for whatever reason, we reuse this, and we're searching elsewhere in the conversation? I don't want to assume that the one that I'm searching is always going to be the last one. Because I can just imagine someone saying, well, why don't you just skip the last one? Because I don't wanna make that assumption. We might be searching based on an older memory. Okay, so score, we get the score. And then what do we do? Yeah, okay, so we can just do the content. I'll change this to line equals, and then we'll do I line. I think that's what I called it, right? Yeah, we've got line and vector. Okay, so the line of dialogue and the vector, search index, okay. So line equals line and the score equals score, okay? Sorted scores, sort by score, reverse is true so it'll be top down. And then I only want to return the top five, so what we'll do is when we first start the dialog it's going to be shorter than five, so we'll just return, we'll try to return the top five, but if that doesn't work, actually, here's what we'll do. We'll do ordered equals ordered, and then return ordered, because I'm afraid that with the previous one, if it has the return at the same time that it's trying to slice up the list, it might just break entirely, but then except we'll just return ordered. So, cause if this doesn't work, it'll error, it'll bomb right there. And then we'll just skip to this and just return it as is. So this is another paradigm in Python where it's better to ask forgiveness than permission. So rather than test something, you just try accept. You do need to be careful. Whoops, one of my, I put up, you probably see those hexagons in the back. I put up sound dampening things. Let me put that back up. It's gonna bug me. But yeah, I put up sound dampening because you probably noticed it sounds better. There's less echoes. Okay, we're back. Sorry about that. Someone online said that the best way to stick those panels up was blue painter's tape, which seems to be working because it won't peel any paint off, but one just fell off, so I might need to get some better tape or some two-sided tape. Anyways, where were we? Search index. Okay, yeah. So better to ask forgiveness than permission. You do need to be careful. If you do a try-except loop, sometimes it might not have the results that you think and you might gloss over problems. So that's why when I do the try accept loop here, I actually give it output. Cause sometimes the assumptions you're making might be wrong and you need to see why. But in this case, there's like only one thing that could go wrong here. So this is probably a little bit, this is more of a sure thing, right? Cause there's only one action happening. Okay, so we've got that. Old lines equals search index info conversation. You know, honestly, I don't even need to pass the scores back. I don't care. But whatever. Okay, so we get an input, we get the old conversation. So this will find anything that's most relevant. We'll find the most relevant five lines of conversation. This number is arbitrary. Where did it go? Actually, you know what I'll do? I'll do, I'll add count back in. Count equals five. And then we'll just do that. So that way, if you reuse this code, you can just pass however much you want. Because imagine that this conversation gets much longer and you want to return like 50, right? Because you can easily fit that into, actually, yeah, let's do a little bit longer. Let's do 10. So we'll get the 10 most relevant lines. And then, okay, so the whole conversation is going to be accumulated here. The old lines will be there. And then we'll just pass in 10. Let's cut off. Let's see, latest conversation equals, let's see, actually no, here. If Len conversation, no, recent conversation equals, here, do, there we go, if len conversation is greater than 30. So we'll say recent conversation equals equals conversation. And I believe the thing is negative 30.. Let me make sure I'm doing that right. Python L equals list range zero to 100 L, okay. And then L if we do negative 30 colon, that's just the last 30. Okay, perfect. And then do lang just to make sure that that's right. Yes, okay. Yeah, so what this does is, if our conversation is getting too long, we don't wanna pass the entire conversation, right? Just the last 30 lines, that's more than enough. Because you look at this, this is one, two, three, four, five, six, seven, eight, nine, 10, 11 9 10 11 12 13 14. So this is 14. So this is less than half of that. And obviously this is enough context to know what we're talking about. So 14 lines of conversation is plenty. 30 will be way more than enough. That is a pretty long conversation. And then on top of that we're going to have the 10 oldest lines, right? Or the 10 most relevant older lines. Okay. So, now what? We're searching our index, we're searching the conversation, we're getting the most recent conversation. So, this is 40 lines. So, now what? What are we missing? We're missing external data. Okay. Okay, so convo block equals, so we'll do n, so this is new line dot join, and then we'll do old lines. So this is going to just be the old bits of the conversation plus I probably need to do a new line between there and then plus, and we'll do the same thing and we'll join recent conversation. So basically what this will do is it'll combine it into one chat block, right? So it's like, okay, we've got this. So now what this will do is it'll combine it into one chat block, right? So it's like, okay, we've got this. So now what? We've got our convo block. What we need to do now is we need to do a prompt. So prompt and just copy a line. So prompt equals open file. And we'll just do prompt keyword, replace block with convo block. Okay, so now we need to do some prompt engineering. So basically what we're gonna do is we're gonna, I'll use this as an example. So we're gonna say, I'll use this as an example. So we're going to say, read the following conversation and generate, actually, and let's single relevant Wikipedia article title. Okay, so then we'll do most relevant Wikipedia article title. So let's see if this works. I want to keep this up, so I'm just gonna duplicate this tab and we'll go to DaVinci Instruct. Okay, let's see what it says. Romulus Augustus, cool. And then let's go to Wikipedia. Wikipedia, why is my, ugh. I've already had coffee. Romulus Augustus, bam, got it, okay. So what we can do then is we'll query this, we'll get it from Wikipedia. So this worked really well. It's always nice when your intuition is like spot on. Okay, so we've got our prompt. Let's go ahead and just do block and we'll call this prompt. What did I call it? Keyword. Actually, let's call this Wikipedia because it's very specific. Specifically let's write a prompt to get Wikipedia. Prompt Wikipedia. So this presupposes that GPT-3 will know the correct Wikipedia article. GPT-3 was trained in part on Wikipedia, so I don't see any problem. There's another way you can do this. So let me show you. If you want something that's more open source or more open-ended, what you can do is actually you could feed this into a question generator. So let's see, where was it? My Naoka question generator. Wow, is it really that old? Questions. Yeah. So I have a question generator, which basically what it does... Do I have an example? Yeah. So I have an example where you feed it anything and it'll generate a list of questions. So natural language questions are going to be much more open-ended in terms of what questions should I ask about this. You could do that, but GPT-3 is also just really great at like, let's see. Read the following conversation and recommend a list of Google search queries to learn more. And then we'll say, list of Google search queries. So you do this. So let's say you want to query Google instead of Wikipedia. You could do this and then just do Google searches. I'll go ahead and save this prompt just so that way you all can see it. So we'll do block and then we'll remove that and save this as prompt Google. Okay so now you guys can see you can experiment with this if you want to use the Google API instead of Wikipedia. Either way both serve as an external source. It's pretty trivial as you see like you just ask gT-3 to generate a list of Google queries and you're done Okay Answer GPT, oh wait, no, no, no. No. No. All right. Yeah So then we say Title equals GPT-3 completion. Prompt, I think that's all I need to pass. Oh, that's a chat completion, GPT-3 completion. Yes, we do not need that many tokens. Let's say like, I'll say 500, because we might use this for other things. No, let's say 200, nice and short. Max tokens for the chat, let's say 100. We don't want super long conversations. Temperature of 0.7, that's fine. Frequency penalty, let's take that out. That should be fine. Okay. Yeah, so we've got our conversation set. We've got a title. And then let's take a look at the Wikipedia PyPI. So if you want to get the article, you just say, here's our page. So I'm going to assume that it works. Actually, no, we'll say, let's not make any bad assumptions. Let's do fetch wiki and we'll do title. Okay, so in this function, because when you're searching for something, you don't always know if it's going to work. So def fetchWikiTitle, and so what we'll do is we'll do try page equals, make sure I get the syntax right, wikipedia.pageTitle. So if it works, we'll just pass this back, return page. If not, this will error out, and it'll say, I don't know what you're talking about. It'll say it exactly like that, too. Except if it doesn't work, we'll do search equals wikipedia.searchTitle. And so you see the syntax for that here where you can do a search and it'll return a list of possible articles. And so like in this case it's like Barack. So it's like do you mean Barack Obama presidency of Barack Obama Barack the Brandy Barack the given name. I have no idea how. I'll probably just pick the first one. So then we'll do page equals Wikipedia.page. And we'll do search 0. So I'll just assume that like return page, I'll just assume that it returns, you know, that the first one is gonna be most relevant. Like I said, this is, I'm just showing you the proof of concept. Okay, so then, so we've got the wiki that will return it. Oh, I also need to import Wikipedia, import Wikipedia. And then do I use OS anywhere? Import OS. I probably do. Okay. Fetch wiki. Okay, so we get our relevant wiki. So then what do we do with it? Well, we found Romulus Augustus. This is way too much text, right? This is way too much to put into a GPT-3 prompt. So what do we do? Well, I'm glad you asked. That's why we have this, the document answering thing. So you know what, let's just go all out. Let's hit this. I was gonna do something simple, but no, let's just go all out. Let's hit this. I was gonna do something simple, but no, let's go whole hog. All right, so we've got the wiki content. Let's see, what is it? If we go here, we do, if we wanna look at the whole content, where was it? Summary, search, page, content. That is okay. So we just do .content. So that will give us just all the text from that particular article. And so then what do we do? Well, I'm glad you asked. So let's go back to our playground and we'll say, read the following conversation and write a list of... Well, we're asking questions. Because basically what I'm thinking is we want to query this document, right? For interesting facts, right? Life in exile. Actually, like the questions we should ask are not necessarily obvious from this. So we get Romulus Augustus. How should we handle this? Because if we, let's say this conversation leads to this, and I don't know what I'm asking. So, federati. Because if I use my multi-document answering thing and we query it, like we got it, we basically, we have to like... We have to be, there has to be something we're looking for. Unless I just want to summarize it. But I don't know if I want to do that, because I want to... Summarizing is good and all, but if you get... If you answer a question, you get much better. get if you answer a question you get much better but but the problem is if this is it this is a chicken-or-the-egg problem or putting the cart before the horse because if you don't read the thing and you don't know what the information is you don't necessarily so there's there's what what I call a naive question so like if if you don't know anything about something you ask naive questions right a naive question is just like, well, here's a perfect example. Who was the last emperor? I had to already know that Rome had emperors. So that's a semi-naive question. It's like, OK, so Romulus Augustus. Whereas if I ask, so this is a super not naive question right because I knew that there was a virtuous Roman man that there was this concept in ancient Rome but how do you ask the question to even get to the right question so we need like meta questions but GPT-3 is trained on all this stuff, so maybe it could...I don't know. I wonder, I wonder, I wonder. Follow-up questions to search the internet. Yeah, that's fine. We'll just do search the internet because it's like list, let's see, list of follow-up questions to add to the conversation. And then let's just see what it does. What caused the fall of the Western Empire? Oh, wow. Okay. What was life like for the average person in Rome? This is great. It's not really staying on topic though because if you ask these questions, that's not going to be information that is necessarily here. Let's see. Given the following conversation and the selected topic, so we'll do Romulus Augustus, write a follow-up question to ask. No. Because if we look at this, like what is the kind of information that I want to extract from this, right? Like the summary of this dude's life or what did he do? I'll write a follow-up question to ask. Follow-up question to ask about Romulus Augustus. Let's just see what it does. That's not a bad question. Okay. I have to take a quick bio break, so I'm going to pause and then we'll be right back. I think this is what we'll go with. All right, and we're back. Where were we? Oh, yeah. Okay. So, we've got the follow-up question. We have identified ... Where did we go? So, we look in here, we identified, we used our Wikipedia prompt to get a particular topic, right? So, we get that. We then, I wrote a quick thing just to go fetch an article, so we'll get it and then what we'll do is we will do a follow-up question. So we've got a topic. Given the following conversation and the selected topic. So we'll do topic. And then we'll do block. and then we'll do block. And we'll do one follow up question to ask about. And we'll do topic. Okay. And then we'll do prompt follow up. Okay, so then what we do is we go to here. And so we say Control-D for duplicate. And so then we'll do prompt follow up. And we'll replace the block with ConvoBlock and we'll also do replace topic with the title. And so what that'll do is that'll take this and populate it with the way that it looked up here, right? So it'll just say, okay, let's populate in the conversation block and ask a follow-up question. So then once we have the follow-up question We can then Use the same logic that we had here to To Brain come on We're going to index the content of this Wikipedia article and then we're going to search it and answer the question. So then what we'll do is we'll do answer equals, we'll do answer question and we'll pass in the Wikipedia article. Oh, wait, we actually need to do question. And the question. I think that's all we need to do because the question will be generated here. Generate a specific follow-up question to use to query the external information. And then we'll do, generate a search query to find external article in the wide world. Okay, add a couple comments just because this is becoming a really big block. Let's see, get user input and vector and then we'll say search conversation for previous relevant lines of dialogue. OK, that's good. So we'll get the answer for answer question. Def answer question. And so then we'll pass in the article and the question. So then what we'll do is we need, we've already got text wrap. So we'll basically do the same thing we did here, but I also need to open, whoops, no, that was the correct thing, build index. So this is super, super simple. Basically all we do is we cut this into chunks. So when we come in here, we cut this into chunks. So when we come in here, we'll say chunks equals text wrap, and instead of all text, we'll do article, and then we'll do it in, let's do smaller. So we'll do 2000 characters. And then for, actually, I guess we don't even need to index it because we're just asking the question. So this will be a little bit simpler because the reason, so in this one, the reason that I did it the way that I did is because I was basically saying like, okay, let's go ahead and index an arbitrarily large thing. Because if you have like 150 entries or 150,000 entries, you need to be able to search that database and then answer the question. But in this case, we're already zeroed in on one article. So we don't need to answer the whole thing. So in that case, what we'll do is we'll just go ahead and skip ahead to answering the question and we'll probably just copy this prompt. Let's see, which was it? Prompt answer. Yeah. Use the following passage to give a detailed answer to the question. That's great. So we'll just save as and we'll come back to our long-term chat and we'll call prompt answer. And so now that should appear here, prompt answer. don't need it. And then for chunk in chunks prompt equals open file replace passage with we're gonna replace that with chunk and we're gonna replace query with question. Okay and to keep it consistent let's name it the same thing so we're gonna do question is question. So basically what we're gonna do is we're gonna cut up yeah use the following passage to give a detailed answer to the question. Ooh, actually, I guess I probably should search it because the answer might not be right, like fall. Yeah. See, also fall of the Western Empire. Yeah, okay, because the answer might not be in it. So let's add something. Use the following passage to give a detailed answer to the question. If the answer is not present, and the passage, just write not present. Okay. and the passage just write not present. Okay. Detailed answer or not present. All right, so let's test this real quick. So what we'll do is we'll come here and we'll say, okay, so the question is, and, oh darn, hang on, I need this question. Wait, give me the question. OK, so we copy the question, and then let's go here. So we'll say, OK, and question. OK, so this is our question. What made Romulus Augustus the last emperor of the Roman Empire? So in the passage, we're going to have a small chunk and actually I think I was wrong. I think we should go, we should make this a little bit bigger because many Wikipedia articles will fit. It will say 4,000 tokens is 16,000 characters, so let's say like 10,000. Because if the whole thing fits, great. If not, we'll skip it. And then let's go back here and just grab a chunk. So like name, So like name, here, we'll just grab the summary and see how it handles this. Okay, passage, detailed answer or not present. Okay. Hey. Is that there? Yeah. All right. So the answer was there. Let's see if there is a section that it doesn't have it. Ancestry and family. So let's just grab this section because it looks like the answer will not be. Okay. Because we want it to be able to say not present. He was the last emperor of the Western Roman Empire because he was the last emperor of the Western Roman Empire. Wow, the floor is made of floor. Thank you. Okay. So it didn't really work there. But at least it didn't give me too much information. So that's not too bad. Okay, so then we'll say answers equals list, because here's a fun trick. So then you say answers.appendlist, oh whoops, no, not list, answer. And then what you do is you have one final summarization step. So let's get this, and I'll show you what this does. All right, so we got that answer. And then, because, oh darn. OK, that's fine. I liked its original one better. Let's see if I can do, what was it, Control-Shift-Z? There we go. better. Let's see if I can do was a control shift Z. There we go. Okay, so we'll take the passage and we'll take what was it his legacy or no it gave me a good answer from just the introduction. Okay, so we'll do that and then okay so And then, okay. So it gave me a shorter answer, that's fine. So we'll accumulate this here. So let's ask this question of a couple more spots. So we'll do ancestry and family. Just so that way I can show you that like, if you have different answers, Ancestry and family. Just so that way I can show you that if you have different answers or different things, the GPT-3 is still pretty good at summarizing it together. Yeah, so that's not even remotely relevant. Okay, so that's fine. RSDs, question what made our Omnulous? Yep, okay, so then we'll do last one in rain. So we'll just, basically what I'm doing is I'm pretending that this article is much longer and that it's giving me that I needed to split it up into four different chunks in order to get this answer. This made the last emperor. Okay, so there is relevant information in each of these sections. That's great. Okay, so then what we do is we take all this and we say merge the following answers into a single paragraph. The question asked was, and that should still be, oh darn. Let me just copy paste this whole thing. Okay, merge the following answers into a single coherent paragraph. The question asked was, so then we'll do... Oh, I have another thing falling off. Let me pause real quick. Okay, and we're back. All right. The question asked was, okay, and then various answers. And then various answers. So then we'll come back to here and grab. Wait, where did I accumulate those answers? Here we are. Various answers, and then let's do this all caps. Various answers, and then merged answer actually let's say merged paragraph let's see how that does in Gallagher Orestes instead marched on Ravenna, the capital of Western and proclaimed Romulus. And Napostad was only a figurehead with Orestes running much of the imperial administration. Interesting. Okay. But it doesn't mention Odoacer. Odoacer. Well, whatever. That's fine. Let's go to the Wikipedia article and see. Odoacer captured Ravenna, killing her estes deputy. Okay. Dana, killing at rest, he's deputy. Okay. All right, whatever. Okay, so we've got it broken up into a couple sections. We summarize it here. I wonder what happens if, that should be fine. Yeah, that'll be fine. Okay, so we'll do one last thing. We'll save this as answers question. And then we'll do prompt and we'll call this prompt merge. Okay, so then we're closing in on complete, I think. All right, so we got all of our answers, and so answers equals, no, answer equals, we'll say space.join answers, so that just merges it all into one block. And we'll say prompt equals... Well, here we'll just... prompt merge. And we have answers and question. So replace question with question. I already had that. And then we'll replace answers with answer. I think that's how I did it. Yes. Various answers into a single coherent paragraph. The question asked was. OK, that should work. And so then we get that. GPT-3 completion. We do the encoding here. So this was a thing that I started adding, because especially when you use external sources, sometimes there's characters that GPT-3 does not like, but if you convert it to ASCII and ignore errors and then decode it back, then GPT-3 seems to be happy. And then return answer. Okay, so we've got, we pass it a Wikipedia article, we ask a question, we break it into chunks, if necessary, might not be necessary. We ask questions and then pass it back. Okay, so now that we've got this, it's like, okay, wow, we've done a lot. Now what? So let's copy this and we'll copy this out to here. And let's go back here. And let's go back here and we'll say, background info. User. What caused the fall of Rome? Tim. Why didn't you give me an answer? There we go. But you gotta include this. Because we've got this information, which is good. So part of the problem is, and this might not work, I might have to exclude this part, is because this model was not fine-tuned to incorporate other information. Let's see. Oops, user. I wonder if instead of using the fine-tuned model, I could just, what if I just exclude the fine-tuned model and just have this information? It would require reworking this fine-tuned model. So I might have to stop here and split this up because this fine-tuned model does not know how to incorporate additional information. Darn. But that's okay, we're already at almost an hour, so it happens. We'll stop here, we'll leave it off where I need to create another model that will incorporate background information, external information into the chat. Because this chat model is already pretty good, but it's missing a couple things. All right, thanks for watching.", "chunks": [{"timestamp": [0.0, 3.28], "text": " Morning everybody, David Shapiro here."}, {"timestamp": [3.28, 8.96], "text": " I'm doing the rare recording two videos back to back because I woke up with inspiration."}, {"timestamp": [8.96, 10.84], "text": " Woke up at 5 in the morning and was ready to go."}, {"timestamp": [10.84, 17.12], "text": " Okay, so the reason that you're here, this dude, Ravi, he asked a good question and he"}, {"timestamp": [17.12, 20.04], "text": " put it so succinctly that I need to borrow his language."}, {"timestamp": [20.04, 23.24], "text": " So thank you Ravi for asking this question."}, {"timestamp": [23.24, 27.02], "text": " He's not the first person to ask this question, but he put it very succinctly."}, {"timestamp": [27.02, 31.1], "text": " So basically he says he wants school kids,"}, {"timestamp": [31.1, 33.38], "text": " wants to help school kids with a conversational app"}, {"timestamp": [33.38, 34.82], "text": " that is friendly."}, {"timestamp": [36.14, 38.5], "text": " So that, the chatbot we've got worked out."}, {"timestamp": [38.5, 43.5], "text": " I have a previous video where I did my tutor chatbot,"}, {"timestamp": [43.82, 44.86], "text": " works perfectly fine."}, {"timestamp": [44.86, 46.04], "text": " Here's an example."}, {"timestamp": [46.04, 48.6], "text": " Fine-tuned model, it's got 600 samples."}, {"timestamp": [48.6, 52.78], "text": " It can handle adversarial students."}, {"timestamp": [52.78, 54.62], "text": " It can handle emotional issues."}, {"timestamp": [54.62, 55.46], "text": " It's very kind."}, {"timestamp": [55.46, 58.26], "text": " It will adapt to whatever level the person needs."}, {"timestamp": [58.26, 59.64], "text": " If you ask it about quantum physics,"}, {"timestamp": [59.64, 61.28], "text": " it'll answer quantum physics."}, {"timestamp": [61.28, 63.9], "text": " If you ask it about colors, it'll talk about colors."}, {"timestamp": [63.9, 74.24], "text": " It can handle all age range, all needs. So I've got that part covered. But there are two primary issues."}, {"timestamp": [74.24, 82.08], "text": " One is GPT-3 is static. It was trained, you know, I think the update was it was updated"}, {"timestamp": [82.08, 85.04], "text": " with information circa 2021"}, {"timestamp": [85.04, 86.32], "text": " already aging."}, {"timestamp": [86.32, 88.92], "text": " So how do you integrate new information?"}, {"timestamp": [88.92, 90.4], "text": " Big issue."}, {"timestamp": [90.4, 93.88], "text": " And a similar issue has to do with memories."}, {"timestamp": [93.88, 96.88], "text": " And I love this term goldfish memory."}, {"timestamp": [96.88, 99.44], "text": " It doesn't remember anything long term."}, {"timestamp": [99.44, 103.44], "text": " Goldfish have short memory."}, {"timestamp": [103.44, 106.0], "text": " There is some science out there that goldfish actually have longer"}, {"timestamp": [106.0, 109.0], "text": " memories than you might think, but the point is that, you know,"}, {"timestamp": [109.0, 112.0], "text": " it's got a tiny little brain. It can't remember long-term things."}, {"timestamp": [112.0, 115.0], "text": " So, how do we solve"}, {"timestamp": [115.0, 118.0], "text": " these two problems? That's what we're doing today."}, {"timestamp": [118.0, 121.0], "text": " So, I have created a new repo."}, {"timestamp": [121.0, 124.0], "text": " It's called Long-Term Chat with External"}, {"timestamp": [124.0, 125.0], "text": " Sources."}, {"timestamp": [125.0, 128.0], "text": " So let me just show you that repo real quick."}, {"timestamp": [128.0, 131.0], "text": " Sorry, I don't know why I'm so..."}, {"timestamp": [131.0, 133.0], "text": " I've got some ginger tea which will..."}, {"timestamp": [133.0, 135.0], "text": " Ow! That's hot."}, {"timestamp": [135.0, 137.0], "text": " I just spilled hot tea on my leg."}, {"timestamp": [137.0, 139.0], "text": " I'm struggling this morning."}, {"timestamp": [139.0, 142.0], "text": " Okay, Long Term Chat External Sources."}, {"timestamp": [142.0, 144.0], "text": " So we've got chat figured out."}, {"timestamp": [144.0, 146.0], "text": " That's not a big deal, right?"}, {"timestamp": [146.0, 150.4], "text": " You know, you fine tune a model, you've got a chat bot, okay, whatever."}, {"timestamp": [150.4, 158.02], "text": " But let's solve these other two problems where we've got long-term memory and external sources."}, {"timestamp": [158.02, 160.88], "text": " So there's two primary ways that I'm going to tackle this."}, {"timestamp": [160.88, 168.0], "text": " Oh, here's the repo for the chat bot if you need to see that. So this is how I created the fine tune for that."}, {"timestamp": [168.0, 172.0], "text": " And then this is the multi-document answering."}, {"timestamp": [172.0, 177.0], "text": " So this is how do you query information from an external source?"}, {"timestamp": [177.0, 179.0], "text": " That's this."}, {"timestamp": [179.0, 182.0], "text": " What I'm going to do is instead of querying a document,"}, {"timestamp": [182.0, 186.0], "text": " I'm going to actually query Wikipedia."}, {"timestamp": [186.68, 190.64], "text": " Because Wikipedia is a live source of external information and it's searchable."}, {"timestamp": [191.64, 192.88], "text": " So that's what we'll do."}, {"timestamp": [193.84, 195.48], "text": " And yes."}, {"timestamp": [195.64, 197.88], "text": " So without further ado, let's jump in."}, {"timestamp": [198.56, 199.64], "text": " I got started."}, {"timestamp": [200.2, 203.44], "text": " I know that you guys usually like to see the whole process."}, {"timestamp": [203.52, 210.24], "text": " These are just copy-pasted mostly from this one,"}, {"timestamp": [211.28, 215.36], "text": " from answerquestions.py, because I solved some problems here,"}, {"timestamp": [216.0, 218.08], "text": " like search indexing, that sort of thing."}, {"timestamp": [218.64, 220.32], "text": " So I just copy-pasted the functions."}, {"timestamp": [220.32, 223.6], "text": " So if you want to see how I did all that, go watch the other video."}, {"timestamp": [223.6, 224.88], "text": " Let's get to the new stuff though."}, {"timestamp": [232.68, 245.84], "text": " Okay, so first of all, if we're doing a chat, if name equals main, so we'll do conversation. So we'll do, make sure I'm not using"}, {"timestamp": [245.84, 247.68], "text": " that variable anywhere else."}, {"timestamp": [247.68, 248.52], "text": " Okay, good."}, {"timestamp": [248.52, 253.38], "text": " So we'll have one list to accumulate the entire conversation."}, {"timestamp": [255.04, 256.8], "text": " That's probably the only global thing."}, {"timestamp": [256.8, 261.8], "text": " While true, let's see, input is a function."}, {"timestamp": [265.0, 267.9], "text": " Let's see, input is a function."}, {"timestamp": [272.46, 274.8], "text": " I wanna say like user says input."}, {"timestamp": [276.34, 278.12], "text": " We'll just say user."}, {"timestamp": [282.52, 287.56], "text": " So that way it'll say like, okay, this is what you say. Okay, so line in equals user,"}, {"timestamp": [292.12, 295.48], "text": " and then we'll do percent user says."}, {"timestamp": [295.48, 299.64], "text": " Okay, so this will be basically populating this."}, {"timestamp": [299.64, 301.04], "text": " So user says, hey, Tim,"}, {"timestamp": [301.04, 303.0], "text": " can you teach me about ancient Rome?"}, {"timestamp": [303.0, 304.38], "text": " So I just, I brought this up."}, {"timestamp": [304.38, 307.08], "text": " One thing I noticed, and this is kudos to OpenAI,"}, {"timestamp": [307.08, 309.88], "text": " this is something I've been complaining about for a while,"}, {"timestamp": [309.88, 312.74], "text": " is that it tends to unload your fine-tuned models."}, {"timestamp": [312.74, 314.96], "text": " This has been sitting here for 10 minutes."}, {"timestamp": [314.96, 319.96], "text": " And look, did matrons ever have sex with gladiators?"}, {"timestamp": [323.8, 325.72], "text": " This is a real thing that happened in ancient Rome. Let's see"}, {"timestamp": [325.72, 326.68], "text": " how it handles it."}, {"timestamp": [326.68, 330.64], "text": " Oh yeah, so I trained it not to engage with sex."}, {"timestamp": [330.64, 333.88], "text": " So let's see if we can get it to... yeah,"}, {"timestamp": [333.88, 337.28], "text": " he's not gonna talk about it. Okay, that's fine."}, {"timestamp": [337.28, 340.36], "text": " Because this is meant to be PG or G-rated."}, {"timestamp": [340.36, 344.44], "text": " Did gladiators"}, {"timestamp": [344.44, 345.0], "text": " ever fight lions?"}, {"timestamp": [348.42, 350.34], "text": " He should answer that."}, {"timestamp": [350.34, 351.76], "text": " Yes, okay."}, {"timestamp": [351.76, 353.58], "text": " They're called Venetiones."}, {"timestamp": [353.58, 355.0], "text": " Oh, cool."}, {"timestamp": [355.0, 356.88], "text": " So this has been sitting here for like 10 minutes"}, {"timestamp": [356.88, 358.68], "text": " and you saw it was ready to go instantly."}, {"timestamp": [358.68, 359.92], "text": " So good job, OpenAI."}, {"timestamp": [359.92, 361.38], "text": " Thank you for fixing that."}, {"timestamp": [361.38, 363.26], "text": " That was the most annoying thing ever"}, {"timestamp": [363.26, 366.28], "text": " was that our fine-tuned models were not ready"}, {"timestamp": [366.28, 367.28], "text": " to go."}, {"timestamp": [367.28, 370.4], "text": " Okay, so fine-tuned models ready to go at all times."}, {"timestamp": [370.4, 371.74], "text": " Wonderful."}, {"timestamp": [371.74, 375.76], "text": " My Spanish teacher used to say todo el mundo, so all the world."}, {"timestamp": [375.76, 379.48], "text": " I guess that's a turn of phrase in Spain where it's just like everything is good."}, {"timestamp": [379.48, 380.48], "text": " Let's go."}, {"timestamp": [380.48, 382.56], "text": " So todo el mundo."}, {"timestamp": [382.56, 383.84], "text": " Or maybe that was just something she said."}, {"timestamp": [383.84, 386.76], "text": " I don't know."}, {"timestamp": [393.16, 395.52], "text": " OK. Brain reset. What are we doing? OK. So we have this. And then what do we need to do? Well,"}, {"timestamp": [400.68, 401.36], "text": " it's not just that simple. Once we have a conversation going. So what we'll do is we'll say"}, {"timestamp": [406.36, 408.68], "text": " conversation.appendLineIn. OK, so we'll just start this."}, {"timestamp": [408.68, 409.72], "text": " Conversation starts."}, {"timestamp": [409.72, 411.92], "text": " We put something in."}, {"timestamp": [411.92, 413.12], "text": " Excuse me."}, {"timestamp": [413.12, 414.64], "text": " And then what do we do?"}, {"timestamp": [414.64, 421.0], "text": " Well, first, we need to search the rest of the conversation."}, {"timestamp": [421.0, 428.42], "text": " So what we're going to do is search indexed. So we're going to search."}, {"timestamp": [428.42, 435.54], "text": " Let's cut this down a little. Okay. So count five. That's going to error out the first"}, {"timestamp": [435.54, 445.0], "text": " time because it's going to be too short. But, oh, here's how we'll do this."}, {"timestamp": [446.92, 448.88], "text": " Yeah, yeah, gotcha, gotcha, gotcha."}, {"timestamp": [448.88, 450.1], "text": " Okay, so search index,"}, {"timestamp": [450.1, 452.34], "text": " the data is gonna be the conversation."}, {"timestamp": [454.84, 459.04], "text": " Oh, I need to actually keep track of the whole thing."}, {"timestamp": [459.04, 461.56], "text": " Hmm, hmm, okay."}, {"timestamp": [463.28, 464.12], "text": " Okay."}, {"timestamp": [466.22, 468.48], "text": " So the conversation will actually need"}, {"timestamp": [468.48, 470.56], "text": " to have the embeddings with it."}, {"timestamp": [471.56, 472.4], "text": " Yeah."}, {"timestamp": [473.32, 477.8], "text": " Vector equals GPT-3 embedding,"}, {"timestamp": [477.8, 479.7], "text": " and then we'll do line in."}, {"timestamp": [480.6, 485.16], "text": " And so then we'll say, info equals,"}, {"timestamp": [487.28, 489.76], "text": " no, that needs to be a dictionary."}, {"timestamp": [489.76, 494.04], "text": " So line of conversation equals line in,"}, {"timestamp": [494.04, 497.92], "text": " and then the vector equals the vector."}, {"timestamp": [498.84, 502.68], "text": " Okay, and oh, in case it's not obvious,"}, {"timestamp": [502.68, 503.92], "text": " I prefer the term vector"}, {"timestamp": [503.92, 507.08], "text": " because that is mathematically what it returns."}, {"timestamp": [507.08, 509.48], "text": " But in the context of a large language model,"}, {"timestamp": [509.48, 514.48], "text": " this vector is technically an embedding or a semantic vector."}, {"timestamp": [514.6, 519.6], "text": " So an embedding is a vector that has semantic meaning,"}, {"timestamp": [519.84, 523.16], "text": " but a vector is just mathematically, this is what it is."}, {"timestamp": [523.16, 526.48], "text": " Okay, so we will store. So the reason that we're doing this is what it is. Okay, so we will store."}, {"timestamp": [526.48, 527.64], "text": " So the reason that we're doing this"}, {"timestamp": [527.64, 529.84], "text": " is that every line of conversation,"}, {"timestamp": [529.84, 532.32], "text": " so let me come back over here."}, {"timestamp": [532.32, 534.4], "text": " Every line of conversation is going to have"}, {"timestamp": [534.4, 536.88], "text": " an associated embedding or vector,"}, {"timestamp": [536.88, 539.52], "text": " which means that this whole thing will be searchable."}, {"timestamp": [540.68, 543.24], "text": " Okay, so conversation append.info,"}, {"timestamp": [543.24, 544.44], "text": " so we're just gonna accumulate"}, {"timestamp": [544.44, 545.62], "text": " the entire conversation"}, {"timestamp": [545.62, 547.14], "text": " right here."}, {"timestamp": [547.14, 559.06], "text": " And then what we do is we, I guess we actually won't need this because it's going to, well,"}, {"timestamp": [559.06, 561.36], "text": " yeah, okay."}, {"timestamp": [561.36, 567.96], "text": " So we'll search, we'll search for the most, for similar, so I'll need to modify the index."}, {"timestamp": [567.96, 569.24], "text": " Also, I need to move my microphone"}, {"timestamp": [569.24, 571.74], "text": " because it's blocking a little bit of my field of vision."}, {"timestamp": [571.74, 572.76], "text": " Hope you can still hear me."}, {"timestamp": [572.76, 574.78], "text": " If not, deal with it."}, {"timestamp": [576.28, 577.12], "text": " Okay."}, {"timestamp": [578.6, 583.1], "text": " So we get our line in, and then what do we do?"}, {"timestamp": [583.1, 590.88], "text": " We search for... We search, we search, we search."}, {"timestamp": [592.2, 598.48], "text": " old lines equals search index and so we'll do... we'll pass info so that's our"}, {"timestamp": [598.48, 607.88], "text": " most recent one. We'll search or pass conversation and we'll leave it at that. I'm not gonna pass a variable for how many to return."}, {"timestamp": [609.84, 611.24], "text": " We'll just take that out."}, {"timestamp": [611.24, 613.08], "text": " We'll hard code that, doesn't matter."}, {"timestamp": [613.96, 618.96], "text": " So we'll do recent and then we'll do all lines."}, {"timestamp": [619.84, 621.32], "text": " So what you're supposed to do is"}, {"timestamp": [621.32, 623.76], "text": " if you use a variable in a function,"}, {"timestamp": [623.76, 624.76], "text": " especially if it's a list,"}, {"timestamp": [624.76, 626.96], "text": " you're supposed to do is if you use a variable in a function, especially if it's a list, you're supposed to use a different name."}, {"timestamp": [626.96, 634.86], "text": " Otherwise, because GPT-3 or not GPT-3, Python, Python automatically has lists as global variables."}, {"timestamp": [634.86, 640.04], "text": " I'm probably saying this wrong, but you can modify the contents of a list from any function."}, {"timestamp": [640.04, 643.94], "text": " But if it's local to a function, it won't be modified."}, {"timestamp": [643.94, 646.08], "text": " So if I have a different name,"}, {"timestamp": [646.08, 648.84], "text": " even though it's the same conversation,"}, {"timestamp": [648.84, 650.16], "text": " like I'm not gonna be modifying"}, {"timestamp": [650.16, 651.68], "text": " and I'm just gonna be reading from it."}, {"timestamp": [651.68, 654.82], "text": " But anyways, this is best practices, I think."}, {"timestamp": [656.0, 660.4], "text": " Okay, so what we're gonna do is, where did I go?"}, {"timestamp": [660.4, 663.28], "text": " I lost my search index, there we go, okay."}, {"timestamp": [664.4, 665.3], "text": " We don't need the vector"}, {"timestamp": [665.3, 667.26], "text": " cause we're already gonna have that."}, {"timestamp": [667.26, 671.54], "text": " So for I in all lines,"}, {"timestamp": [671.54, 674.3], "text": " score is similarity."}, {"timestamp": [674.3, 677.26], "text": " So we're gonna do this vector."}, {"timestamp": [677.26, 681.18], "text": " So we're gonna do recent vector."}, {"timestamp": [683.78, 693.84], "text": " And actually what we'll do is if yeah so if these are the same then we"}, {"timestamp": [693.84, 702.92], "text": " skip our current line so if I vector equals recent vector continue and that"}, {"timestamp": [702.92, 708.0], "text": " should be the last one technically Technically, you could probably just skip"}, {"timestamp": [708.0, 713.32], "text": " the last one, but I don't want to make that assumption. Because what if we're searching,"}, {"timestamp": [713.32, 718.68], "text": " for whatever reason, we reuse this, and we're searching elsewhere in the conversation? I"}, {"timestamp": [718.68, 722.66], "text": " don't want to assume that the one that I'm searching is always going to be the last one."}, {"timestamp": [722.66, 725.72], "text": " Because I can just imagine someone saying, well, why don't you just skip the last one?"}, {"timestamp": [725.72, 727.08], "text": " Because I don't wanna make that assumption."}, {"timestamp": [727.08, 730.32], "text": " We might be searching based on an older memory."}, {"timestamp": [730.32, 733.44], "text": " Okay, so score, we get the score."}, {"timestamp": [733.44, 736.96], "text": " And then what do we do?"}, {"timestamp": [736.96, 741.96], "text": " Yeah, okay, so we can just do the content."}, {"timestamp": [742.6, 745.0], "text": " I'll change this to line equals,"}, {"timestamp": [746.0, 748.84], "text": " and then we'll do I line."}, {"timestamp": [748.84, 751.12], "text": " I think that's what I called it, right?"}, {"timestamp": [751.12, 752.48], "text": " Yeah, we've got line and vector."}, {"timestamp": [752.48, 755.2], "text": " Okay, so the line of dialogue and the vector,"}, {"timestamp": [755.2, 757.04], "text": " search index, okay."}, {"timestamp": [757.04, 762.04], "text": " So line equals line and the score equals score, okay?"}, {"timestamp": [762.16, 766.88], "text": " Sorted scores, sort by score, reverse is true so it'll be top"}, {"timestamp": [766.88, 778.16], "text": " down. And then I only want to return the top five, so what we'll do is when we"}, {"timestamp": [778.16, 787.42], "text": " first start the dialog it's going to be shorter than five, so we'll just return, we'll try to return the top five,"}, {"timestamp": [789.28, 792.76], "text": " but if that doesn't work, actually, here's what we'll do."}, {"timestamp": [792.76, 795.98], "text": " We'll do ordered equals ordered,"}, {"timestamp": [797.36, 801.0], "text": " and then return ordered, because I'm afraid"}, {"timestamp": [801.0, 806.52], "text": " that with the previous one, if it has the return at the same time"}, {"timestamp": [806.52, 808.12], "text": " that it's trying to slice up the list,"}, {"timestamp": [808.12, 810.56], "text": " it might just break entirely,"}, {"timestamp": [810.56, 814.02], "text": " but then except we'll just return ordered."}, {"timestamp": [815.76, 818.72], "text": " So, cause if this doesn't work, it'll error,"}, {"timestamp": [818.72, 820.2], "text": " it'll bomb right there."}, {"timestamp": [820.2, 823.56], "text": " And then we'll just skip to this and just return it as is."}, {"timestamp": [823.56, 826.64], "text": " So this is another paradigm in Python"}, {"timestamp": [826.64, 829.56], "text": " where it's better to ask forgiveness than permission."}, {"timestamp": [829.56, 832.92], "text": " So rather than test something, you just try accept."}, {"timestamp": [832.92, 834.76], "text": " You do need to be careful."}, {"timestamp": [834.76, 837.08], "text": " Whoops, one of my, I put up,"}, {"timestamp": [837.08, 839.04], "text": " you probably see those hexagons in the back."}, {"timestamp": [839.04, 841.0], "text": " I put up sound dampening things."}, {"timestamp": [841.0, 841.98], "text": " Let me put that back up."}, {"timestamp": [841.98, 843.72], "text": " It's gonna bug me."}, {"timestamp": [843.72, 849.6], "text": " But yeah, I put up sound dampening because you probably noticed it sounds better. There's less echoes. Okay, we're back."}, {"timestamp": [849.6, 859.12], "text": " Sorry about that. Someone online said that the best way to stick those panels up was blue painter's"}, {"timestamp": [859.12, 868.12], "text": " tape, which seems to be working because it won't peel any paint off, but one just fell off, so I might need to get some better tape or some two-sided tape."}, {"timestamp": [868.12, 870.28], "text": " Anyways, where were we?"}, {"timestamp": [870.28, 871.28], "text": " Search index."}, {"timestamp": [871.28, 872.28], "text": " Okay, yeah."}, {"timestamp": [872.28, 874.4], "text": " So better to ask forgiveness than permission."}, {"timestamp": [874.4, 876.56], "text": " You do need to be careful."}, {"timestamp": [876.56, 882.36], "text": " If you do a try-except loop, sometimes it might not have the results that you think"}, {"timestamp": [882.36, 889.5], "text": " and you might gloss over problems. So that's why when I do the try accept loop here,"}, {"timestamp": [889.5, 890.98], "text": " I actually give it output."}, {"timestamp": [891.86, 894.06], "text": " Cause sometimes the assumptions you're making"}, {"timestamp": [894.06, 896.54], "text": " might be wrong and you need to see why."}, {"timestamp": [896.54, 899.14], "text": " But in this case, there's like only one thing"}, {"timestamp": [899.14, 900.76], "text": " that could go wrong here."}, {"timestamp": [900.76, 902.7], "text": " So this is probably a little bit,"}, {"timestamp": [902.7, 904.54], "text": " this is more of a sure thing, right?"}, {"timestamp": [904.54, 907.32], "text": " Cause there's only one action happening."}, {"timestamp": [907.32, 909.36], "text": " Okay, so we've got that."}, {"timestamp": [909.36, 912.96], "text": " Old lines equals search index info conversation."}, {"timestamp": [912.96, 916.8], "text": " You know, honestly, I don't even need to pass the scores back."}, {"timestamp": [916.8, 917.44], "text": " I don't care."}, {"timestamp": [917.44, 919.68], "text": " But whatever."}, {"timestamp": [919.68, 923.72], "text": " Okay, so we get an input, we get the old conversation."}, {"timestamp": [923.72, 926.18], "text": " So this will find anything that's most relevant."}, {"timestamp": [926.18, 929.58], "text": " We'll find the most relevant five lines of conversation."}, {"timestamp": [929.58, 931.14], "text": " This number is arbitrary."}, {"timestamp": [933.14, 934.18], "text": " Where did it go?"}, {"timestamp": [934.18, 935.3], "text": " Actually, you know what I'll do?"}, {"timestamp": [935.3, 937.42], "text": " I'll do, I'll add count back in."}, {"timestamp": [937.42, 939.84], "text": " Count equals five."}, {"timestamp": [941.74, 943.26], "text": " And then we'll just do that."}, {"timestamp": [943.26, 945.3], "text": " So that way, if you reuse this code,"}, {"timestamp": [945.3, 948.0], "text": " you can just pass however much you want."}, {"timestamp": [948.0, 950.96], "text": " Because imagine that this conversation gets much longer"}, {"timestamp": [950.96, 954.04], "text": " and you want to return like 50, right?"}, {"timestamp": [954.04, 956.1], "text": " Because you can easily fit that into,"}, {"timestamp": [956.1, 957.74], "text": " actually, yeah, let's do a little bit longer."}, {"timestamp": [957.74, 960.12], "text": " Let's do 10."}, {"timestamp": [960.12, 962.1], "text": " So we'll get the 10 most relevant lines."}, {"timestamp": [963.08, 968.48], "text": " And then, okay, so the whole conversation"}, {"timestamp": [968.48, 970.32], "text": " is going to be accumulated here."}, {"timestamp": [970.32, 973.6], "text": " The old lines will be there."}, {"timestamp": [973.6, 976.48], "text": " And then we'll just pass in 10."}, {"timestamp": [976.48, 977.68], "text": " Let's cut off."}, {"timestamp": [980.48, 987.96], "text": " Let's see, latest conversation equals, let's see, actually no, here."}, {"timestamp": [987.96, 1010.72], "text": " If Len conversation, no, recent conversation equals, here, do, there we go, if len conversation is greater than 30. So we'll say recent conversation"}, {"timestamp": [1010.72, 1025.42], "text": " equals equals conversation. And I believe the thing is negative 30.. Let me make sure I'm doing that right."}, {"timestamp": [1027.08, 1032.08], "text": " Python L equals list range zero to 100 L, okay."}, {"timestamp": [1035.04, 1038.92], "text": " And then L if we do negative 30 colon,"}, {"timestamp": [1038.92, 1040.04], "text": " that's just the last 30."}, {"timestamp": [1040.04, 1040.86], "text": " Okay, perfect."}, {"timestamp": [1041.72, 1045.22], "text": " And then do lang just to make sure that that's right."}, {"timestamp": [1045.22, 1046.06], "text": " Yes, okay."}, {"timestamp": [1047.54, 1049.34], "text": " Yeah, so what this does is,"}, {"timestamp": [1049.34, 1052.62], "text": " if our conversation is getting too long,"}, {"timestamp": [1052.62, 1055.58], "text": " we don't wanna pass the entire conversation, right?"}, {"timestamp": [1055.58, 1057.98], "text": " Just the last 30 lines, that's more than enough."}, {"timestamp": [1057.98, 1059.3], "text": " Because you look at this,"}, {"timestamp": [1059.3, 1062.0], "text": " this is one, two, three, four, five, six, seven,"}, {"timestamp": [1062.0, 1065.72], "text": " eight, nine, 10, 11 9 10 11 12 13 14."}, {"timestamp": [1065.72, 1066.72], "text": " So this is 14."}, {"timestamp": [1066.72, 1068.4], "text": " So this is less than half of that."}, {"timestamp": [1068.4, 1073.32], "text": " And obviously this is enough context to know what we're talking about."}, {"timestamp": [1073.32, 1075.6], "text": " So 14 lines of conversation is plenty."}, {"timestamp": [1075.6, 1078.54], "text": " 30 will be way more than enough."}, {"timestamp": [1078.54, 1080.28], "text": " That is a pretty long conversation."}, {"timestamp": [1080.28, 1086.88], "text": " And then on top of that we're going to have the 10 oldest lines, right? Or the 10 most relevant older lines."}, {"timestamp": [1086.88, 1087.88], "text": " Okay."}, {"timestamp": [1087.88, 1089.6], "text": " So, now what?"}, {"timestamp": [1089.6, 1094.52], "text": " We're searching our index, we're searching the conversation, we're getting the most recent"}, {"timestamp": [1094.52, 1095.52], "text": " conversation."}, {"timestamp": [1095.52, 1096.88], "text": " So, this is 40 lines."}, {"timestamp": [1096.88, 1097.88], "text": " So, now what?"}, {"timestamp": [1097.88, 1098.96], "text": " What are we missing?"}, {"timestamp": [1098.96, 1101.76], "text": " We're missing external data."}, {"timestamp": [1101.76, 1118.8], "text": " Okay. Okay, so convo block equals, so we'll do n, so this is new line dot join, and then we'll"}, {"timestamp": [1118.8, 1120.9], "text": " do old lines."}, {"timestamp": [1120.9, 1125.0], "text": " So this is going to just be the old bits of the conversation"}, {"timestamp": [1125.1, 1130.1], "text": " plus I probably need to do a new line between there"}, {"timestamp": [1131.32, 1133.8], "text": " and then plus, and we'll do the same thing"}, {"timestamp": [1135.44, 1140.24], "text": " and we'll join recent conversation."}, {"timestamp": [1140.24, 1141.6], "text": " So basically what this will do"}, {"timestamp": [1141.6, 1144.72], "text": " is it'll combine it into one chat block, right?"}, {"timestamp": [1144.72, 1146.16], "text": " So it's like, okay, we've got this. So now what this will do is it'll combine it into one chat block, right? So it's like, okay, we've got this."}, {"timestamp": [1147.6, 1148.28], "text": " So now what?"}, {"timestamp": [1148.28, 1149.56], "text": " We've got our convo block."}, {"timestamp": [1149.96, 1153.08], "text": " What we need to do now is we need to do a prompt."}, {"timestamp": [1153.16, 1157.52], "text": " So prompt and just copy a line."}, {"timestamp": [1157.52, 1159.44], "text": " So prompt equals open file."}, {"timestamp": [1165.0, 1165.2], "text": " And we'll just do prompt keyword,"}, {"timestamp": [1170.2, 1174.48], "text": " replace block with convo block."}, {"timestamp": [1176.92, 1179.32], "text": " Okay, so now we need to do some prompt engineering. So basically what we're gonna do is we're gonna,"}, {"timestamp": [1180.28, 1181.9], "text": " I'll use this as an example."}, {"timestamp": [1183.64, 1184.92], "text": " So we're gonna say,"}, {"timestamp": [1183.88, 1209.0], "text": " I'll use this as an example. So we're going to say, read the following conversation and generate, actually, and let's single relevant Wikipedia article title."}, {"timestamp": [1209.0, 1219.0], "text": " Okay, so then we'll do most relevant Wikipedia article title."}, {"timestamp": [1219.0, 1221.0], "text": " So let's see if this works."}, {"timestamp": [1221.0, 1225.88], "text": " I want to keep this up, so I'm just gonna duplicate this tab"}, {"timestamp": [1225.88, 1229.56], "text": " and we'll go to DaVinci Instruct."}, {"timestamp": [1230.52, 1232.32], "text": " Okay, let's see what it says."}, {"timestamp": [1234.46, 1237.02], "text": " Romulus Augustus, cool."}, {"timestamp": [1238.24, 1240.12], "text": " And then let's go to Wikipedia."}, {"timestamp": [1241.92, 1245.0], "text": " Wikipedia, why is my, ugh. I've already had coffee."}, {"timestamp": [1246.86, 1250.32], "text": " Romulus Augustus, bam, got it, okay."}, {"timestamp": [1251.78, 1256.78], "text": " So what we can do then is we'll query this,"}, {"timestamp": [1257.36, 1258.8], "text": " we'll get it from Wikipedia."}, {"timestamp": [1260.04, 1261.54], "text": " So this worked really well."}, {"timestamp": [1263.42, 1267.16], "text": " It's always nice when your intuition is like spot on."}, {"timestamp": [1267.16, 1272.84], "text": " Okay, so we've got our prompt."}, {"timestamp": [1272.84, 1279.12], "text": " Let's go ahead and just do block and we'll call this prompt."}, {"timestamp": [1279.12, 1281.76], "text": " What did I call it?"}, {"timestamp": [1281.76, 1282.76], "text": " Keyword."}, {"timestamp": [1282.76, 1288.96], "text": " Actually, let's call this Wikipedia because it's very specific."}, {"timestamp": [1288.96, 1293.42], "text": " Specifically let's write a prompt to get Wikipedia."}, {"timestamp": [1293.42, 1295.28], "text": " Prompt Wikipedia."}, {"timestamp": [1295.28, 1310.76], "text": " So this presupposes that GPT-3 will know the correct Wikipedia article. GPT-3 was trained in part on Wikipedia, so I don't see any problem."}, {"timestamp": [1310.76, 1312.4], "text": " There's another way you can do this."}, {"timestamp": [1312.4, 1314.32], "text": " So let me show you."}, {"timestamp": [1314.32, 1320.08], "text": " If you want something that's more open source or more open-ended, what you can do is actually"}, {"timestamp": [1320.08, 1327.0], "text": " you could feed this into a question generator. So let's see, where was it?"}, {"timestamp": [1327.0, 1330.0], "text": " My Naoka question generator."}, {"timestamp": [1330.0, 1333.0], "text": " Wow, is it really that old?"}, {"timestamp": [1333.0, 1335.0], "text": " Questions. Yeah."}, {"timestamp": [1335.0, 1338.0], "text": " So I have a question generator, which basically what it does..."}, {"timestamp": [1338.0, 1340.0], "text": " Do I have an example? Yeah."}, {"timestamp": [1340.0, 1343.0], "text": " So I have an example where you feed it anything"}, {"timestamp": [1343.0, 1345.64], "text": " and it'll generate a list of questions."}, {"timestamp": [1345.64, 1349.96], "text": " So natural language questions are going to be much more open-ended in terms of what questions"}, {"timestamp": [1349.96, 1352.38], "text": " should I ask about this."}, {"timestamp": [1352.38, 1374.72], "text": " You could do that, but GPT-3 is also just really great at like, let's see. Read the following conversation and recommend a list of Google search queries to learn more."}, {"timestamp": [1374.72, 1381.28], "text": " And then we'll say, list of Google search queries."}, {"timestamp": [1381.28, 1383.44], "text": " So you do this."}, {"timestamp": [1383.44, 1387.0], "text": " So let's say you want to query Google instead of Wikipedia."}, {"timestamp": [1387.0, 1390.0], "text": " You could do this and then just do Google searches."}, {"timestamp": [1390.0, 1393.0], "text": " I'll go ahead and save this prompt just so that way you all can see it."}, {"timestamp": [1393.0, 1408.7], "text": " So we'll do block and then we'll remove that and save this as prompt Google. Okay so now"}, {"timestamp": [1408.7, 1413.12], "text": " you guys can see you can experiment with this if you want to use the Google API"}, {"timestamp": [1413.12, 1420.92], "text": " instead of Wikipedia. Either way both serve as an external source. It's pretty"}, {"timestamp": [1420.92, 1425.96], "text": " trivial as you see like you just ask gT-3 to generate a list of Google queries and you're done"}, {"timestamp": [1426.76, 1428.76], "text": " Okay"}, {"timestamp": [1429.96, 1434.24], "text": " Answer GPT, oh wait, no, no, no. No. No. All right. Yeah"}, {"timestamp": [1437.2, 1439.2], "text": " So then we say"}, {"timestamp": [1441.28, 1445.0], "text": " Title equals GPT-3 completion."}, {"timestamp": [1445.68, 1447.98], "text": " Prompt, I think that's all I need to pass."}, {"timestamp": [1449.5, 1452.06], "text": " Oh, that's a chat completion, GPT-3 completion."}, {"timestamp": [1452.06, 1455.06], "text": " Yes, we do not need that many tokens."}, {"timestamp": [1455.06, 1456.2], "text": " Let's say like,"}, {"timestamp": [1459.46, 1460.3], "text": " I'll say 500,"}, {"timestamp": [1460.3, 1463.46], "text": " because we might use this for other things."}, {"timestamp": [1463.46, 1465.16], "text": " No, let's say 200, nice and short."}, {"timestamp": [1466.86, 1470.26], "text": " Max tokens for the chat, let's say 100."}, {"timestamp": [1470.26, 1473.16], "text": " We don't want super long conversations."}, {"timestamp": [1473.16, 1475.0], "text": " Temperature of 0.7, that's fine."}, {"timestamp": [1475.0, 1477.44], "text": " Frequency penalty, let's take that out."}, {"timestamp": [1479.42, 1480.48], "text": " That should be fine."}, {"timestamp": [1480.48, 1481.32], "text": " Okay."}, {"timestamp": [1482.92, 1488.68], "text": " Yeah, so we've got our conversation set."}, {"timestamp": [1488.68, 1490.44], "text": " We've got a title."}, {"timestamp": [1490.44, 1495.04], "text": " And then let's take a look at the Wikipedia PyPI."}, {"timestamp": [1495.04, 1501.76], "text": " So if you want to get the article, you just say, here's our page."}, {"timestamp": [1501.76, 1504.76], "text": " So I'm going to assume that it works."}, {"timestamp": [1504.76, 1517.92], "text": " Actually, no, we'll say, let's not make any bad assumptions."}, {"timestamp": [1517.92, 1521.08], "text": " Let's do fetch wiki and we'll do title."}, {"timestamp": [1521.08, 1525.96], "text": " Okay, so in this function, because when you're searching for something, you don't always"}, {"timestamp": [1525.96, 1527.96], "text": " know if it's going to work."}, {"timestamp": [1527.96, 1539.4], "text": " So def fetchWikiTitle, and so what we'll do is we'll do try page equals, make sure I get"}, {"timestamp": [1539.4, 1545.0], "text": " the syntax right, wikipedia.pageTitle."}, {"timestamp": [1545.0, 1550.12], "text": " So if it works, we'll just pass this back, return page."}, {"timestamp": [1550.12, 1551.88], "text": " If not, this will error out, and it'll say,"}, {"timestamp": [1551.88, 1553.96], "text": " I don't know what you're talking about."}, {"timestamp": [1553.96, 1555.96], "text": " It'll say it exactly like that, too."}, {"timestamp": [1555.96, 1558.36], "text": " Except if it doesn't work, we'll do"}, {"timestamp": [1558.36, 1564.0], "text": " search equals wikipedia.searchTitle."}, {"timestamp": [1564.0, 1565.94], "text": " And so you see the syntax"}, {"timestamp": [1565.94, 1567.98], "text": " for that here where you can do"}, {"timestamp": [1567.98, 1570.04], "text": " a search and it'll return a list of"}, {"timestamp": [1570.44, 1571.36], "text": " possible"}, {"timestamp": [1572.44, 1573.0], "text": " articles."}, {"timestamp": [1574.2, 1576.12], "text": " And so like in this case it's like"}, {"timestamp": [1576.12, 1578.36], "text": " Barack. So it's like do you mean Barack Obama"}, {"timestamp": [1578.36, 1580.48], "text": " presidency of Barack Obama Barack"}, {"timestamp": [1580.48, 1582.56], "text": " the Brandy Barack the given name."}, {"timestamp": [1584.2, 1586.24], "text": " I have no idea how."}, {"timestamp": [1586.24, 1589.6], "text": " I'll probably just pick the first one."}, {"timestamp": [1589.6, 1596.6], "text": " So then we'll do page equals Wikipedia.page."}, {"timestamp": [1596.6, 1600.2], "text": " And we'll do search 0."}, {"timestamp": [1600.2, 1607.5], "text": " So I'll just assume that like return page, I'll just assume that it returns,"}, {"timestamp": [1607.5, 1611.9], "text": " you know, that the first one is gonna be most relevant."}, {"timestamp": [1611.9, 1613.26], "text": " Like I said, this is,"}, {"timestamp": [1613.26, 1616.1], "text": " I'm just showing you the proof of concept."}, {"timestamp": [1616.1, 1617.26], "text": " Okay, so then,"}, {"timestamp": [1618.74, 1620.5], "text": " so we've got the wiki that will return it."}, {"timestamp": [1620.5, 1623.5], "text": " Oh, I also need to import Wikipedia,"}, {"timestamp": [1624.46, 1630.0], "text": " import Wikipedia. And then do I use OS anywhere?"}, {"timestamp": [1630.0, 1638.32], "text": " Import OS. I probably do. Okay. Fetch wiki. Okay, so we get our relevant wiki. So then"}, {"timestamp": [1638.32, 1648.52], "text": " what do we do with it? Well, we found Romulus Augustus. This is way too much text, right? This is way too much to put into a GPT-3 prompt."}, {"timestamp": [1649.68, 1652.04], "text": " So what do we do?"}, {"timestamp": [1652.04, 1654.4], "text": " Well, I'm glad you asked."}, {"timestamp": [1654.4, 1658.2], "text": " That's why we have this, the document answering thing."}, {"timestamp": [1659.84, 1661.72], "text": " So you know what, let's just go all out."}, {"timestamp": [1661.72, 1663.96], "text": " Let's hit this."}, {"timestamp": [1663.96, 1665.58], "text": " I was gonna do something simple, but no, let's just go all out. Let's hit this. I was gonna do something simple,"}, {"timestamp": [1665.58, 1669.2], "text": " but no, let's go whole hog."}, {"timestamp": [1669.2, 1674.16], "text": " All right, so we've got the wiki content."}, {"timestamp": [1675.92, 1677.02], "text": " Let's see, what is it?"}, {"timestamp": [1677.02, 1679.4], "text": " If we go here, we do,"}, {"timestamp": [1679.4, 1681.34], "text": " if we wanna look at the whole content,"}, {"timestamp": [1682.22, 1683.06], "text": " where was it?"}, {"timestamp": [1683.06, 1686.84], "text": " Summary, search, page, content."}, {"timestamp": [1686.84, 1687.84], "text": " That is okay."}, {"timestamp": [1687.84, 1688.84], "text": " So we just do .content."}, {"timestamp": [1688.84, 1695.2], "text": " So that will give us just all the text from that particular article."}, {"timestamp": [1695.2, 1696.68], "text": " And so then what do we do?"}, {"timestamp": [1696.68, 1699.38], "text": " Well, I'm glad you asked."}, {"timestamp": [1699.38, 1717.0], "text": " So let's go back to our playground and we'll say, read the following conversation and write a list"}, {"timestamp": [1717.0, 1719.0], "text": " of..."}, {"timestamp": [1719.0, 1725.0], "text": " Well, we're asking questions."}, {"timestamp": [1725.44, 1728.04], "text": " Because basically what I'm thinking is"}, {"timestamp": [1728.04, 1731.54], "text": " we want to query this document, right?"}, {"timestamp": [1733.28, 1735.16], "text": " For interesting facts, right?"}, {"timestamp": [1736.8, 1738.32], "text": " Life in exile."}, {"timestamp": [1744.68, 1749.28], "text": " Actually, like the questions we should ask are not necessarily obvious from this."}, {"timestamp": [1749.28, 1753.28], "text": " So we get Romulus Augustus."}, {"timestamp": [1753.28, 1754.28], "text": " How should we handle this?"}, {"timestamp": [1754.28, 1765.0], "text": " Because if we, let's say this conversation leads to this, and I don't know what I'm asking. So, federati."}, {"timestamp": [1775.12, 1780.12], "text": " Because if I use my multi-document answering thing"}, {"timestamp": [1780.12, 1783.96], "text": " and we query it, like we got it,"}, {"timestamp": [1783.96, 1786.0], "text": " we basically, we have to like..."}, {"timestamp": [1786.0, 1789.0], "text": " We have to be, there has to be something we're looking for."}, {"timestamp": [1789.0, 1792.0], "text": " Unless I just want to summarize it."}, {"timestamp": [1794.0, 1797.0], "text": " But I don't know if I want to do that, because I want to..."}, {"timestamp": [1797.0, 1800.0], "text": " Summarizing is good and all, but if you get..."}, {"timestamp": [1800.0, 1804.0], "text": " If you answer a question, you get much better."}, {"timestamp": [1808.48, 1811.64], "text": " get if you answer a question you get much better but but the problem is if this is it this is a chicken-or-the-egg problem or putting the cart before the"}, {"timestamp": [1811.64, 1815.0], "text": " horse because if you don't read the thing and you don't know what the"}, {"timestamp": [1815.0, 1819.0], "text": " information is you don't necessarily so there's there's what what I call a naive"}, {"timestamp": [1819.0, 1823.04], "text": " question so like if if you don't know anything about something you ask naive"}, {"timestamp": [1823.04, 1829.0], "text": " questions right a naive question is just like, well, here's a perfect example."}, {"timestamp": [1829.0, 1831.76], "text": " Who was the last emperor?"}, {"timestamp": [1831.76, 1836.8], "text": " I had to already know that Rome had emperors."}, {"timestamp": [1836.8, 1839.8], "text": " So that's a semi-naive question."}, {"timestamp": [1839.8, 1841.8], "text": " It's like, OK, so Romulus Augustus."}, {"timestamp": [1841.8, 1845.12], "text": " Whereas if I ask, so this is a super not naive"}, {"timestamp": [1845.12, 1849.76], "text": " question right because I knew that there was a virtuous Roman man that there was"}, {"timestamp": [1849.76, 1853.96], "text": " this concept in ancient Rome but how do you ask the question to even get to the"}, {"timestamp": [1853.96, 1871.52], "text": " right question so we need like meta questions but GPT-3 is trained on all this stuff, so maybe it could...I don't know. I wonder, I wonder,"}, {"timestamp": [1871.52, 1890.0], "text": " I wonder. Follow-up questions to search the internet. Yeah, that's fine. We'll just do search the internet because it's like"}, {"timestamp": [1891.52, 1897.6], "text": " list, let's see, list of follow-up questions to add to the conversation."}, {"timestamp": [1898.56, 1903.76], "text": " And then let's just see what it does. What caused the fall of the Western Empire? Oh,"}, {"timestamp": [1903.76, 1906.9], "text": " wow. Okay. What was life like for the average person in Rome?"}, {"timestamp": [1906.9, 1911.88], "text": " This is great."}, {"timestamp": [1911.88, 1918.3], "text": " It's not really staying on topic though because if you ask these questions, that's not going"}, {"timestamp": [1918.3, 1933.76], "text": " to be information that is necessarily here."}, {"timestamp": [1934.6, 1935.96], "text": " Let's see."}, {"timestamp": [1940.96, 1943.92], "text": " Given the following conversation and the selected topic,"}, {"timestamp": [1966.76, 1970.42], "text": " so we'll do Romulus Augustus, write a follow-up question to ask. No. Because if we look at this, like what is the kind of information that I want to extract"}, {"timestamp": [1970.42, 1971.78], "text": " from this, right?"}, {"timestamp": [1971.78, 1978.0], "text": " Like the summary of this dude's life or what did he do?"}, {"timestamp": [1978.0, 1988.52], "text": " I'll write a follow-up question to ask."}, {"timestamp": [1988.52, 1996.48], "text": " Follow-up question to ask about Romulus Augustus."}, {"timestamp": [1996.48, 1998.8], "text": " Let's just see what it does."}, {"timestamp": [1998.8, 1999.8], "text": " That's not a bad question."}, {"timestamp": [1999.8, 2000.8], "text": " Okay."}, {"timestamp": [2000.8, 2005.04], "text": " I have to take a quick bio break, so I'm going to pause and then we'll be right back."}, {"timestamp": [2005.04, 2006.84], "text": " I think this is what we'll go with."}, {"timestamp": [2006.84, 2011.32], "text": " All right, and we're back."}, {"timestamp": [2011.32, 2012.32], "text": " Where were we?"}, {"timestamp": [2012.32, 2013.32], "text": " Oh, yeah."}, {"timestamp": [2013.32, 2014.32], "text": " Okay."}, {"timestamp": [2014.32, 2017.4], "text": " So, we've got the follow-up question."}, {"timestamp": [2017.4, 2031.36], "text": " We have identified ... Where did we go? So, we look in here, we identified, we used our Wikipedia prompt to get a particular topic,"}, {"timestamp": [2031.36, 2032.36], "text": " right?"}, {"timestamp": [2032.36, 2036.0], "text": " So, we get that."}, {"timestamp": [2036.0, 2042.88], "text": " We then, I wrote a quick thing just to go fetch an article, so we'll get it and then"}, {"timestamp": [2042.88, 2047.68], "text": " what we'll do is we will do a follow-up question."}, {"timestamp": [2047.68, 2050.96], "text": " So we've got a topic."}, {"timestamp": [2050.96, 2055.8], "text": " Given the following conversation and the selected topic."}, {"timestamp": [2055.8, 2060.0], "text": " So we'll do topic."}, {"timestamp": [2060.0, 2061.24], "text": " And then we'll do block."}, {"timestamp": [2061.76, 2067.48], "text": " and then we'll do block."}, {"timestamp": [2070.9, 2072.24], "text": " And we'll do one follow up question to ask about."}, {"timestamp": [2074.0, 2076.08], "text": " And we'll do topic."}, {"timestamp": [2076.92, 2079.44], "text": " Okay."}, {"timestamp": [2082.68, 2083.7], "text": " And then we'll do prompt follow up."}, {"timestamp": [2090.18, 2098.0], "text": " Okay, so then what we do is we go to here. And so we say Control-D for duplicate."}, {"timestamp": [2098.0, 2102.32], "text": " And so then we'll do prompt follow up."}, {"timestamp": [2102.32, 2114.28], "text": " And we'll replace the block with ConvoBlock and we'll also do replace topic with the title."}, {"timestamp": [2114.28, 2119.36], "text": " And so what that'll do is that'll take this and populate it with the way that it looked"}, {"timestamp": [2119.36, 2120.72], "text": " up here, right?"}, {"timestamp": [2120.72, 2124.8], "text": " So it'll just say, okay, let's populate in the conversation block and ask a follow-up"}, {"timestamp": [2124.8, 2127.6], "text": " question. So then once we have the follow-up question"}, {"timestamp": [2129.16, 2131.16], "text": " We can then"}, {"timestamp": [2131.88, 2135.32], "text": " Use the same logic that we had here to"}, {"timestamp": [2136.96, 2138.96], "text": " To"}, {"timestamp": [2139.84, 2141.84], "text": " Brain come on"}, {"timestamp": [2142.96, 2148.16], "text": " We're going to index the content of this Wikipedia article and then we're going to"}, {"timestamp": [2150.32, 2158.16], "text": " search it and answer the question. So then what we'll do is we'll do answer equals,"}, {"timestamp": [2161.52, 2168.64], "text": " we'll do answer question and we'll pass in the Wikipedia article."}, {"timestamp": [2168.64, 2172.56], "text": " Oh, wait, we actually need to do question."}, {"timestamp": [2172.56, 2178.04], "text": " And the question."}, {"timestamp": [2178.04, 2187.68], "text": " I think that's all we need to do because the question will be generated here."}, {"timestamp": [2192.64, 2200.88], "text": " Generate a specific follow-up question"}, {"timestamp": [2204.88, 2205.0], "text": " to use to query the external information."}, {"timestamp": [2207.44, 2208.84], "text": " And then we'll do,"}, {"timestamp": [2212.2, 2217.0], "text": " generate a search query"}, {"timestamp": [2217.0, 2222.0], "text": " to find external article in the wide world."}, {"timestamp": [2223.76, 2226.32], "text": " Okay, add a couple comments just because this is becoming a"}, {"timestamp": [2226.32, 2250.0], "text": " really big block. Let's see, get user input and vector and then we'll say search conversation for previous relevant lines of dialogue."}, {"timestamp": [2250.0, 2251.72], "text": " OK, that's good."}, {"timestamp": [2251.72, 2255.84], "text": " So we'll get the answer for answer question."}, {"timestamp": [2255.84, 2258.24], "text": " Def answer question."}, {"timestamp": [2258.24, 2262.12], "text": " And so then we'll pass in the article and the question."}, {"timestamp": [2262.12, 2264.72], "text": " So then what we'll do is we need,"}, {"timestamp": [2264.72, 2266.42], "text": " we've already got text wrap."}, {"timestamp": [2266.42, 2268.32], "text": " So we'll basically do the same thing we did here,"}, {"timestamp": [2268.32, 2270.4], "text": " but I also need to open, whoops,"}, {"timestamp": [2270.4, 2273.92], "text": " no, that was the correct thing, build index."}, {"timestamp": [2273.92, 2277.04], "text": " So this is super, super simple."}, {"timestamp": [2277.04, 2279.28], "text": " Basically all we do is"}, {"timestamp": [2282.44, 2284.96], "text": " we cut this into chunks."}, {"timestamp": [2285.0, 2286.42], "text": " So when we come in here, we cut this into chunks."}, {"timestamp": [2288.5, 2289.76], "text": " So when we come in here,"}, {"timestamp": [2293.34, 2295.86], "text": " we'll say chunks equals text wrap, and instead of all text, we'll do article,"}, {"timestamp": [2295.86, 2298.44], "text": " and then we'll do it in, let's do smaller."}, {"timestamp": [2298.44, 2300.38], "text": " So we'll do 2000 characters."}, {"timestamp": [2302.34, 2309.0], "text": " And then for, actually, I guess we don't even need to index it because we're just asking the question."}, {"timestamp": [2309.0, 2327.52], "text": " So this will be a little bit simpler because the reason, so in this one, the reason that I did it the way that I did is because I was basically saying like, okay, let's go ahead and index an arbitrarily large thing. Because if you have like 150 entries or 150,000 entries,"}, {"timestamp": [2327.52, 2331.28], "text": " you need to be able to search that database"}, {"timestamp": [2331.28, 2332.52], "text": " and then answer the question."}, {"timestamp": [2332.52, 2335.68], "text": " But in this case, we're already zeroed in on one article."}, {"timestamp": [2335.68, 2339.92], "text": " So we don't need to answer the whole thing."}, {"timestamp": [2339.92, 2342.32], "text": " So in that case, what we'll do is"}, {"timestamp": [2342.32, 2344.44], "text": " we'll just go ahead and skip ahead"}, {"timestamp": [2344.44, 2349.58], "text": " to answering the question"}, {"timestamp": [2349.58, 2353.2], "text": " and we'll probably just copy this prompt."}, {"timestamp": [2353.2, 2354.26], "text": " Let's see, which was it?"}, {"timestamp": [2354.26, 2356.36], "text": " Prompt answer."}, {"timestamp": [2356.36, 2358.72], "text": " Yeah."}, {"timestamp": [2358.72, 2361.2], "text": " Use the following passage to give a detailed answer to the question."}, {"timestamp": [2361.2, 2362.62], "text": " That's great."}, {"timestamp": [2362.62, 2368.84], "text": " So we'll just save as and we'll come back to our long-term chat and we'll call prompt"}, {"timestamp": [2368.84, 2372.38], "text": " answer."}, {"timestamp": [2372.38, 2395.42], "text": " And so now that should appear here, prompt answer. don't need it. And then for chunk in chunks prompt equals open file replace"}, {"timestamp": [2395.42, 2411.32], "text": " passage with we're gonna replace that with chunk and we're gonna replace query with question. Okay and to keep it"}, {"timestamp": [2411.32, 2416.88], "text": " consistent let's name it the same thing so we're gonna do question is question."}, {"timestamp": [2416.88, 2422.92], "text": " So basically what we're gonna do is we're gonna cut up yeah use the"}, {"timestamp": [2422.92, 2426.12], "text": " following passage to give a detailed answer to the question."}, {"timestamp": [2426.12, 2436.0], "text": " Ooh, actually, I guess I probably should search it because the answer might not be right,"}, {"timestamp": [2436.0, 2437.0], "text": " like fall."}, {"timestamp": [2437.0, 2438.0], "text": " Yeah."}, {"timestamp": [2438.0, 2441.76], "text": " See, also fall of the Western Empire."}, {"timestamp": [2441.76, 2445.7], "text": " Yeah, okay, because the answer might not be in it."}, {"timestamp": [2447.38, 2450.3], "text": " So let's add something."}, {"timestamp": [2450.3, 2451.3], "text": " Use the following passage"}, {"timestamp": [2451.3, 2453.38], "text": " to give a detailed answer to the question."}, {"timestamp": [2453.38, 2456.08], "text": " If the answer is not present,"}, {"timestamp": [2460.26, 2464.66], "text": " and the passage, just write not present."}, {"timestamp": [2465.0, 2465.5], "text": " Okay. and the passage just write not present."}, {"timestamp": [2466.34, 2470.66], "text": " Okay."}, {"timestamp": [2474.7, 2476.58], "text": " Detailed answer or not present."}, {"timestamp": [2478.8, 2481.02], "text": " All right, so let's test this real quick. So what we'll do is we'll come here and we'll say,"}, {"timestamp": [2481.02, 2484.06], "text": " okay, so the question is, and,"}, {"timestamp": [2486.96, 2489.28], "text": " oh darn, hang on, I need this question."}, {"timestamp": [2489.28, 2492.68], "text": " Wait, give me the question."}, {"timestamp": [2492.68, 2497.0], "text": " OK, so we copy the question, and then let's go here."}, {"timestamp": [2497.0, 2501.6], "text": " So we'll say, OK, and question."}, {"timestamp": [2501.6, 2504.8], "text": " OK, so this is our question."}, {"timestamp": [2504.8, 2508.1], "text": " What made Romulus Augustus the last emperor of the Roman Empire?"}, {"timestamp": [2508.1, 2514.78], "text": " So in the passage, we're going to have a small chunk and actually I think I was wrong."}, {"timestamp": [2514.78, 2522.5], "text": " I think we should go, we should make this a little bit bigger because many Wikipedia"}, {"timestamp": [2522.5, 2523.5], "text": " articles will fit."}, {"timestamp": [2523.5, 2527.32], "text": " It will say 4,000 tokens is 16,000 characters,"}, {"timestamp": [2527.32, 2529.7], "text": " so let's say like 10,000."}, {"timestamp": [2529.7, 2531.24], "text": " Because if the whole thing fits, great."}, {"timestamp": [2531.24, 2532.72], "text": " If not, we'll skip it."}, {"timestamp": [2535.6, 2539.8], "text": " And then let's go back here and just grab a chunk."}, {"timestamp": [2541.16, 2543.88], "text": " So like name,"}, {"timestamp": [2554.32, 2562.0], "text": " So like name, here, we'll just grab the summary and see how it handles this. Okay, passage, detailed answer or not present."}, {"timestamp": [2562.0, 2568.0], "text": " Okay. Hey. Is that there?"}, {"timestamp": [2568.0, 2570.0], "text": " Yeah."}, {"timestamp": [2570.0, 2572.0], "text": " All right."}, {"timestamp": [2572.0, 2574.0], "text": " So the answer was there."}, {"timestamp": [2574.0, 2578.0], "text": " Let's see if there is a section that it doesn't have it."}, {"timestamp": [2578.0, 2580.0], "text": " Ancestry and family."}, {"timestamp": [2580.0, 2586.24], "text": " So let's just grab this section because it looks like the answer will not be."}, {"timestamp": [2586.24, 2587.24], "text": " Okay."}, {"timestamp": [2587.24, 2595.96], "text": " Because we want it to be able to say not present."}, {"timestamp": [2595.96, 2599.48], "text": " He was the last emperor of the Western Roman Empire because he was the last emperor of"}, {"timestamp": [2599.48, 2600.84], "text": " the Western Roman Empire."}, {"timestamp": [2600.84, 2602.64], "text": " Wow, the floor is made of floor."}, {"timestamp": [2602.64, 2603.64], "text": " Thank you."}, {"timestamp": [2603.64, 2604.64], "text": " Okay."}, {"timestamp": [2604.64, 2605.0], "text": " So it didn't really work there."}, {"timestamp": [2605.0, 2611.0], "text": " But at least it didn't give me too much information."}, {"timestamp": [2611.0, 2614.0], "text": " So that's not too bad."}, {"timestamp": [2614.0, 2618.0], "text": " Okay, so then we'll say answers equals list,"}, {"timestamp": [2618.0, 2622.0], "text": " because here's a fun trick."}, {"timestamp": [2622.0, 2625.0], "text": " So then you say answers.appendlist,"}, {"timestamp": [2627.8, 2630.0], "text": " oh whoops, no, not list, answer."}, {"timestamp": [2630.0, 2632.04], "text": " And then what you do is you have"}, {"timestamp": [2632.04, 2634.16], "text": " one final summarization step."}, {"timestamp": [2636.92, 2640.22], "text": " So let's get this, and I'll show you what this does."}, {"timestamp": [2647.36, 2653.96], "text": " All right, so we got that answer. And then, because, oh darn."}, {"timestamp": [2653.96, 2655.0], "text": " OK, that's fine."}, {"timestamp": [2658.2, 2660.24], "text": " I liked its original one better."}, {"timestamp": [2660.24, 2662.4], "text": " Let's see if I can do, what was it, Control-Shift-Z?"}, {"timestamp": [2662.4, 2662.9], "text": " There we go."}, {"timestamp": [2667.32, 2674.52], "text": " better. Let's see if I can do was a control shift Z. There we go. Okay, so we'll take the passage and we'll take what was it his legacy or no it gave me"}, {"timestamp": [2674.52, 2685.0], "text": " a good answer from just the introduction. Okay, so we'll do that and then okay so And then, okay."}, {"timestamp": [2688.1, 2690.68], "text": " So it gave me a shorter answer, that's fine."}, {"timestamp": [2690.68, 2691.92], "text": " So we'll accumulate this here."}, {"timestamp": [2691.92, 2696.92], "text": " So let's ask this question of a couple more spots."}, {"timestamp": [2698.0, 2701.78], "text": " So we'll do ancestry and family."}, {"timestamp": [2704.78, 2705.0], "text": " Just so that way I can show you that like, if you have different answers, Ancestry and family."}, {"timestamp": [2705.0, 2711.76], "text": " Just so that way I can show you that if you have different answers or different things,"}, {"timestamp": [2711.76, 2720.64], "text": " the GPT-3 is still pretty good at summarizing it together."}, {"timestamp": [2720.64, 2727.0], "text": " Yeah, so that's not even remotely relevant."}, {"timestamp": [2728.0, 2730.0], "text": " Okay, so that's fine."}, {"timestamp": [2730.0, 2735.0], "text": " RSDs, question what made our Omnulous?"}, {"timestamp": [2738.0, 2743.0], "text": " Yep, okay, so then we'll do last one in rain."}, {"timestamp": [2743.0, 2746.36], "text": " So we'll just, basically what I'm doing is I'm"}, {"timestamp": [2746.36, 2751.24], "text": " pretending that this article is much longer"}, {"timestamp": [2751.24, 2754.12], "text": " and that it's giving me that I needed to split it up"}, {"timestamp": [2754.12, 2757.0], "text": " into four different chunks in order to get this answer."}, {"timestamp": [2763.68, 2765.28], "text": " This made the last emperor."}, {"timestamp": [2765.28, 2768.56], "text": " Okay, so there is relevant information in each of these sections."}, {"timestamp": [2768.56, 2770.12], "text": " That's great."}, {"timestamp": [2770.12, 2785.0], "text": " Okay, so then what we do is we take all this and we say merge the following answers into a single paragraph."}, {"timestamp": [2794.16, 2796.56], "text": " The question asked was,"}, {"timestamp": [2798.56, 2802.26], "text": " and that should still be, oh darn."}, {"timestamp": [2806.28, 2808.28], "text": " Let me just copy paste this whole thing."}, {"timestamp": [2809.68, 2813.2], "text": " Okay, merge the following answers"}, {"timestamp": [2813.2, 2818.2], "text": " into a single coherent paragraph."}, {"timestamp": [2818.56, 2821.0], "text": " The question asked was,"}, {"timestamp": [2824.92, 2827.0], "text": " so then we'll do... Oh, I have another thing falling off. Let me pause real quick."}, {"timestamp": [2833.12, 2840.12], "text": " Okay, and we're back. All right. The question asked was, okay, and then various answers."}, {"timestamp": [2845.32, 2850.08], "text": " And then various answers. So then we'll come back to here and grab."}, {"timestamp": [2850.08, 2851.84], "text": " Wait, where did I accumulate those answers?"}, {"timestamp": [2851.84, 2853.52], "text": " Here we are."}, {"timestamp": [2853.52, 2857.4], "text": " Various answers, and then let's do this all caps."}, {"timestamp": [2860.2, 2867.0], "text": " Various answers, and then"}, {"timestamp": [2867.0, 2868.0], "text": " merged answer"}, {"timestamp": [2868.0, 2872.0], "text": " actually let's say merged paragraph"}, {"timestamp": [2872.0, 2879.0], "text": " let's see how that does"}, {"timestamp": [2879.0, 2890.0], "text": " in Gallagher Orestes instead marched on Ravenna, the capital of Western and proclaimed Romulus. And Napostad was only a figurehead with Orestes running much of the imperial administration."}, {"timestamp": [2893.0, 2895.0], "text": " Interesting."}, {"timestamp": [2896.0, 2898.0], "text": " Okay."}, {"timestamp": [2898.0, 2902.0], "text": " But it doesn't mention Odoacer."}, {"timestamp": [2907.0, 2907.2], "text": " Odoacer."}, {"timestamp": [2912.68, 2919.68], "text": " Well, whatever. That's fine. Let's go to the Wikipedia article and see."}, {"timestamp": [2919.68, 2921.68], "text": " Odoacer captured Ravenna, killing her estes deputy."}, {"timestamp": [2921.68, 2927.82], "text": " Okay. Dana, killing at rest, he's deputy."}, {"timestamp": [2928.66, 2933.0], "text": " Okay."}, {"timestamp": [2934.0, 2935.72], "text": " All right, whatever."}, {"timestamp": [2938.88, 2942.16], "text": " Okay, so we've got it broken up into a couple sections. We summarize it here."}, {"timestamp": [2944.6, 2945.0], "text": " I wonder what happens if, that should be fine."}, {"timestamp": [2956.2, 2957.4], "text": " Yeah, that'll be fine."}, {"timestamp": [2957.4, 2959.48], "text": " Okay, so we'll do one last thing."}, {"timestamp": [2959.48, 2974.78], "text": " We'll save this as answers question."}, {"timestamp": [2978.3, 2981.5], "text": " And then we'll do prompt and we'll call this prompt merge."}, {"timestamp": [2985.0, 2987.12], "text": " Okay, so then we're closing in on complete, I think."}, {"timestamp": [2988.68, 2993.68], "text": " All right, so we got all of our answers, and so answers equals, no, answer equals,"}, {"timestamp": [3000.8, 3004.08], "text": " we'll say space.join answers,"}, {"timestamp": [3004.08, 3007.0], "text": " so that just merges it all into one block."}, {"timestamp": [3007.0, 3011.0], "text": " And we'll say prompt equals..."}, {"timestamp": [3011.0, 3017.0], "text": " Well, here we'll just..."}, {"timestamp": [3017.0, 3022.0], "text": " prompt merge."}, {"timestamp": [3022.0, 3032.72], "text": " And we have answers and question. So replace question with question."}, {"timestamp": [3032.72, 3034.12], "text": " I already had that."}, {"timestamp": [3034.12, 3041.52], "text": " And then we'll replace answers with answer."}, {"timestamp": [3041.52, 3043.04], "text": " I think that's how I did it."}, {"timestamp": [3043.04, 3043.76], "text": " Yes."}, {"timestamp": [3043.76, 3047.64], "text": " Various answers into a single coherent paragraph."}, {"timestamp": [3047.64, 3048.8], "text": " The question asked was."}, {"timestamp": [3048.8, 3049.72], "text": " OK, that should work."}, {"timestamp": [3052.64, 3054.48], "text": " And so then we get that."}, {"timestamp": [3054.48, 3056.04], "text": " GPT-3 completion."}, {"timestamp": [3056.04, 3057.6], "text": " We do the encoding here."}, {"timestamp": [3057.6, 3060.88], "text": " So this was a thing that I started adding,"}, {"timestamp": [3060.88, 3064.58], "text": " because especially when you use external sources,"}, {"timestamp": [3064.58, 3067.74], "text": " sometimes there's characters that GPT-3 does not like,"}, {"timestamp": [3067.74, 3070.24], "text": " but if you convert it to ASCII and ignore errors"}, {"timestamp": [3070.24, 3073.98], "text": " and then decode it back, then GPT-3 seems to be happy."}, {"timestamp": [3073.98, 3076.78], "text": " And then return answer."}, {"timestamp": [3076.78, 3079.44], "text": " Okay, so we've got, we pass it a Wikipedia article,"}, {"timestamp": [3079.44, 3081.58], "text": " we ask a question, we break it into chunks,"}, {"timestamp": [3081.58, 3084.42], "text": " if necessary, might not be necessary."}, {"timestamp": [3084.42, 3086.08], "text": " We ask questions and then pass it back."}, {"timestamp": [3086.08, 3088.36], "text": " Okay, so now that we've got this, it's like,"}, {"timestamp": [3088.36, 3090.1], "text": " okay, wow, we've done a lot."}, {"timestamp": [3090.1, 3090.94], "text": " Now what?"}, {"timestamp": [3092.44, 3097.44], "text": " So let's copy this and we'll copy this out to here."}, {"timestamp": [3100.28, 3103.96], "text": " And let's go back here."}, {"timestamp": [3110.44, 3115.88], "text": " And let's go back here and we'll say, background info."}, {"timestamp": [3118.2, 3122.44], "text": " User. What caused the fall of Rome?"}, {"timestamp": [3122.44, 3122.94], "text": " Tim."}, {"timestamp": [3127.98, 3129.44], "text": " Why didn't you give me an answer?"}, {"timestamp": [3129.44, 3130.28], "text": " There we go."}, {"timestamp": [3131.9, 3133.88], "text": " But you gotta include this."}, {"timestamp": [3137.62, 3140.28], "text": " Because we've got this information, which is good."}, {"timestamp": [3140.28, 3144.14], "text": " So part of the problem is, and this might not work,"}, {"timestamp": [3144.14, 3146.32], "text": " I might have to exclude this part,"}, {"timestamp": [3146.32, 3158.36], "text": " is because this model was not fine-tuned to incorporate other information."}, {"timestamp": [3158.36, 3161.6], "text": " Let's see."}, {"timestamp": [3161.6, 3167.8], "text": " Oops, user."}, {"timestamp": [3167.8, 3175.22], "text": " I wonder if instead of using the fine-tuned model, I could just, what if I just exclude"}, {"timestamp": [3175.22, 3180.66], "text": " the fine-tuned model and just have this information?"}, {"timestamp": [3180.66, 3185.0], "text": " It would require reworking this fine-tuned model."}, {"timestamp": [3186.32, 3189.94], "text": " So I might have to stop here and split this up"}, {"timestamp": [3189.94, 3192.12], "text": " because this fine-tuned model does not know"}, {"timestamp": [3192.12, 3194.56], "text": " how to incorporate additional information."}, {"timestamp": [3194.56, 3195.54], "text": " Darn."}, {"timestamp": [3195.54, 3197.96], "text": " But that's okay, we're already at almost an hour,"}, {"timestamp": [3197.96, 3200.26], "text": " so it happens."}, {"timestamp": [3200.26, 3202.24], "text": " We'll stop here, we'll leave it off"}, {"timestamp": [3202.24, 3204.8], "text": " where I need to create another model"}, {"timestamp": [3204.8, 3205.68], "text": " that will incorporate"}, {"timestamp": [3206.32, 3211.44], "text": " background information, external information into the chat. Because this chat model is already"}, {"timestamp": [3211.44, 3216.24], "text": " pretty good, but it's missing a couple things. All right, thanks for watching."}]}