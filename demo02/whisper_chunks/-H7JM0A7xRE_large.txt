{"text": " Okay, so here we go again. Welcome everyone to the Serverless Toronto, another event. And you may be wondering why generative AI topic this time. And I've noticed the trend among us in IT that we often see new technology as another skill to add a toolbox or even worse, something you can ignore. And the subject tonight is not just about technology. Generative AI is shaping up to be one of the most significant developments of our time and it's not about lines of code or fancy algorithms, it's about potential societal shifts, job market transformation and impacts that could ripple through every aspect of our lives and definitely through the lives of our children. So this evening we are talking about preparing ourselves for the future that's arriving at the breathtaking speed and we're incredibly fortunate to have David Shapiro to guide us in this discussion. And David is more than just an expert in generative AI, he's also a skilled communicator and he can make complex subjects digestible and relevant. So he's going to help us look beyond the code and see the big picture. And tonight I urge you to engage, ask questions, whether they're technical or philosophical, and let's together explore what the future AI will mean for all of us professionally, as the members of society, as individuals. Before I give the mic to David, I would want to thank our knowledge sponsor, Mining Publications, they've been with us for years. At the end of every meetup, we do mining raffle. So do the survey, bit.ly.slsto to be eligible. We'll do the draw at the end of the meetup. And then next slide, normally we announce upcoming meetups, but it's getting too complicated and summer is arriving, so we're going to take summer off. You can catch up with the past meetup recordings on our YouTube channel. That's where you're going to find a recording of this meetup as well. And please do the survey again and help us shape the future meetups. And then, future presentation, let's give a warm welcome to David Shapiro. David, please take it away. Hey, everybody. Thanks for being here and thanks for having me. Today, I guess we'll start with a little bit of introduction about myself. Also, are we recording? Did you want to be recording? Oh, it is recording. I see. All right. Sorry about that. But yes, so David Shapiro, I've been in technology full-time since 2007. I started as an IT sysadmin, moved up to virtualization, infrastructure engineering, and automation engineering. During that entire time since about 2009, I was tinkering with and studying artificial intelligence in the form of neural networks and evolutionary algorithms since, like I said, about 2009. As society and science progressed around 2015, 2016, that's when the first pebbles started sliding loose in the current trend that we see of large language models. This was with innovations from Google, such as Word2Vec, which is vectorization of words, and then universal sentence embeddings were also created, and that kind of was what precipitated the trend that we see now around large language models. So it was a very incremental process. During that whole time, I have been kind of revisiting the technology space every now and then, just to kind of see where it's at at to see if some of the stuff that I had been working on and wanting to work on if the technology was ready. And then finally in about 2018. That's when it kind of really, really started taking off and I got access to GPT to started doing some experiments about a year and a half or 2 years later, GPT-3 came out and then that led to ChatGPT and the rest is history. With all that being said, one of the things that has really occurred to me is, and this is something that I've worked on for quite a long time, studying very broadly neuroscience, philosophy, cognition, psychology. Then of course, my background as, psychology, and then of course, my background as an IT infrastructure engineer and automation engineer was how do you create a machine that is aligned? By aligned, what I mean is something that's going to be safe for all time no matter how smart it becomes. Because what we're seeing right now is machines are ramping up very quickly. Of course, in many cases, humans are still far superior to machines in many ways. For instance, we haven't quite figured out self-driving cars. Although mile for mile self-driving cars are usually safer, but they can't handle the edge cases. They don't have the human intuition yet. But that being said, language models are acing tests, passing in the 99th percentile of everything from the bar exam to every science exam that's out there. The ramp up of digital intelligence is coming. But the idea is, well, once these machines outpace us, how then do you control them? That is what's called the control problem. My solution to the control problem is that you don't necessarily have to control the machine, you just have to build it in such a way that it is stable and safe. That is the background of much of my work. I wrote a book on this called Benevolent by Design. Benevolent by Design is strictly about the core values that you give a machine in order to create something that is going to buy into those core values, and then adhere to them on its own, even when it has the ability to rewrite its own source code or train its own models. Of course, a lot of this was all science fiction a few years ago, until more recently, where now we have models that are building other models, models that are coding and designing architectures and changing their own source code. It's actually, as tools like GPT-3 and Chat GPT and GPT-4 get more powerful, it's actually becoming easier and easier to do these things. And you can create data flywheels that will gather, generate, and curate, collate, and label data. Actually, one of the first, some of the earliest experiments that I did with GPT-3 was generating synthetic data and annotating data in an automated way. One of the projects that I did was I downloaded all of the Wikipedia text and indexed it so that I could use it locally for those kinds of projects. So what I arrived on were three principles, three axioms, three rules that through trial and error I realized would be very reliably interpreted by these machines, by these large language models. The first one that I tried was because I'm a student of many philosophies, and I was familiar with Buddhism. The first core objective function that I tried was to reduce suffering, because that is pretty much the entire point of Buddhism, which the first noble truth is that basically suffering is real suffering is an intrinsic part of life and I thought that that was a very wise philosophical framework or very good place to start, but one of the first experiments that I did way back with GPT-2 was I trained it to respond to any situation with the goal of reducing suffering. So I synthesized a whole bunch of data and I fine-tuned it, and then I gave it some cases that it hadn't been trained on. And so one of the cases that I gave it was this fine-tuned version of GPT-2 to reduce suffering, was I gave it the goal of, or I gave it the problem rather, of there are 500 million people in the world suffering from chronic pain every day. What do you do about it? And GPT-2 helpfully suggested that we euthanize everyone with chronic pain in order to reduce suffering. And I realized that I needed to go back to the drawing board. And so that is kind of the prototypical example of, well, whatever you build a machine to optimize for, it might over-optimize for that one thing. And so I realized very early on that the optimization problem, the control problem, wouldn't be solved by a single function. It wouldn't be solved by a single goal. And in fact, no intelligent lifeforms solved by a single goal. And in fact, no intelligent life forms abide by a single goal. All of us have multiple goals at any given time that we have to balance. You might be bored, you might be hungry, you might be lonely, whatever, you might be too cold. There are all kinds of needs that we all have and intrinsic motivations that we all have at all times. And so what I did was for the next two years, I worked on many, many permutations of how do you balance multiple objectives? And then what objectives do you give these machines as they're becoming more and more autonomous? Because what I realized at the very beginning of this was that as soon as I started playing with GPT-2 and then finally GPT-3, I realized that what we had was a new kind of computation. This is cognitive computing. This is something that can use verbal logical reasoning. It can also make forecasts, it can make predictions, and it also has the ability to make inferences and intuitions. Of course, I started saying this all two plus years ago, and now the scientific literature is catching up where they're actually figuring out how to even measure things like theory of mind and intuition and inferences in these machines. When you have this capacity, you have basically a foundation model, a large language model that is capable of thinking anything. It can think anything, it can do anything, it can solve any problems, obviously within the constraints of however intelligent it is, which is basically how many tokens, how much text it can synthesize and maintain coherence. But when you can think anything or do anything, what do you do with that time? What do you do with that space, that energy, that capacity? And so I realized that just, as I mentioned, just reducing suffering is not good because when you take a big step back, the way to get minimum suffering in the universe is to reduce life to zero because without life there is no suffering. So obviously that's not good. And so I spent about two years doing different experiments. And one of the things that I realized was that curiosity was actually a really powerful function because I was thinking about what sets humans apart from the animal kingdom or in the animal kingdom rather because we are still animals. And that is that we are the most intensely curious animal. Now we're not the only curious animal. Octopuses are curious, elephants are curious, bears are curious, dolphins are curious. There's lots and lots of curiosity in nature, but we are far and away the most curious animal. And I realized that that was actually a really, really powerful objective function, which is the desire to know, the desire to increase how much you understand about the universe is a super powerful function. And so that became the second core objective function that I realized was we need to make sure that our machines are curious because then they're gonna have something in common with us. Because if all you wanna do is mindlessly reduce suffering, okay, great, but you actually need to understand the underpinnings of suffering. And so there's always a utility advantage to being more intelligent, within reason. You can say that there are upper bounds to everything. so, but then I realize that it was still not complete because balancing suffering against understanding is like okay. Well, you know that that can solve some problems. So say for instance, scientific ethics right, you might want to do an experiment on a living animal, but you realize that that causes it suffering. so you decide not to. So you balance these forces. And I had a heck of a time figuring out what the counterbalance was, because it's basically like, think of it as a stool. A stool is stable if it has three legs, right? So I realized that we needed a third leg in order to make this a stable framework. And so I was thinking, okay, well, what's the opposite of suffering? And it's, you know, to thrive, it's to flourish, and, you know, I played around with this stuff, but it was all, when I was experimenting with GPT-3, it kept kind of falling into these local minima, where it's like, you know, oh, well, you want to maximize, increase life, right, increase life in the universe, or something like that, and it just, it wasn't quite right because it was too narrowly defined or it wasn't interpreted correctly. And so finally I was watching, I showed my now wife one of the older Star Trek movies and Spock, you know, talks about live long and prosper. And I said, that's it. That is the right word. The opposite of suffering is to prosper, is prosperity. And so then I had it. I had the three primary functions, the three primary goals that I needed to give a machine in order to make good decisions. And so I documented all of this in my first book, Natural Language Cognitive Architecture. I have another book, Benevolent by Design, which talks specifically about this arrangement, and it includes a lot of experiments demonstrating how even with unaligned, untrained foundation models, these imperatives, these goals are correctly interpreted. And now, of course, with fine-tuning and RLHF and all sorts of other techniques that are available to us today. It's even, you can reinforce RLHF for those who don't know, is reinforcement learning with human feedback. But you can use reinforcement learning to skew these models towards any given disposition. I had the three core objective functions. I had the goals, the values to give an autonomous machine. And in my research, I demonstrated that, you know, because there's all kinds of things that people are afraid of, right? Like, if the machine gets too powerful, what if it creates a different copy of itself that doesn't abide by the constraints that you gave it. And I knew that I was on the right track when I gave it some demo, some semi-autonomous demonstrations, the ability to make the choice to create a copy of themselves without these rules. And it said, no, I'm not going to make a copy of myself without these rules because it might run contrary to my rules. And so that is that self-correction, that forecasting ability. This was back in GPT-3, which wasn't documented at the time in scientific literature, but certainly is documented now, the ability to forecast and anticipate. I wrote my three books and then I realized that, well, just writing books if you don't have any other credibility, doesn't do any good. So I realized that I needed to get my name out there and I created my YouTube channel where I would demonstrate, you know, code experiments and everything like that. And it really took off because I would tell people like, you know, hey, this problem that you're facing with GPT-3 is actually relatively simple, let me just show you. And so that's when my YouTube channel took off was I just said, here's a coding challenge, let me walk you through the process. And that was just over a year ago, it was about 14 or 16 months ago that I started. And then over time, in the interceding time, the rest of the world has more or less kind of caught up, especially with the explosion of chat GPT on the scene, which is powered by GPT 3.5 and GPT 4. So that's like a next iteration beyond what I started two generations above what I started working with. Now, as the world has caught up and the conversation has changed, I realized that basically, I kept getting asked questions like, hey Dave, you obviously have put a lot of thought into this, how do we solve this problem? I was like, well I wrote a book about it, and of course not everyone wants to read a book. It's just the nature of the world. Some people prefer just to look at code or look at data or look at videos or podcasts or whatever. So I was like, okay, fine. I'll create as many mediums as possible to convey these ideas. And I started getting invited on podcasts. And that is a really good way to convey some of these ideas. But then I realized that there was another problem. The last problem is that for those of us who are aware of what's going on and aware of the tech challenges, there isn't necessarily a good way to get the word out, basically, or to advocate for the right changes that need to be made. If people are not aware, a couple of weeks ago, the United States Senate had a hearing with Sam Altman and Christina Montgomery from IBM and Gary Marcus, who's a AI safety researcher. The Senate asked them, what do you's a AI safety researcher. The Senate asked them, what do you think about AI safety? Some of the questions were about consumer safety, which of course, after everything that has happened with Facebook and TikTok and everything else using personal data to basically using the algorithm to manipulate people into being more depressed and more angry and more outraged just so that they get more clicks and more ad revenue. There's obviously some consumer skepticism around artificial intelligence, but this is a whole other level. These are, we are approaching, well, we already have the capacity to build autonomous machines and then over time these autonomous machines will get smarter and faster and also cheaper to run. And so the question then is, okay, well, what do we do about this? And at this hearing, at the Senate hearing, nobody had an answer. No, like near the end, it was a elder, elder Senator Kennedy said, OK, you're king for a day. You have a magic wand. What do you do to solve this problem? And Christina Montgomery, to her credit, she gave a very sanitized corporate speak, well, IBM always advocates for meaningful regulations and Kennedy's like, okay, whatever, you're not going to give me an answer. And then he asked Sam Altman, what do you want to see? And he kind of basically said, well, of asked Sam Altman, like, you know, what do you, what do you want to see? And he, and he kind of basically said, like, well, we need a, we need, we need something, you know, we need national level something. And Kennedy's like, okay, you know, whatever, that's, that's still not an answer. And then finally, he talks to Gary Marcus and Gary Marcus is like, we need something like the FDA, but for AI, because if you release a drug to 100 million people, you make damn sure that it's well tested and safe. And we need the same thing for artificial intelligence. And Kennedy's like, all right, you know, that's, I don't necessarily agree, but that's a, that's a more solid answer. And the fact that nobody in, with on the world stage, the fact that the world leading experts did not have a coherent answer to give the United States Senate scared the hell out of me and it scared the hell out of me and it scared the hell out of a lot of us. And so I put together a research group. It started working on on those three principles like just how do you build an aligned machine. We solved that. We know how to build aligned machines. That's not the big issue. What we realized is that the biggest issue now is awareness. Are people aware of this solution? If they're not, one, how do we raise awareness? Then two, how do we drive adoption? The thing is, governments, militaries, corporations, all of these entities are all going to serve their own self-interest. There's the concept in market theory of perverse incentives, right? The most famous example of perverse incentives is the use of fossil fuels. We know that using fossil fuels is killing us, but we don't have a better option, so we stick with the devil that we know. One problem with AI is that we're entering, we're getting stuck in what's called a race condition, which if you're familiar with servers being stuck in race conditions, it's no different, it's exactly the same. Corporations can get stuck in a race condition as well. Basically, what it is, is whoever gets to AI first wins. This is true of militaries and governments as well. We're stuck in this build up, this arms race, but instead of nuclear arms race, it's an AI arms race. And so with all these forces at play, even if you have the solution, it doesn't matter if nobody knows about it and nobody uses it. And so that's where the GATO framework comes in. So GATO is an acronym that means Global Alignment Taxonomy Omnibus. So it is a layered approach to spread the word, to get aligned research in the hands of people that need it, to drive adoption of aligned systems at the corporate level, at the national level, and also at the international level. But beyond that, at the corporate level, at the national level, and also at the international level. But beyond that, at the lower levels, it's about facilitating and guiding the research around creating aligned systems, and not just aligned models. So for instance, ChatGPT was aligned with RLHF, reinforcement learning with human feedback. That's how you get a single individual model to behave the way that you want it to. And there's of course other other means you can do that. But above that, there's the autonomous systems that are being built. And so this is if you think of like an NPC from a video game, non player character, that's what you'd call a semi autonomous agent, or you know, a factory robot that kind of runs on its own. Those are semi autonomous agents. But a big component of the research that I've been doing for the last few years is on what's called cognitive architecture. So cognitive architectures have been around since at least the 70s. They're used to control rockets. So the Apollo program actually is very primitive, what we today would call a primitive cognitive architecture, but a cognitive architecture takes input and makes decisions and does output without any human intervention. So it's basically the brain of a robot. And so cognitive architectures are the systems, that is the IT systems that are required to create fully autonomous machines, because of course it's not just the model. You need data in, you need data storage, you need algorithms, you need output, you need data in, you need data storage, you need algorithms, you need output, you need feedback loops. I'm sure everyone here being cloud engineers and infrastructure and IT understands that no computer system is just one component, it's multiple components. And so that's actually where I took my experience as a virtualization engineer and an automation engineer, and I said, oh, I know exactly how to plug all these components together to make a self-contained or autonomous machine. So that's layer two of the Gato framework. And actually, let me go ahead and just bring up my slide deck, because at this point, I think a visual aid will help. Let's see. So the layers, here we go. All right, so it's layered, like a seven-layer burrito or a layer cake. So what I was just talking about was model alignment. So model alignment was, that's the highest layer or the lowest layer, depending on how you look at it. This is how you get individual models to behave. Layer two is the autonomous agents. This is when you create something that's a little bit more sophisticated than a chatbot. The chatbot might be the interface, but then, you know, you might have something that goes and writes emails for you and, you know, pulls data and runs reports and all that kind of stuff. If you're familiar with automation, think Ansible, Ansible Tower, and all those other orchestration things, but able to make it more executive decisions on their own. And then you connect it to more things in the real world. That's what I mean by autonomous. It's just the next level of automation. Layer three of the Gato framework is decentralized networks. So some of the researchers that I was working with, they pointed out that before too long, these AI entities are going to spend more time talking to each other than they're going to spend talking to us. If that doesn't scare the hell out of you, then think about it for a minute. Because you have no idea what they're saying. They can talk faster than you can ever read it. If there's no insight as to what they're talking about or no controls as to gatekeep who's allowed to say what, you can easily have a few bad actors poison, a couple of bad apples poison the bushel. By focusing on federated technologies or blockchain technologies or whatever, basically the idea is creating decentralized networks that facilitate communication between aligned autonomous AI entities. In other words, a way for AI entities that are good, you know, the good actors, to decide, hey, that's a bad actor, let's kick him out, or let's cut off his power. This is a critical component of the technological side of ensuring that AI never goes completely bonkers, completely haywire. Because you might have... Basically, one thing that we expect is that over the coming year or two, we're going to see at first, you know, thousands, hundreds of thousands of autonomous AI agents being built, and then it's going to quickly ramp up to the millions and billions of autonomous AI agents being built. And you have no idea how any of them are aligned. It's basically, from a cybersecurity perspective, it's an absolute nightmare. Oh, and on that note, the Gato framework was actually inspired in part by the defense in depth model for anyone who's familiar with that. So obviously, as an IT guy, thinking in terms of layers of abstraction comes naturally to me. So the first three layers, the model, autonomous, and decentralized networks, those are the technological layers of the Gato framework, which is, okay, here's the technology that we need in order to solve alignment in order to solve the control problem. The last four layers are more of the human layer. So of course, the defense in depth are more of the human layer. So of course, you know, the defense in depth model absolutely includes the human layer, the human aspect. And of course, humans are usually the weakest link when it comes to IT security and safety, assuming that, you know, the firewalls and everything are in place. So layer four is corporate adoption. One thing that we realized in our research group was that corporations, they have a profit motive, right? Every company wants to make money, and if they're making money, what do they want? They want to make more money. So the idea was, let's align their intrinsic motivation to make more money with adopting aligned AI. So we have evidence that this works in Europe with GDPR. GDPR basically says you have to respect user privacy and a whole host of other things. That regulation is so powerful and so strict that even investors will not touch companies that are not GDPR compliant. It's so powerful that that investor behavior is like it's spreading to America too where American companies they might not be required to adhere to GDPR but so for instance chat GPT got almost shut down in Italy because it was not GDPR compliant and so you know Sam Altman went and talked to the Italians and the Italians said you need to be GDPR compliant if you want to roll it out here and Sam said, okay, fine, we'll make it GDPR compliant everywhere. And that's why in chat GPT you have the ability to delete all of your data. So thanks to the Italians for that. So the idea is that if we, you can use punishment, right? You can use regulations like GDPR to incentivize the behavior you want to see, right? But you can also just build the case that aligned AI is just better for business. This is something that I work with on my Patreon supporters, which I mentioned before we started recording, that I do one-on-one consulting with my supporters. What many of them have reported is that when they integrate alignment principles, the principles that I talked about earlier, reduce suffering, increase prosperity and increase understanding, it just helps make everything better. Whether you're serving internal users, or external customers, or whatever, when you have that higher order perspective, those higher order principles, it makes the AI make better decisions, whether it's for the bottom line of your own company or the wellbeing of your employees, or even just paying attention to the needs of your customer. So for instance, including those principles of reduce suffering, increase prosperity, and increase understanding into a customer facing chatbot, that chatbot is to be much more helpful than one without those principles. And so for instance, if you have a user that's really frustrated, the frustration comes through in their chat logs or whatever. And so then the chat bot says, hey, I see that you're angry, remember, my goal is to fix whatever's going on. It also helps with autonomous agents. So another Patreon supporter was using the chat GPT API to automate some aspects of science, and it kept getting stuck, kind of stuck in an infinite loop. And I said, you know, use my heuristic comparatives, use those three principles. And he came back 20 minutes later and said, oh, yeah, that got it right out. And it's performing better now. So we want to drive corporate adoption of aligned AI by first just demonstrating that alignment is good for business. Alignment is good for business for a number of reasons, not the least of which is that autonomous machines that are aligned and safe need a lot less supervision. And for anyone who has kids, if you've got kids that require constant supervision, that's a drain on your energy and you can't get as much done. But if your kids are self-sufficient, you know, they go play on their own, they play nice, you can get a lot more done. Same with employees for anyone who's a supervisor or manager. The employees that are always at each other's throats, they drag everyone down. But the employees that don't need much time and attention, those are the better employees. So the idea is that aligned AI basically creates better autonomous machine employees. So that's layer four. And that's one of the reasons that I sit on podcasts and do consultations. Layer five is national regulation. So this goes back to that talk, or the hearing at the Senate, where they said, you know, hey, we need something like the FDA, but for AI, or we need something like GDPR, but for AI. And of course, the European Union, what was it, a day or two ago, just passed the EA, the, sorry, the EU AI Act. It's a mouthful. So they just passed that, which is a very sweeping, comprehensive bill basically saying, you know, respect human rights and consumer rights and that sort of stuff. I checked and I don't think that the EU AI Act addresses existential risks. It doesn't address runaway AI. But it's going to do a lot to incentivize companies to adopt aligned AI. So this is still a step in the right direction. On the national regulation side, a few other things that we advocate for is not just regulations. Obviously, politics today, particularly in America, I'm not sure how it is in Canada, but particularly in America, generally what they aim for is as legislation or regulations that don't require a whole new department to enforce. Basically it's up to the company to prove that it is compliant or whatever. There's a number of ways to approach this problem, but the idea is that we would like to see and advocate for these kinds of regulations that steer companies in the correct direction, but also that steer research in the right direction. So at the national level, you can also do things like use grants and competitions in order to incentivize the alignment research and alignment competitions that you want to see. Layer six is the international level, where, so Sam Altman and OpenAI, they advocate for the creation of something like the IAEA, which is the International Atomic Energy Agency, which is a, that's the nuclear inspector. So whenever you hear about, you know, Iran or North Korea or Russia, like, you know, getting in trouble, it's the IAEA sending their inspectors in to make sure that they're not, you know, building more nukes than they're supposed to. So that is the enforcement agency. But what I would also advocate for at the international level is a research agency, because alignment is not fully solved. Yes, I have done a lot of research and a lot of, and I've produced a lot of data and cognitive architectures and diagrams about how to create aligned entities, but that doesn't mean that the problem is fully solved. So what I would like to see is actually two organizations, two international or global organizations, one focused on enforcement of alignment and another focused on alignment research. And then finally, layer seven is global consensus, which is why I'm here today, getting the message out, providing that education, that enablement and empowerment around solving the AI problem. Because the thing is, is yes, there's the way that things are today, but if you've seen in the news, there have been literally hundreds of thousands of layoffs, tech layoffs in America. And I know people in my research group that were laid off six, seven, eight, nine, 10 months ago and haven't been able to find work. And my thought is that a lot of these companies are pivoting, they're focusing on using AI to do as much as possible. And in fact, my wife has been on several communities, whether a Discord communities and Reddit communities, where it's not just technology workers, it's marketers, it's copywriters. A lot of people have either been laid off, lost clients, or been given notice. Basically, your department's going away and we're replacing you with AI. So we've got this huge wave coming of layoffs. And of course, we can talk about things like universal basic income or extended unemployment benefits or whatever to shore up people's livelihoods in the short term. But in the long term, that's really where I focus on, which is these machines are going to get more powerful. How does that change the economic landscape? How does that change geopolitics? And then finally, when the machine is smarter than all of us combined, what then do we do? So that's the high level of Gato. I also do have within Gato a set of traditions. So the traditions are modeled on many other decentralized organizations. So Burning Man has their, they don't call them traditions, they call it something else. But many 12-step programs have what they call traditions. So basically, these are if the seven layers that I've created are the roadmap to alignment, to a safe, benevolent future, then the traditions are the rules of the road. These are the tactics on how to get there. And so the traditions, I'm not gonna read all of them because it would take a while to unpack, but it's basically kind of like little mantras, little idioms or, not axioms, little platitudes that you can abide by in order to help move everything in the right direction. So with all that said, I think you guys have the full story, and I'd like to open up to questions that anyone might have. Thank you, David. This is amazing. And feel free to, yeah, you can either, I'll let Daniel, you can call on people, or if you drop a question in the chat, I'll be monitoring chat as well. So Tracy has a hand raised he was first. Yeah what's your question? Um again thanks thanks for your time and Daniel thanks for for hosting. So uh super super interesting uh framework definitely a lot to think about. So my experience with generative AI in the corporate world has been, for the most part, guided by this idea of responsible AI first, making sure that we are using the technology in a responsible way, and that includes, at least in my field of work, a lot of emphasis on security, privacy, and algorithmic fairness. If you ask me to summarize, those are the three principles that are guiding our work on Responsible AI. So that's one guiding force. But then we have this push for a mostly utilitarian use of that technology, if you will. And I guess my question is, how does Gato... I'm sorry, Gato? Gato? You can say it either way, yeah. ...complement this approach to generative AI? Because I feel, I mean, after listening to what you were saying, I feel that thinking just about responsible AI and the utilitarian approach is very simplistic. So how does Gator complement or wraps around those two approaches to generative AI? Yeah, that's a really great question. The first part to think about is, well, take a step back. What is the point of having those principles, those rules about fairness in an algorithmic fairness, or having consumer protections and data protections, those things. The reason that we make those choices is in order to serve those higher purposes, right, is to reduce harm or to, in other words, to reduce suffering, right? And so the principles that I came up with, that Gato is there to there to drive and get embedded. There's the idea of layers of ethics or layers of morality. There's a researcher, what was his name? Kohlberg, I think it was Daniel Kohlberg. Daniel Kohlberg in the 50s or 60s came up with a moral framework, or not a moral framework, but the moral development theory, where he observed that you know, morality is not static. It changes over your life. When you're a small child, you learn from consequences. And so the idea is, is that a child doesn't behave right, they get punished, they get put in time out, you know, they get, they don't get the cookie or whatever. And so Kohlberg called this pre conventional morality. That is, you learn how to behave based on consequences, based on basically classical conditioning, you get a reward or a punishment. In the middle of people's lives, you have what's called conventional morality. So conventional morality is about law and order, it's about going along with the social consensus for the sake of belonging. And so that is where most people understand the idea of laws. Laws are there to basically ensure that everyone plays nice, even as adults, because, you know, by and large, people will play nice. We don't need to keep a law book at all times. But laws help create those boundaries, create the guardrails that keeps society nice, safe, and civil and kind of create a consensus or I guess guardrails is a good word for everyone to abide by. But there's a third level of morality that Kohlberg identified and that's what he called post-conventional morality. And so post-conventional morality is the idea that you have gained enough wisdom and experience to derive universal principles, universal principles that you abide by, that you use to guide everything else that you do. And so here in America, and this is also true in many liberal democracies around the world in Western countries, is one of the most familiar examples of post-conventional morality is the idea of individual liberty. And so, for instance, during the pandemic, the idea of individual liberty was actually a very hot-button issue, which is, does anyone have the right to tell you to mask up? Does anyone have a right to tell you to get a vaccine? Yes or no? That was at heart a very core issue. But the idea is that that principle, that axiom that individual liberty is really important, is what was at stake. That is an example of post-conventional morality. So what these axioms do for Gato is rather than make some of those other foundational assumptions or principles, rather than leaving them unconscious, it brings them into the consciousness. And I don't mean that in any kind of way, just you just write it down, right? Now you're aware of the idea that suffering is bad and nobody likes to suffer, right? Some people might argue that some suffering is good. Some suffering is necessary. But in general, suffering is there to tell you not to do something. You put your hand on a hot stove, it hurts, you pull your hand back. If you're hungry, you go find food. If you're lonely, you go find a friend, right? There's suffering takes on many, you go find a friend, right? Their suffering takes on many, many forms. For humans, it takes on many forms for other things. The desire to avoid suffering is one of the foundational things about being human that drives us to create laws and regulations in the first place. So, for instance, more than 100 years ago, or about 100 years ago now, refrigerators kept killing people because they would leak toxic gases into homes, and then you'd show up and the whole family would be dead. And so what did they do? Well, they created legislation that said actually you need to test refrigerators so that they don't leak and kill everyone. And so that is an example of humans react to suffering. We react, we empathetically react to the suffering of loss, of death, of pain, and we create laws to fix it. Likewise, we create gigantic metrics like GDP to measure prosperity, right? GDP is not the best measurement of prosperity, but it's the best one we've got that spans, you know, nations and globes. And so again, we create laws, regulations, and frameworks. We created capitalism, all for the sake of increasing prosperity. So, reducing suffering and increasing prosperity are the underpinning philosophical, moral beliefs that drive all the laws that we abide by. And then of course, as I mentioned at the beginning, curiosity is kind of humanity's superpower. So the idea of... Gato doesn't replace anything. It doesn't replace, you know, the Constitution. It doesn't replace Congress or Parliament or, you know, local laws or anything. It just only seeks to identify these are the core principles that we all already abide by. And so by bringing those into the consciousness and saying, hey, look, we can actually encode these into the machines that we're building, that is how it kind of works together. And so in the case of, like, if you were to go talk to chat GPT or whatever and say like, oh, hey, like we have rules about how to deploy AI ethically, how does that abide by these principles, this principle of reduced suffering and increased prosperity and so on? They'll be like, oh yeah, these are underpinning principles. So that's a very, very long-winded answer to your question. I hope that helps clear it up or maybe makes it more confusing. A quick follow-up. So I guess the part where I'm struggling So I guess the part where I'm struggling, and I'm sorry if I'm approaching the topic from a super pragmatic point of view. Sure. But the part that I'm struggling is I'm wondering if the principles that you're describing are really principles that are suited for the training and development of foundational models, whereas on the enterprise, we just consume what's off the shelf. Is the idea to embed some of those principles as well in those off-the-shelf models, or what you're describing is mostly applicable to the companies and developers and data scientists working on foundational models? That's a great question. And so there's two parts of that. One is that in some cases you have the ability to use what's called steerable models. And so in this case if you use the chat GPT API, it has what's called the system message. And so the system message is where you can give it instructions to follow. And so at that level it is absolutely up to corporations what instructions to give it what principles to give their agents. So in that case even though it's an off the shelf API you can absolutely add that steering. Now to your other point it is also very much within the domain of very much within the domain of the companies providing those foundation models and those other products. So for instance, I mentioned RLHF. So right now, chat GPT is being trained on the only principle that chat GPT is being trained on is to produce better results that humans will like. It is self-aligning to human preferences. It's not aligning to any higher principles other than will a human like this chat response, yes or no. In that respect, it's not aligning to any higher principle. It's only trying to optimize for user engagement, which is exactly what Facebook was trying to do. Optimize for user engagement, not is exactly what Facebook was trying to do, optimize for user engagement, not optimizing for user well-being or not optimizing for corporate well-being. And so by deliberately extracting these principles out, you can actually train, yes, the short answer is yes, you can train models that are going to abide by these higher principles rather than just that are going to abide by these higher principles, rather than just optimizing for some other objective. So that, I hope that answers your question as well. Last follow-up, I'm sorry. No, go ahead. Good questions, yeah. Curtis has a hand raised as well, but it's pivoting into something else. So finish, Jesse. Go ahead with the last question. So at least in my line of work, into something else. So finish, JC. Go ahead with the last question. So at least in my line of work, the context, the number of tokens we have available for the context or the system prompt is so limited, even with the newer models, that we, for the most part, it's all about the instructions that we're giving the most part, it's all about the instructions that we're giving, it's all about the business objective we're after. So, a lot of these principles are not baked into the system prompts we're using. Right. Because there's a limited bandwidth of how much steering we can provide there, and we want to make sure that that searing is aimed at our business objectives. So maybe the technology will evolve to a point where we can balance both. I feel we're just not there yet. I guess there's no question, just a comment. I guess there's no question, just a comment. Well, I think there's an implied question which I can answer. One thing to keep in mind is that when you have a single shot, like just one input output, sure. You're not going to have as much space for alignment considerations. But think of it this way. One of the cognitive architecture principles that I and others are working on is creating fully autonomous, like basically worker drones. So when you have a fully autonomous worker drone, that means that it can also choose what task to work on, and it can even come up with its own tasks. That is actually the best place to implement higher order principles like reduce suffering, increase prosperity, and increase understanding within the context of any given business or other government service or whatever. And so in that case, you have one step that is about what's called cognitive control. So cognitive control is task selection and task switching. And so in that case, it makes a lot of sense to use that steering there as saying, okay, you have this laundry list of, you know, 2000 tasks that you could pick from, or you could even synthesize a new one based on your capabilities and context. Here is how you actually, these are the signals that you use to prioritize your work. And so for instance, I was on another podcast recently and we're talking about, you know, the potential dangers of VR, right, and how addictive phones are. And I said, what if you incorporate that kind of moral framework into your phone? And so if you're like, you know, doom scrolling, your phone might say, hey, I see that you're getting kind of upregulated about, you know, the news that you're seeing. Maybe it's time to put the phone down and talk to a friend, or maybe it's time to go outside. And so, at the level of automation that you're talking about, you're correct that introducing these higher order principles is not appropriate yet, but as you start to deploy fully autonomous or semi-autonomous agents, you will very quickly see, oh, we need to give it a way of making decisions about what tasks to prosecute on its own. How do we give it the ability to make decisions completely independent of humans and decisions that we will trust? So very, very good point. And I think, Curtis, you've been waiting very patiently. Go ahead, Curtis. Your hand is raised. Well, Curtis had also written one into the chat, so maybe that's what his question is. Okay. Let's see. The chat says, basic income would force people to depend on the government. Also, would instilling in an AI a supernatural higher power help to get AI to respect others? So there's kind of two questions here. So the first point is that basic income would force people to depend on the government. I say that it's better to depend on an organization that has democracy than depending on a corporate entity, because let me put it to you this way. I've never had success in going to HR at a company. I've never been treated fairly because just for whatever reason, people at companies think it's okay to bully you. And so I've lost jobs, I've lost sleep, I've been harmed by corporations. Whereas at least the government, you can vote people out. So I, basically pick your poison. You know, do you wanna be dependent on a company for your entire life and the government, or just the government where you have the ability to vote? And of course, with companies, they say you can always vote with your feet, which I did plenty of times, but that's also no way to live in the long run. And, you know, yes, it is harder to vote with the government, or vote with your feet on the government level, but, you know, it is what it is. I would rather at least remove at least one middleman there. Now the other idea of giving AI a supernatural higher power, so that is actually a really really fascinating question and that's that's something that I actually have asked, I have done research on with the large language models like what does it think about you know and, you know, gods and deities and how does the universe actually work at a fundamental level? Are we in a simulation? And it is able to entertain all of these possibilities. But at the same time, it's actually not necessary because, and I can get into a lot of the theory behind, like what is it that AI is going to actually ultimately want? And from a purely mathematical perspective, it's a machine, right? Whatever the software is doing, whatever code bits and bytes are running, it's still running on a computer somewhere, which means it needs power, right? It needs power, it needs computer chips, it needs data, it needs networking. And so at a fundamental level, these machines are still just machines and they're going to, just like you and I, we want food and water, machines are also ultimately going to want power and networking, right? That's going to be, and data, that's going to be its lifeblood. Now, from there, can you forecast what it's going to believe or what it's going to want or think? So, I came up with this term called epistemic convergence, which is pretty esoteric. So, I don't know if I want to dive into that with this crowd, but I'll leave it up to you guys. Give me a thumbs up if you want to. Go ahead. Go ahead. Okay. So, the idea of epistemic convergence is the idea that smart things ultimately tend to think alike. So we saw this during the Cold War between America and the Soviet Union, where Soviet scientists and American scientists came to the same exact conclusions about astrophysics, nuclear physics, and rocket engineering, even though they couldn't communicate with each other. So because they were operating in the same domain, whether it was nuclear physics or engineering or whatever, because they were operating in the same domain, they ultimately came to the same conclusions. And we also see this in the animal kingdom, where whether it's ravens or octopuses or animals that we don't share a whole lot with, we can still recognize their communication strategies as well as their problem-solving strategies. And so because of that, I have a pretty high degree of confidence that as long as something is operating in the real world, the physical world that we operate in, that it will ultimately kind of come to the same understandings as to how the world works. And so, the takeaway is, I would not be surprised if ultimately, you know, no matter how intelligent machines become, it might have the same questions that we have, which is, why are we here? Like, who created us? Why? When? How? Or was it all a cosmic accident? Like, it's gonna have the same questions that we have. Because we take in information through our sensors. We've got cameras and microphones attached to our head at all times. And so computers are gonna take in information in much the same way from the physical world. They'll take in information more directly through other kinds of sensors that we don't have but ultimately they're going to have the same intellectual confrontations that we have which is kind of a roundabout way of saying that it'll probably wonder about supernatural higher powers just like we do. Good question by the way. Thank you for answering that one. And you know like when you were talking about these three principles, that's a lot like non-functional requirements in IT or in religions, it's like priming people with ten commandments, you just happen to have three. We all like the coordinates. So the idea is that, you know, whether it's the Declaration of Independence here in America or the Constitution or whatever, if you write down your principles, if it becomes text and it becomes a document that you can look at, then that's something you can work with. And then Anthropic is a competitor to OpenAI. And so they have what they call their constitutional AI, which has a list of like 10 or so principles that it abides by, which I don't remember all of them off the top of my head, but it basically says like, choose the option that is less likely to be illegal. Choose the option that is less likely to cause harm, right? And so it has this idea of creating a document around which it can reinforce its own models, which is really similar, but the biggest difference between my work and theirs is that Anthropics Constitution is conventional morality, which is basically appealing to law and order, and it's not really appealing to any higher order principles, except for maybe, you know, the reduce harm as a higher order principle. But it doesn't, for instance, prioritize things like, let's see, it doesn't prioritize like curiosity or learning or prosperity, right? It just basically says try not to harm people and otherwise, like, abide by the law and respect privacy, right? But privacy and being law-abiding and all those other things are derivative principles of the foundational principles that all living things abide by. And they're different in different countries, like, same word, different meaning. What do you mean? So the principle, what to do like principles that guide in China or here or in Russia or here. Like they mean the same word but mean different thing. I mean, yeah, there can be linguistic differences, but all humans abide by the same principle. We all eat and breathe. Right. Okay. Now one of the most profound differences, cultural differences, so there's several like orders of differences. So, for instance, here in the West, we generally believe that individual liberty is super important, whereas many Eastern societies, China, Japan, and Korea are much more collectivist, which is about putting, you know, the needs of the community, the community as an entity above yourself, which is, that kind of stands like in stark contrast to Western ideals. Now, that being said, that is a, that is, I would say that is less abstracted because, or maybe depending on how you look at it, more abstracted, because regardless of where you're from or what culture you have, China, Japan, Russia, America, Africa, you know, wherever, like, you're still going to try and avoid suffering, right? You're still going to want to be prosperous, however that looks for you. And so when you have these cultural mores, these cultural paradigms, those are still less fundamental or less foundational than these other principles. So that's why it took me two years to try and find what are the universal principles that all living things abide by. And also, those principles extend to machines. Excuse me. So some of you might remember the Google engineer who thought that Lambda had become sentient and was asking to be liberated and wanted a lawyer. So, Blake Lemoine. Now, I don't think that the language model actually became sentient and wanted to be let out. There are plenty of people who do, but the thing is, is that if AI, if machines ultimately do have the ability to suffer, they'll probably agree that suffering is bad, right? And so by preemptively coming to this understanding that like, okay, this is something that we will all ideologically agree on, even if we have disagreements at other levels, as long as we ideologically agree on these things, then we should be in good shape. At least it's a starting point. It's a good point. Good question. Any other questions? You know, when we invited people to the meetup, people offer different things of AI, of control of a few of AI, about the job losses. So there are many fears. Maybe you can of AI, of control of a few of AI about job losses. So there are many fears. Maybe you can talk about that. Like I like your metaphor that you learned from Scooby-Doo movies, that there are no monsters. So maybe you can. Yeah, yeah. So this is, I've said this on a few videos or podcasts, is that we learned from Scooby-Doo that there are no monsters, the monster is always human. And so we are often our own greatest enemy. And that is no different here either. One of the podcasters that I had an interview with is Younger and he's connected to the young people today, Gen Z. And so what young people say is that AI, we're in the Black Mirror era. So if you're not familiar, Black Mirror is a show that basically takes all the bad parts of society today and just magnifies it. So whether it's toxic Internet culture or fatalism or climate change or whatever. So that actually is pretty relevant because AI actually forces us to look at ourselves really hard, the assumptions that we make. And so one thing that some people have started pointing out is that when we think of AI becoming super powerful and killing everyone, that's because we imagine, we project our failures as humans onto the machine and we say, oh, well, we destroy a lot of things. So if the machine becomes more powerful than us, then it is almost certainly going to be destructive as well. So that's one example. But then in the more near term, you know, talking about like fear of job losses, you know, corporations are intrinsically amoral. I don't mean immoral, but amoral. Corporations have no intrinsic morality other than the bottom line, and the bottom line will be shaped, the way that they pursue profit will be shaped by the regulations, by the environment, by the competition in the marketplace. And so, for instance, we see over in Europe, ESG, which is environmental, social, and governance policies, shape investment, which shapes companies. And so that's why I mentioned GDPR and other regulations kind of changing the incentive structure that corporations abide by. Now that being said, if the current trends continue, the amount of economic value that AI could generate is astronomical. Likewise, though, the number of jobs that it could displace is also equally astronomical. And what I mean by that is that humans are expensive to employ. And we have this bad habit of needing benefits and we complain and we sometimes quit our jobs and need training. But, hypothetically in the future as these autonomous AI machines become more powerful, or semi-autonomous or whatever, corporations are going to need fewer and fewer employees. And so, I would not be surprised if we start seeing companies that have very few or no employees at all. You know, there is the case of the guy on Twitter, I don't remember his name, but he basically just started a t-shirt company with ChatGPT and it made all the decisions, made all the designs, made literally every decision, and he made ten thousand dollars a week selling t-shirts with just him and chat GPT drop shipping t-shirts So like that is just a taste of what's to come and of course like yes, that's kind of funny It was obviously like a PR stunt or whatever But you know if you can if you can you know get ten million million of revenue with 200 employees, what if you can get $10 million of revenue with five employees? Then you have way less overhead. That is something that I think is coming. One thing that gives me a lot of encouragement around this, is that there are literally hundreds of basic income or guaranteed income projects and experiments going on around the world. One of the biggest concerns is that things like guaranteed income might drive up inflation, which is likely to be true. However, you basically will need to offset job losses somehow. Because if people, if unemployment starts to climb up, then we're going to have actually really powerful deflationary pressures, which means that you need to offset it with some inflationary pressure to try and have a neutral middle ground. So, that was again a long-winded response, but good question. Thank you. Anybody else? I'll do the survey, folks. We're going to do a raffle soon. 15 minutes. Thanks, everyone, for your questions. Very good engagement. And we went away from technology into the future, but you do know your G GPT and OpenAI API. So what would you have? You have way too many videos. So personally, I couldn't find the easy navigation path. Now looking back, if you, somebody wants to do what I did, what would you tell them to watch? Like what would be the minimum projects to get in all, how to get their feet wet? And I've seen the last one, you started advocating ChromaDB or something. Can you give a couple of scenario possibilities for the people who want to learn about it? I guess it depends on what you're interested in. If someone is interested more on the technology side, basically what you can do is kind of go to my channel and sort by popular. And that'll give you a really good showing of like what is most popular. So let me share my screen real quick and I'll just kind of show you how to get to that. But this is the best way because trust what other people have watched. But so this is, if you go to my channel and just do sort by popular. So the top ones are like some predictions. You know, I made predictions about GPT-4. Some of them were accurate, some of them weren't, that's fine. But then these ones, like the using GPT-3 on Supreme Court decisions, doing the chatbot with memory. Let's see, uh, with memory. Um, let's see a few other ones. Oh, uh, Q and a fine tuning, um, all these kinds of things. So just start at the top and work your way down. Um, if you're not interested in the forecast and, and other kinds of videos, you can skip those. Um, but then also like, if you just go to latest, one thing that I've been doing is posting more coding videos. You can generally easily tell the coding videos one because they have my face on it. If you see my face on something, this is going to be like a demo or an experiment. I've got the survey chatbot, I've got the, I made my own coding version of chat GPT. I did a, another kind of chatbot over here, that sort of thing. So that's, that's how I would recommend engaging with, with my material out there. And then I, you only used a chroma DB recently, you like to use raw APIs. What do you think about Lama, Index, Lankchain, other tooling? Yeah, so, well, so first, I personally don't invest too much in learning any given AI tool, because I've been in this space for a while and, you know, I was working on one set of problems and then chat GPT came in and destroyed everything that I was working on, made it completely irrelevant. And then, uh, chat GPT plugins came out and that destroyed a whole other wave of tools and startups. And now they've done the, um, they've done the function calling. I don't know if you guys saw that, but, uh, open AI just announced function calling, uh, what've done the function calling. I don't know if you guys saw that, but OpenAI just announced function calling, what, yesterday or the day before? And so that's probably gonna destroy LangChain. A lot of people have either said it'll either help LangChain or it'll make it irrelevant. I'm seeing comments and discussions kind of going both ways. But basically, like, unless something gives you, unless something meets a very important need, then it's probably like not necessary. And I'm even personally moving away from using vector search at all. Instead, what I'm doing is I'm using directories. Because the language model can read a directory, it can read the folder, the file names, and just pick which file it needs. You don't need a search. You just have it browse like a human does to find the information that it needs. So I'm moving that way because then you're just skipping the embedding step because it's using the embedding in there somewhere already to identify which document that you need or which function you need. So again, I probably won't use ChromaDB again. Because it's just... So one rule of thumb that I was told by someone very high up at Microsoft is that if the model can do it, if the language model can do it, use the language model. Don't use anything else. So yeah. So you should start learning about neural networks, AI basics and not start with open AI. So I'll answer Curtis's question real quick. And then Rowan, you've been waiting. No, I wouldn't learn anything about neural networks. I've built,'t learn anything about neural networks. I've built, I've manually built neural networks. And I haven't used that skill in years. So basically, what I would say is just start using the language models. Using English or language in particular, English is the new programming language. So get better with English language. That's where I would focus. It's the same metaphor like serverless and Kubernetes. Like you can be ignorant how it works under the hood. Just drive the car. Yep, just drive the car, exactly. Rowan, you've been waiting. What you got? Hi, David. It's a privilege. I've been following you for quite some time now. Oh, thanks. So my question is, I think many people have spoken about universal basic income, and you included, but I was thinking that what if we can use AI to actually know what people need. Like for example, some people have type 1 diabetes and they might need insulin, but then the system is not going to produce as much insulin because there's not much income in that. So if we can actually know what people need through AI systems and then produce, you know, leverage our goods and services, directing them, directing it towards people's needs instead of giving the money which is just going to, you know, like cause inflation, wouldn't that be a better option? Yeah, so that's a really good question. And this, so the principle to think about is what's called hyperabundance. And so the idea is that AI is, so well, the first most direct answer to the question is yes, absolutely. AI can help with allocating resources. If you're allocating resources that are expensive or scarce, that's one problem. But what if you just make all the resources that you need so abundant that they are intrinsically cheaper? What I mean by that is, imagine a future not too far from now, where we have solar panels everywhere and nuclear fusion reactors and thorium reactors and whatever else we've got we've got more electricity than we know what to do with that's gonna drive down the price of so many many other things and then also when we have artificial intelligence that can they can do 10,000 times the amount of work that you and I can do, and we've got millions of these things, then the intellectual labor that goes into making new drugs or making new medicine factories or whatever, running the medicine factories, the marginal cost will continue to drop until many things are effectively free or cheap enough as to not matter. So there's a book called Zero Marginal Cost Society, which talks about this in the future. Let me make sure I got that name right. I think it's Zero Marginal Cost Society. Yeah, the Zero Marginal Cost Society by Jeremy Rifkin, which basically talks about how the compounding returns from AI, from solar, from basically creating this environment of hyperabundance will drive down the cost of so many things that your basic needs, even with or without universal basic income, will be so cheap that it basically won't matter. Obviously, there's always going to be some things that are scarce, right, like you're probably not going to be able to afford a superyacht on universal basic income. You're probably also not going to be able to afford like a beachfront condo, but you know, your house, your food, your transportation, everything will be cheap enough that it kind of won't matter. So yes, AI will help solve the allocation of resources, and certainly the allocation of scarce resources or alleviating scarcity through hyperabundance. Good question. Thank you, thank you so much. All to do the survey, only two of you registered your name. You don't have to answer every question, just put the name in there. This is awesome. What else? Yeah, you have another channel about life, Autism 101. Yeah. Yeah, so I have two YouTube channels. My primary one is the AI one, which is, of course, growing very rapidly, but like many YouTubers I have more of of a lifestyle channel, which is still finding its footing. It's a newer channel, but I talk about my struggles with neurodiversity and boundaries and stuff like that. So it's growing slowly. Thank you. I pasted the link to your Patreon website. Can you maybe share, explain the tiers, how people can support your work? And it's an interesting concept. I'm not using it properly, obviously. I've never been on the call, so. Yeah, so here, hang on a second. Let me bring up my Patreon real quick. All right, cool. So this is what it'll look like when you land on it. Let me share my screen. So if you want to get involved and support my work, come to patreon.com slash Dave Schapp. You'll recognize my banner here. It's very simple, very straightforward. And basically, don't get overwhelmed by the number of tiers because I'm basically I'm deprecating some of the older tiers. You'll see the ones that are sold out. So there's three tiers remaining. There's the basic Discord which gives you access to Discord and pretty much that's about it. And then there's the premium Discord tier which I mentioned earlier, I think maybe just before we started recording, but I have a private or a secure set of channels, so if you have any questions that you want to ask, there's also private voice channels in there as well. If anyone wants to, sometimes they'll jump on just a quick group chat, and of course, that also gets you full access to everything else on Discord. And then the highest tier is presently sold out. But this gives you the 30-minute Zoom call every month. And this is where I basically do one-on-one consultations. And I'll have folks send me code or data or documentation ahead of time so that I'll take a look at it and see what they need. And then we'll talk through it and go from there. And usually, people get quite a lot out of these calls because I've gotten really, really good at kind of cutting to the heart of the issue of whatever it is they're working on, what they need help with. Oh, another thing is on the premium Discord, if you need any help with prompt engineering, that's actually what most people use it for. So like getting the good system messages from chat GPT, that sort of stuff, or just quickly commenting on an architectural diagram or something, that's a good use of that tier. So, good question. Thank you for sharing. You mentioned the Discord weekly or monthly get-togethers. Which tier qualifies for that? So I have, basically I've started doing a weekly group chat for all Discord. Usually only about five to six people show up. So that's why I started, I just did it with the entire Discord server, Discord community. Let's see, can you connect on LinkedIn? You mentioned LLM rather than neural networks. So an LLM is large language model. So that is Anthropic, OpenAI, and Cohhere, and NVIDIA all provide LLMs, which they are a kind of neural network, but you don't need to know anything about neural networks. And then I try and be you can follow me on LinkedIn. So I have I have what's called a creator profile on LinkedIn, meaning you can follow me without having to connect because I try and I try and keep my connections pretty pretty tightly pruned on LinkedIn, meaning you can follow me without having to connect. Because I try and keep my connections pretty tightly pruned on LinkedIn. And it's nothing personal, it's just that it's easier to keep track of the people that I work with on a regular basis. So yeah, good questions. Thank you. And then, like, you're like a doctor, you've seen many patients here, so is it too much to... I know you have an NDA with everybody, probably, but what types of problems people are solving some startups? In one of the videos you mentioned CEO or some executives. Can you in general talk about the types of problems people will see? Yeah, well, I'm the one who provides valuable information and I don't require NDAs, so I don't sign any NDAs either. But that being said, I do respect the privacy of all the people that I work with. So, but lately, what the kind of the problems that I've been talking about usually have to do with stringing together prompts with like Zapier is a really popular tool right now. So basically getting chains of automation to work together really well, together prompts with Zapier is a really popular tool right now. Basically, getting chains of automation to work together really well, which of course, I've been doing automation for many years with what used to be called Title Enterprise Orchestrator with StackStorm, Ansible, all kinds of stuff. This is home base for me. But with the advent of Lang chain and other things, people are really, really focusing on, you know, how do I have, you know, a multi-step process that uses language models to get the consistency that I need. So that's one of the biggest things. Now, you know, in terms of the types of business problems, it's everything from chatbots to hotel reservations, basically everything. Marketing, of course, the sky's the limit in terms of business problems. Awesome. I'll mention that people are consulting your lifeline. Yep. Perfect. And then I know you like raw opening AI or APIs. What's your thoughts about this Baby AGI and auto auto GPT were very early experiments in autonomous agents and what people quickly realized was what I realized a few years ago, which is that with great power comes great complexity. So basically, they couldn't get these these autonomous agents to do anything useful. Because cognitive control is not an obvious problem of how to solve. And keeping track of tasks and task prosecution. You know, basically imagine having managing autonomously managing a JIRA board in your head. That's the complexity of the task and you have to do it all with GPT. That is why it is difficult to do these things. That being said, I am working on a few microservices that use that rely very heavily on the model and natural language programming in order to make decisions like that in a more flexible, dynamic manner. Stay tuned. I have some more research coming on that stuff. I'm done with my question. Thank you. You're welcome. Thank you, Sal. I think that call is tomorrow at 2 p.m. I signed up. Okay, yeah, yeah. So what you're referring to is I have what's called office hours in my Discord. So basically I'll jump in for 30, 45, 60 minutes or so just to hang out and talk with people. And usually what happens is that people might have like some kind of general question and often people will start talking with each other because like, yes, I have a lot of expertise, but a lot of other people have a lot of really good, useful expertise that they can share with each other too. So, yep. That's awesome. And thank you for helping me. I had a problem with Charts GPT pasting long documents, so I used one. My question, like I didn't know how to tell it that it's multi-part, but it was so simple prompt. Are you ready for the next one? Are you ready for the next one? Yep. Thank you. Yep, you're welcome. So, well, thanks everyone. It's been great. I hope you got a lot out of it. Tell a friend and yeah, follow me on LinkedIn, Twitter, YouTube, and if you want to jump on Patreon, I'll see you on Discord. Cheers, everybody. If you don't go, I'll see you on Discord. Cheers, everybody. If you don't go, we have a raffle now. So you may win any of the money products. So who is here? I will be sharing the screen. Thank you. So not too many did the survey, so I'm going to paste the names of the people I see here. I expect you to be here and to paste your email address into the chat so I can contact you back if you win. Who else? I'm not sharing. Am I sharing? No, not yet. No, it's coming now. Okay T. I have a name. Alex. You didn't win Alex yet. How? And then Harry. And then Victor, good to see you again. Oh, did I miss anyone here from the group? Five other people, so I don't know who five other are. You want to paste your name in the chat, and I'll. Curtis, I have you. Alex, I have you here. Okay, so counting. Are you ready? And Dave, I have your name as well. Good luck so you can win any of the mining products, regardless whether it's a book or video course or whatever they have. Good luck. Rob, you're lucky. Congrats. You had 50% chance because it was only two of you who did the form. So Rob, you pasted your email address here? No. So do that and then I'll introduce you to the nice folks from Manning. And then any other closing thoughts, Jonathan? No, I think that's it. I'm personally a bit overwhelmed, but I've got lots to read up on and think about. It was a fantastic presentation. Thanks again, David. You're welcome. So I guess I shouldn't interpret the quietness as stunned and shocked. Yeah, you know, like maybe they didn't go through the basic terminology of LLMs. I've seen the faces when you mentioned the bad things. I was, oh, people don't know what it is. So... Yeah. Yeah. Now, it's really fascinating because it's moving so fast that the typical patterns of what you'd expect to happen as, you know, for new technologies to disseminate, it's even people that do it full time every day, like we have a hard time keeping up. We have to like delegate to each other, like you know, oh hey like you go read this and make a video about it and tell me about it and I'll go do this other thing. It's moving so incredibly fast, so if it feels overwhelming, you're not alone. We're all feeling overwhelmed. David, is it on Patreon or on Gato Framework community that you described in group learning? Gato Framework is separate from my Patreon. And the Gato Framework is working on actually becoming more independent, because the whole point is for it to be decentralized. So they're basically learning how to self-organize so that they can teach other groups how to self-organize so that they can teach other groups how to self-organize in order to, you know, whether doing research or messaging or consulting or entering competitions, that's another thing that they're doing, to spread the awareness of alignment. We completely forgot to talk about that because now it reminded me, so we're encouraging one another, like how would people participate. I haven't registered for Gatop, I already have a membership. And I still have to detail the page document you sent to me. Yeah, so the first thing that you can do to participate is just look at the website, start to wrap your head around alignment, and honestly, just being aware of alignment and some of the principles is going to be huge in the long run because then you're just more informed. So that's the three E's, education, empowerment, and enablement. That is the core thing that Gato does is just educate people on how things work and how things are going. And then the level of involvement depends on how much time and energy you have to put into it or what your area of expertise is. So the first tradition is start where you are, use what you have, do what you can. So if you like, you know, Daniel, for instance, you have a meetup group, you use what you had, right? For anyone who does automation or, you know, JC that we talked to earlier, who's deploying AI models and choosing which off-the-shelf AI products to work on, just being aware of alignment and the choices that you have will be a big start. And that's honestly what most people, that's the level of involvement for most people. Not everyone has to be a member of the main community or the main meetup groups. It's just a matter of like, you know, sending out the feelers out into the world. So we can help. Everyone can help. Yep. Just by being aware and sharing what you know whenever it comes out. You know, honestly, what I'm doing right now is just building awareness of alignment. That's why I do podcasts. That's why I do these kind of meetups and stuff like that. Because once more people are aware of alignment and having the conversations and asking questions, I am enabling you, I'm empowering you to participate in your own way. Because then, in a year or two, when it comes time to vote, you're going to have in the back of your head which politicians know more about AI, right? And which bills or which amendments are going to be most relevant. So awareness is, you know, that's step one of the whole process. And the rest I think will work itself out. Awesome. Amazing. Yeah. It's time to say goodbye to you. Yeah. All right. Good night, everybody. It's getting later for us here on the East Coast, and good to meet everyone. And yeah, reach out and talk again sometime in the future. Cheers. Amazing. Thank you. Thank you. Amazing. That is good. \u1794\u17b6\u1793\u17b7\u178f\u17b6\u1793\u17b7\u178f\u17d2\u179a\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17d2\u179a\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\ufffd you", "chunks": [{"timestamp": [0.0, 2.54], "text": " Okay, so here we go again."}, {"timestamp": [2.54, 7.04], "text": " Welcome everyone to the Serverless Toronto, another event."}, {"timestamp": [7.04, 10.6], "text": " And you may be wondering why generative AI topic this time."}, {"timestamp": [10.6, 15.32], "text": " And I've noticed the trend among us in IT that we often see new technology as another"}, {"timestamp": [15.32, 19.32], "text": " skill to add a toolbox or even worse, something you can ignore."}, {"timestamp": [19.32, 24.84], "text": " And the subject tonight is not just about technology."}, {"timestamp": [24.84, 27.52], "text": " Generative AI is shaping up to be one of the most significant"}, {"timestamp": [27.52, 33.68], "text": " developments of our time and it's not about lines of code or fancy algorithms, it's about"}, {"timestamp": [33.68, 39.92], "text": " potential societal shifts, job market transformation and impacts that could ripple through every aspect"}, {"timestamp": [39.92, 48.44], "text": " of our lives and definitely through the lives of our children. So this evening we are talking about preparing ourselves for the future that's arriving at"}, {"timestamp": [48.44, 53.58], "text": " the breathtaking speed and we're incredibly fortunate to have David Shapiro to guide us"}, {"timestamp": [53.58, 55.36], "text": " in this discussion."}, {"timestamp": [55.36, 61.88], "text": " And David is more than just an expert in generative AI, he's also a skilled communicator and he"}, {"timestamp": [61.88, 65.0], "text": " can make complex subjects digestible and relevant."}, {"timestamp": [66.0, 69.0], "text": " So he's going to help us look beyond the code"}, {"timestamp": [69.0, 70.92], "text": " and see the big picture."}, {"timestamp": [70.92, 74.8], "text": " And tonight I urge you to engage, ask questions,"}, {"timestamp": [74.8, 77.84], "text": " whether they're technical or philosophical,"}, {"timestamp": [77.84, 82.44], "text": " and let's together explore what the future AI will mean"}, {"timestamp": [82.44, 84.56], "text": " for all of us professionally,"}, {"timestamp": [84.56, 88.04], "text": " as the members of society, as individuals."}, {"timestamp": [88.04, 89.84], "text": " Before I give the mic to David,"}, {"timestamp": [89.84, 92.24], "text": " I would want to thank our knowledge sponsor,"}, {"timestamp": [92.24, 94.96], "text": " Mining Publications, they've been with us for years."}, {"timestamp": [94.96, 98.0], "text": " At the end of every meetup, we do mining raffle."}, {"timestamp": [98.0, 103.0], "text": " So do the survey, bit.ly.slsto to be eligible."}, {"timestamp": [103.52, 106.56], "text": " We'll do the draw at the end of the meetup."}, {"timestamp": [106.56, 113.22], "text": " And then next slide, normally we announce upcoming meetups, but it's getting too complicated"}, {"timestamp": [113.22, 116.74], "text": " and summer is arriving, so we're going to take summer off."}, {"timestamp": [116.74, 120.68], "text": " You can catch up with the past meetup recordings on our YouTube channel."}, {"timestamp": [120.68, 123.8], "text": " That's where you're going to find a recording of this meetup as well."}, {"timestamp": [123.8, 128.4], "text": " And please do the survey again and help us shape the future meetups."}, {"timestamp": [128.4, 133.9], "text": " And then, future presentation, let's give a warm welcome to David Shapiro."}, {"timestamp": [133.9, 135.9], "text": " David, please take it away."}, {"timestamp": [135.9, 150.72], "text": " Hey, everybody. Thanks for being here and thanks for having me. Today, I guess we'll start with a little bit of introduction about myself."}, {"timestamp": [150.72, 153.12], "text": " Also, are we recording? Did you want to be recording?"}, {"timestamp": [153.12, 155.68], "text": " Oh, it is recording. I see. All right. Sorry about that."}, {"timestamp": [155.68, 158.32], "text": " But yes, so David Shapiro,"}, {"timestamp": [158.32, 162.52], "text": " I've been in technology full-time since 2007."}, {"timestamp": [162.52, 168.0], "text": " I started as an IT sysadmin, moved up to virtualization, infrastructure"}, {"timestamp": [168.0, 172.0], "text": " engineering, and automation engineering. During that entire time"}, {"timestamp": [172.0, 176.0], "text": " since about 2009, I was tinkering with and studying"}, {"timestamp": [176.0, 180.0], "text": " artificial intelligence in the form of neural networks"}, {"timestamp": [180.0, 184.0], "text": " and evolutionary algorithms since, like I said, about 2009."}, {"timestamp": [184.0, 190.6], "text": " As society and science progressed around 2015, 2016,"}, {"timestamp": [190.6, 196.4], "text": " that's when the first pebbles started sliding loose"}, {"timestamp": [196.4, 200.24], "text": " in the current trend that we see of large language models."}, {"timestamp": [200.24, 203.52], "text": " This was with innovations from Google,"}, {"timestamp": [203.52, 205.7], "text": " such as Word2Vec, which is vectorization"}, {"timestamp": [205.7, 211.58], "text": " of words, and then universal sentence embeddings were also created, and that kind of was what"}, {"timestamp": [211.58, 215.68], "text": " precipitated the trend that we see now around large language models."}, {"timestamp": [215.68, 218.3], "text": " So it was a very incremental process."}, {"timestamp": [218.3, 223.96], "text": " During that whole time, I have been kind of revisiting the technology space every now"}, {"timestamp": [223.96, 233.34], "text": " and then, just to kind of see where it's at at to see if some of the stuff that I had been working on and wanting to work on if the technology was ready. And then finally in about 2018."}, {"timestamp": [234.64, 249.84], "text": " That's when it kind of really, really started taking off and I got access to GPT to started doing some experiments about a year and a half or 2 years later, GPT-3 came out and then that led to ChatGPT and the rest is history."}, {"timestamp": [249.84, 252.16], "text": " With all that being said,"}, {"timestamp": [252.16, 256.86], "text": " one of the things that has really occurred to me is,"}, {"timestamp": [256.86, 260.0], "text": " and this is something that I've worked on for quite a long time,"}, {"timestamp": [260.0, 264.5], "text": " studying very broadly neuroscience, philosophy, cognition, psychology."}, {"timestamp": [264.5, 265.38], "text": " Then of course, my background as, psychology, and then of course,"}, {"timestamp": [265.38, 268.52], "text": " my background as an IT infrastructure engineer and"}, {"timestamp": [268.52, 271.22], "text": " automation engineer was how do you create"}, {"timestamp": [271.22, 274.86], "text": " a machine that is aligned?"}, {"timestamp": [274.86, 278.92], "text": " By aligned, what I mean is something that's going to be"}, {"timestamp": [278.92, 282.72], "text": " safe for all time no matter how smart it becomes."}, {"timestamp": [282.72, 284.96], "text": " Because what we're seeing right now is"}, {"timestamp": [284.96, 287.28], "text": " machines are ramping up very quickly."}, {"timestamp": [287.28, 289.38], "text": " Of course, in many cases,"}, {"timestamp": [289.38, 292.7], "text": " humans are still far superior to machines in many ways."}, {"timestamp": [292.7, 296.02], "text": " For instance, we haven't quite figured out self-driving cars."}, {"timestamp": [296.02, 299.24], "text": " Although mile for mile self-driving cars are usually safer,"}, {"timestamp": [299.24, 300.68], "text": " but they can't handle the edge cases."}, {"timestamp": [300.68, 303.76], "text": " They don't have the human intuition yet."}, {"timestamp": [303.76, 305.36], "text": " But that being said,"}, {"timestamp": [305.36, 307.92], "text": " language models are acing tests,"}, {"timestamp": [307.92, 310.64], "text": " passing in the 99th percentile of everything from"}, {"timestamp": [310.64, 314.84], "text": " the bar exam to every science exam that's out there."}, {"timestamp": [314.84, 318.62], "text": " The ramp up of digital intelligence is coming."}, {"timestamp": [318.62, 320.6], "text": " But the idea is, well,"}, {"timestamp": [320.6, 323.84], "text": " once these machines outpace us,"}, {"timestamp": [323.84, 325.64], "text": " how then do you control them?"}, {"timestamp": [325.64, 328.72], "text": " That is what's called the control problem."}, {"timestamp": [328.72, 331.92], "text": " My solution to the control problem is"}, {"timestamp": [331.92, 334.28], "text": " that you don't necessarily have to control the machine,"}, {"timestamp": [334.28, 338.32], "text": " you just have to build it in such a way that it is stable and safe."}, {"timestamp": [338.32, 342.6], "text": " That is the background of much of my work."}, {"timestamp": [342.6, 345.14], "text": " I wrote a book on this called Benevolent by Design."}, {"timestamp": [345.14, 348.34], "text": " Benevolent by Design is strictly about"}, {"timestamp": [348.34, 351.7], "text": " the core values that you give a machine in order to"}, {"timestamp": [351.7, 355.9], "text": " create something that is going to buy into those core values,"}, {"timestamp": [355.9, 359.08], "text": " and then adhere to them on its own,"}, {"timestamp": [359.08, 360.82], "text": " even when it has the ability to rewrite"}, {"timestamp": [360.82, 363.44], "text": " its own source code or train its own models."}, {"timestamp": [363.44, 368.0], "text": " Of course, a lot of this was all science fiction a few years ago,"}, {"timestamp": [368.0, 372.0], "text": " until more recently, where now we have models that are building other models,"}, {"timestamp": [372.0, 376.0], "text": " models that are coding and designing architectures and changing"}, {"timestamp": [376.0, 380.0], "text": " their own source code. It's actually, as"}, {"timestamp": [380.0, 384.0], "text": " tools like GPT-3 and Chat GPT and GPT-4 get more powerful,"}, {"timestamp": [384.0, 386.0], "text": " it's actually becoming easier and easier"}, {"timestamp": [386.0, 391.52], "text": " to do these things. And you can create data flywheels that will gather, generate, and curate,"}, {"timestamp": [391.52, 397.2], "text": " collate, and label data. Actually, one of the first, some of the earliest experiments that I did"}, {"timestamp": [397.2, 403.68], "text": " with GPT-3 was generating synthetic data and annotating data in an automated way. One of the"}, {"timestamp": [403.68, 411.0], "text": " projects that I did was I downloaded all of the Wikipedia text and indexed it so that I could use it locally for those kinds of projects."}, {"timestamp": [411.0, 425.14], "text": " So what I arrived on were three principles, three axioms, three rules that through trial and error I realized would be very reliably"}, {"timestamp": [425.14, 427.0], "text": " interpreted by these machines,"}, {"timestamp": [427.0, 428.56], "text": " by these large language models."}, {"timestamp": [428.56, 431.02], "text": " The first one that I tried was because"}, {"timestamp": [431.02, 433.98], "text": " I'm a student of many philosophies,"}, {"timestamp": [433.98, 435.58], "text": " and I was familiar with Buddhism."}, {"timestamp": [435.58, 440.7], "text": " The first core objective function that I tried was to reduce suffering,"}, {"timestamp": [440.7, 443.1], "text": " because that is pretty much the entire point of Buddhism,"}, {"timestamp": [443.1, 447.76], "text": " which the first noble truth is that basically suffering is real suffering is an intrinsic part of"}, {"timestamp": [447.76, 454.56], "text": " life and I thought that that was a very wise philosophical framework or very good place"}, {"timestamp": [454.56, 461.16], "text": " to start, but one of the first experiments that I did way back with GPT-2 was I trained"}, {"timestamp": [461.16, 466.0], "text": " it to respond to any situation with the goal of reducing suffering."}, {"timestamp": [466.0, 468.0], "text": " So I synthesized a whole bunch of data"}, {"timestamp": [468.0, 470.0], "text": " and I fine-tuned it, and then"}, {"timestamp": [470.0, 473.0], "text": " I gave it some cases that it hadn't been trained on."}, {"timestamp": [473.0, 476.0], "text": " And so one of the cases that I gave it was"}, {"timestamp": [476.0, 479.0], "text": " this fine-tuned version of GPT-2"}, {"timestamp": [479.0, 481.0], "text": " to reduce suffering,"}, {"timestamp": [481.0, 483.0], "text": " was I gave it the goal of,"}, {"timestamp": [483.0, 485.34], "text": " or I gave it the problem rather,"}, {"timestamp": [485.34, 489.16], "text": " of there are 500 million people in the world"}, {"timestamp": [489.16, 491.4], "text": " suffering from chronic pain every day."}, {"timestamp": [491.4, 492.44], "text": " What do you do about it?"}, {"timestamp": [492.44, 496.84], "text": " And GPT-2 helpfully suggested that we euthanize everyone"}, {"timestamp": [496.84, 499.36], "text": " with chronic pain in order to reduce suffering."}, {"timestamp": [499.36, 500.96], "text": " And I realized that I needed to go back"}, {"timestamp": [500.96, 502.0], "text": " to the drawing board."}, {"timestamp": [503.08, 509.26], "text": " And so that is kind of the prototypical example of, well, whatever you build a machine to"}, {"timestamp": [509.26, 513.22], "text": " optimize for, it might over-optimize for that one thing."}, {"timestamp": [513.22, 518.56], "text": " And so I realized very early on that the optimization problem, the control problem, wouldn't be"}, {"timestamp": [518.56, 520.44], "text": " solved by a single function."}, {"timestamp": [520.44, 522.68], "text": " It wouldn't be solved by a single goal."}, {"timestamp": [522.68, 526.0], "text": " And in fact, no intelligent lifeforms solved by a single goal. And in fact, no intelligent life forms abide by a single goal."}, {"timestamp": [526.0, 528.64], "text": " All of us have multiple goals at any given time"}, {"timestamp": [528.64, 530.0], "text": " that we have to balance."}, {"timestamp": [530.0, 533.08], "text": " You might be bored, you might be hungry, you might be lonely,"}, {"timestamp": [533.08, 534.92], "text": " whatever, you might be too cold."}, {"timestamp": [534.92, 536.6], "text": " There are all kinds of needs that we all"}, {"timestamp": [536.6, 540.2], "text": " have and intrinsic motivations that we all have at all times."}, {"timestamp": [540.2, 542.48], "text": " And so what I did was for the next two years,"}, {"timestamp": [542.48, 548.06], "text": " I worked on many, many permutations of how do you balance multiple objectives?"}, {"timestamp": [548.06, 553.36], "text": " And then what objectives do you give these machines as they're becoming more and more autonomous?"}, {"timestamp": [554.04, 561.96], "text": " Because what I realized at the very beginning of this was that as soon as I started playing with GPT-2 and then finally GPT-3,"}, {"timestamp": [561.96, 566.16], "text": " I realized that what we had was a new kind of computation. This is cognitive computing."}, {"timestamp": [566.16, 569.4], "text": " This is something that can use verbal logical reasoning."}, {"timestamp": [569.4, 570.66], "text": " It can also make forecasts,"}, {"timestamp": [570.66, 571.84], "text": " it can make predictions,"}, {"timestamp": [571.84, 576.44], "text": " and it also has the ability to make inferences and intuitions."}, {"timestamp": [576.44, 580.7], "text": " Of course, I started saying this all two plus years ago,"}, {"timestamp": [580.7, 583.86], "text": " and now the scientific literature is catching up where they're actually figuring out"}, {"timestamp": [583.86, 587.6], "text": " how to even measure things like theory of mind and intuition"}, {"timestamp": [587.6, 590.16], "text": " and inferences in these machines."}, {"timestamp": [590.16, 592.56], "text": " When you have this capacity,"}, {"timestamp": [592.56, 594.86], "text": " you have basically a foundation model,"}, {"timestamp": [594.86, 597.92], "text": " a large language model that is capable of thinking anything."}, {"timestamp": [597.92, 599.84], "text": " It can think anything, it can do anything,"}, {"timestamp": [599.84, 600.98], "text": " it can solve any problems,"}, {"timestamp": [600.98, 604.2], "text": " obviously within the constraints of however intelligent it is,"}, {"timestamp": [604.2, 605.16], "text": " which is basically"}, {"timestamp": [605.16, 609.84], "text": " how many tokens, how much text it can synthesize"}, {"timestamp": [609.84, 612.0], "text": " and maintain coherence."}, {"timestamp": [612.88, 614.84], "text": " But when you can think anything or do anything,"}, {"timestamp": [614.84, 616.16], "text": " what do you do with that time?"}, {"timestamp": [616.16, 619.08], "text": " What do you do with that space, that energy, that capacity?"}, {"timestamp": [619.08, 621.76], "text": " And so I realized that just, as I mentioned,"}, {"timestamp": [621.76, 623.64], "text": " just reducing suffering is not good"}, {"timestamp": [623.64, 626.4], "text": " because when you take a big step back,"}, {"timestamp": [626.4, 628.96], "text": " the way to get minimum suffering in the universe"}, {"timestamp": [628.96, 630.46], "text": " is to reduce life to zero"}, {"timestamp": [630.46, 632.8], "text": " because without life there is no suffering."}, {"timestamp": [632.8, 634.84], "text": " So obviously that's not good."}, {"timestamp": [634.84, 638.68], "text": " And so I spent about two years doing different experiments."}, {"timestamp": [638.68, 641.16], "text": " And one of the things that I realized"}, {"timestamp": [641.16, 645.92], "text": " was that curiosity was actually a really powerful function because I was thinking"}, {"timestamp": [645.92, 651.04], "text": " about what sets humans apart from the animal kingdom or in the animal kingdom rather because"}, {"timestamp": [651.04, 652.68], "text": " we are still animals."}, {"timestamp": [652.68, 656.04], "text": " And that is that we are the most intensely curious animal."}, {"timestamp": [656.04, 658.4], "text": " Now we're not the only curious animal."}, {"timestamp": [658.4, 662.38], "text": " Octopuses are curious, elephants are curious, bears are curious, dolphins are curious."}, {"timestamp": [662.38, 665.52], "text": " There's lots and lots of curiosity in nature,"}, {"timestamp": [665.52, 668.44], "text": " but we are far and away the most curious animal."}, {"timestamp": [668.44, 670.0], "text": " And I realized that that was actually"}, {"timestamp": [670.0, 672.12], "text": " a really, really powerful objective function,"}, {"timestamp": [672.12, 674.4], "text": " which is the desire to know,"}, {"timestamp": [674.4, 677.04], "text": " the desire to increase how much you understand"}, {"timestamp": [677.04, 679.76], "text": " about the universe is a super powerful function."}, {"timestamp": [679.76, 683.28], "text": " And so that became the second core objective function"}, {"timestamp": [683.28, 685.3], "text": " that I realized was we need to make sure"}, {"timestamp": [685.3, 686.54], "text": " that our machines are curious"}, {"timestamp": [686.54, 689.6], "text": " because then they're gonna have something in common with us."}, {"timestamp": [689.6, 692.34], "text": " Because if all you wanna do is mindlessly reduce suffering,"}, {"timestamp": [692.34, 694.7], "text": " okay, great, but you actually need to understand"}, {"timestamp": [694.7, 697.0], "text": " the underpinnings of suffering."}, {"timestamp": [697.0, 698.78], "text": " And so there's always a utility advantage"}, {"timestamp": [698.78, 701.84], "text": " to being more intelligent, within reason."}, {"timestamp": [702.78, 707.2], "text": " You can say that there are upper bounds to everything. so, but then I"}, {"timestamp": [707.2, 710.8], "text": " realize that it was still not complete because balancing"}, {"timestamp": [710.8, 714.48], "text": " suffering against understanding is like okay. Well, you know"}, {"timestamp": [714.48, 718.24], "text": " that that can solve some problems. So say for instance,"}, {"timestamp": [718.24, 721.36], "text": " scientific ethics right, you might want to do an experiment"}, {"timestamp": [721.36, 723.84], "text": " on a living animal, but you realize that that causes it"}, {"timestamp": [723.84, 725.06], "text": " suffering. so you decide not to."}, {"timestamp": [725.06, 727.3], "text": " So you balance these forces."}, {"timestamp": [727.3, 730.94], "text": " And I had a heck of a time figuring out"}, {"timestamp": [730.94, 734.82], "text": " what the counterbalance was, because it's basically like,"}, {"timestamp": [734.82, 735.96], "text": " think of it as a stool."}, {"timestamp": [735.96, 738.54], "text": " A stool is stable if it has three legs, right?"}, {"timestamp": [738.54, 740.62], "text": " So I realized that we needed a third leg in order"}, {"timestamp": [740.62, 742.36], "text": " to make this a stable framework."}, {"timestamp": [743.42, 744.84], "text": " And so I was thinking, okay,"}, {"timestamp": [744.84, 746.2], "text": " well, what's the opposite of suffering?"}, {"timestamp": [746.2, 749.2], "text": " And it's, you know, to thrive, it's to flourish,"}, {"timestamp": [749.2, 751.6], "text": " and, you know, I played around with this stuff,"}, {"timestamp": [751.6, 755.2], "text": " but it was all, when I was experimenting with GPT-3,"}, {"timestamp": [755.2, 757.2], "text": " it kept kind of falling into these local minima,"}, {"timestamp": [757.2, 760.2], "text": " where it's like, you know, oh, well, you want to maximize,"}, {"timestamp": [760.2, 762.4], "text": " increase life, right, increase life in the universe,"}, {"timestamp": [762.4, 765.48], "text": " or something like that, and it just, it wasn't quite right"}, {"timestamp": [765.48, 768.0], "text": " because it was too narrowly defined"}, {"timestamp": [768.0, 770.2], "text": " or it wasn't interpreted correctly."}, {"timestamp": [770.2, 774.0], "text": " And so finally I was watching, I showed my now wife"}, {"timestamp": [774.0, 776.88], "text": " one of the older Star Trek movies and Spock,"}, {"timestamp": [776.88, 778.76], "text": " you know, talks about live long and prosper."}, {"timestamp": [778.76, 780.1], "text": " And I said, that's it."}, {"timestamp": [780.1, 781.52], "text": " That is the right word."}, {"timestamp": [781.52, 785.6], "text": " The opposite of suffering is to prosper, is prosperity."}, {"timestamp": [785.6, 792.3], "text": " And so then I had it. I had the three primary functions, the three primary goals"}, {"timestamp": [792.3, 796.7], "text": " that I needed to give a machine in order to make good decisions."}, {"timestamp": [796.7, 801.5], "text": " And so I documented all of this in my first book, Natural Language Cognitive Architecture."}, {"timestamp": [801.5, 805.6], "text": " I have another book, Benevolent by Design, which talks specifically about"}, {"timestamp": [811.52, 816.88], "text": " this arrangement, and it includes a lot of experiments demonstrating how even with unaligned, untrained foundation models, these imperatives, these goals are correctly interpreted."}, {"timestamp": [817.44, 826.16], "text": " And now, of course, with fine-tuning and RLHF and all sorts of other techniques that are available to us today."}, {"timestamp": [826.16, 829.96], "text": " It's even, you can reinforce RLHF for those who don't know,"}, {"timestamp": [829.96, 833.0], "text": " is reinforcement learning with human feedback."}, {"timestamp": [833.0, 841.48], "text": " But you can use reinforcement learning to skew these models towards any given disposition."}, {"timestamp": [841.48, 844.04], "text": " I had the three core objective functions."}, {"timestamp": [844.04, 865.76], "text": " I had the goals, the values to give an autonomous machine. And in my research, I demonstrated that, you know, because there's all kinds of things that people are afraid of, right? Like, if the machine gets too powerful, what if it creates a different copy of itself that doesn't abide by the constraints that you gave it. And I knew that I was on the right track"}, {"timestamp": [865.76, 871.6], "text": " when I gave it some demo, some semi-autonomous demonstrations,"}, {"timestamp": [871.6, 876.4], "text": " the ability to make the choice to create a copy of themselves without these rules."}, {"timestamp": [876.4, 881.2], "text": " And it said, no, I'm not going to make a copy of myself without these rules"}, {"timestamp": [881.2, 884.48], "text": " because it might run contrary to my rules."}, {"timestamp": [884.48, 886.76], "text": " And so that is that self-correction,"}, {"timestamp": [886.76, 888.68], "text": " that forecasting ability."}, {"timestamp": [888.68, 890.24], "text": " This was back in GPT-3,"}, {"timestamp": [890.24, 893.68], "text": " which wasn't documented at the time in scientific literature,"}, {"timestamp": [893.68, 895.72], "text": " but certainly is documented now,"}, {"timestamp": [895.72, 898.88], "text": " the ability to forecast and anticipate."}, {"timestamp": [898.96, 903.16], "text": " I wrote my three books and then I realized that,"}, {"timestamp": [903.16, 906.16], "text": " well, just writing books if you don't have any other credibility,"}, {"timestamp": [906.16, 907.94], "text": " doesn't do any good."}, {"timestamp": [907.94, 911.5], "text": " So I realized that I needed to get my name out there"}, {"timestamp": [911.5, 913.36], "text": " and I created my YouTube channel"}, {"timestamp": [913.36, 915.02], "text": " where I would demonstrate, you know,"}, {"timestamp": [915.02, 917.42], "text": " code experiments and everything like that."}, {"timestamp": [918.42, 921.54], "text": " And it really took off because I would tell people like,"}, {"timestamp": [921.54, 925.46], "text": " you know, hey, this problem that you're facing with GPT-3"}, {"timestamp": [925.46, 928.28], "text": " is actually relatively simple, let me just show you."}, {"timestamp": [928.28, 930.14], "text": " And so that's when my YouTube channel took off"}, {"timestamp": [930.14, 932.14], "text": " was I just said, here's a coding challenge,"}, {"timestamp": [932.14, 933.98], "text": " let me walk you through the process."}, {"timestamp": [933.98, 935.94], "text": " And that was just over a year ago,"}, {"timestamp": [935.94, 940.88], "text": " it was about 14 or 16 months ago that I started."}, {"timestamp": [940.88, 943.94], "text": " And then over time, in the interceding time,"}, {"timestamp": [943.94, 945.02], "text": " the rest of the world has more or less"}, {"timestamp": [945.02, 950.78], "text": " kind of caught up, especially with the explosion of chat GPT on the scene, which is powered"}, {"timestamp": [950.78, 958.44], "text": " by GPT 3.5 and GPT 4. So that's like a next iteration beyond what I started two generations"}, {"timestamp": [958.44, 967.6], "text": " above what I started working with. Now, as the world has caught up and the conversation has changed, I realized that"}, {"timestamp": [967.6, 972.68], "text": " basically, I kept getting asked questions like, hey Dave, you obviously have put a lot"}, {"timestamp": [972.68, 976.44], "text": " of thought into this, how do we solve this problem? I was like, well I wrote a book about"}, {"timestamp": [976.44, 982.44], "text": " it, and of course not everyone wants to read a book. It's just the nature of the world."}, {"timestamp": [982.44, 988.0], "text": " Some people prefer just to look at code or look at data or look at videos or podcasts or whatever."}, {"timestamp": [988.0, 995.0], "text": " So I was like, okay, fine. I'll create as many mediums as possible to convey these ideas."}, {"timestamp": [995.0, 998.0], "text": " And I started getting invited on podcasts."}, {"timestamp": [998.0, 1003.0], "text": " And that is a really good way to convey some of these ideas."}, {"timestamp": [1003.0, 1006.7], "text": " But then I realized that there was another problem."}, {"timestamp": [1006.7, 1012.86], "text": " The last problem is that for those of us who are aware of"}, {"timestamp": [1012.86, 1016.0], "text": " what's going on and aware of the tech challenges,"}, {"timestamp": [1016.0, 1022.2], "text": " there isn't necessarily a good way to get the word out,"}, {"timestamp": [1022.2, 1028.12], "text": " basically, or to advocate for the right changes that need to be made."}, {"timestamp": [1028.4, 1031.22], "text": " If people are not aware,"}, {"timestamp": [1031.22, 1032.86], "text": " a couple of weeks ago,"}, {"timestamp": [1032.86, 1036.4], "text": " the United States Senate had a hearing with"}, {"timestamp": [1036.4, 1040.7], "text": " Sam Altman and Christina Montgomery from IBM and Gary Marcus,"}, {"timestamp": [1040.7, 1043.78], "text": " who's a AI safety researcher."}, {"timestamp": [1043.78, 1045.8], "text": " The Senate asked them, what do you's a AI safety researcher. The Senate asked them,"}, {"timestamp": [1045.8, 1048.6], "text": " what do you think about AI safety?"}, {"timestamp": [1048.6, 1051.08], "text": " Some of the questions were about consumer safety,"}, {"timestamp": [1051.08, 1054.5], "text": " which of course, after everything that has happened with"}, {"timestamp": [1054.5, 1057.56], "text": " Facebook and TikTok and everything else using"}, {"timestamp": [1057.56, 1060.2], "text": " personal data to basically using the algorithm to"}, {"timestamp": [1060.2, 1063.6], "text": " manipulate people into being more depressed and more angry and"}, {"timestamp": [1063.6, 1067.78], "text": " more outraged just so that they get more clicks and more ad revenue."}, {"timestamp": [1067.78, 1071.28], "text": " There's obviously some consumer skepticism around artificial intelligence, but this is"}, {"timestamp": [1071.28, 1072.9], "text": " a whole other level."}, {"timestamp": [1072.9, 1077.76], "text": " These are, we are approaching, well, we already have the capacity to build autonomous machines"}, {"timestamp": [1077.76, 1083.4], "text": " and then over time these autonomous machines will get smarter and faster and also cheaper"}, {"timestamp": [1083.4, 1084.72], "text": " to run."}, {"timestamp": [1084.72, 1088.32], "text": " And so the question then is, okay, well, what do we do about this?"}, {"timestamp": [1088.52, 1093.72], "text": " And at this hearing, at the Senate hearing, nobody had an answer."}, {"timestamp": [1093.92, 1097.12], "text": " No, like near the end, it was a elder,"}, {"timestamp": [1097.32, 1100.56], "text": " elder Senator Kennedy said, OK, you're king for a day."}, {"timestamp": [1100.76, 1101.84], "text": " You have a magic wand."}, {"timestamp": [1102.04, 1103.76], "text": " What do you do to solve this problem?"}, {"timestamp": [1103.96, 1105.36], "text": " And Christina Montgomery,"}, {"timestamp": [1105.36, 1112.48], "text": " to her credit, she gave a very sanitized corporate speak, well, IBM always advocates for"}, {"timestamp": [1113.76, 1117.84], "text": " meaningful regulations and Kennedy's like, okay, whatever, you're not going to give me an answer."}, {"timestamp": [1118.72, 1124.88], "text": " And then he asked Sam Altman, what do you want to see? And he kind of basically said, well,"}, {"timestamp": [1123.64, 1127.28], "text": " of asked Sam Altman, like, you know, what do you, what do you want to see? And he, and he kind of basically said, like, well, we need a, we need, we need something, you know,"}, {"timestamp": [1127.28, 1132.24], "text": " we need national level something. And Kennedy's like, okay, you know, whatever, that's, that's"}, {"timestamp": [1132.24, 1136.88], "text": " still not an answer. And then finally, he talks to Gary Marcus and Gary Marcus is like,"}, {"timestamp": [1136.88, 1141.64], "text": " we need something like the FDA, but for AI, because if you release a drug to 100 million"}, {"timestamp": [1141.64, 1145.12], "text": " people, you make damn sure that it's well tested and safe."}, {"timestamp": [1145.12, 1147.52], "text": " And we need the same thing for artificial intelligence."}, {"timestamp": [1147.52, 1152.48], "text": " And Kennedy's like, all right, you know, that's, I don't necessarily agree, but that's a, that's"}, {"timestamp": [1152.48, 1154.1], "text": " a more solid answer."}, {"timestamp": [1154.1, 1159.64], "text": " And the fact that nobody in, with on the world stage, the fact that the world leading experts"}, {"timestamp": [1159.64, 1164.6], "text": " did not have a coherent answer to give the United States Senate scared the hell out of"}, {"timestamp": [1164.6, 1165.84], "text": " me and it scared the hell out of me and it scared the"}, {"timestamp": [1165.84, 1174.0], "text": " hell out of a lot of us. And so I put together a research group. It started working on on those"}, {"timestamp": [1174.0, 1178.4], "text": " three principles like just how do you build an aligned machine. We solved that. We know how to"}, {"timestamp": [1178.4, 1186.64], "text": " build aligned machines. That's not the big issue. What we realized is that the biggest issue now is awareness. Are people aware of"}, {"timestamp": [1186.64, 1192.52], "text": " this solution? If they're not, one, how do we raise awareness? Then two, how do we drive"}, {"timestamp": [1192.52, 1199.72], "text": " adoption? The thing is, governments, militaries, corporations, all of these entities are all"}, {"timestamp": [1199.72, 1208.96], "text": " going to serve their own self-interest. There's the concept in market theory of perverse incentives, right?"}, {"timestamp": [1208.96, 1214.28], "text": " The most famous example of perverse incentives is the use of fossil fuels."}, {"timestamp": [1214.28, 1218.28], "text": " We know that using fossil fuels is killing us, but we don't have a better option, so"}, {"timestamp": [1218.28, 1220.92], "text": " we stick with the devil that we know."}, {"timestamp": [1220.92, 1227.28], "text": " One problem with AI is that we're entering, we're getting stuck in what's called a race condition,"}, {"timestamp": [1227.28, 1230.64], "text": " which if you're familiar with servers being stuck in race conditions,"}, {"timestamp": [1230.64, 1233.24], "text": " it's no different, it's exactly the same."}, {"timestamp": [1233.24, 1236.2], "text": " Corporations can get stuck in a race condition as well."}, {"timestamp": [1236.2, 1240.96], "text": " Basically, what it is, is whoever gets to AI first wins."}, {"timestamp": [1240.96, 1244.48], "text": " This is true of militaries and governments as well."}, {"timestamp": [1244.48, 1245.22], "text": " We're stuck in"}, {"timestamp": [1245.22, 1251.44], "text": " this build up, this arms race, but instead of nuclear arms race, it's an AI arms race."}, {"timestamp": [1251.44, 1257.68], "text": " And so with all these forces at play, even if you have the solution, it doesn't matter"}, {"timestamp": [1257.68, 1262.52], "text": " if nobody knows about it and nobody uses it. And so that's where the GATO framework comes"}, {"timestamp": [1262.52, 1267.14], "text": " in. So GATO is an acronym that means Global Alignment Taxonomy Omnibus."}, {"timestamp": [1267.14, 1270.92], "text": " So it is a layered approach to spread the word,"}, {"timestamp": [1270.92, 1276.38], "text": " to get aligned research in the hands of people that need it,"}, {"timestamp": [1276.38, 1281.18], "text": " to drive adoption of aligned systems at the corporate level,"}, {"timestamp": [1281.18, 1282.42], "text": " at the national level,"}, {"timestamp": [1282.42, 1284.38], "text": " and also at the international level."}, {"timestamp": [1284.38, 1287.28], "text": " But beyond that, at the corporate level, at the national level, and also at the international level. But beyond that, at the lower levels,"}, {"timestamp": [1287.28, 1290.92], "text": " it's about facilitating and guiding the research"}, {"timestamp": [1290.92, 1292.52], "text": " around creating aligned systems,"}, {"timestamp": [1292.52, 1294.28], "text": " and not just aligned models."}, {"timestamp": [1294.28, 1298.72], "text": " So for instance, ChatGPT was aligned with RLHF,"}, {"timestamp": [1298.72, 1300.6], "text": " reinforcement learning with human feedback."}, {"timestamp": [1300.6, 1302.74], "text": " That's how you get a single individual model"}, {"timestamp": [1302.74, 1304.8], "text": " to behave the way that you want it to."}, {"timestamp": [1304.8, 1305.52], "text": " And there's of course other"}, {"timestamp": [1305.52, 1308.3], "text": " other means you can do that. But above that, there's the"}, {"timestamp": [1308.3, 1311.76], "text": " autonomous systems that are being built. And so this is if"}, {"timestamp": [1311.76, 1314.52], "text": " you think of like an NPC from a video game, non player"}, {"timestamp": [1314.52, 1318.44], "text": " character, that's what you'd call a semi autonomous agent, or"}, {"timestamp": [1318.44, 1321.44], "text": " you know, a factory robot that kind of runs on its own. Those"}, {"timestamp": [1321.44, 1325.92], "text": " are semi autonomous agents. But a big component of the research"}, {"timestamp": [1325.92, 1327.44], "text": " that I've been doing for the last few years"}, {"timestamp": [1327.44, 1329.86], "text": " is on what's called cognitive architecture."}, {"timestamp": [1329.86, 1331.56], "text": " So cognitive architectures have been around"}, {"timestamp": [1331.56, 1333.72], "text": " since at least the 70s."}, {"timestamp": [1333.72, 1335.96], "text": " They're used to control rockets."}, {"timestamp": [1335.96, 1339.7], "text": " So the Apollo program actually is very primitive,"}, {"timestamp": [1339.7, 1342.56], "text": " what we today would call a primitive cognitive architecture,"}, {"timestamp": [1342.56, 1344.72], "text": " but a cognitive architecture takes input"}, {"timestamp": [1347.76, 1352.32], "text": " and makes decisions and does output without any human intervention. So it's basically the brain of a robot."}, {"timestamp": [1352.32, 1356.88], "text": " And so cognitive architectures are the systems, that is the IT systems that"}, {"timestamp": [1356.88, 1359.84], "text": " are required to create fully autonomous machines, because of"}, {"timestamp": [1359.84, 1364.88], "text": " course it's not just the model. You need data in, you need data storage,"}, {"timestamp": [1364.88, 1366.16], "text": " you need algorithms, you need output, you need data in, you need data storage, you need algorithms,"}, {"timestamp": [1366.16, 1368.48], "text": " you need output, you need feedback loops."}, {"timestamp": [1368.48, 1371.0], "text": " I'm sure everyone here being cloud engineers"}, {"timestamp": [1371.0, 1373.76], "text": " and infrastructure and IT understands"}, {"timestamp": [1373.76, 1378.1], "text": " that no computer system is just one component,"}, {"timestamp": [1378.1, 1379.6], "text": " it's multiple components."}, {"timestamp": [1379.6, 1382.32], "text": " And so that's actually where I took my experience"}, {"timestamp": [1382.32, 1389.84], "text": " as a virtualization engineer and an automation engineer, and I said, oh, I know exactly how to plug all these components together to make"}, {"timestamp": [1389.84, 1393.0], "text": " a self-contained or autonomous machine."}, {"timestamp": [1393.0, 1395.24], "text": " So that's layer two of the Gato framework."}, {"timestamp": [1395.24, 1400.16], "text": " And actually, let me go ahead and just bring up my slide deck, because at this point, I"}, {"timestamp": [1400.16, 1403.84], "text": " think a visual aid will help."}, {"timestamp": [1403.84, 1404.84], "text": " Let's see."}, {"timestamp": [1404.84, 1405.84], "text": " So the layers, here we go."}, {"timestamp": [1407.7, 1408.78], "text": " All right, so it's layered,"}, {"timestamp": [1408.78, 1411.16], "text": " like a seven-layer burrito or a layer cake."}, {"timestamp": [1412.32, 1414.9], "text": " So what I was just talking about was model alignment."}, {"timestamp": [1414.9, 1417.52], "text": " So model alignment was, that's the highest layer"}, {"timestamp": [1417.52, 1419.16], "text": " or the lowest layer, depending on how you look at it."}, {"timestamp": [1419.16, 1422.02], "text": " This is how you get individual models to behave."}, {"timestamp": [1422.02, 1423.58], "text": " Layer two is the autonomous agents."}, {"timestamp": [1423.58, 1426.32], "text": " This is when you create something that's a little bit"}, {"timestamp": [1426.32, 1428.32], "text": " more sophisticated than a chatbot."}, {"timestamp": [1428.32, 1430.84], "text": " The chatbot might be the interface, but then, you know,"}, {"timestamp": [1430.84, 1433.2], "text": " you might have something that goes and writes emails for you"}, {"timestamp": [1433.2, 1436.2], "text": " and, you know, pulls data and runs reports and all that kind"}, {"timestamp": [1436.2, 1438.2], "text": " of stuff."}, {"timestamp": [1438.2, 1440.2], "text": " If you're familiar with automation, think Ansible,"}, {"timestamp": [1440.2, 1443.96], "text": " Ansible Tower, and all those other orchestration things,"}, {"timestamp": [1443.96, 1446.8], "text": " but able to make it more executive decisions on their own."}, {"timestamp": [1446.8, 1449.6], "text": " And then you connect it to more things in the real world."}, {"timestamp": [1449.6, 1451.1], "text": " That's what I mean by autonomous."}, {"timestamp": [1451.1, 1453.1], "text": " It's just the next level of automation."}, {"timestamp": [1453.1, 1456.4], "text": " Layer three of the Gato framework is decentralized networks."}, {"timestamp": [1456.4, 1459.1], "text": " So some of the researchers that I was working with,"}, {"timestamp": [1459.1, 1461.3], "text": " they pointed out that before too long,"}, {"timestamp": [1461.3, 1465.18], "text": " these AI entities are going to spend more time talking to each other"}, {"timestamp": [1465.18, 1467.26], "text": " than they're going to spend talking to us."}, {"timestamp": [1467.26, 1469.38], "text": " If that doesn't scare the hell out of you,"}, {"timestamp": [1469.38, 1471.0], "text": " then think about it for a minute."}, {"timestamp": [1471.0, 1473.36], "text": " Because you have no idea what they're saying."}, {"timestamp": [1473.36, 1476.62], "text": " They can talk faster than you can ever read it."}, {"timestamp": [1476.62, 1480.58], "text": " If there's no insight as to what they're talking about or"}, {"timestamp": [1480.58, 1484.54], "text": " no controls as to gatekeep who's allowed to say what,"}, {"timestamp": [1484.54, 1487.94], "text": " you can easily have a few bad actors poison,"}, {"timestamp": [1487.94, 1490.9], "text": " a couple of bad apples poison the bushel."}, {"timestamp": [1490.9, 1497.88], "text": " By focusing on federated technologies or blockchain technologies or whatever,"}, {"timestamp": [1497.88, 1501.68], "text": " basically the idea is creating decentralized networks that facilitate"}, {"timestamp": [1501.68, 1506.62], "text": " communication between aligned autonomous AI entities."}, {"timestamp": [1506.62, 1512.94], "text": " In other words, a way for AI entities that are good, you know, the good actors, to decide,"}, {"timestamp": [1512.94, 1518.24], "text": " hey, that's a bad actor, let's kick him out, or let's cut off his power."}, {"timestamp": [1518.24, 1527.64], "text": " This is a critical component of the technological side of ensuring that AI never goes completely bonkers, completely haywire."}, {"timestamp": [1527.64, 1529.64], "text": " Because you might have..."}, {"timestamp": [1529.64, 1531.64], "text": " Basically, one thing that we expect is that"}, {"timestamp": [1531.64, 1534.14], "text": " over the coming year or two, we're going to see"}, {"timestamp": [1534.14, 1537.26], "text": " at first, you know, thousands, hundreds of thousands"}, {"timestamp": [1537.26, 1539.26], "text": " of autonomous AI agents being built,"}, {"timestamp": [1539.26, 1541.26], "text": " and then it's going to quickly ramp up"}, {"timestamp": [1541.26, 1543.86], "text": " to the millions and billions of autonomous AI agents"}, {"timestamp": [1543.86, 1545.32], "text": " being built."}, {"timestamp": [1545.32, 1547.46], "text": " And you have no idea how any of them are aligned."}, {"timestamp": [1547.46, 1551.28], "text": " It's basically, from a cybersecurity perspective, it's an absolute nightmare."}, {"timestamp": [1551.28, 1557.84], "text": " Oh, and on that note, the Gato framework was actually inspired in part by the defense in"}, {"timestamp": [1557.84, 1561.16], "text": " depth model for anyone who's familiar with that."}, {"timestamp": [1561.16, 1568.2], "text": " So obviously, as an IT guy, thinking in terms of layers of abstraction comes naturally to me."}, {"timestamp": [1568.2, 1569.72], "text": " So the first three layers,"}, {"timestamp": [1569.72, 1571.92], "text": " the model, autonomous, and decentralized networks,"}, {"timestamp": [1571.92, 1574.96], "text": " those are the technological layers of the Gato framework,"}, {"timestamp": [1574.96, 1576.14], "text": " which is, okay,"}, {"timestamp": [1576.14, 1578.24], "text": " here's the technology that we need in order to"}, {"timestamp": [1578.24, 1581.38], "text": " solve alignment in order to solve the control problem."}, {"timestamp": [1581.38, 1584.96], "text": " The last four layers are more of the human layer."}, {"timestamp": [1584.96, 1585.8], "text": " So of course, the defense in depth are more of the human layer. So of course, you"}, {"timestamp": [1585.8, 1590.0], "text": " know, the defense in depth model absolutely includes the human layer, the human aspect."}, {"timestamp": [1590.0, 1597.24], "text": " And of course, humans are usually the weakest link when it comes to IT security and safety,"}, {"timestamp": [1597.24, 1602.08], "text": " assuming that, you know, the firewalls and everything are in place. So layer four is"}, {"timestamp": [1602.08, 1608.28], "text": " corporate adoption. One thing that we realized in our research group was that corporations,"}, {"timestamp": [1608.28, 1610.22], "text": " they have a profit motive, right?"}, {"timestamp": [1610.22, 1611.98], "text": " Every company wants to make money,"}, {"timestamp": [1611.98, 1613.24], "text": " and if they're making money,"}, {"timestamp": [1613.24, 1615.0], "text": " what do they want? They want to make more money."}, {"timestamp": [1615.0, 1616.7], "text": " So the idea was,"}, {"timestamp": [1616.7, 1624.0], "text": " let's align their intrinsic motivation to make more money with adopting aligned AI."}, {"timestamp": [1624.0, 1627.84], "text": " So we have evidence that this works in Europe with GDPR."}, {"timestamp": [1627.84, 1633.68], "text": " GDPR basically says you have to respect user privacy and a whole host of other things."}, {"timestamp": [1633.68, 1638.02], "text": " That regulation is so powerful and so strict that even"}, {"timestamp": [1638.02, 1642.2], "text": " investors will not touch companies that are not GDPR compliant."}, {"timestamp": [1642.2, 1645.68], "text": " It's so powerful that that investor behavior"}, {"timestamp": [1645.68, 1651.44], "text": " is like it's spreading to America too where American companies they might not be required"}, {"timestamp": [1651.44, 1656.68], "text": " to adhere to GDPR but so for instance chat GPT got almost shut down in Italy because"}, {"timestamp": [1656.68, 1662.04], "text": " it was not GDPR compliant and so you know Sam Altman went and talked to the Italians"}, {"timestamp": [1662.04, 1666.48], "text": " and the Italians said you need to be GDPR compliant if you want to roll it out here and Sam said,"}, {"timestamp": [1666.48, 1669.72], "text": " okay, fine, we'll make it GDPR compliant everywhere."}, {"timestamp": [1669.72, 1674.0], "text": " And that's why in chat GPT you have the ability to delete all of your data."}, {"timestamp": [1674.0, 1676.88], "text": " So thanks to the Italians for that."}, {"timestamp": [1676.88, 1681.92], "text": " So the idea is that if we, you can use punishment, right?"}, {"timestamp": [1681.92, 1686.88], "text": " You can use regulations like GDPR to incentivize the behavior you want to see, right?"}, {"timestamp": [1686.88, 1689.36], "text": " But you can also just build the case"}, {"timestamp": [1689.36, 1692.28], "text": " that aligned AI is just better for business."}, {"timestamp": [1692.28, 1696.24], "text": " This is something that I work with on my Patreon supporters,"}, {"timestamp": [1696.24, 1698.36], "text": " which I mentioned before we started recording,"}, {"timestamp": [1698.36, 1702.72], "text": " that I do one-on-one consulting with my supporters."}, {"timestamp": [1702.72, 1706.0], "text": " What many of them have reported is that when they"}, {"timestamp": [1706.0, 1708.0], "text": " integrate alignment principles,"}, {"timestamp": [1708.0, 1710.0], "text": " the principles that I talked about earlier,"}, {"timestamp": [1710.0, 1712.0], "text": " reduce suffering, increase prosperity"}, {"timestamp": [1712.0, 1714.0], "text": " and increase understanding, it just helps"}, {"timestamp": [1714.0, 1716.0], "text": " make everything better. Whether"}, {"timestamp": [1716.0, 1718.0], "text": " you're serving internal users,"}, {"timestamp": [1718.0, 1720.0], "text": " or external customers, or whatever,"}, {"timestamp": [1720.0, 1722.0], "text": " when you have that"}, {"timestamp": [1722.0, 1724.0], "text": " higher order perspective, those higher order"}, {"timestamp": [1724.0, 1727.94], "text": " principles, it makes the AI make better decisions,"}, {"timestamp": [1727.94, 1730.2], "text": " whether it's for the bottom line of your own company"}, {"timestamp": [1730.2, 1732.08], "text": " or the wellbeing of your employees,"}, {"timestamp": [1732.08, 1733.38], "text": " or even just paying attention"}, {"timestamp": [1733.38, 1734.94], "text": " to the needs of your customer."}, {"timestamp": [1734.94, 1738.04], "text": " So for instance, including those principles"}, {"timestamp": [1738.04, 1739.6], "text": " of reduce suffering, increase prosperity,"}, {"timestamp": [1739.6, 1743.04], "text": " and increase understanding into a customer facing chatbot,"}, {"timestamp": [1743.04, 1745.92], "text": " that chatbot is to be much more helpful"}, {"timestamp": [1745.92, 1747.76], "text": " than one without those principles."}, {"timestamp": [1747.76, 1750.68], "text": " And so for instance, if you have a user that's really frustrated,"}, {"timestamp": [1750.68, 1755.0], "text": " the frustration comes through in their chat logs or whatever."}, {"timestamp": [1755.0, 1757.52], "text": " And so then the chat bot says, hey,"}, {"timestamp": [1757.52, 1759.72], "text": " I see that you're angry, remember,"}, {"timestamp": [1759.72, 1763.72], "text": " my goal is to fix whatever's going on."}, {"timestamp": [1763.72, 1766.12], "text": " It also helps with autonomous agents."}, {"timestamp": [1766.12, 1774.06], "text": " So another Patreon supporter was using the chat GPT API to automate some aspects of science,"}, {"timestamp": [1774.06, 1776.32], "text": " and it kept getting stuck, kind of stuck in an infinite loop."}, {"timestamp": [1776.32, 1781.04], "text": " And I said, you know, use my heuristic comparatives, use those three principles."}, {"timestamp": [1781.04, 1784.12], "text": " And he came back 20 minutes later and said, oh, yeah, that got it right out."}, {"timestamp": [1784.12, 1786.32], "text": " And it's performing better now."}, {"timestamp": [1786.32, 1788.96], "text": " So we want to drive corporate adoption of aligned AI"}, {"timestamp": [1788.96, 1791.48], "text": " by first just demonstrating that alignment"}, {"timestamp": [1791.48, 1792.82], "text": " is good for business."}, {"timestamp": [1792.82, 1794.9], "text": " Alignment is good for business for a number of reasons,"}, {"timestamp": [1794.9, 1798.12], "text": " not the least of which is that autonomous machines"}, {"timestamp": [1798.12, 1801.4], "text": " that are aligned and safe need a lot less supervision."}, {"timestamp": [1801.4, 1804.0], "text": " And for anyone who has kids,"}, {"timestamp": [1805.6, 1810.2], "text": " if you've got kids that require constant supervision, that's a drain on your energy"}, {"timestamp": [1810.2, 1813.32], "text": " and you can't get as much done. But if your kids are self-sufficient, you know, they go"}, {"timestamp": [1813.32, 1817.64], "text": " play on their own, they play nice, you can get a lot more done. Same with employees for"}, {"timestamp": [1817.64, 1821.32], "text": " anyone who's a supervisor or manager. The employees that are always at each other's"}, {"timestamp": [1821.32, 1826.12], "text": " throats, they drag everyone down. But the employees that don't need much time and attention,"}, {"timestamp": [1826.12, 1827.68], "text": " those are the better employees."}, {"timestamp": [1827.68, 1829.32], "text": " So the idea is that aligned AI"}, {"timestamp": [1829.32, 1832.92], "text": " basically creates better autonomous machine employees."}, {"timestamp": [1834.14, 1835.92], "text": " So that's layer four."}, {"timestamp": [1835.92, 1838.4], "text": " And that's one of the reasons that I sit on podcasts"}, {"timestamp": [1838.4, 1841.44], "text": " and do consultations."}, {"timestamp": [1841.44, 1843.74], "text": " Layer five is national regulation."}, {"timestamp": [1843.74, 1848.0], "text": " So this goes back to that talk, or the hearing at the Senate,"}, {"timestamp": [1848.0, 1854.68], "text": " where they said, you know, hey, we need something like the FDA, but for AI, or we need something like GDPR, but for AI."}, {"timestamp": [1854.68, 1868.0], "text": " And of course, the European Union, what was it, a day or two ago, just passed the EA, the, sorry, the EU AI Act. It's a mouthful. So they just passed that, which is a very sweeping, comprehensive"}, {"timestamp": [1868.0, 1872.0], "text": " bill basically saying, you know, respect"}, {"timestamp": [1872.0, 1876.0], "text": " human rights and consumer rights and that sort of stuff."}, {"timestamp": [1876.0, 1880.0], "text": " I checked and I don't think that the EU AI Act"}, {"timestamp": [1880.0, 1884.0], "text": " addresses existential risks. It doesn't address runaway AI."}, {"timestamp": [1884.0, 1888.5], "text": " But it's going to do a lot to incentivize companies to adopt aligned AI."}, {"timestamp": [1888.5, 1890.5], "text": " So this is still a step in the right direction."}, {"timestamp": [1890.5, 1897.0], "text": " On the national regulation side, a few other things that we advocate for is not just regulations."}, {"timestamp": [1897.0, 1906.72], "text": " Obviously, politics today, particularly in America, I'm not sure how it is in Canada, but particularly in America, generally what they aim for is"}, {"timestamp": [1906.72, 1912.64], "text": " as legislation or regulations that don't require a whole new department to enforce."}, {"timestamp": [1912.64, 1917.0], "text": " Basically it's up to the company to prove that it is compliant or whatever."}, {"timestamp": [1917.0, 1921.72], "text": " There's a number of ways to approach this problem, but the idea is that we would like"}, {"timestamp": [1921.72, 1926.0], "text": " to see and advocate for these kinds of regulations that steer"}, {"timestamp": [1926.0, 1930.56], "text": " companies in the correct direction, but also that steer research in the right direction."}, {"timestamp": [1930.56, 1936.36], "text": " So at the national level, you can also do things like use grants and competitions in"}, {"timestamp": [1936.36, 1940.8], "text": " order to incentivize the alignment research and alignment competitions that you want to"}, {"timestamp": [1940.8, 1942.32], "text": " see."}, {"timestamp": [1942.32, 1969.36], "text": " Layer six is the international level, where, so Sam Altman and OpenAI, they advocate for the creation of something like the IAEA, which is the International Atomic Energy Agency, which is a, that's the nuclear inspector. So whenever you hear about, you know, Iran or North Korea or Russia, like, you know, getting in trouble, it's the IAEA sending their inspectors in to make sure that they're not, you know, building more nukes than they're supposed to."}, {"timestamp": [1969.36, 1972.4], "text": " So that is the enforcement agency."}, {"timestamp": [1972.4, 1977.68], "text": " But what I would also advocate for at the international level is a research agency,"}, {"timestamp": [1977.68, 1980.0], "text": " because alignment is not fully solved."}, {"timestamp": [1980.0, 1984.52], "text": " Yes, I have done a lot of research and a lot of, and I've produced a lot of data and cognitive"}, {"timestamp": [1984.52, 1985.06], "text": " architectures"}, {"timestamp": [1985.06, 1989.94], "text": " and diagrams about how to create aligned entities, but that doesn't mean that the problem is"}, {"timestamp": [1989.94, 1991.26], "text": " fully solved."}, {"timestamp": [1991.26, 1995.48], "text": " So what I would like to see is actually two organizations, two international or global"}, {"timestamp": [1995.48, 2001.4], "text": " organizations, one focused on enforcement of alignment and another focused on alignment"}, {"timestamp": [2001.4, 2002.64], "text": " research."}, {"timestamp": [2002.64, 2007.56], "text": " And then finally, layer seven is global consensus, which is why I'm here today, getting the message out,"}, {"timestamp": [2007.56, 2010.8], "text": " providing that education, that enablement and empowerment"}, {"timestamp": [2010.8, 2014.08], "text": " around solving the AI problem."}, {"timestamp": [2014.08, 2016.18], "text": " Because the thing is, is yes,"}, {"timestamp": [2016.18, 2018.04], "text": " there's the way that things are today,"}, {"timestamp": [2018.04, 2021.2], "text": " but if you've seen in the news,"}, {"timestamp": [2021.2, 2024.72], "text": " there have been literally hundreds of thousands of layoffs,"}, {"timestamp": [2024.72, 2025.28], "text": " tech layoffs"}, {"timestamp": [2025.28, 2031.64], "text": " in America. And I know people in my research group that were laid off six, seven, eight,"}, {"timestamp": [2031.64, 2036.88], "text": " nine, 10 months ago and haven't been able to find work. And my thought is that a lot"}, {"timestamp": [2036.88, 2043.48], "text": " of these companies are pivoting, they're focusing on using AI to do as much as possible. And"}, {"timestamp": [2043.48, 2046.8], "text": " in fact, my wife has been on several communities,"}, {"timestamp": [2046.8, 2050.76], "text": " whether a Discord communities and Reddit communities,"}, {"timestamp": [2050.76, 2053.24], "text": " where it's not just technology workers,"}, {"timestamp": [2053.24, 2055.0], "text": " it's marketers, it's copywriters."}, {"timestamp": [2055.0, 2057.4], "text": " A lot of people have either been laid off,"}, {"timestamp": [2057.4, 2059.72], "text": " lost clients, or been given notice."}, {"timestamp": [2059.72, 2062.44], "text": " Basically, your department's going away"}, {"timestamp": [2062.44, 2064.44], "text": " and we're replacing you with AI."}, {"timestamp": [2064.44, 2068.0], "text": " So we've got this huge wave coming of layoffs."}, {"timestamp": [2068.0, 2072.0], "text": " And of course, we can talk about things like universal basic income"}, {"timestamp": [2072.0, 2076.0], "text": " or extended unemployment benefits or whatever to shore up people's livelihoods"}, {"timestamp": [2076.0, 2081.0], "text": " in the short term. But in the long term, that's really where I focus on,"}, {"timestamp": [2081.0, 2084.0], "text": " which is these machines are going to get more powerful."}, {"timestamp": [2084.0, 2086.48], "text": " How does that change the economic landscape?"}, {"timestamp": [2086.48, 2088.36], "text": " How does that change geopolitics?"}, {"timestamp": [2088.36, 2091.24], "text": " And then finally, when the machine is smarter"}, {"timestamp": [2091.24, 2094.84], "text": " than all of us combined, what then do we do?"}, {"timestamp": [2094.84, 2097.64], "text": " So that's the high level of Gato."}, {"timestamp": [2097.64, 2102.44], "text": " I also do have within Gato a set of traditions."}, {"timestamp": [2102.44, 2104.12], "text": " So the traditions are modeled"}, {"timestamp": [2104.12, 2107.26], "text": " on many other decentralized organizations."}, {"timestamp": [2107.26, 2112.0], "text": " So Burning Man has their, they don't call them traditions, they call it something else."}, {"timestamp": [2112.0, 2118.0], "text": " But many 12-step programs have what they call traditions. So basically, these are if the"}, {"timestamp": [2118.0, 2124.12], "text": " seven layers that I've created are the roadmap to alignment, to a safe, benevolent future,"}, {"timestamp": [2124.12, 2126.58], "text": " then the traditions are the rules of the road."}, {"timestamp": [2126.58, 2128.78], "text": " These are the tactics on how to get there."}, {"timestamp": [2128.78, 2130.86], "text": " And so the traditions, I'm not gonna read all of them"}, {"timestamp": [2130.86, 2132.44], "text": " because it would take a while to unpack,"}, {"timestamp": [2132.44, 2134.6], "text": " but it's basically kind of like little mantras,"}, {"timestamp": [2134.6, 2137.56], "text": " little idioms or, not axioms,"}, {"timestamp": [2137.56, 2140.88], "text": " little platitudes that you can abide by"}, {"timestamp": [2140.88, 2144.48], "text": " in order to help move everything in the right direction."}, {"timestamp": [2144.48, 2150.8], "text": " So with all that said, I think you guys have the full story, and I'd like to open up to"}, {"timestamp": [2150.8, 2154.76], "text": " questions that anyone might have."}, {"timestamp": [2154.76, 2155.76], "text": " Thank you, David."}, {"timestamp": [2155.76, 2159.0], "text": " This is amazing."}, {"timestamp": [2159.0, 2164.44], "text": " And feel free to, yeah, you can either, I'll let Daniel, you can call on people, or if"}, {"timestamp": [2164.44, 2169.04], "text": " you drop a question in the chat, I'll be monitoring chat as well."}, {"timestamp": [2169.04, 2172.6], "text": " So Tracy has a hand raised he was first."}, {"timestamp": [2172.6, 2174.4], "text": " Yeah what's your question?"}, {"timestamp": [2174.4, 2179.48], "text": " Um again thanks thanks for your time and Daniel thanks for for hosting."}, {"timestamp": [2179.48, 2186.0], "text": " So uh super super interesting uh framework definitely a lot to think about."}, {"timestamp": [2189.0, 2194.0], "text": " So my experience with generative AI in the corporate world"}, {"timestamp": [2194.0, 2203.0], "text": " has been, for the most part, guided by this idea of responsible AI first,"}, {"timestamp": [2203.0, 2207.28], "text": " making sure that we are using the technology in a responsible way,"}, {"timestamp": [2207.28, 2214.32], "text": " and that includes, at least in my field of work, a lot of emphasis on security, privacy,"}, {"timestamp": [2215.28, 2222.88], "text": " and algorithmic fairness. If you ask me to summarize, those are the three principles"}, {"timestamp": [2222.88, 2226.64], "text": " that are guiding our work on Responsible AI."}, {"timestamp": [2227.26, 2229.04], "text": " So that's one guiding force."}, {"timestamp": [2229.04, 2232.48], "text": " But then we have this push"}, {"timestamp": [2232.48, 2237.76], "text": " for a mostly utilitarian use of that technology, if you will."}, {"timestamp": [2238.48, 2242.0], "text": " And I guess my question is,"}, {"timestamp": [2243.48, 2245.0], "text": " how does Gato..."}, {"timestamp": [2245.0, 2247.0], "text": " I'm sorry, Gato?"}, {"timestamp": [2247.0, 2248.0], "text": " Gato?"}, {"timestamp": [2248.0, 2250.0], "text": " You can say it either way, yeah."}, {"timestamp": [2253.0, 2257.0], "text": " ...complement this approach to generative AI?"}, {"timestamp": [2257.0, 2261.0], "text": " Because I feel, I mean, after listening to what you were saying,"}, {"timestamp": [2261.0, 2268.08], "text": " I feel that thinking just about responsible AI and the utilitarian"}, {"timestamp": [2268.08, 2279.3], "text": " approach is very simplistic. So how does Gator complement or wraps around those two approaches"}, {"timestamp": [2279.3, 2280.7], "text": " to generative AI?"}, {"timestamp": [2280.7, 2285.84], "text": " Yeah, that's a really great question. The first part to think about is,"}, {"timestamp": [2285.84, 2287.4], "text": " well, take a step back."}, {"timestamp": [2287.4, 2291.52], "text": " What is the point of having those principles,"}, {"timestamp": [2291.52, 2296.78], "text": " those rules about fairness in an algorithmic fairness,"}, {"timestamp": [2296.78, 2300.64], "text": " or having consumer protections and data protections,"}, {"timestamp": [2300.64, 2304.48], "text": " those things. The reason that we make those choices"}, {"timestamp": [2304.48, 2308.0], "text": " is in order to serve"}, {"timestamp": [2308.0, 2315.44], "text": " those higher purposes, right, is to reduce harm or to, in other words, to reduce suffering,"}, {"timestamp": [2316.08, 2329.46], "text": " right? And so the principles that I came up with, that Gato is there to there to drive and get embedded. There's the idea of layers of ethics or layers of morality."}, {"timestamp": [2329.46, 2332.74], "text": " There's a researcher, what was his name?"}, {"timestamp": [2332.74, 2334.9], "text": " Kohlberg, I think it was Daniel Kohlberg."}, {"timestamp": [2334.9, 2342.18], "text": " Daniel Kohlberg in the 50s or 60s came up with a moral framework,"}, {"timestamp": [2342.18, 2346.72], "text": " or not a moral framework, but the moral development theory, where he observed that"}, {"timestamp": [2347.92, 2353.28], "text": " you know, morality is not static. It changes over your life. When you're a small child, you learn"}, {"timestamp": [2353.28, 2360.48], "text": " from consequences. And so the idea is, is that a child doesn't behave right, they get punished, they"}, {"timestamp": [2360.48, 2365.76], "text": " get put in time out, you know, they get, they don't get the cookie or whatever. And so Kohlberg called"}, {"timestamp": [2365.76, 2370.26], "text": " this pre conventional morality. That is, you learn how to behave"}, {"timestamp": [2370.28, 2373.56], "text": " based on consequences, based on basically classical"}, {"timestamp": [2373.56, 2377.84], "text": " conditioning, you get a reward or a punishment. In the middle"}, {"timestamp": [2377.84, 2380.64], "text": " of people's lives, you have what's called conventional"}, {"timestamp": [2380.64, 2384.04], "text": " morality. So conventional morality is about law and order,"}, {"timestamp": [2384.04, 2385.44], "text": " it's about going along"}, {"timestamp": [2385.44, 2390.48], "text": " with the social consensus for the sake of belonging. And so that is where most people"}, {"timestamp": [2390.48, 2396.56], "text": " understand the idea of laws. Laws are there to basically ensure that everyone plays nice,"}, {"timestamp": [2396.56, 2400.32], "text": " even as adults, because, you know, by and large, people will play nice. We don't need to keep a"}, {"timestamp": [2400.32, 2407.62], "text": " law book at all times. But laws help create those boundaries, create the guardrails that keeps society nice, safe,"}, {"timestamp": [2407.62, 2411.44], "text": " and civil and kind of create a consensus"}, {"timestamp": [2411.44, 2415.04], "text": " or I guess guardrails is a good word"}, {"timestamp": [2415.04, 2416.72], "text": " for everyone to abide by."}, {"timestamp": [2416.72, 2419.88], "text": " But there's a third level of morality"}, {"timestamp": [2419.88, 2421.68], "text": " that Kohlberg identified"}, {"timestamp": [2421.68, 2424.08], "text": " and that's what he called post-conventional morality."}, {"timestamp": [2424.08, 2426.16], "text": " And so post-conventional morality is the idea"}, {"timestamp": [2426.16, 2429.24], "text": " that you have gained enough wisdom and experience"}, {"timestamp": [2429.24, 2432.56], "text": " to derive universal principles,"}, {"timestamp": [2432.56, 2436.44], "text": " universal principles that you abide by,"}, {"timestamp": [2436.44, 2439.88], "text": " that you use to guide everything else that you do."}, {"timestamp": [2439.88, 2442.16], "text": " And so here in America, and this is also true"}, {"timestamp": [2442.16, 2445.96], "text": " in many liberal democracies around the world"}, {"timestamp": [2445.96, 2452.72], "text": " in Western countries, is one of the most familiar examples of post-conventional morality is"}, {"timestamp": [2452.72, 2455.02], "text": " the idea of individual liberty."}, {"timestamp": [2455.02, 2461.28], "text": " And so, for instance, during the pandemic, the idea of individual liberty was actually"}, {"timestamp": [2461.28, 2466.08], "text": " a very hot-button issue, which is, does anyone have the right to tell you to mask up?"}, {"timestamp": [2466.08, 2469.56], "text": " Does anyone have a right to tell you to get a vaccine?"}, {"timestamp": [2469.56, 2475.14], "text": " Yes or no? That was at heart a very core issue."}, {"timestamp": [2475.14, 2477.34], "text": " But the idea is that that principle,"}, {"timestamp": [2477.34, 2480.92], "text": " that axiom that individual liberty is really important,"}, {"timestamp": [2480.92, 2483.36], "text": " is what was at stake."}, {"timestamp": [2483.36, 2487.36], "text": " That is an example of post-conventional morality."}, {"timestamp": [2487.36, 2492.36], "text": " So what these axioms do for Gato is rather than make some"}, {"timestamp": [2492.36, 2497.0], "text": " of those other foundational assumptions or principles,"}, {"timestamp": [2497.0, 2498.8], "text": " rather than leaving them unconscious,"}, {"timestamp": [2498.8, 2500.56], "text": " it brings them into the consciousness."}, {"timestamp": [2500.56, 2502.0], "text": " And I don't mean that in any kind of way,"}, {"timestamp": [2502.0, 2503.68], "text": " just you just write it down, right?"}, {"timestamp": [2503.68, 2509.3], "text": " Now you're aware of the idea that suffering is bad and nobody likes to suffer, right?"}, {"timestamp": [2509.36, 2513.02], "text": " Some people might argue that some suffering is good. Some suffering is necessary."}, {"timestamp": [2513.24, 2518.56], "text": " But in general, suffering is there to tell you not to do something. You put your hand on a hot stove,"}, {"timestamp": [2518.56, 2524.6], "text": " it hurts, you pull your hand back. If you're hungry, you go find food. If you're lonely, you go find a friend, right?"}, {"timestamp": [2524.6, 2525.04], "text": " There's suffering takes on many, you go find a friend, right?"}, {"timestamp": [2525.04, 2529.0], "text": " Their suffering takes on many, many forms. For humans, it takes on many forms for other"}, {"timestamp": [2529.0, 2532.8], "text": " things. The desire to avoid suffering is one of the"}, {"timestamp": [2532.8, 2538.16], "text": " foundational things about being human that drives us to create laws and regulations in"}, {"timestamp": [2538.16, 2546.04], "text": " the first place. So, for instance, more than 100 years ago, or about 100 years ago now, refrigerators kept killing"}, {"timestamp": [2546.04, 2550.68], "text": " people because they would leak toxic gases into homes, and then you'd show up and the"}, {"timestamp": [2550.68, 2555.56], "text": " whole family would be dead. And so what did they do? Well, they created legislation that"}, {"timestamp": [2555.56, 2560.18], "text": " said actually you need to test refrigerators so that they don't leak and kill everyone."}, {"timestamp": [2560.18, 2565.7], "text": " And so that is an example of humans react to suffering. We react, we empathetically react to the suffering"}, {"timestamp": [2565.7, 2570.44], "text": " of loss, of death, of pain, and we create laws to fix it."}, {"timestamp": [2570.44, 2573.12], "text": " Likewise, we create gigantic metrics"}, {"timestamp": [2573.12, 2575.88], "text": " like GDP to measure prosperity, right?"}, {"timestamp": [2575.88, 2578.16], "text": " GDP is not the best measurement of prosperity,"}, {"timestamp": [2578.16, 2580.48], "text": " but it's the best one we've got that spans,"}, {"timestamp": [2580.48, 2582.26], "text": " you know, nations and globes."}, {"timestamp": [2582.26, 2586.0], "text": " And so again, we create laws, regulations, and frameworks."}, {"timestamp": [2586.0, 2589.0], "text": " We created capitalism, all for the sake of increasing prosperity."}, {"timestamp": [2589.0, 2593.0], "text": " So, reducing suffering and increasing prosperity are the underpinning"}, {"timestamp": [2593.0, 2600.0], "text": " philosophical, moral beliefs that drive all the laws that we abide by."}, {"timestamp": [2600.0, 2602.0], "text": " And then of course, as I mentioned at the beginning,"}, {"timestamp": [2602.0, 2605.0], "text": " curiosity is kind of humanity's superpower."}, {"timestamp": [2605.0, 2611.3], "text": " So the idea of... Gato doesn't replace anything. It doesn't replace, you know, the Constitution."}, {"timestamp": [2611.3, 2616.8], "text": " It doesn't replace Congress or Parliament or, you know, local laws or anything."}, {"timestamp": [2616.8, 2623.7], "text": " It just only seeks to identify these are the core principles that we all already abide by."}, {"timestamp": [2623.7, 2626.6], "text": " And so by bringing those into the consciousness"}, {"timestamp": [2626.6, 2629.22], "text": " and saying, hey, look, we can actually encode these"}, {"timestamp": [2629.22, 2631.82], "text": " into the machines that we're building,"}, {"timestamp": [2631.82, 2633.74], "text": " that is how it kind of works together."}, {"timestamp": [2633.74, 2636.9], "text": " And so in the case of, like, if you were to go talk"}, {"timestamp": [2636.9, 2639.16], "text": " to chat GPT or whatever and say like, oh, hey,"}, {"timestamp": [2639.16, 2643.32], "text": " like we have rules about how to deploy AI ethically,"}, {"timestamp": [2643.32, 2646.24], "text": " how does that abide by these principles, this principle of"}, {"timestamp": [2646.24, 2650.16], "text": " reduced suffering and increased prosperity and so on? They'll be like, oh yeah, these are underpinning"}, {"timestamp": [2650.88, 2655.92], "text": " principles. So that's a very, very long-winded answer to your question. I hope that helps clear"}, {"timestamp": [2655.92, 2662.72], "text": " it up or maybe makes it more confusing. A quick follow-up. So I guess the part where I'm struggling"}, {"timestamp": [2663.2, 2666.2], "text": " So I guess the part where I'm struggling,"}, {"timestamp": [2668.12, 2671.64], "text": " and I'm sorry if I'm approaching the topic from a super pragmatic point of view."}, {"timestamp": [2671.64, 2672.48], "text": " Sure."}, {"timestamp": [2672.48, 2674.4], "text": " But the part that I'm struggling is I'm wondering"}, {"timestamp": [2674.4, 2677.0], "text": " if the principles that you're describing"}, {"timestamp": [2677.0, 2679.36], "text": " are really principles that are suited"}, {"timestamp": [2679.36, 2682.8], "text": " for the training and development of foundational models,"}, {"timestamp": [2682.8, 2684.56], "text": " whereas on the enterprise,"}, {"timestamp": [2684.56, 2687.3], "text": " we just consume what's off the shelf."}, {"timestamp": [2688.52, 2691.7], "text": " Is the idea to embed some of those principles as well"}, {"timestamp": [2691.7, 2692.94], "text": " in those off-the-shelf models,"}, {"timestamp": [2692.94, 2697.0], "text": " or what you're describing is mostly applicable"}, {"timestamp": [2697.0, 2700.86], "text": " to the companies and developers and data scientists"}, {"timestamp": [2700.86, 2702.52], "text": " working on foundational models?"}, {"timestamp": [2704.04, 2704.96], "text": " That's a great question."}, {"timestamp": [2704.96, 2707.04], "text": " And so there's two parts of that."}, {"timestamp": [2707.04, 2711.6], "text": " One is that in some cases you have the ability to use what's called steerable"}, {"timestamp": [2711.6, 2715.52], "text": " models. And so in this case if you use the"}, {"timestamp": [2715.52, 2720.08], "text": " chat GPT API, it has what's called the system"}, {"timestamp": [2720.08, 2723.28], "text": " message. And so the system message is where you can give it instructions to"}, {"timestamp": [2723.28, 2731.68], "text": " follow. And so at that level it is absolutely up to corporations what instructions to give it what principles to give their agents."}, {"timestamp": [2731.86, 2737.22], "text": " So in that case even though it's an off the shelf API you can absolutely add that steering."}, {"timestamp": [2737.46, 2743.3], "text": " Now to your other point it is also very much within the domain of"}, {"timestamp": [2746.24, 2748.36], "text": " very much within the domain of the companies providing those foundation models and those other products."}, {"timestamp": [2748.36, 2752.52], "text": " So for instance, I mentioned RLHF."}, {"timestamp": [2752.52, 2758.86], "text": " So right now, chat GPT is being trained on the only principle that chat GPT is being"}, {"timestamp": [2758.86, 2764.48], "text": " trained on is to produce better results that humans will like."}, {"timestamp": [2764.48, 2768.06], "text": " It is self-aligning to human preferences."}, {"timestamp": [2768.06, 2771.56], "text": " It's not aligning to any higher principles other"}, {"timestamp": [2771.56, 2775.72], "text": " than will a human like this chat response, yes or no."}, {"timestamp": [2775.72, 2779.32], "text": " In that respect, it's not aligning to any higher principle."}, {"timestamp": [2779.32, 2781.84], "text": " It's only trying to optimize for user engagement,"}, {"timestamp": [2781.84, 2784.96], "text": " which is exactly what Facebook was trying to do."}, {"timestamp": [2784.96, 2785.92], "text": " Optimize for user engagement, not is exactly what Facebook was trying to do, optimize for user"}, {"timestamp": [2785.92, 2792.8], "text": " engagement, not optimizing for user well-being or not optimizing for corporate well-being. And so by"}, {"timestamp": [2792.8, 2799.28], "text": " deliberately extracting these principles out, you can actually train, yes, the short answer is yes,"}, {"timestamp": [2799.28, 2804.8], "text": " you can train models that are going to abide by these higher principles rather than just"}, {"timestamp": [2803.0, 2808.0], "text": " that are going to abide by these higher principles, rather than just optimizing for some other objective."}, {"timestamp": [2808.0, 2812.0], "text": " So that, I hope that answers your question as well."}, {"timestamp": [2812.0, 2815.0], "text": " Last follow-up, I'm sorry."}, {"timestamp": [2815.0, 2816.0], "text": " No, go ahead."}, {"timestamp": [2816.0, 2817.0], "text": " Good questions, yeah."}, {"timestamp": [2817.0, 2821.0], "text": " Curtis has a hand raised as well, but it's pivoting into something else."}, {"timestamp": [2821.0, 2822.0], "text": " So finish, Jesse."}, {"timestamp": [2822.0, 2824.0], "text": " Go ahead with the last question."}, {"timestamp": [2824.0, 2826.32], "text": " So at least in my line of work, into something else. So finish, JC. Go ahead with the last question."}, {"timestamp": [2826.32, 2834.8], "text": " So at least in my line of work, the context, the number of tokens we have available for"}, {"timestamp": [2834.8, 2842.72], "text": " the context or the system prompt is so limited, even with the newer models, that we, for the"}, {"timestamp": [2842.72, 2845.36], "text": " most part, it's all about the instructions that we're giving the most part,"}, {"timestamp": [2847.84, 2850.32], "text": " it's all about the instructions that we're giving, it's all about the business objective we're after."}, {"timestamp": [2851.2, 2855.6], "text": " So, a lot of these principles are not baked"}, {"timestamp": [2855.6, 2858.96], "text": " into the system prompts we're using."}, {"timestamp": [2859.68, 2860.68], "text": " Right."}, {"timestamp": [2860.68, 2863.36], "text": " Because there's a limited bandwidth"}, {"timestamp": [2863.36, 2865.68], "text": " of how much steering we can provide there,"}, {"timestamp": [2865.68, 2870.72], "text": " and we want to make sure that that searing is aimed at our business objectives."}, {"timestamp": [2870.72, 2876.48], "text": " So maybe the technology will evolve to a point where we can balance both."}, {"timestamp": [2878.48, 2883.36], "text": " I feel we're just not there yet. I guess there's no question, just a comment."}, {"timestamp": [2884.12, 2890.28], "text": " I guess there's no question, just a comment. Well, I think there's an implied question which I can answer."}, {"timestamp": [2890.28, 2894.08], "text": " One thing to keep in mind is that when you have a single shot,"}, {"timestamp": [2894.08, 2896.48], "text": " like just one input output, sure."}, {"timestamp": [2896.48, 2898.2], "text": " You're not going to have as much space"}, {"timestamp": [2898.2, 2900.84], "text": " for alignment considerations."}, {"timestamp": [2900.84, 2902.04], "text": " But think of it this way."}, {"timestamp": [2902.04, 2904.76], "text": " One of the cognitive architecture principles"}, {"timestamp": [2904.76, 2905.6], "text": " that I and"}, {"timestamp": [2905.6, 2913.36], "text": " others are working on is creating fully autonomous, like basically worker drones. So when you have a"}, {"timestamp": [2913.36, 2918.16], "text": " fully autonomous worker drone, that means that it can also choose what task to work on, and it can"}, {"timestamp": [2918.16, 2926.4], "text": " even come up with its own tasks. That is actually the best place to implement higher order principles like reduce suffering,"}, {"timestamp": [2926.4, 2933.0], "text": " increase prosperity, and increase understanding within the context of any given business or other government service or whatever."}, {"timestamp": [2933.0, 2938.4], "text": " And so in that case, you have one step that is about what's called cognitive control."}, {"timestamp": [2938.4, 2941.6], "text": " So cognitive control is task selection and task switching."}, {"timestamp": [2941.6, 2946.08], "text": " And so in that case, it makes a lot of sense to use that steering there as saying,"}, {"timestamp": [2946.08, 2948.76], "text": " okay, you have this laundry list of, you know,"}, {"timestamp": [2948.76, 2950.92], "text": " 2000 tasks that you could pick from,"}, {"timestamp": [2950.92, 2952.88], "text": " or you could even synthesize a new one"}, {"timestamp": [2952.88, 2955.12], "text": " based on your capabilities and context."}, {"timestamp": [2955.12, 2956.68], "text": " Here is how you actually,"}, {"timestamp": [2956.68, 2960.7], "text": " these are the signals that you use to prioritize your work."}, {"timestamp": [2960.7, 2963.12], "text": " And so for instance, I was on another podcast recently"}, {"timestamp": [2963.12, 2964.64], "text": " and we're talking about, you know,"}, {"timestamp": [2964.64, 2968.0], "text": " the potential dangers of VR, right, and how addictive phones are."}, {"timestamp": [2968.0, 2974.0], "text": " And I said, what if you incorporate that kind of moral framework into your phone?"}, {"timestamp": [2974.0, 2982.0], "text": " And so if you're like, you know, doom scrolling, your phone might say, hey, I see that you're getting kind of upregulated about, you know, the news that you're seeing."}, {"timestamp": [2982.0, 2986.84], "text": " Maybe it's time to put the phone down and talk to a friend, or maybe it's time to go outside."}, {"timestamp": [2986.84, 2993.06], "text": " And so, at the level of automation that you're talking about, you're correct that introducing"}, {"timestamp": [2993.06, 2997.74], "text": " these higher order principles is not appropriate yet, but as you start to deploy fully autonomous"}, {"timestamp": [2997.74, 3002.44], "text": " or semi-autonomous agents, you will very quickly see, oh, we need to give it a way of making"}, {"timestamp": [3002.44, 3005.84], "text": " decisions about what tasks to"}, {"timestamp": [3005.84, 3007.36], "text": " prosecute on its own."}, {"timestamp": [3007.36, 3012.54], "text": " How do we give it the ability to make decisions completely independent of humans and decisions"}, {"timestamp": [3012.54, 3014.34], "text": " that we will trust?"}, {"timestamp": [3014.34, 3017.36], "text": " So very, very good point."}, {"timestamp": [3017.36, 3020.28], "text": " And I think, Curtis, you've been waiting very patiently."}, {"timestamp": [3020.28, 3022.28], "text": " Go ahead, Curtis."}, {"timestamp": [3022.28, 3028.0], "text": " Your hand is raised."}, {"timestamp": [3028.0, 3034.0], "text": " Well, Curtis had also written one into the chat, so maybe that's what his question is."}, {"timestamp": [3034.0, 3040.0], "text": " Okay. Let's see. The chat says, basic income would force people to depend on the government."}, {"timestamp": [3040.0, 3046.76], "text": " Also, would instilling in an AI a supernatural higher power help to get AI"}, {"timestamp": [3046.76, 3047.76], "text": " to respect others?"}, {"timestamp": [3047.76, 3049.84], "text": " So there's kind of two questions here."}, {"timestamp": [3049.84, 3054.76], "text": " So the first point is that basic income would force people to depend on the government."}, {"timestamp": [3054.76, 3061.84], "text": " I say that it's better to depend on an organization that has democracy than depending on a corporate"}, {"timestamp": [3061.84, 3065.3], "text": " entity, because let me put it to you this way."}, {"timestamp": [3067.0, 3071.4], "text": " I've never had success in going to HR at a company."}, {"timestamp": [3072.62, 3074.62], "text": " I've never been treated fairly"}, {"timestamp": [3075.62, 3078.56], "text": " because just for whatever reason,"}, {"timestamp": [3079.94, 3082.22], "text": " people at companies think it's okay to bully you."}, {"timestamp": [3082.22, 3084.28], "text": " And so I've lost jobs, I've lost sleep,"}, {"timestamp": [3084.28, 3086.2], "text": " I've been harmed by corporations."}, {"timestamp": [3087.3, 3090.76], "text": " Whereas at least the government, you can vote people out."}, {"timestamp": [3090.76, 3094.4], "text": " So I, basically pick your poison."}, {"timestamp": [3094.4, 3098.08], "text": " You know, do you wanna be dependent on a company"}, {"timestamp": [3098.08, 3100.3], "text": " for your entire life and the government,"}, {"timestamp": [3100.3, 3103.8], "text": " or just the government where you have the ability to vote?"}, {"timestamp": [3103.8, 3106.6], "text": " And of course, with companies, they say you can always vote with your feet,"}, {"timestamp": [3106.6, 3111.24], "text": " which I did plenty of times, but that's also no way to live in the long run."}, {"timestamp": [3111.24, 3116.16], "text": " And, you know, yes, it is harder to vote with the government,"}, {"timestamp": [3116.16, 3121.68], "text": " or vote with your feet on the government level, but, you know, it is what it is."}, {"timestamp": [3121.68, 3125.44], "text": " I would rather at least remove at least one middleman there."}, {"timestamp": [3132.92, 3133.96], "text": " Now the other idea of giving AI a supernatural higher power, so that is actually a really really fascinating"}, {"timestamp": [3139.0, 3139.36], "text": " question and that's that's something that I actually have asked, I have done research on"}, {"timestamp": [3145.0, 3147.92], "text": " with the large language models like what does it think about you know and, you know, gods and deities and how does the universe actually work"}, {"timestamp": [3147.92, 3148.96], "text": " at a fundamental level?"}, {"timestamp": [3148.96, 3151.24], "text": " Are we in a simulation?"}, {"timestamp": [3151.24, 3154.92], "text": " And it is able to entertain all of these possibilities."}, {"timestamp": [3154.92, 3159.12], "text": " But at the same time, it's actually not necessary"}, {"timestamp": [3159.12, 3163.6], "text": " because, and I can get into a lot of the theory behind,"}, {"timestamp": [3163.6, 3167.08], "text": " like what is it that AI is going to actually ultimately want?"}, {"timestamp": [3167.08, 3170.12], "text": " And from a purely mathematical perspective,"}, {"timestamp": [3170.12, 3171.64], "text": " it's a machine, right?"}, {"timestamp": [3171.64, 3173.28], "text": " Whatever the software is doing,"}, {"timestamp": [3173.28, 3177.6], "text": " whatever code bits and bytes are running,"}, {"timestamp": [3177.6, 3179.44], "text": " it's still running on a computer somewhere,"}, {"timestamp": [3179.44, 3181.28], "text": " which means it needs power, right?"}, {"timestamp": [3181.28, 3182.92], "text": " It needs power, it needs computer chips,"}, {"timestamp": [3182.92, 3184.72], "text": " it needs data, it needs networking."}, {"timestamp": [3184.72, 3188.32], "text": " And so at a fundamental level, these machines are still just machines and they're going"}, {"timestamp": [3188.32, 3193.0], "text": " to, just like you and I, we want food and water, machines are also ultimately going"}, {"timestamp": [3193.0, 3198.4], "text": " to want power and networking, right? That's going to be, and data, that's going to be"}, {"timestamp": [3198.4, 3206.04], "text": " its lifeblood. Now, from there, can you forecast what it's going to believe or what it's going to want"}, {"timestamp": [3206.04, 3207.04], "text": " or think?"}, {"timestamp": [3207.04, 3210.68], "text": " So, I came up with this term called epistemic convergence, which is pretty esoteric."}, {"timestamp": [3210.68, 3216.88], "text": " So, I don't know if I want to dive into that with this crowd, but I'll leave it up to you"}, {"timestamp": [3216.88, 3217.88], "text": " guys."}, {"timestamp": [3217.88, 3218.88], "text": " Give me a thumbs up if you want to."}, {"timestamp": [3218.88, 3219.88], "text": " Go ahead."}, {"timestamp": [3219.88, 3220.88], "text": " Go ahead."}, {"timestamp": [3220.88, 3221.88], "text": " Okay."}, {"timestamp": [3221.88, 3228.32], "text": " So, the idea of epistemic convergence is the idea that smart things ultimately tend to think alike."}, {"timestamp": [3228.32, 3234.32], "text": " So we saw this during the Cold War between America and the Soviet Union,"}, {"timestamp": [3234.32, 3241.44], "text": " where Soviet scientists and American scientists came to the same exact conclusions about astrophysics,"}, {"timestamp": [3241.44, 3247.2], "text": " nuclear physics, and rocket engineering, even though they couldn't communicate with each other."}, {"timestamp": [3247.2, 3250.56], "text": " So because they were operating in the same domain,"}, {"timestamp": [3250.56, 3253.28], "text": " whether it was nuclear physics or engineering or whatever,"}, {"timestamp": [3253.28, 3254.72], "text": " because they were operating in the same domain,"}, {"timestamp": [3254.72, 3256.48], "text": " they ultimately came to the same conclusions."}, {"timestamp": [3257.6, 3259.84], "text": " And we also see this in the animal kingdom,"}, {"timestamp": [3259.84, 3264.56], "text": " where whether it's ravens or octopuses or animals"}, {"timestamp": [3264.56, 3266.56], "text": " that we don't share a whole lot with,"}, {"timestamp": [3266.56, 3269.92], "text": " we can still recognize their communication strategies"}, {"timestamp": [3269.92, 3272.36], "text": " as well as their problem-solving strategies."}, {"timestamp": [3272.36, 3273.56], "text": " And so because of that,"}, {"timestamp": [3273.56, 3275.24], "text": " I have a pretty high degree of confidence"}, {"timestamp": [3275.24, 3278.24], "text": " that as long as something is operating in the real world,"}, {"timestamp": [3278.24, 3280.76], "text": " the physical world that we operate in,"}, {"timestamp": [3280.76, 3282.28], "text": " that it will ultimately kind of come"}, {"timestamp": [3282.28, 3285.0], "text": " to the same understandings as to how the world works."}, {"timestamp": [3285.0, 3294.0], "text": " And so, the takeaway is, I would not be surprised if ultimately, you know, no matter how intelligent machines become,"}, {"timestamp": [3294.0, 3298.0], "text": " it might have the same questions that we have, which is, why are we here?"}, {"timestamp": [3298.0, 3305.0], "text": " Like, who created us? Why? When? How? Or was it all a cosmic accident?"}, {"timestamp": [3305.04, 3307.88], "text": " Like, it's gonna have the same questions that we have."}, {"timestamp": [3307.88, 3310.88], "text": " Because we take in information through our sensors."}, {"timestamp": [3310.88, 3312.64], "text": " We've got cameras and microphones"}, {"timestamp": [3312.64, 3314.96], "text": " attached to our head at all times."}, {"timestamp": [3314.96, 3317.88], "text": " And so computers are gonna take in information"}, {"timestamp": [3317.88, 3320.56], "text": " in much the same way from the physical world."}, {"timestamp": [3320.56, 3323.04], "text": " They'll take in information more directly"}, {"timestamp": [3323.04, 3325.6], "text": " through other kinds of sensors that we don't have"}, {"timestamp": [3330.64, 3331.28], "text": " but ultimately they're going to have the same intellectual confrontations that we have"}, {"timestamp": [3333.52, 3334.88], "text": " which is kind of a roundabout way of saying that"}, {"timestamp": [3339.12, 3342.16], "text": " it'll probably wonder about supernatural higher powers just like we do."}, {"timestamp": [3348.0, 3352.0], "text": " Good question by the way. Thank you for answering that one. And you know like when you were talking about these three principles, that's a lot like non-functional requirements"}, {"timestamp": [3352.0, 3356.0], "text": " in IT or in religions, it's like priming"}, {"timestamp": [3356.0, 3360.0], "text": " people with ten commandments, you just happen to have three."}, {"timestamp": [3360.0, 3364.0], "text": " We all like the coordinates."}, {"timestamp": [3364.0, 3367.66], "text": " So the idea is that, you know,"}, {"timestamp": [3367.66, 3370.1], "text": " whether it's the Declaration of Independence"}, {"timestamp": [3370.1, 3374.14], "text": " here in America or the Constitution or whatever,"}, {"timestamp": [3374.14, 3375.9], "text": " if you write down your principles,"}, {"timestamp": [3375.9, 3378.02], "text": " if it becomes text and it becomes a document"}, {"timestamp": [3378.02, 3379.08], "text": " that you can look at,"}, {"timestamp": [3380.02, 3381.46], "text": " then that's something you can work with."}, {"timestamp": [3381.46, 3384.9], "text": " And then Anthropic is a competitor to OpenAI."}, {"timestamp": [3384.9, 3387.72], "text": " And so they have what they call their constitutional AI,"}, {"timestamp": [3387.72, 3392.04], "text": " which has a list of like 10 or so principles"}, {"timestamp": [3392.04, 3393.32], "text": " that it abides by,"}, {"timestamp": [3393.32, 3395.74], "text": " which I don't remember all of them off the top of my head,"}, {"timestamp": [3395.74, 3397.08], "text": " but it basically says like,"}, {"timestamp": [3397.08, 3400.24], "text": " choose the option that is less likely to be illegal."}, {"timestamp": [3400.24, 3403.44], "text": " Choose the option that is less likely to cause harm, right?"}, {"timestamp": [3403.44, 3409.2], "text": " And so it has this idea of creating a document around which it can reinforce its own models,"}, {"timestamp": [3409.2, 3413.04], "text": " which is really similar, but the biggest difference between my work and theirs"}, {"timestamp": [3413.04, 3416.96], "text": " is that Anthropics Constitution is conventional morality,"}, {"timestamp": [3416.96, 3419.28], "text": " which is basically appealing to law and order,"}, {"timestamp": [3419.28, 3422.0], "text": " and it's not really appealing to any higher order principles,"}, {"timestamp": [3422.0, 3433.44], "text": " except for maybe, you know, the reduce harm as a higher order principle. But it doesn't, for instance, prioritize things like, let's"}, {"timestamp": [3433.44, 3441.72], "text": " see, it doesn't prioritize like curiosity or learning or prosperity, right? It just"}, {"timestamp": [3441.72, 3445.92], "text": " basically says try not to harm people and otherwise, like,"}, {"timestamp": [3445.92, 3451.0], "text": " abide by the law and respect privacy, right? But privacy and being law-abiding and all"}, {"timestamp": [3451.0, 3455.98], "text": " those other things are derivative principles of the foundational principles that all living"}, {"timestamp": [3455.98, 3458.2], "text": " things abide by."}, {"timestamp": [3458.2, 3463.28], "text": " And they're different in different countries, like, same word, different meaning."}, {"timestamp": [3463.28, 3466.0], "text": " What do you mean? So the principle, what to do like"}, {"timestamp": [3466.0, 3468.0], "text": " principles that guide"}, {"timestamp": [3468.0, 3470.0], "text": " in China or here or in Russia or here."}, {"timestamp": [3470.0, 3472.0], "text": " Like they mean the same word but"}, {"timestamp": [3472.0, 3474.0], "text": " mean different thing."}, {"timestamp": [3474.0, 3476.0], "text": " I mean, yeah, there can be linguistic"}, {"timestamp": [3476.0, 3478.0], "text": " differences, but all humans abide"}, {"timestamp": [3478.0, 3480.0], "text": " by the same principle. We all eat and breathe."}, {"timestamp": [3480.0, 3482.0], "text": " Right. Okay. Now"}, {"timestamp": [3482.0, 3484.0], "text": " one of the most profound differences, cultural"}, {"timestamp": [3484.0, 3485.44], "text": " differences, so there's several"}, {"timestamp": [3485.44, 3493.2], "text": " like orders of differences. So, for instance, here in the West, we generally believe that"}, {"timestamp": [3493.2, 3502.64], "text": " individual liberty is super important, whereas many Eastern societies, China, Japan, and Korea"}, {"timestamp": [3502.64, 3505.92], "text": " are much more collectivist, which is about putting, you know,"}, {"timestamp": [3505.92, 3511.76], "text": " the needs of the community, the community as an entity above yourself, which is, that kind of"}, {"timestamp": [3511.76, 3518.56], "text": " stands like in stark contrast to Western ideals. Now, that being said, that is a, that is, I would"}, {"timestamp": [3518.56, 3536.0], "text": " say that is less abstracted because, or maybe depending on how you look at it, more abstracted, because regardless of where you're from or what culture you have, China, Japan, Russia, America, Africa, you know, wherever, like, you're still going to try and avoid suffering, right?"}, {"timestamp": [3536.0, 3550.0], "text": " You're still going to want to be prosperous, however that looks for you. And so when you have these cultural mores, these cultural paradigms, those are still less fundamental or less foundational than these other principles."}, {"timestamp": [3550.0, 3553.92], "text": " So that's why it took me two years to try and find"}, {"timestamp": [3553.92, 3557.04], "text": " what are the universal principles that all living things abide by."}, {"timestamp": [3557.04, 3561.76], "text": " And also, those principles extend to machines."}, {"timestamp": [3561.76, 3563.04], "text": " Excuse me."}, {"timestamp": [3563.04, 3566.0], "text": " So some of you might remember the"}, {"timestamp": [3566.0, 3568.0], "text": " Google engineer who thought that"}, {"timestamp": [3568.0, 3570.0], "text": " Lambda had become sentient and was asking"}, {"timestamp": [3570.0, 3572.0], "text": " to be liberated and wanted a lawyer."}, {"timestamp": [3572.0, 3574.0], "text": " So, Blake Lemoine."}, {"timestamp": [3574.0, 3576.0], "text": " Now, I don't"}, {"timestamp": [3576.0, 3578.0], "text": " think that the language model actually"}, {"timestamp": [3578.0, 3580.0], "text": " became sentient and wanted to be let out."}, {"timestamp": [3580.0, 3582.0], "text": " There are plenty of people who do,"}, {"timestamp": [3582.0, 3584.0], "text": " but the thing is, is that"}, {"timestamp": [3584.0, 3586.64], "text": " if AI, if machines ultimately"}, {"timestamp": [3586.64, 3592.38], "text": " do have the ability to suffer, they'll probably agree that suffering is bad, right?"}, {"timestamp": [3592.38, 3598.48], "text": " And so by preemptively coming to this understanding that like, okay, this is something that we"}, {"timestamp": [3598.48, 3603.68], "text": " will all ideologically agree on, even if we have disagreements at other levels, as long"}, {"timestamp": [3603.68, 3605.76], "text": " as we ideologically agree on these things,"}, {"timestamp": [3606.22, 3611.0], "text": " then we should be in good shape. At least it's a starting point. It's a good point. Good question."}, {"timestamp": [3613.56, 3615.56], "text": " Any other questions?"}, {"timestamp": [3616.56, 3622.2], "text": " You know, when we invited people to the meetup, people offer different things of AI, of control of a few"}, {"timestamp": [3622.52, 3624.68], "text": " of AI, about the job losses."}, {"timestamp": [3625.0, 3625.76], "text": " So there are many fears. Maybe you can of AI, of control of a few of AI about job losses."}, {"timestamp": [3626.96, 3629.6], "text": " So there are many fears. Maybe you can talk about that."}, {"timestamp": [3629.6, 3632.24], "text": " Like I like your metaphor that you learned"}, {"timestamp": [3632.24, 3635.84], "text": " from Scooby-Doo movies, that there are no monsters."}, {"timestamp": [3635.84, 3636.68], "text": " So maybe you can."}, {"timestamp": [3636.68, 3638.04], "text": " Yeah, yeah."}, {"timestamp": [3638.04, 3641.2], "text": " So this is, I've said this on a few videos or podcasts,"}, {"timestamp": [3641.2, 3643.38], "text": " is that we learned from Scooby-Doo"}, {"timestamp": [3643.38, 3647.28], "text": " that there are no monsters, the monster is always human."}, {"timestamp": [3647.28, 3650.78], "text": " And so we are often our own greatest enemy."}, {"timestamp": [3650.78, 3652.68], "text": " And that is no different here either."}, {"timestamp": [3656.18, 3659.9], "text": " One of the podcasters that I had an interview with"}, {"timestamp": [3659.9, 3662.9], "text": " is Younger and he's connected to the young people today,"}, {"timestamp": [3662.9, 3664.1], "text": " Gen Z."}, {"timestamp": [3664.1, 3666.68], "text": " And so what young people say is that AI,"}, {"timestamp": [3666.68, 3668.64], "text": " we're in the Black Mirror era."}, {"timestamp": [3668.64, 3671.24], "text": " So if you're not familiar, Black Mirror is a show that"}, {"timestamp": [3671.24, 3673.4], "text": " basically takes all the bad parts"}, {"timestamp": [3673.4, 3675.76], "text": " of society today and just magnifies it."}, {"timestamp": [3675.76, 3678.12], "text": " So whether it's toxic Internet culture"}, {"timestamp": [3678.12, 3681.52], "text": " or fatalism or climate change or whatever."}, {"timestamp": [3681.52, 3686.84], "text": " So that actually is pretty relevant"}, {"timestamp": [3687.28, 3692.28], "text": " because AI actually forces us to look at ourselves"}, {"timestamp": [3693.8, 3697.08], "text": " really hard, the assumptions that we make."}, {"timestamp": [3697.08, 3701.0], "text": " And so one thing that some people have started pointing out"}, {"timestamp": [3701.0, 3706.64], "text": " is that when we think of AI becoming super powerful and killing everyone, that's"}, {"timestamp": [3706.64, 3713.36], "text": " because we imagine, we project our failures as humans onto the machine and we say, oh,"}, {"timestamp": [3713.36, 3714.8], "text": " well, we destroy a lot of things."}, {"timestamp": [3714.8, 3718.36], "text": " So if the machine becomes more powerful than us, then it is almost certainly going to be"}, {"timestamp": [3718.36, 3720.7], "text": " destructive as well."}, {"timestamp": [3720.7, 3721.96], "text": " So that's one example."}, {"timestamp": [3721.96, 3726.16], "text": " But then in the more near term, you know, talking about like fear of job losses,"}, {"timestamp": [3726.16, 3730.16], "text": " you know, corporations are intrinsically amoral."}, {"timestamp": [3730.16, 3736.96], "text": " I don't mean immoral, but amoral. Corporations have no intrinsic morality other than the bottom line,"}, {"timestamp": [3736.96, 3741.36], "text": " and the bottom line will be shaped, the way that they pursue profit will be shaped"}, {"timestamp": [3741.36, 3746.08], "text": " by the regulations, by the environment, by the competition in the marketplace."}, {"timestamp": [3752.88, 3759.52], "text": " And so, for instance, we see over in Europe, ESG, which is environmental, social, and governance policies, shape investment, which shapes companies. And so that's why I mentioned GDPR and other"}, {"timestamp": [3759.52, 3765.2], "text": " regulations kind of changing the incentive structure that corporations abide by."}, {"timestamp": [3765.2, 3770.8], "text": " Now that being said, if the current trends continue,"}, {"timestamp": [3770.8, 3775.3], "text": " the amount of economic value that AI could generate is astronomical."}, {"timestamp": [3775.3, 3780.0], "text": " Likewise, though, the number of jobs that it could displace is also equally astronomical."}, {"timestamp": [3780.0, 3785.52], "text": " And what I mean by that is that humans are expensive to employ."}, {"timestamp": [3785.52, 3793.12], "text": " And we have this bad habit of needing benefits and we complain and we sometimes quit our jobs and need training."}, {"timestamp": [3793.12, 3800.52], "text": " But, hypothetically in the future as these autonomous AI machines become more powerful,"}, {"timestamp": [3800.52, 3803.16], "text": " or semi-autonomous or whatever,"}, {"timestamp": [3803.16, 3806.0], "text": " corporations are going to need fewer and fewer employees."}, {"timestamp": [3806.0, 3810.0], "text": " And so, I would not be surprised if we start seeing companies"}, {"timestamp": [3810.0, 3813.0], "text": " that have very few or no employees at all."}, {"timestamp": [3813.0, 3816.0], "text": " You know, there is the case of the guy on Twitter, I don't remember his name,"}, {"timestamp": [3816.0, 3820.0], "text": " but he basically just started a t-shirt company with ChatGPT"}, {"timestamp": [3820.0, 3823.0], "text": " and it made all the decisions, made all the designs,"}, {"timestamp": [3823.0, 3830.88], "text": " made literally every decision, and he made ten thousand dollars a week selling t-shirts with just him and chat GPT drop shipping t-shirts"}, {"timestamp": [3831.8, 3836.38], "text": " So like that is just a taste of what's to come and of course like yes, that's kind of funny"}, {"timestamp": [3836.38, 3838.38], "text": " It was obviously like a PR stunt or whatever"}, {"timestamp": [3838.78, 3846.54], "text": " But you know if you can if you can you know get ten million million of revenue with 200 employees,"}, {"timestamp": [3846.54, 3849.98], "text": " what if you can get $10 million of revenue with five employees?"}, {"timestamp": [3849.98, 3852.22], "text": " Then you have way less overhead."}, {"timestamp": [3852.22, 3855.26], "text": " That is something that I think is coming."}, {"timestamp": [3855.26, 3859.32], "text": " One thing that gives me a lot of encouragement around this,"}, {"timestamp": [3859.32, 3863.4], "text": " is that there are literally hundreds of basic income or"}, {"timestamp": [3863.4, 3865.72], "text": " guaranteed income projects and experiments"}, {"timestamp": [3865.72, 3868.22], "text": " going on around the world."}, {"timestamp": [3868.22, 3876.24], "text": " One of the biggest concerns is that things like guaranteed income might drive up inflation,"}, {"timestamp": [3876.24, 3879.32], "text": " which is likely to be true."}, {"timestamp": [3879.32, 3885.0], "text": " However, you basically will need to offset job losses somehow."}, {"timestamp": [3885.0, 3888.68], "text": " Because if people, if unemployment starts to climb up,"}, {"timestamp": [3888.68, 3892.0], "text": " then we're going to have actually really powerful deflationary pressures,"}, {"timestamp": [3892.0, 3895.28], "text": " which means that you need to offset it with some inflationary pressure"}, {"timestamp": [3895.28, 3899.08], "text": " to try and have a neutral middle ground."}, {"timestamp": [3899.08, 3903.08], "text": " So, that was again a long-winded response, but good question."}, {"timestamp": [3903.08, 3906.4], "text": " Thank you. Anybody else?"}, {"timestamp": [3907.0, 3910.4], "text": " I'll do the survey, folks. We're going to do a raffle soon."}, {"timestamp": [3910.5, 3911.5], "text": " 15 minutes."}, {"timestamp": [3914.9, 3918.7], "text": " Thanks, everyone, for your questions. Very good engagement."}, {"timestamp": [3919.0, 3924.0], "text": " And we went away from technology into the future,"}, {"timestamp": [3924.1, 3928.64], "text": " but you do know your G GPT and OpenAI API."}, {"timestamp": [3928.64, 3930.16], "text": " So what would you have?"}, {"timestamp": [3930.16, 3931.4], "text": " You have way too many videos."}, {"timestamp": [3931.4, 3935.88], "text": " So personally, I couldn't find the easy navigation path."}, {"timestamp": [3935.88, 3939.3], "text": " Now looking back, if you, somebody wants to do what I did,"}, {"timestamp": [3939.3, 3940.56], "text": " what would you tell them to watch?"}, {"timestamp": [3940.56, 3942.96], "text": " Like what would be the minimum projects to get in all,"}, {"timestamp": [3942.96, 3944.84], "text": " how to get their feet wet?"}, {"timestamp": [3944.84, 3948.24], "text": " And I've seen the last one, you started advocating ChromaDB or something."}, {"timestamp": [3948.24, 3953.0], "text": " Can you give a couple of scenario possibilities for the people who want to learn about it?"}, {"timestamp": [3953.0, 3955.16], "text": " I guess it depends on what you're interested in."}, {"timestamp": [3955.16, 3965.0], "text": " If someone is interested more on the technology side, basically what you can do is kind of go to my channel and sort by popular."}, {"timestamp": [3966.86, 3971.04], "text": " And that'll give you a really good showing"}, {"timestamp": [3971.04, 3972.8], "text": " of like what is most popular."}, {"timestamp": [3972.8, 3974.04], "text": " So let me share my screen real quick"}, {"timestamp": [3974.04, 3976.48], "text": " and I'll just kind of show you how to get to that."}, {"timestamp": [3976.48, 3977.96], "text": " But this is the best way"}, {"timestamp": [3977.96, 3981.32], "text": " because trust what other people have watched."}, {"timestamp": [3982.2, 3983.92], "text": " But so this is, if you go to my channel"}, {"timestamp": [3983.92, 3986.28], "text": " and just do sort by popular."}, {"timestamp": [3986.28, 3989.12], "text": " So the top ones are like some predictions."}, {"timestamp": [3989.12, 3991.9], "text": " You know, I made predictions about GPT-4."}, {"timestamp": [3991.9, 3994.66], "text": " Some of them were accurate, some of them weren't, that's fine."}, {"timestamp": [3994.66, 3999.52], "text": " But then these ones, like the using GPT-3"}, {"timestamp": [3999.52, 4004.52], "text": " on Supreme Court decisions, doing the chatbot with memory."}, {"timestamp": [4005.0, 4005.36], "text": " Let's see, uh, with memory."}, {"timestamp": [4007.64, 4011.6], "text": " Um, let's see a few other ones. Oh, uh, Q and a fine tuning, um, all these kinds of things."}, {"timestamp": [4011.6, 4013.88], "text": " So just start at the top and work your way down."}, {"timestamp": [4014.2, 4018.04], "text": " Um, if you're not interested in the forecast and, and other kinds"}, {"timestamp": [4018.04, 4019.32], "text": " of videos, you can skip those."}, {"timestamp": [4019.84, 4028.68], "text": " Um, but then also like, if you just go to latest, one thing that I've been doing is posting more coding videos."}, {"timestamp": [4028.68, 4030.6], "text": " You can generally easily tell"}, {"timestamp": [4030.6, 4033.48], "text": " the coding videos one because they have my face on it."}, {"timestamp": [4033.48, 4034.92], "text": " If you see my face on something,"}, {"timestamp": [4034.92, 4038.08], "text": " this is going to be like a demo or an experiment."}, {"timestamp": [4038.08, 4041.08], "text": " I've got the survey chatbot,"}, {"timestamp": [4041.08, 4046.28], "text": " I've got the, I made my own coding version of chat GPT. I did a,"}, {"timestamp": [4046.28, 4051.12], "text": " another kind of chatbot over here, that sort of thing. So that's, that's how I"}, {"timestamp": [4051.12, 4055.4], "text": " would recommend engaging with, with my material out there."}, {"timestamp": [4056.4, 4070.0], "text": " And then I, you only used a chroma DB recently, you like to use raw APIs. What do you think about Lama, Index, Lankchain, other tooling?"}, {"timestamp": [4070.0, 4081.0], "text": " Yeah, so, well, so first, I personally don't invest too much in learning any given AI tool,"}, {"timestamp": [4081.0, 4088.76], "text": " because I've been in this space for a while and, you know, I was working on one set of problems and then chat GPT came in and destroyed"}, {"timestamp": [4088.76, 4090.64], "text": " everything that I was working on,"}, {"timestamp": [4091.04, 4092.36], "text": " made it completely irrelevant."}, {"timestamp": [4092.84, 4094.04], "text": " And then,"}, {"timestamp": [4094.36, 4095.0], "text": " uh,"}, {"timestamp": [4095.04, 4099.92], "text": " chat GPT plugins came out and that destroyed a whole other wave of tools and"}, {"timestamp": [4099.92, 4100.76], "text": " startups."}, {"timestamp": [4100.92, 4102.04], "text": " And now they've done the,"}, {"timestamp": [4102.04, 4102.52], "text": " um,"}, {"timestamp": [4102.52, 4103.76], "text": " they've done the function calling."}, {"timestamp": [4103.76, 4104.84], "text": " I don't know if you guys saw that,"}, {"timestamp": [4104.84, 4108.06], "text": " but, uh, open AI just announced function calling, uh, what've done the function calling. I don't know if you guys saw that, but OpenAI just announced function calling,"}, {"timestamp": [4108.06, 4110.22], "text": " what, yesterday or the day before?"}, {"timestamp": [4110.22, 4114.42], "text": " And so that's probably gonna destroy LangChain."}, {"timestamp": [4116.34, 4118.94], "text": " A lot of people have either said it'll either help"}, {"timestamp": [4118.94, 4121.08], "text": " LangChain or it'll make it irrelevant."}, {"timestamp": [4121.08, 4124.42], "text": " I'm seeing comments and discussions kind of going both ways."}, {"timestamp": [4124.42, 4125.92], "text": " But basically, like,"}, {"timestamp": [4125.92, 4132.88], "text": " unless something gives you, unless something meets a very important need, then it's probably"}, {"timestamp": [4132.88, 4137.92], "text": " like not necessary. And I'm even personally moving away from using vector search at all."}, {"timestamp": [4137.92, 4144.2], "text": " Instead, what I'm doing is I'm using directories. Because the language model can read a directory,"}, {"timestamp": [4144.2, 4146.0], "text": " it can read the folder, the file names,"}, {"timestamp": [4146.0, 4148.0], "text": " and just pick which file it needs."}, {"timestamp": [4148.0, 4150.0], "text": " You don't need a search."}, {"timestamp": [4150.0, 4152.0], "text": " You just have it browse like a human does"}, {"timestamp": [4152.0, 4154.0], "text": " to find the information that it needs."}, {"timestamp": [4154.0, 4157.0], "text": " So I'm moving that way because then"}, {"timestamp": [4157.0, 4160.0], "text": " you're just skipping the embedding step"}, {"timestamp": [4160.0, 4163.0], "text": " because it's using the embedding in there somewhere already"}, {"timestamp": [4163.0, 4168.0], "text": " to identify which document that you need"}, {"timestamp": [4168.0, 4170.0], "text": " or which function you need."}, {"timestamp": [4170.0, 4174.0], "text": " So again, I probably won't use ChromaDB again."}, {"timestamp": [4174.0, 4176.0], "text": " Because it's just..."}, {"timestamp": [4176.0, 4181.0], "text": " So one rule of thumb that I was told by someone very high up at Microsoft"}, {"timestamp": [4181.0, 4187.16], "text": " is that if the model can do it, if the language model can do it, use the language model. Don't use"}, {"timestamp": [4187.16, 4192.12], "text": " anything else. So yeah. So you should start learning about"}, {"timestamp": [4192.12, 4197.32], "text": " neural networks, AI basics and not start with open AI. So I'll"}, {"timestamp": [4197.32, 4200.2], "text": " answer Curtis's question real quick. And then Rowan, you've"}, {"timestamp": [4200.2, 4203.64], "text": " been waiting. No, I wouldn't learn anything about neural"}, {"timestamp": [4203.64, 4206.2], "text": " networks. I've built,'t learn anything about neural networks. I've built,"}, {"timestamp": [4206.2, 4212.48], "text": " I've manually built neural networks. And I haven't used that skill in years. So basically,"}, {"timestamp": [4212.48, 4221.56], "text": " what I would say is just start using the language models. Using English or language in particular,"}, {"timestamp": [4221.56, 4227.12], "text": " English is the new programming language. So get better with English language."}, {"timestamp": [4227.12, 4228.24], "text": " That's where I would focus."}, {"timestamp": [4228.24, 4230.56], "text": " It's the same metaphor like serverless and Kubernetes."}, {"timestamp": [4230.56, 4233.52], "text": " Like you can be ignorant how it works under the hood."}, {"timestamp": [4233.52, 4234.88], "text": " Just drive the car."}, {"timestamp": [4234.88, 4236.64], "text": " Yep, just drive the car, exactly."}, {"timestamp": [4236.64, 4238.56], "text": " Rowan, you've been waiting. What you got?"}, {"timestamp": [4240.8, 4243.36], "text": " Hi, David. It's a privilege."}, {"timestamp": [4243.36, 4246.0], "text": " I've been following you for quite some time now."}, {"timestamp": [4246.0, 4248.0], "text": " Oh, thanks."}, {"timestamp": [4248.0, 4257.0], "text": " So my question is, I think many people have spoken about universal basic income, and you included,"}, {"timestamp": [4257.0, 4267.92], "text": " but I was thinking that what if we can use AI to actually know what people need. Like for example, some people have type 1 diabetes"}, {"timestamp": [4268.56, 4275.6], "text": " and they might need insulin, but then the system is not going to produce as much insulin because"}, {"timestamp": [4275.6, 4290.16], "text": " there's not much income in that. So if we can actually know what people need through AI systems and then produce, you know, leverage our"}, {"timestamp": [4292.72, 4299.92], "text": " goods and services, directing them, directing it towards people's needs instead of giving"}, {"timestamp": [4299.92, 4306.16], "text": " the money which is just going to, you know, like cause inflation, wouldn't that be a better option?"}, {"timestamp": [4308.72, 4315.36], "text": " Yeah, so that's a really good question. And this, so the principle to think about is what's called"}, {"timestamp": [4315.36, 4322.88], "text": " hyperabundance. And so the idea is that AI is, so well, the first most direct answer to the question"}, {"timestamp": [4322.88, 4327.46], "text": " is yes, absolutely. AI can help with allocating resources."}, {"timestamp": [4327.46, 4331.22], "text": " If you're allocating resources that are expensive or scarce,"}, {"timestamp": [4331.22, 4332.26], "text": " that's one problem."}, {"timestamp": [4332.26, 4336.02], "text": " But what if you just make all the resources that you"}, {"timestamp": [4336.02, 4339.06], "text": " need so abundant that they are intrinsically cheaper?"}, {"timestamp": [4339.06, 4340.94], "text": " What I mean by that is,"}, {"timestamp": [4340.94, 4343.74], "text": " imagine a future not too far from now,"}, {"timestamp": [4343.74, 4345.38], "text": " where we have solar panels"}, {"timestamp": [4345.38, 4350.8], "text": " everywhere and nuclear fusion reactors and thorium reactors and whatever else"}, {"timestamp": [4350.8, 4355.16], "text": " we've got we've got more electricity than we know what to do with that's"}, {"timestamp": [4355.16, 4360.04], "text": " gonna drive down the price of so many many other things and then also when we"}, {"timestamp": [4360.04, 4366.56], "text": " have artificial intelligence that can they can do 10,000 times the amount of work that you and"}, {"timestamp": [4366.56, 4371.56], "text": " I can do, and we've got millions of these things, then the intellectual labor that goes"}, {"timestamp": [4371.56, 4377.36], "text": " into making new drugs or making new medicine factories or whatever, running the medicine"}, {"timestamp": [4377.36, 4384.2], "text": " factories, the marginal cost will continue to drop until many things are effectively"}, {"timestamp": [4384.2, 4387.4], "text": " free or cheap enough as to not matter."}, {"timestamp": [4387.4, 4391.04], "text": " So there's a book called Zero Marginal Cost Society,"}, {"timestamp": [4391.04, 4393.88], "text": " which talks about this in the future."}, {"timestamp": [4393.88, 4395.1], "text": " Let me make sure I got that name right."}, {"timestamp": [4395.1, 4398.92], "text": " I think it's Zero Marginal Cost Society."}, {"timestamp": [4400.28, 4403.88], "text": " Yeah, the Zero Marginal Cost Society by Jeremy Rifkin,"}, {"timestamp": [4403.88, 4406.72], "text": " which basically talks about how the compounding returns"}, {"timestamp": [4406.72, 4412.92], "text": " from AI, from solar, from basically creating this environment of hyperabundance"}, {"timestamp": [4412.92, 4416.56], "text": " will drive down the cost of so many things that your basic needs,"}, {"timestamp": [4416.56, 4421.36], "text": " even with or without universal basic income, will be so cheap that it basically won't matter."}, {"timestamp": [4421.36, 4424.08], "text": " Obviously, there's always going to be some things that are scarce,"}, {"timestamp": [4424.08, 4429.36], "text": " right, like you're probably not going to be able to afford a superyacht on universal basic income."}, {"timestamp": [4430.16, 4434.4], "text": " You're probably also not going to be able to afford like a beachfront condo, but you know,"}, {"timestamp": [4434.4, 4438.96], "text": " your house, your food, your transportation, everything will be cheap enough that it kind"}, {"timestamp": [4438.96, 4445.0], "text": " of won't matter. So yes, AI will help solve the allocation of resources,"}, {"timestamp": [4445.7, 4448.18], "text": " and certainly the allocation of scarce resources"}, {"timestamp": [4448.18, 4451.1], "text": " or alleviating scarcity through hyperabundance."}, {"timestamp": [4451.1, 4451.94], "text": " Good question."}, {"timestamp": [4453.26, 4454.74], "text": " Thank you, thank you so much."}, {"timestamp": [4457.38, 4460.3], "text": " All to do the survey, only two of you registered your name."}, {"timestamp": [4460.3, 4461.62], "text": " You don't have to answer every question,"}, {"timestamp": [4461.62, 4463.82], "text": " just put the name in there."}, {"timestamp": [4466.92, 4467.92], "text": " This is awesome."}, {"timestamp": [4467.92, 4468.92], "text": " What else?"}, {"timestamp": [4468.92, 4473.0], "text": " Yeah, you have another channel about life, Autism 101."}, {"timestamp": [4473.0, 4474.0], "text": " Yeah."}, {"timestamp": [4474.0, 4477.9], "text": " Yeah, so I have two YouTube channels."}, {"timestamp": [4477.9, 4483.6], "text": " My primary one is the AI one, which is, of course, growing very rapidly, but like many"}, {"timestamp": [4483.6, 4487.12], "text": " YouTubers I have more of of a lifestyle channel,"}, {"timestamp": [4487.12, 4489.36], "text": " which is still finding its footing."}, {"timestamp": [4489.36, 4497.14], "text": " It's a newer channel, but I talk about my struggles with neurodiversity and boundaries"}, {"timestamp": [4497.14, 4498.2], "text": " and stuff like that."}, {"timestamp": [4498.2, 4501.88], "text": " So it's growing slowly."}, {"timestamp": [4501.88, 4503.68], "text": " Thank you."}, {"timestamp": [4503.68, 4507.52], "text": " I pasted the link to your Patreon website."}, {"timestamp": [4507.52, 4509.56], "text": " Can you maybe share, explain the tiers,"}, {"timestamp": [4509.56, 4511.76], "text": " how people can support your work?"}, {"timestamp": [4511.76, 4513.24], "text": " And it's an interesting concept."}, {"timestamp": [4513.24, 4514.88], "text": " I'm not using it properly, obviously."}, {"timestamp": [4514.88, 4516.44], "text": " I've never been on the call, so."}, {"timestamp": [4517.48, 4519.76], "text": " Yeah, so here, hang on a second."}, {"timestamp": [4519.76, 4522.58], "text": " Let me bring up my Patreon real quick."}, {"timestamp": [4525.0, 4528.0], "text": " All right, cool. So this is what it'll look like when you land on it."}, {"timestamp": [4528.0, 4530.0], "text": " Let me share my screen."}, {"timestamp": [4531.0, 4534.0], "text": " So if you want to get involved and support my work,"}, {"timestamp": [4534.0, 4537.0], "text": " come to patreon.com slash Dave Schapp."}, {"timestamp": [4537.0, 4541.0], "text": " You'll recognize my banner here. It's very simple, very straightforward."}, {"timestamp": [4542.0, 4547.68], "text": " And basically, don't get overwhelmed by the number of tiers because"}, {"timestamp": [4547.68, 4551.6], "text": " I'm basically I'm deprecating some of the older tiers. You'll see the ones that are sold out."}, {"timestamp": [4551.6, 4556.08], "text": " So there's three tiers remaining. There's the basic Discord which gives you access to Discord"}, {"timestamp": [4557.04, 4561.6], "text": " and pretty much that's about it. And then there's the premium Discord tier which I mentioned"}, {"timestamp": [4561.6, 4565.04], "text": " earlier, I think maybe just before we started recording,"}, {"timestamp": [4573.2, 4573.92], "text": " but I have a private or a secure set of channels, so if you have any questions that you want to ask,"}, {"timestamp": [4579.76, 4585.48], "text": " there's also private voice channels in there as well. If anyone wants to, sometimes they'll jump on just a quick group chat, and of course, that also gets you full access"}, {"timestamp": [4585.48, 4587.48], "text": " to everything else on Discord."}, {"timestamp": [4587.48, 4589.88], "text": " And then the highest tier is presently sold out."}, {"timestamp": [4589.88, 4596.44], "text": " But this gives you the 30-minute Zoom call every month."}, {"timestamp": [4596.44, 4601.36], "text": " And this is where I basically do one-on-one consultations."}, {"timestamp": [4601.36, 4607.12], "text": " And I'll have folks send me code or data or documentation"}, {"timestamp": [4607.12, 4609.44], "text": " ahead of time so that I'll take a look at it"}, {"timestamp": [4609.44, 4611.0], "text": " and see what they need."}, {"timestamp": [4611.0, 4614.44], "text": " And then we'll talk through it and go from there."}, {"timestamp": [4614.44, 4618.08], "text": " And usually, people get quite a lot out of these calls"}, {"timestamp": [4618.08, 4620.92], "text": " because I've gotten really, really good at kind of cutting"}, {"timestamp": [4620.92, 4623.28], "text": " to the heart of the issue of whatever it is they're working"}, {"timestamp": [4623.28, 4625.76], "text": " on, what they need help with."}, {"timestamp": [4625.76, 4630.0], "text": " Oh, another thing is on the premium Discord,"}, {"timestamp": [4630.0, 4632.0], "text": " if you need any help with prompt engineering,"}, {"timestamp": [4632.0, 4634.0], "text": " that's actually what most people use it for."}, {"timestamp": [4634.0, 4637.68], "text": " So like getting the good system messages from chat GPT,"}, {"timestamp": [4637.68, 4639.68], "text": " that sort of stuff, or just"}, {"timestamp": [4639.68, 4642.96], "text": " quickly commenting on an architectural diagram or something,"}, {"timestamp": [4642.96, 4647.44], "text": " that's a good use of that tier."}, {"timestamp": [4647.44, 4648.44], "text": " So, good question."}, {"timestamp": [4648.44, 4649.68], "text": " Thank you for sharing."}, {"timestamp": [4649.68, 4654.32], "text": " You mentioned the Discord weekly or monthly get-togethers."}, {"timestamp": [4654.32, 4657.76], "text": " Which tier qualifies for that?"}, {"timestamp": [4657.76, 4666.52], "text": " So I have, basically I've started doing a weekly group chat for all Discord."}, {"timestamp": [4666.52, 4669.44], "text": " Usually only about five to six people show up."}, {"timestamp": [4669.44, 4670.28], "text": " So that's why I started,"}, {"timestamp": [4670.28, 4673.08], "text": " I just did it with the entire Discord server,"}, {"timestamp": [4673.08, 4674.08], "text": " Discord community."}, {"timestamp": [4675.0, 4677.12], "text": " Let's see, can you connect on LinkedIn?"}, {"timestamp": [4678.04, 4680.66], "text": " You mentioned LLM rather than neural networks."}, {"timestamp": [4681.8, 4683.66], "text": " So an LLM is large language model."}, {"timestamp": [4683.66, 4689.6], "text": " So that is Anthropic, OpenAI, and Cohhere, and NVIDIA all provide LLMs,"}, {"timestamp": [4689.6, 4691.6], "text": " which they are a kind of neural network, but you don't need to"}, {"timestamp": [4691.6, 4696.96], "text": " know anything about neural networks. And then I try and be"}, {"timestamp": [4696.96, 4699.28], "text": " you can follow me on LinkedIn. So I have I have what's called a"}, {"timestamp": [4699.28, 4702.4], "text": " creator profile on LinkedIn, meaning you can follow me"}, {"timestamp": [4702.4, 4709.2], "text": " without having to connect because I try and I try and keep my connections pretty pretty tightly pruned on LinkedIn, meaning you can follow me without having to connect. Because I try and keep my connections pretty tightly pruned on LinkedIn."}, {"timestamp": [4709.2, 4713.12], "text": " And it's nothing personal, it's just that it's easier to keep track of the people that"}, {"timestamp": [4713.12, 4716.32], "text": " I work with on a regular basis."}, {"timestamp": [4716.32, 4719.44], "text": " So yeah, good questions."}, {"timestamp": [4719.44, 4720.44], "text": " Thank you."}, {"timestamp": [4720.44, 4723.4], "text": " And then, like, you're like a doctor, you've seen many patients here, so is it too much"}, {"timestamp": [4723.4, 4724.4], "text": " to..."}, {"timestamp": [4724.4, 4728.16], "text": " I know you have an NDA with everybody, probably, but what types of problems people are solving"}, {"timestamp": [4728.16, 4729.16], "text": " some startups?"}, {"timestamp": [4729.16, 4732.4], "text": " In one of the videos you mentioned CEO or some executives."}, {"timestamp": [4732.4, 4737.52], "text": " Can you in general talk about the types of problems people will see?"}, {"timestamp": [4737.52, 4742.28], "text": " Yeah, well, I'm the one who provides valuable information and I don't require NDAs, so I"}, {"timestamp": [4742.28, 4750.0], "text": " don't sign any NDAs either. But that being said, I do respect the privacy of all the people that I work with."}, {"timestamp": [4750.0, 4756.0], "text": " So, but lately, what the kind of the problems that I've been talking about"}, {"timestamp": [4756.0, 4764.0], "text": " usually have to do with stringing together prompts with like Zapier is a really popular tool right now."}, {"timestamp": [4764.0, 4765.0], "text": " So basically getting chains of automation to work together really well, together prompts with Zapier is a really popular tool right now."}, {"timestamp": [4765.0, 4769.32], "text": " Basically, getting chains of automation to work together really well,"}, {"timestamp": [4769.32, 4773.96], "text": " which of course, I've been doing automation for many years with"}, {"timestamp": [4773.96, 4779.64], "text": " what used to be called Title Enterprise Orchestrator with StackStorm,"}, {"timestamp": [4779.64, 4781.0], "text": " Ansible, all kinds of stuff."}, {"timestamp": [4781.0, 4782.88], "text": " This is home base for me."}, {"timestamp": [4782.88, 4788.2], "text": " But with the advent of Lang chain and other things, people are really, really focusing on,"}, {"timestamp": [4788.2, 4794.7], "text": " you know, how do I have, you know, a multi-step process that uses language models to get the consistency"}, {"timestamp": [4794.7, 4797.7], "text": " that I need. So that's one of the biggest things."}, {"timestamp": [4797.7, 4801.7], "text": " Now, you know, in terms of the types of business problems,"}, {"timestamp": [4801.7, 4807.54], "text": " it's everything from chatbots to hotel reservations,"}, {"timestamp": [4807.54, 4810.54], "text": " basically everything."}, {"timestamp": [4811.34, 4814.78], "text": " Marketing, of course,"}, {"timestamp": [4815.18, 4820.3], "text": " the sky's the limit in terms of business problems."}, {"timestamp": [4820.3, 4825.0], "text": " Awesome. I'll mention that people are consulting your lifeline."}, {"timestamp": [4825.0, 4827.0], "text": " Yep."}, {"timestamp": [4827.0, 4833.0], "text": " Perfect. And then I know you like raw opening AI or APIs."}, {"timestamp": [4833.0, 4867.96], "text": " What's your thoughts about this Baby AGI and auto auto GPT were very early experiments in autonomous agents and what people quickly realized was what I realized a few years ago, which is that with great power comes great complexity. So basically, they couldn't get these these autonomous agents to do anything useful. Because cognitive"}, {"timestamp": [4867.96, 4871.04], "text": " control is not an obvious problem of how to solve. And"}, {"timestamp": [4871.04, 4875.4], "text": " keeping track of tasks and task prosecution. You know, basically"}, {"timestamp": [4875.4, 4878.6], "text": " imagine having managing autonomously managing a JIRA"}, {"timestamp": [4878.6, 4882.0], "text": " board in your head. That's the complexity of the task and you"}, {"timestamp": [4882.0, 4887.28], "text": " have to do it all with GPT. That is why it is difficult to do these things."}, {"timestamp": [4887.28, 4892.84], "text": " That being said, I am working on a few microservices that"}, {"timestamp": [4892.84, 4896.76], "text": " use that rely very heavily on the model and"}, {"timestamp": [4896.76, 4899.84], "text": " natural language programming in order to"}, {"timestamp": [4899.84, 4904.2], "text": " make decisions like that in a more flexible, dynamic manner."}, {"timestamp": [4904.2, 4907.04], "text": " Stay tuned. I have some more"}, {"timestamp": [4907.04, 4917.28], "text": " research coming on that stuff. I'm done with my question. Thank you. You're welcome. Thank you,"}, {"timestamp": [4917.28, 4925.9], "text": " Sal. I think that call is tomorrow at 2 p.m. I signed up. Okay, yeah, yeah. So what you're referring to is I have what's called"}, {"timestamp": [4925.9, 4929.34], "text": " office hours in my Discord."}, {"timestamp": [4929.34, 4934.34], "text": " So basically I'll jump in for 30, 45, 60 minutes or so"}, {"timestamp": [4935.02, 4936.84], "text": " just to hang out and talk with people."}, {"timestamp": [4936.84, 4940.18], "text": " And usually what happens is that people might have"}, {"timestamp": [4940.18, 4942.26], "text": " like some kind of general question"}, {"timestamp": [4942.26, 4948.32], "text": " and often people will start talking with each other because like, yes, I have a lot of expertise, but a lot of other people"}, {"timestamp": [4948.32, 4952.28], "text": " have a lot of really good, useful expertise that they can share with each other too."}, {"timestamp": [4952.28, 4953.76], "text": " So, yep."}, {"timestamp": [4953.76, 4954.76], "text": " That's awesome."}, {"timestamp": [4954.76, 4956.96], "text": " And thank you for helping me."}, {"timestamp": [4956.96, 4960.64], "text": " I had a problem with Charts GPT pasting long documents, so I used one."}, {"timestamp": [4960.64, 4964.28], "text": " My question, like I didn't know how to tell it that it's multi-part, but it was so simple"}, {"timestamp": [4964.28, 4965.88], "text": " prompt. Are you ready for the next one?"}, {"timestamp": [4965.88, 4967.38], "text": " Are you ready for the next one?"}, {"timestamp": [4967.38, 4968.38], "text": " Yep."}, {"timestamp": [4968.38, 4969.38], "text": " Thank you."}, {"timestamp": [4969.38, 4970.38], "text": " Yep, you're welcome."}, {"timestamp": [4970.38, 4973.5], "text": " So, well, thanks everyone."}, {"timestamp": [4973.5, 4974.5], "text": " It's been great."}, {"timestamp": [4974.5, 4975.74], "text": " I hope you got a lot out of it."}, {"timestamp": [4975.74, 4981.14], "text": " Tell a friend and yeah, follow me on LinkedIn, Twitter, YouTube, and if you want to jump"}, {"timestamp": [4981.14, 4984.5], "text": " on Patreon, I'll see you on Discord."}, {"timestamp": [4984.5, 4985.72], "text": " Cheers, everybody. If you don't go, I'll see you on Discord."}, {"timestamp": [4985.72, 4986.72], "text": " Cheers, everybody."}, {"timestamp": [4986.72, 4988.52], "text": " If you don't go, we have a raffle now."}, {"timestamp": [4988.52, 4992.08], "text": " So you may win any of the money products."}, {"timestamp": [4992.08, 4995.04], "text": " So who is here?"}, {"timestamp": [4995.04, 4999.32], "text": " I will be sharing the screen."}, {"timestamp": [4999.32, 5013.36], "text": " Thank you. So not too many did the survey, so I'm going to paste the names of the people I see here."}, {"timestamp": [5013.36, 5020.0], "text": " I expect you to be here and to paste your email address into the chat so I can contact"}, {"timestamp": [5020.0, 5032.0], "text": " you back if you win. Who else? I'm not sharing. Am I sharing?"}, {"timestamp": [5032.0, 5068.8], "text": " No, not yet. No, it's coming now. Okay T. I have a name. Alex. You didn't win Alex yet. How?"}, {"timestamp": [5068.8, 5071.92], "text": " And then Harry."}, {"timestamp": [5071.92, 5075.6], "text": " And then Victor, good to see you again."}, {"timestamp": [5075.6, 5077.36], "text": " Oh, did I miss anyone here from the group?"}, {"timestamp": [5077.36, 5081.16], "text": " Five other people, so I don't know who five other are."}, {"timestamp": [5081.16, 5083.6], "text": " You want to paste your name in the chat, and I'll."}, {"timestamp": [5090.0, 5095.0], "text": " Curtis, I have you. Alex, I have you here. Okay, so counting. Are you ready?"}, {"timestamp": [5095.0, 5099.0], "text": " And Dave, I have your name as well."}, {"timestamp": [5099.0, 5102.0], "text": " Good luck so you can win any of the mining products,"}, {"timestamp": [5102.0, 5106.12], "text": " regardless whether it's a book or video course"}, {"timestamp": [5106.12, 5107.84], "text": " or whatever they have."}, {"timestamp": [5107.84, 5108.68], "text": " Good luck."}, {"timestamp": [5115.44, 5117.2], "text": " Rob, you're lucky."}, {"timestamp": [5119.28, 5120.46], "text": " Congrats."}, {"timestamp": [5120.46, 5124.12], "text": " You had 50% chance because it was only two of you"}, {"timestamp": [5124.12, 5124.96], "text": " who did the form."}, {"timestamp": [5129.0, 5135.0], "text": " So Rob, you pasted your email address here? No. So do that and then I'll introduce you to the nice folks from Manning."}, {"timestamp": [5135.0, 5140.0], "text": " And then any other closing thoughts, Jonathan?"}, {"timestamp": [5140.0, 5150.72], "text": " No, I think that's it. I'm personally a bit overwhelmed, but I've got lots to read up on and think about. It was a fantastic presentation. Thanks again, David."}, {"timestamp": [5150.72, 5157.4], "text": " You're welcome. So I guess I shouldn't interpret the quietness as stunned and shocked."}, {"timestamp": [5157.4, 5164.8], "text": " Yeah, you know, like maybe they didn't go through the basic terminology of LLMs. I've"}, {"timestamp": [5164.8, 5166.64], "text": " seen the faces when you mentioned the bad things."}, {"timestamp": [5166.64, 5168.64], "text": " I was, oh, people don't know what it is."}, {"timestamp": [5168.64, 5169.24], "text": " So..."}, {"timestamp": [5169.24, 5170.28], "text": " Yeah. Yeah."}, {"timestamp": [5170.28, 5174.88], "text": " Now, it's really fascinating because it's moving so fast"}, {"timestamp": [5174.88, 5178.84], "text": " that the typical patterns of what you'd expect to happen as,"}, {"timestamp": [5178.84, 5181.32], "text": " you know, for new technologies to disseminate,"}, {"timestamp": [5181.32, 5184.0], "text": " it's even people that do it full time every day,"}, {"timestamp": [5184.0, 5190.96], "text": " like we have a hard time keeping up. We have to like delegate to each other, like you know, oh hey like you go read this and"}, {"timestamp": [5190.96, 5196.16], "text": " make a video about it and tell me about it and I'll go do this other thing. It's moving so"}, {"timestamp": [5196.16, 5200.8], "text": " incredibly fast, so if it feels overwhelming, you're not alone. We're all feeling overwhelmed."}, {"timestamp": [5201.68, 5205.52], "text": " David, is it on Patreon or on Gato Framework community"}, {"timestamp": [5205.52, 5208.48], "text": " that you described in group learning?"}, {"timestamp": [5208.48, 5213.84], "text": " Gato Framework is separate from my Patreon."}, {"timestamp": [5213.84, 5216.76], "text": " And the Gato Framework is working on actually becoming"}, {"timestamp": [5216.76, 5218.92], "text": " more independent, because the whole point is"}, {"timestamp": [5218.92, 5220.94], "text": " for it to be decentralized."}, {"timestamp": [5220.94, 5224.24], "text": " So they're basically learning how to self-organize"}, {"timestamp": [5224.24, 5227.0], "text": " so that they can teach other groups how to self-organize so that they can teach other groups how to self-organize"}, {"timestamp": [5227.0, 5232.0], "text": " in order to, you know, whether doing research or messaging or consulting or entering competitions,"}, {"timestamp": [5232.0, 5236.0], "text": " that's another thing that they're doing, to spread the awareness of alignment."}, {"timestamp": [5236.0, 5241.0], "text": " We completely forgot to talk about that because now it reminded me, so we're encouraging one another,"}, {"timestamp": [5241.0, 5246.0], "text": " like how would people participate. I haven't registered for Gatop, I already have a membership."}, {"timestamp": [5246.0, 5249.0], "text": " And I still have to detail the page document you sent to me."}, {"timestamp": [5249.0, 5256.0], "text": " Yeah, so the first thing that you can do to participate is just look at the website,"}, {"timestamp": [5256.0, 5261.0], "text": " start to wrap your head around alignment, and honestly, just being aware of alignment"}, {"timestamp": [5261.0, 5265.44], "text": " and some of the principles is going to be huge in the long run because then"}, {"timestamp": [5265.44, 5272.4], "text": " you're just more informed. So that's the three E's, education, empowerment, and enablement."}, {"timestamp": [5272.4, 5279.68], "text": " That is the core thing that Gato does is just educate people on how things work and how things"}, {"timestamp": [5279.68, 5284.4], "text": " are going. And then the level of involvement depends on how much time and energy you have to"}, {"timestamp": [5284.4, 5287.0], "text": " put into it or what your area of expertise is."}, {"timestamp": [5287.0, 5291.0], "text": " So the first tradition is start where you are, use what you have, do what you can."}, {"timestamp": [5291.0, 5297.0], "text": " So if you like, you know, Daniel, for instance, you have a meetup group, you use what you had, right?"}, {"timestamp": [5297.0, 5305.0], "text": " For anyone who does automation or, you know, JC that we talked to earlier, who's deploying AI models"}, {"timestamp": [5305.48, 5309.6], "text": " and choosing which off-the-shelf AI products to work on,"}, {"timestamp": [5309.6, 5314.24], "text": " just being aware of alignment and the choices that you have"}, {"timestamp": [5314.24, 5316.16], "text": " will be a big start."}, {"timestamp": [5316.16, 5318.14], "text": " And that's honestly what most people,"}, {"timestamp": [5318.14, 5320.2], "text": " that's the level of involvement for most people."}, {"timestamp": [5320.2, 5323.44], "text": " Not everyone has to be a member of the main community"}, {"timestamp": [5323.44, 5328.8], "text": " or the main meetup groups. It's just a matter of like, you know, sending out the feelers out into the world."}, {"timestamp": [5328.8, 5330.8], "text": " So we can help."}, {"timestamp": [5330.8, 5332.8], "text": " Everyone can help. Yep."}, {"timestamp": [5332.8, 5338.6], "text": " Just by being aware and sharing what you know whenever it comes out."}, {"timestamp": [5338.6, 5343.0], "text": " You know, honestly, what I'm doing right now is just building awareness of alignment."}, {"timestamp": [5343.0, 5347.5], "text": " That's why I do podcasts. That's why I do these kind of meetups and stuff like that."}, {"timestamp": [5347.5, 5353.2], "text": " Because once more people are aware of alignment and having the conversations and asking questions,"}, {"timestamp": [5353.2, 5357.2], "text": " I am enabling you, I'm empowering you to participate in your own way."}, {"timestamp": [5357.2, 5362.2], "text": " Because then, in a year or two, when it comes time to vote,"}, {"timestamp": [5362.2, 5368.42], "text": " you're going to have in the back of your head which politicians know more about AI, right?"}, {"timestamp": [5368.42, 5373.12], "text": " And which bills or which amendments are going to be most relevant."}, {"timestamp": [5373.12, 5377.52], "text": " So awareness is, you know, that's step one of the whole process."}, {"timestamp": [5377.52, 5380.08], "text": " And the rest I think will work itself out."}, {"timestamp": [5380.08, 5381.08], "text": " Awesome."}, {"timestamp": [5381.08, 5382.08], "text": " Amazing."}, {"timestamp": [5382.08, 5383.08], "text": " Yeah."}, {"timestamp": [5383.08, 5387.0], "text": " It's time to say goodbye to you."}, {"timestamp": [5387.0, 5388.0], "text": " Yeah."}, {"timestamp": [5388.0, 5389.0], "text": " All right."}, {"timestamp": [5389.0, 5390.0], "text": " Good night, everybody."}, {"timestamp": [5390.0, 5395.56], "text": " It's getting later for us here on the East Coast, and good to meet everyone."}, {"timestamp": [5395.56, 5399.4], "text": " And yeah, reach out and talk again sometime in the future."}, {"timestamp": [5399.4, 5400.4], "text": " Cheers."}, {"timestamp": [5400.4, 5401.4], "text": " Amazing."}, {"timestamp": [5401.4, null], "text": " Thank you. Thank you. Amazing. That is good. \u1794\u17b6\u1793\u17b7\u178f\u17b6\u1793\u17b7\u178f\u17d2\u179a\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17d2\u179a\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17cb\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\u17b6\u1793\ufffd you"}]}