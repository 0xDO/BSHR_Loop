{"text": " What does AI actually understand one of the most common things that we see and hear is That AI is no more than a stochastic parrot and of course I'm one of the first people to say that humans are no more than a stochastic parrot because why when you say that AI is A stochastic parrot you are just parroting something that you heard based on your training data now something that you heard based on your training data. Now, rather than getting lost in a petty internet squabble over what is true intelligence and what is true understanding, there was this bombshell paper that came out just a few days ago, October 3rd, by Max Tegmark and Wes Gurney of Massachusetts Institute of Technology, MIT. Max Tegmark is, as you may or may not know, the guy who penned the famous PAUSE paper calling for a six-month moratorium on AI. I watched some interviews with him and he knew that it wouldn't work. That was not the point. The point was just to raise awareness, which it seems like that succeeded. But yeah, so Max Tegmark also wrote the book Life 3.0 where he talks about the possibility of substrate independence, meaning that there's no reason that machines could not eventually be conscious in their own way. So this paper, it is titled quite simply, Language Models Represent Space and Time. The long story short is that just by training on corpora of text, large language models, in this case they tested Lama 2, so not even a frontier model, is able to embed the concept of geospatial data and chronologically linear time. And this is something that one is not surprising to me and I'll explain to you why in just a few minutes. But it's really important because not only does it encode the ability to understand you know relative spatial data and chronologically linear time, they were actually able to identify the specific parameters or the neurons that allow it to encode this. And so what this implies, and why it's not surprising to me, is that by reading gigantic mountains of training data, just through text, just through semantic similarity, it has to learn to generalize into more and more abstract concepts, such as space, such as time. So we saw this with other recent papers where just training language models on two-dimensional data, they are able to generate a three-dimensional understanding of the world. And there's another video, or another paper that came out recently, talking about how just training models on linear sequences of moves for such as like chess and other games, even though their input data was one dimensional, they were able to understand and infer and in generalize a two dimensional play space. So the pattern that is emerging, and again, this is not surprising to me, the pattern that is emerging is and again, this is not surprising to me, the pattern that is emerging is that when you have enough training data, and when you have a sufficiently deep neural network, it is able to generate increasingly abstract representations of the problem space in which it is operating. So when you talk about the ability to generalize knowledge, the ability to generalize intelligence, it is no longer just a stochastic parent. It has an internal model of the space that it's working in. Image generators, they have a three-dimensional model of the world so that they can understand three-dimensional relationships between objects in order to better generate output images. So, this is not surprising to me because of a book that is very popular. Now I was not a big fan of this book, Thinking Fast and Slow by Daniel Kahneman. You can see I bought this back in 2015. The reason that this book was not impressive to me is because anyone with a modicum of like metacognitive skills or meditation or anything is like, well, duh. So if you're not familiar with this book, basically he argues that there are two, quote, systems of the brain. And he goes through great lengths to say that these are not neurological systems, that these are basically just two modes of using your brain. So there's system one thinking, which is fast thinking, which is intuitive, it's instantaneous, it's knee-jerk, it's off the cuff. And then there's slow thinking, which is deliberately slowing down to think through things and piece through it and use your brain to kind of steer the thought process and iterate upon thoughts. So if you're familiar with my work in cognitive architectures, this is basically how I approach cognitive architecture, where a large language model with one instance is system one thinking. It just gives you an off-the-cuff intuitive response with no double checking. The point of a cognitive architecture is explicitly to give it system two thinking. So that's that, but having watched this space for a while, starting with Data Skeptic, a podcast that i started listening to many years ago. And brain inspired a neuroscience podcast about a i. It has been obvious to me for a while that deep neural networks depending on what the problem space is they generally learn to abstract. learn to abstract kind of the space that they're working in, in order to come up with these, some of these more abstract concepts. In vision models, we see that what they do is they discover boundaries, they discover gradients, they discover all the same kinds of things that the human optic nerve and occipital discover about how to process images. Likewise, as deep neural networks learn, as elucidated in this paper by Max Tegmark and some of these other papers talking about theory of mind, basically in order... so even though the objective function is just to predict the next word, in order to accurately predict the next word it needs an internal model so that it can accurately predict the next word. And this is very close to how human brains work Because you know, it's not as popular now, but five ten years ago people were saying oh, yeah The human brain is mostly just a prediction engine that's what we do like we predict the next word in a comedy routine and when and when you you you know, Your expectations are subverted. That's when it's funny. That's what comedians do is they play on those expectations? This is how you drive a car you can anticipate what cars gonna be where? based on your knowledge of how cars move and how people move and the traffic rules and You're able to anticipate where you need to be because you remember like driving to work yesterday and so then you pay attention to the same cues that kind of get you to work on autopilot the following day And so a lot of these things are all system one thinking Basically where it's like, okay based on your training data based on your experience and recall You just kind of can get through life mostly on autopilot. And in fact, there are all kinds of like no tropic brain training Schemes out there that basically cause you to bank on system 2 thinking, which is you deliberately put yourself in situations that force you to think through things so that you're engaging your brain rather than working on autopilot. And so one way to think of system 1 versus system 2 is autopilot versus like manual flying or whatever, cruiseilot versus manual flying or whatever, cruise control versus manual driving. Anyways, so this is all very profound and the reason that I wanted to make this video is because there's this pattern emerging, this trend emerging and there's this growing awareness of what is understanding? What does it mean to truly understand something? And there's a few ways to unpack this. You can look at understanding from a functional perspective. Does it produce the right answer? How does it produce the right answer? Why does it produce the wrong answer? Right, and so just looking at things in terms of benchmarks and those avenues, there's a lot of value in that. But we're getting at a more fundamental question, which is what is the similarity between humans and machines? Because the substrate of human brains versus large language models is very different. The training methods are very different. Yet despite that, we're seeing more and more convergence. And what I mean by convergence is not that like we're going to merge into Borg or anything like that. But what I mean is that despite the fact that the training data is very different, despite the fact that the substrate is very different, the thing that we have in common is the problem space. We're training image generators to generate images that we find appealing and they mathematically derive how to do that, but then they also, in the process of doing that, they derive three-dimensional models of the world and other understandings of things in order to better generate images that we like. Likewise, language models, we train them on text that by and large humans wrote and that is comprehensible to humans. And so it's no surprise that a mathematical model is able to eventually generate text that we find, you know, useful, meaningful and so on. It was forcing us to question like, what does it mean for us to understand? Because like, I often have a tongue in cheek joke like, humans don't understand anything. We only think that we do. And likewise language models don't understand anything. They only think that they do. But when you have like tree of thought and multi-step reasoning, that's basically using large language models for system 2 thinking. And it's like, hey, you just generated this off-the-cuff thing, now check your work. And it's like, oh well, okay, we could do better. Or let's brainstorm this and have a more structured approach. And so this is basically what a cognitive architecture is. How do we implement systems-to-thinking with language models so that that way it's not just an intuitive, off-the-cuff, you know, immediate just guess. Anyways, that's kind of the long story short of this is this is how I think of large language models and AI models in general. So the rule of thumb moving forward, long story short, here's your takeaway, is if an AI model, if a neural network is a single pass forward, a single feed forward inference, then that is a system one thinking machine. If it has loops, if it has feedback mechanisms, if it can iterate on the material that it's thinking about, it is a system two device. So this is what we're doing with the ACE framework, the Autonomous Cognitive Entity framework, where it's actually many, many, many loops that are basically able to form dynamically as needed. That's over complicated. It's much simpler than that if you go look at the diagrams. Anyways, thanks for watching. I hope you got a lot out of this. It just felt really important to say this, especially in light of Max Tegmark's paper, which is amazing and you should read it. Even if you don't understand it, just skim it and look at the pretty colors. The things that they're talking about will drastically change the way that you understand how language models work. And again, this is LLAMA 2. This is not even multimodal models. This is not even frontier models that are going to come out next year. So anyways, thanks for watching. Like, subscribe, etc, etc. Have a good one. out next year. So, anyways, thanks for watching, like, subscribe, etc, etc. Have a good one.", "chunks": [{"timestamp": [0.0, 7.52], "text": " What does AI actually understand one of the most common things that we see and hear is"}, {"timestamp": [7.84, 11.16], "text": " That AI is no more than a stochastic parrot"}, {"timestamp": [11.16, 11.56], "text": " and of course"}, {"timestamp": [11.56, 18.06], "text": " I'm one of the first people to say that humans are no more than a stochastic parrot because why when you say that AI is"}, {"timestamp": [18.06, 21.74], "text": " A stochastic parrot you are just parroting something that you heard based on your training data"}, {"timestamp": [22.64, 28.08], "text": " now something that you heard based on your training data. Now, rather than getting lost in a petty internet squabble"}, {"timestamp": [28.2, 29.72], "text": " over what is true intelligence"}, {"timestamp": [29.72, 31.8], "text": " and what is true understanding,"}, {"timestamp": [31.8, 34.08], "text": " there was this bombshell paper that came out"}, {"timestamp": [34.08, 36.56], "text": " just a few days ago, October 3rd,"}, {"timestamp": [36.56, 38.76], "text": " by Max Tegmark and Wes Gurney"}, {"timestamp": [38.76, 42.44], "text": " of Massachusetts Institute of Technology, MIT."}, {"timestamp": [42.44, 45.6], "text": " Max Tegmark is, as you may or may not know,"}, {"timestamp": [45.6, 48.02], "text": " the guy who penned the famous PAUSE paper"}, {"timestamp": [48.02, 50.72], "text": " calling for a six-month moratorium on AI."}, {"timestamp": [50.72, 51.84], "text": " I watched some interviews with him"}, {"timestamp": [51.84, 53.24], "text": " and he knew that it wouldn't work."}, {"timestamp": [53.24, 54.12], "text": " That was not the point."}, {"timestamp": [54.12, 55.44], "text": " The point was just to raise awareness,"}, {"timestamp": [55.44, 57.24], "text": " which it seems like that succeeded."}, {"timestamp": [58.76, 62.04], "text": " But yeah, so Max Tegmark also wrote the book Life 3.0"}, {"timestamp": [62.04, 63.96], "text": " where he talks about the possibility"}, {"timestamp": [63.96, 70.3], "text": " of substrate independence, meaning that there's no reason that machines could not eventually be conscious"}, {"timestamp": [70.3, 72.58], "text": " in their own way."}, {"timestamp": [72.58, 80.38], "text": " So this paper, it is titled quite simply, Language Models Represent Space and Time."}, {"timestamp": [80.38, 89.0], "text": " The long story short is that just by training on corpora of text,"}, {"timestamp": [89.0, 92.0], "text": " large language models, in this case they tested Lama 2,"}, {"timestamp": [92.0, 94.0], "text": " so not even a frontier model,"}, {"timestamp": [94.0, 104.0], "text": " is able to embed the concept of geospatial data and chronologically linear time."}, {"timestamp": [104.0, 107.04], "text": " And this is something that one is not surprising to me"}, {"timestamp": [107.04, 113.2], "text": " and I'll explain to you why in just a few minutes. But it's really important because not only does"}, {"timestamp": [113.2, 120.16], "text": " it encode the ability to understand you know relative spatial data and chronologically linear"}, {"timestamp": [120.16, 126.0], "text": " time, they were actually able to identify the specific parameters or the neurons"}, {"timestamp": [126.0, 128.12], "text": " that allow it to encode this."}, {"timestamp": [128.12, 130.76], "text": " And so what this implies, and why it's not surprising"}, {"timestamp": [130.76, 135.1], "text": " to me, is that by reading gigantic mountains"}, {"timestamp": [135.1, 138.42], "text": " of training data, just through text,"}, {"timestamp": [138.42, 140.36], "text": " just through semantic similarity,"}, {"timestamp": [140.36, 143.92], "text": " it has to learn to generalize into more"}, {"timestamp": [143.92, 145.92], "text": " and more abstract concepts,"}, {"timestamp": [145.92, 148.4], "text": " such as space, such as time."}, {"timestamp": [148.4, 150.54], "text": " So we saw this with other recent papers"}, {"timestamp": [150.54, 153.96], "text": " where just training language models on two-dimensional data,"}, {"timestamp": [153.96, 155.28], "text": " they are able to generate"}, {"timestamp": [155.28, 157.74], "text": " a three-dimensional understanding of the world."}, {"timestamp": [157.74, 160.06], "text": " And there's another video,"}, {"timestamp": [160.06, 162.32], "text": " or another paper that came out recently,"}, {"timestamp": [162.32, 165.58], "text": " talking about how just training models"}, {"timestamp": [165.58, 168.16], "text": " on linear sequences of moves"}, {"timestamp": [168.16, 170.48], "text": " for such as like chess and other games,"}, {"timestamp": [170.48, 172.88], "text": " even though their input data was one dimensional,"}, {"timestamp": [172.88, 175.14], "text": " they were able to understand and infer"}, {"timestamp": [175.14, 179.26], "text": " and in generalize a two dimensional play space."}, {"timestamp": [179.26, 182.44], "text": " So the pattern that is emerging,"}, {"timestamp": [182.44, 183.82], "text": " and again, this is not surprising to me,"}, {"timestamp": [183.82, 185.4], "text": " the pattern that is emerging is and again, this is not surprising to me, the pattern that is emerging is that"}, {"timestamp": [185.4, 187.82], "text": " when you have enough training data,"}, {"timestamp": [187.82, 191.08], "text": " and when you have a sufficiently deep neural network,"}, {"timestamp": [191.08, 196.04], "text": " it is able to generate increasingly abstract representations"}, {"timestamp": [196.04, 198.96], "text": " of the problem space in which it is operating."}, {"timestamp": [198.96, 202.24], "text": " So when you talk about the ability to generalize knowledge,"}, {"timestamp": [202.24, 204.48], "text": " the ability to generalize intelligence,"}, {"timestamp": [204.48, 206.34], "text": " it is no longer just a stochastic parent."}, {"timestamp": [206.34, 211.34], "text": " It has an internal model of the space that it's working in."}, {"timestamp": [211.4, 214.38], "text": " Image generators, they have a three-dimensional model"}, {"timestamp": [214.38, 215.78], "text": " of the world so that they can understand"}, {"timestamp": [215.78, 219.66], "text": " three-dimensional relationships between objects"}, {"timestamp": [219.66, 222.66], "text": " in order to better generate output images."}, {"timestamp": [223.6, 230.36], "text": " So, this is not surprising to me because of a book that is very popular."}, {"timestamp": [230.36, 234.6], "text": " Now I was not a big fan of this book, Thinking Fast and Slow by Daniel Kahneman."}, {"timestamp": [234.6, 237.92], "text": " You can see I bought this back in 2015."}, {"timestamp": [237.92, 242.48], "text": " The reason that this book was not impressive to me is because anyone with a modicum of"}, {"timestamp": [242.48, 245.92], "text": " like metacognitive skills or meditation or anything"}, {"timestamp": [245.92, 247.68], "text": " is like, well, duh."}, {"timestamp": [247.68, 250.08], "text": " So if you're not familiar with this book,"}, {"timestamp": [250.08, 253.24], "text": " basically he argues that there are two, quote,"}, {"timestamp": [253.24, 254.38], "text": " systems of the brain."}, {"timestamp": [254.38, 256.42], "text": " And he goes through great lengths to say"}, {"timestamp": [256.42, 258.12], "text": " that these are not neurological systems,"}, {"timestamp": [258.12, 261.32], "text": " that these are basically just two modes of using your brain."}, {"timestamp": [261.32, 264.38], "text": " So there's system one thinking, which is fast thinking,"}, {"timestamp": [264.38, 267.06], "text": " which is intuitive, it's instantaneous,"}, {"timestamp": [267.06, 268.88], "text": " it's knee-jerk, it's off the cuff."}, {"timestamp": [268.88, 270.32], "text": " And then there's slow thinking,"}, {"timestamp": [270.32, 273.16], "text": " which is deliberately slowing down to think through things"}, {"timestamp": [273.16, 274.9], "text": " and piece through it and use your brain"}, {"timestamp": [274.9, 277.24], "text": " to kind of steer the thought process"}, {"timestamp": [277.24, 279.12], "text": " and iterate upon thoughts."}, {"timestamp": [279.12, 281.34], "text": " So if you're familiar with my work"}, {"timestamp": [281.34, 282.6], "text": " in cognitive architectures,"}, {"timestamp": [282.6, 285.46], "text": " this is basically how I approach cognitive architecture,"}, {"timestamp": [285.46, 289.08], "text": " where a large language model with one instance"}, {"timestamp": [289.08, 290.8], "text": " is system one thinking."}, {"timestamp": [290.8, 293.56], "text": " It just gives you an off-the-cuff intuitive response"}, {"timestamp": [293.56, 295.2], "text": " with no double checking."}, {"timestamp": [295.2, 296.8], "text": " The point of a cognitive architecture"}, {"timestamp": [296.8, 299.76], "text": " is explicitly to give it system two thinking."}, {"timestamp": [300.62, 304.58], "text": " So that's that, but having watched this space for a while,"}, {"timestamp": [304.58, 308.88], "text": " starting with Data Skeptic, a podcast that i started listening to many years ago."}, {"timestamp": [309.3, 312.46], "text": " And brain inspired a neuroscience podcast about a i."}, {"timestamp": [313.14, 321.66], "text": " It has been obvious to me for a while that deep neural networks depending on what the problem space is they generally learn to abstract."}, {"timestamp": [326.2, 327.6], "text": " learn to abstract kind of the space that they're working in, in order to come up with these, some of these more abstract concepts."}, {"timestamp": [327.6, 334.28], "text": " In vision models, we see that what they do is they discover boundaries, they discover"}, {"timestamp": [334.28, 338.58], "text": " gradients, they discover all the same kinds of things that the human optic nerve and occipital"}, {"timestamp": [338.58, 342.04], "text": " discover about how to process images."}, {"timestamp": [342.04, 347.72], "text": " Likewise, as deep neural networks learn, as elucidated in"}, {"timestamp": [347.72, 351.4], "text": " this paper by Max Tegmark and some of these other papers talking about theory"}, {"timestamp": [351.4, 356.32], "text": " of mind, basically in order... so even though the objective function is just to"}, {"timestamp": [356.32, 360.32], "text": " predict the next word, in order to accurately predict the next word it"}, {"timestamp": [360.32, 363.92], "text": " needs an internal model so that it can accurately predict the next word. And"}, {"timestamp": [363.92, 367.1], "text": " this is very close to how human brains work"}, {"timestamp": [367.1, 371.94], "text": " Because you know, it's not as popular now, but five ten years ago people were saying oh, yeah"}, {"timestamp": [371.94, 373.94], "text": " The human brain is mostly just a prediction engine"}, {"timestamp": [373.94, 380.5], "text": " that's what we do like we predict the next word in a comedy routine and when and when you you you know,"}, {"timestamp": [380.5, 386.34], "text": " Your expectations are subverted. That's when it's funny. That's what comedians do is they play on those expectations?"}, {"timestamp": [386.4, 390.04], "text": " This is how you drive a car you can anticipate what cars gonna be where?"}, {"timestamp": [390.24, 394.52], "text": " based on your knowledge of how cars move and how people move and the traffic rules and"}, {"timestamp": [394.68, 400.32], "text": " You're able to anticipate where you need to be because you remember like driving to work yesterday"}, {"timestamp": [400.32, 405.54], "text": " and so then you pay attention to the same cues that kind of get you to work on autopilot the following day"}, {"timestamp": [405.54, 409.78], "text": " And so a lot of these things are all system one thinking"}, {"timestamp": [410.42, 414.62], "text": " Basically where it's like, okay based on your training data based on your experience and recall"}, {"timestamp": [414.66, 421.22], "text": " You just kind of can get through life mostly on autopilot. And in fact, there are all kinds of like no tropic"}, {"timestamp": [421.66, 423.22], "text": " brain training"}, {"timestamp": [423.22, 426.54], "text": " Schemes out there that basically cause you to bank on"}, {"timestamp": [426.54, 431.3], "text": " system 2 thinking, which is you deliberately put yourself in situations that force you"}, {"timestamp": [431.3, 437.4], "text": " to think through things so that you're engaging your brain rather than working on autopilot."}, {"timestamp": [437.4, 444.74], "text": " And so one way to think of system 1 versus system 2 is autopilot versus like manual flying"}, {"timestamp": [444.74, 445.28], "text": " or whatever, cruiseilot versus manual flying or whatever,"}, {"timestamp": [445.28, 447.28], "text": " cruise control versus manual driving."}, {"timestamp": [449.28, 453.02], "text": " Anyways, so this is all very profound"}, {"timestamp": [453.02, 456.0], "text": " and the reason that I wanted to make this video"}, {"timestamp": [456.0, 459.44], "text": " is because there's this pattern emerging,"}, {"timestamp": [459.44, 462.0], "text": " this trend emerging and there's this growing awareness"}, {"timestamp": [462.0, 464.8], "text": " of what is understanding?"}, {"timestamp": [464.8, 467.16], "text": " What does it mean to truly understand something?"}, {"timestamp": [467.16, 469.24], "text": " And there's a few ways to unpack this."}, {"timestamp": [469.24, 473.4], "text": " You can look at understanding from a functional perspective."}, {"timestamp": [473.4, 475.12], "text": " Does it produce the right answer?"}, {"timestamp": [475.12, 477.46], "text": " How does it produce the right answer?"}, {"timestamp": [477.46, 479.52], "text": " Why does it produce the wrong answer?"}, {"timestamp": [479.52, 482.84], "text": " Right, and so just looking at things in terms of benchmarks"}, {"timestamp": [482.84, 487.38], "text": " and those avenues, there's a lot of value in that."}, {"timestamp": [487.38, 490.48], "text": " But we're getting at a more fundamental question,"}, {"timestamp": [490.48, 494.24], "text": " which is what is the similarity between humans and machines?"}, {"timestamp": [494.24, 497.2], "text": " Because the substrate of human brains"}, {"timestamp": [497.2, 500.2], "text": " versus large language models is very different."}, {"timestamp": [500.2, 502.9], "text": " The training methods are very different."}, {"timestamp": [502.9, 506.6], "text": " Yet despite that, we're seeing more and more convergence."}, {"timestamp": [506.6, 511.8], "text": " And what I mean by convergence is not that like we're going to merge into Borg or anything like that."}, {"timestamp": [511.8, 516.8], "text": " But what I mean is that despite the fact that the training data is very different,"}, {"timestamp": [516.8, 519.2], "text": " despite the fact that the substrate is very different,"}, {"timestamp": [519.2, 522.4], "text": " the thing that we have in common is the problem space."}, {"timestamp": [522.4, 527.36], "text": " We're training image generators to generate images"}, {"timestamp": [527.36, 532.68], "text": " that we find appealing and they mathematically derive how to do that, but then they also,"}, {"timestamp": [532.68, 536.68], "text": " in the process of doing that, they derive three-dimensional models of the world and"}, {"timestamp": [536.68, 542.36], "text": " other understandings of things in order to better generate images that we like. Likewise,"}, {"timestamp": [542.36, 545.28], "text": " language models, we train them on text that by and large humans"}, {"timestamp": [545.28, 549.6], "text": " wrote and that is comprehensible to humans. And so it's no surprise that a mathematical"}, {"timestamp": [549.6, 556.24], "text": " model is able to eventually generate text that we find, you know, useful, meaningful"}, {"timestamp": [556.24, 561.16], "text": " and so on. It was forcing us to question like, what does it mean for us to understand? Because"}, {"timestamp": [561.16, 568.0], "text": " like, I often have a tongue in cheek joke like, humans don't understand anything. We only think that we do. And likewise"}, {"timestamp": [568.0, 572.0], "text": " language models don't understand anything. They only think that they do. But"}, {"timestamp": [572.0, 576.0], "text": " when you have like tree of thought and multi-step"}, {"timestamp": [576.0, 580.0], "text": " reasoning, that's basically using large language models for system 2 thinking."}, {"timestamp": [580.0, 584.0], "text": " And it's like, hey, you just generated this off-the-cuff thing, now check your work."}, {"timestamp": [584.0, 588.0], "text": " And it's like, oh well, okay, we could do better. Or let's brainstorm this and have a more"}, {"timestamp": [588.0, 592.0], "text": " structured approach. And so this is basically what a cognitive architecture is."}, {"timestamp": [592.0, 596.0], "text": " How do we implement systems-to-thinking with language models"}, {"timestamp": [596.0, 600.0], "text": " so that that way it's not just an intuitive, off-the-cuff, you know, immediate"}, {"timestamp": [600.0, 604.0], "text": " just guess. Anyways, that's kind of"}, {"timestamp": [604.0, 607.46], "text": " the long story short of this is this is how I"}, {"timestamp": [607.46, 613.86], "text": " think of large language models and AI models in general. So the rule of thumb moving forward,"}, {"timestamp": [613.86, 620.42], "text": " long story short, here's your takeaway, is if an AI model, if a neural network is a single"}, {"timestamp": [620.42, 625.0], "text": " pass forward, a single feed forward inference,"}, {"timestamp": [625.0, 627.76], "text": " then that is a system one thinking machine."}, {"timestamp": [627.76, 631.24], "text": " If it has loops, if it has feedback mechanisms,"}, {"timestamp": [631.24, 633.56], "text": " if it can iterate on the material"}, {"timestamp": [633.56, 636.04], "text": " that it's thinking about, it is a system two device."}, {"timestamp": [636.04, 637.78], "text": " So this is what we're doing with the ACE framework,"}, {"timestamp": [637.78, 639.7], "text": " the Autonomous Cognitive Entity framework,"}, {"timestamp": [639.7, 642.72], "text": " where it's actually many, many, many loops"}, {"timestamp": [642.72, 647.72], "text": " that are basically able to form dynamically as needed."}, {"timestamp": [647.72, 649.04], "text": " That's over complicated."}, {"timestamp": [649.04, 651.24], "text": " It's much simpler than that if you go look at the diagrams."}, {"timestamp": [651.24, 652.88], "text": " Anyways, thanks for watching."}, {"timestamp": [652.88, 654.4], "text": " I hope you got a lot out of this."}, {"timestamp": [654.4, 659.88], "text": " It just felt really important to say this, especially in light of Max Tegmark's paper,"}, {"timestamp": [659.88, 662.08], "text": " which is amazing and you should read it."}, {"timestamp": [662.08, 670.8], "text": " Even if you don't understand it, just skim it and look at the pretty colors. The things that they're talking about will drastically change"}, {"timestamp": [670.8, 675.18], "text": " the way that you understand how language models work. And again, this is LLAMA 2. This is"}, {"timestamp": [675.18, 680.26], "text": " not even multimodal models. This is not even frontier models that are going to come out"}, {"timestamp": [680.26, 684.72], "text": " next year. So anyways, thanks for watching. Like, subscribe, etc, etc. Have a good one."}, {"timestamp": [679.88, 680.88], "text": " out next year."}, {"timestamp": [680.88, 684.24], "text": " So, anyways, thanks for watching, like, subscribe, etc, etc."}, {"timestamp": [684.24, 684.6], "text": " Have a good one."}]}