{"text": " Okay, today we're going over two papers, the heuristic imperatives and the Gatow framework. If you're not familiar, these are the brainchildren of David Shapiro, a big YouTuber in the AI space. I generally watch his stuff. I disagree with quite a bit of what he says and does, but for the most part, it's pretty freaking good. I'm pretty amazing. We're going to get into some disagreements here. I basically wanted to go over these two papers because I am interested in potentially proposing more specific ideas, kind of in this space sometime in the future, so I wanted to just go through and see what he's got going on, make sure I don't replicate anything, and give people a better sense, as well as just spread the word. I think it's, although I disagree with some stuff in these papers, I think for the most part there's some good ideas, some honest attempts going on here, so I like it quite a lot. With that, let's get started. Heuristic Imperatives for Autonomous AI Systems. Traditional approaches to AI ethics have focused on hard-coding specific rules or guidelines, but these methods can be inflexible and fail to account for the diverse and evolving nature of human values and context.\" Yeah, the traditional approach is really just so bad. Oh my god. It's like, people are trying to define actual utility functions, actual goals effectively for the AI, but it's, it's, they've never sat through an economics class. And if you sat through enough of them you'd realize that The idea of utility function of goals of rational actors of clarifying this stuff. It's all bullshit. It's no good That's a weird way the page moves. What the heck? Not one of that anyways, let's get to it Heuristic imperatives are set of fundamental guiding principles designed to be embedded into autonomous AI systems at various levels. My big problem here with this whole heuristic imperative thing is just how do you embed them? It's not quite clear. The methods that they've worked with up till now are just like putting it in the prompts, which is absolutely laughable, no offense. It's just not going to do anything in the long term, but hopefully maybe someone will find an actual, legit way to embed embed these, not to just provide them and hope that they get used by the AI. Anyway, what they are, the three here's the comparatives, reduce suffering in the universe, increase prosperity in the universe, and increase understanding in the universe. I see why they put in this in the universe phrasing, but I'm not very fond. I would have liked both locally and in the universe, local and global level kind of thing in the actual phrasing, but whatever, that's just semantics. I don't like how this is scrolling right now. Okay, what is a heuristic? A heuristic is a problem-solving strategy or technique that simplifies complex decision-making processes by using shortcuts, approximations, or generalizations. Why did it not switch? I'm so confused. I swear. What is with this? I don't like... Is my view messed up? View? Oh, continue with scroll. That's why it's all messed up. Imperative. An imperative is a command, rule, or duty that must be followed or adhered to. The term heuristic imperative implies that the principles are not exhaustive or absolute, the principles are flexible and adaptive, and the principles require balancing and tradeoffs. I love all those. Great. I mean, not exhaustive or absolute is bad in the sense of like a first glance you might think oh no if it's not gonna be exhaustive absolute What if it's missing something big? And yeah, that's a valid question But the idea here of being not as awesome and absolute and actually the models or whatever we're using the AGI Hopefully I guess knowing the non exhaustive or absolute is kind of embedding a bit of a humility like we're being specifically broad and specifically not exhaustive or absolute because we want to allow some wiggle room and some chance for discussion and some humility some humbleness in the actual AI is making big decisions flexible and adaptive great require balancing and trade-offs I think you're gonna have to do that with anything you can't just give it one rule like you're gonna have to go that with anything. You can't just give it one rule. You're gonna have to give a bunch of rules. It has to balance between them because the world is too complicated for one rule, really. The principle serves as intrinsic motivations. They never actually clarified why this is true. Like I said, the previous experiments embedding these imperatives into AI systems have been just literally putting in the prompt for an LLM. That'll do great for the prompts for a little short prompt kind of thing for the meantime, but come on, like, we have no actual way to guarantee that the AIs want to follow this. We can give them a constitution, we can tell them rules, we can ask them for something, but if you're going to do this instead of an actual utility function, then that's just not actually guaranteed to be followed, and utility functions have their own issues, so it's just a whole nightmare of can of worms, whatever. Each imperative, when considered in isolation, could potentially lead to undesirable outcomes. However, when combined, they complement and counterbalance each other. Yeah, cool. combined they complement and counterbalance each other. Yeah cool, immutable constitution. So we're going for the same idea as the US Constitution here. I think, I don't know if the US actually even had a constitution, but I think they might have the United States. I'm probably wrong on that, but the idea here being we have this broad set of rules and it can't change, but all other rules downstream must be made based off those broad rules kind of thing Embedding the imperative at multiple levels How you gonna do it? I don't know Periodic evaluation self-assessment. How are you gonna make it do that to itself? I don't know like this is easy I think the problem with this documents and with the gado as well I I mean, just this one on the Gato, the problem with this document is the context, the mindset is so framed by LLMs, by language models, like that's what they're building this for effectively, is they're assuming language models are the thing that actually go crazy. Might be true. That's very possible. But if it's not LLMs, then a lot of this stuff just makes no sense, like it's not going to be actually applicable. And also, same problem I have with all alignment stuff is like, sure you can align your AGI, but what if someone else doesn't? Like you need a system that is robust to bad actors. Human oversight and collaboration. Good luck overseeing these things when there are 10,000 John von Neumanns working at 10 times, like 100,000 times the speed of people thoughts or something like yeah, no thanks the closest I see to human collaboration and oversight is like maybe some blockchain decentralized market mechanisms of the AI's need to provide us what we want in order to get compute power like there's like a market mechanism going on where they just to pay for compute power maybe maybe that's possible but it's silly to me to think we can just like keep them running at human speed and like pause every every two thoughts and only with them like keep going with it resume those we've got one of those two thoughts are good like it's i don't think so transparent explainable a i uh... if you know my channel you know that explainable a i so far as absolute baloney uh... some like eighty percent of explainable a i techniques actually explainable all this like they make up a technique and the color explainable but they give no actual metric or really solid explanation or reasoning as to why that's actually super explainable. All they're doing is pulling out features most of the time. And even the ones that do attempt to actually measure this stuff, they fail miserably. Like it's not a good field right now. Explainable AI is absolutely just floundering, drowning. It's not doing anything. Reason being, these are big complex systems that just are probably not going to be explainable to a certain extent. Like, if we had already figured out the human brain, I'd be confident, but I don't think this is going to happen. Explainable AI is, I think, a pipe dream. I hope not a pipe dream. I hope I'm wrong, but whatever. As our understanding of ethical AI and the control problem evolves, it is essential to continuously refine the implementation of heuristic imperatives in AI systems.\" Yeah, cool. As long as the takeoff is slow, this is doable, and I certainly agree we should do it. This is a bunch of stuff I found less interesting. AI as a facilitator, not a dictator. I would love to reach that too, but I don't know, man. Good luck. Good luck. Personal autonomy. Individual autonomy is not explicitly stated in these three heuristic imperatives, however it is implicitly derived from the combination of these imperatives and their underlying intentions. Red means I must have disagreed somehow. So first, reducing suffering in the universe. One of the key aspects of reducing suffering is respecting and protecting individual rights, including the right to make choices about one's own life. I can't say I fully agree with that. In theory, in ideal, yeah, but I think a lot of people don't actually want their individual rights. I think a lot of people are not quite that set of consciousness where they actually like can think for themselves very well or should be thinking for themselves very well. No offense, it's just certain people, it's that, what's it called? I'm not gonna get into it, I'm not gonna get into it. But I also just don't see how the connection from reduced suffering to individual rights, I mean If anything I feel it'd be easier to argue that it'd be easier to reduce suffering if you took away rights and just like Made a forced utopia like I'm like Wally like that movie kind of thing, right? I'm not so confident in this connection here. Prosperity is closely linked to personal well-being, which is often connected to individual autonomy. Kind of. A lot of people, I think, just do better with directions. I'm not going to lie. A lot of people need to be in the military because they need that that regiment that Although you could also argue that a discipline creates freedoms as a whole separate argument. I don't know but these are very loose Conjectures. All right, and I get that that's all we can do right now and that's fine But I just I don't want to bet my life on it or the bet humanity on it either But promoting understanding requires respecting and appreciating the diversity of human experiences, beliefs, and values. Yeah, I agree with this idea of increased understanding which is, I guess, I think he inspired Elon Musk's new like, curiosity is the goal AI whatever for his company XAI. Yeah, I think in an innately curious system might want to keep individuality and diversity at rounds even if that stuff is inherently useless. The same way like we value or we are starting our culture starting to value cultures that have been stomped to the dust kind of thing. We're starting to say like, oh we shouldn't have stomped them into the dust, we wish we hadn't, we want to rejuvenate them kind of thing, or even just like the planet and like nature species. I do think there is an aspect of curiosity that might lead us to individual rights, autonomy, like thriving for all. That's the the one heuristic that I'm actually pretty excited about, this idea, basically. Trustworthy AI. In order to effectively reduce suffering, AI systems must be perceived as trustworthy by users and stakeholders. Yeah, fair. Trust is essential for promoting prosperity, as people are more likely to engage with and benefit from AI systems that they perceive as reliable and dependable. Yeah, I'm not sure how much it'll need to engage this to actually do what it wants to do for the long term, but especially in the short term. Yeah. Cool. I think that's it for the Curious Comparatives. Oh, I wanted to point out here also, so GPT-4 wrote this paper basically, and you can kind of tell, or at least post-hoc I can tell, I guess maybe I couldn't tell if I didn't know this, but the language is very robust and repetitive in structure, and in many ways does not show an awareness of the whole, gbd4 can only output so much at a time kind of thing in many ways like the fact that it says like in conclusion at the end of every single paragraph every single conclusion paragraph every single section is kind of a lot in conclusion probably but there's one a bit closer here okay i'm wrong it's not doing every single time but um i don't know i't know if I could tell but that being said It's a lot less work if you have GPT before right it for concepts as simple as this Yeah, man, like go for it have GPT for right? It's and I love Before I saw that he did this I love them that he just included the raw transcript in there and like stated GPT for wrote it That's something I want to do in the future is just including the raw transcripts from every paper that I basically write, which I've had a few now although not all of them have been like fully finished and fleshed out for various reasons, but I love this idea of including the actual raw transcript. That's great. And the font change theme I thought was pretty good font to choose I don't know why I care about that but I do anyways that one's done let's hit the gato framework the decentralized path to utopia ignore the yellow this is the bigger whole thing this view is messed up again. Continue with scroll. It's like 60 pages, 70 pages, something dumb. It's too much in here. It's like I said, GPT-4 was just so verbose. Although I don't think GPT-4 wrote this one, I think it's, but I can tell there's quite a bit in there that's just, y'all could have trimmed this to 40 pages easily, just saying. A grim picture was painted by AI ethicist Connor Leahy, love him. There are techno-elites that want to be trillionaires and want to live forever, and they're willing to risk everything to get what they want. Yeah, and that sucks, and we might fall victim to that. It's just a shame. I really believe that that kind of stuff is a trauma. I'm really not. It's just a shame. Let me get my, sorry, that was annoying me. I really think that the trauma the billionaire mindset is a trauma like they don't understand what enough money is I know the difference between like actual cash billions versus company owning asset billions whatever I was an economist I get it but end of the day still even just a hundred millionaires or borderline tens a million kind of thing There's such thing as enough guys there's such thing as enough and I really believe that's that lack of awareness of enough that lack of ability to say that there's enough is a trauma response just like People who are hoarders of goods like of like you go and like they're hoarders like in their house You can't walk and I think they're hoarders. I, like you go in their house and you can't walk, kind of thing, they're hoarders. I think it's the same thing with wealth. The more ominous risk lies in the potential emergence of artificial superintelligence. Yeah, yeah, yeah, yeah, yeah, yeah. We've all heard it before. Heuristic imperatives are a big part of what they're doing here, apparently. It comes up a lot. The global alignment taxonomy omnibus, G GATO framework represents a blueprint for a global initiative, a decentralized effort that requires contributions from all corners of the world. This is basically a big call to action paper for a decentralized movement, just like everyone doing the best they can basically. This thing called GATO layers, the big pillars of the actual whole setup. First one is model alignment. I've discussed before my opinions on alignment but we're gonna skip past that for now. They are pro-autonomous agents. Love this. You would think that if they're big on alignment that they're anti-autonomy in agents but the goal here is a very optimistic one. They want utopia is the goal of Gato, which is pretty cool. Decentralized networks, recognizing the strength and potential of distributed systems, this layer focuses on utilizing decentralized networks like blockchain, DAOs, and federations. That'll be a huge part, I think. I was very anti-crypto blockchain all this stuff while it was Big and happening. I guess still is kind of I was saying since the beginning basically that not quite the beginning but pretty much once I understood everything that The investment nature of it is just so messed up and it's shooting us off in the foot Basically, and the mutual benefits of alignment. It advocates for businesses to perceive AI alignment not as an obligation, but as a strategic advantage that enhances business performance and contributes positively to their bottom line. Good luck with that. Dan Hendricks No, businesses recently talking about this kind of stuff. Um, no businesses like big ones at Google, whatever. Um, they're in a race to publish the best thing and they are lowering their safety checks, um, for sake of competition. And, uh, the, when you start having like AI's take over companies and stuff, the AIs that are both smart enough and immoral enough to lie, cheat, steal, whatever are going to have an advantage in our current economic system just the same way that people who are smart and immoral enough to lie, cheat, steal, and get away with it have been doing so and have been getting rich off of that thing for a while now. This sentence only makes sense if you believe that all billionaires got there based off of actually contributing billions worth of value to the economy no, no, no, they contributed millions worth and then they took advantage of the labor power of their employees and didn't allow Returns to labor to track the same returns of capital. That's what happened. Anyways National regulation. I'm pretty anti in general what happened anyways national regulation I'm pretty anti in general a few civic things I'm okay with but a lot in the large sense I think that the regulatory bodies are not smart enough to do anything here I'm certainly not capable enough to do it fast enough strategic national interest that enhances GDP 4% national security and bolsters geopolitical influence yeah yeah like AI is going to be big for that. They said alignment will be big for national interest and national security and geopolitical influence. I don't know how confident I am in that. I think countries are going to run like war machines basically, to a large extent. I'm not very confident in what the US Army will do when um, it gets an AGI that's not independent if it gets like a baby AGI. International Treaty international entity akin to CERN. Yeah, throw some throw billions in here please. Oh my god I would love that. Specifically a fully open source everything published international entity yeah, yeah, and with like collaboration with businesses and stuff like I Don't like the idea of Corporations like Google and whatever being the first to hit the big model the big AGI I don't like the idea of individual governments doing it either but AGI. I don't like the idea of individual governments doing it either, but if we had an actual huge treaty between, let's say, at least NATO members, if not also including China and BRICS and everything, I would be pretty excited for, I would be much more optimistic as to the existence of a trillion parameter model within that entity than I would among an individual government or among a corporation. Global Consensus The apex layer is centered around achieving a worldwide agreement on the principles of the Gatto framework. It's about creating a global discourse. Global discourse, sure. Gatto framework, global, no. I mean, it'll exist globally, but will it actually be global consensus? No. It's gonna be way more methods of alignments, way more contradictory opinions, huge bad actor regimes. Our inspirations are as diverse as they are profound, ranging from the transformative ethos of 12-step programs to the radical inclusivity of Burning Man and the grassroots empowerment of the Occupy movement. Cool. Cool cool cool. Start where you are, use what you have, do what you can. Big believer in this. Tend to the garden with which you can reach, the parts of the garden with which you can reach. Broadcast your findings. Huge believer. Let's go full open source, people. Please. Everyone. Everything. Full open source. Think globally, act locally. Yeah, I'm not very good at that, but I'd like to get better. Think exponentially. Big one. People don't understand how fast stuff is moving. This is huge. Think exponentially. Although, we can't think exponentially, really. It's too fucking hard, but whatever. I hadn't heard this before. I hadn't heard the actual word, the definition of this before, the vocab word used. Mesa optimization. Happens when a trained model termed the base Optimizer creates a MESA Optimizer, a model that optimizes for a different objective. This can lead to a disconnect between the Base Optimizer's intended goal and the MESA Optimizer's actual objective, raising concerns over the model's alignment with its original training intent. So we are MESA, base optimizers currently creating MESA optimizers that are AIs. The scary thing is when AI start to create their own MESA optimizers. We must set clear and measurable milestones and KPIs. I was surprised to see this in here. Only ones that I thought were worth talking about. Number of open source aligned models. I about, number of open source aligned models. I had to number open source models, period, but aligned, sure, I guess if you want to. Number of open source data sets. Yes, yes. I really, as far as regulation goes, only one of the few things I would support are a full open sourcing of all data sets, or maybe not full open sourcing of all data sets, maybe like a period of time like if you're a big company with your own big proprietary data set that the acute advantage of kind of thing I think we should inspiration how patents only last 20 years and we should say that if you have a data set that any data that is greater than 20 years old must be released open source 20 is extreme though 20 is like a figment of the constitutional times, the 1700s, 1800s when this stuff was invented for patents. I think five years is more realistic, five years is gonna be a crazy long time period in this new world we're heading into. I'm also gonna do a paper on that I think, potentially, on the idea of proprietary data sets and time limits on them. Alignment drift over iterations. Over multiple training iterations or after self-modification, it's crucial to measure how well models maintain their alignment. Good luck with that! Huh, silly. One key advocacy point is to ensure all communication between components occurs in natural language, making it human-readable. Haha, good luck with that again. Are you kidding me? That's so inefficient. Um, if they can create their own languages they're going to, um, and even so like, let's say even if you were to do this, even if you were to like get them to actually talk in English to each other, all the AI is right at all times, just English language and maybe some Python code for example, stuff, right. We envisioned a future teeming with autonomous agents, their numbers could reach into the trillions existing in diverse forms and fulfilling various roles. Yes, this will not be a uniform monolithic group of agents, but rather a diverse array of entities, each with their unique capabilities, preferences, and area of specialization. So first off, this assumes a slow takeoff. If we have a fast takeoff Sorry, I'm not gonna happen. But um, also the thing I was calling the question is like Is it even accurate to consider them all separate at that point like if we have this huge ecosystem of AI agents I think if I think people imagine that like an intelligent entity is a single units is a single thing a single being but like I Think intelligence is like a fractal like structure going up groups kind of thing like a Communities groups nations corporations, whatever. They are super intelligence in themselves and you yourself be a Cut that skull open of yours We split down the center on the corporates colossum and then we like seal it back up again and we let your different sides of the brain we put a little divider here It's sorry look up um Look up split brain experiments. The CGP gray is a great video on this. It's called a you are to basically like your own in your brain is multiple consciousnesses is multiple intelligences all Like contributing and combining together to make one intelligence, right? I think it's at a certain point if you have this big thing happening we all become one hive mind effectively, but not a hive mind with a centralized source of decision-making more like a hive mind dependent on individual nodes a bottom-up hive mind Interesting as our agents evolve it's essential to track their level of autonomy. We could adapt a scale similar to the levels of autonomy used for self-driving cars. Higher levels of autonomy should be celebrated, but also treated as opportunities for further scrutiny of alignment. It's vital to test the agent's resilience against alignment drift. By simulating various scenarios and challenges, we can evaluate how robustly the agents maintain their alignment over time and under different conditions. One second, I think I'm going to turn off the second display because this is not helping my... Oh god, the battery's draining. Yeah, no thank you. Let's shut this off. My meter's lagging, this is not good. M2 can't handle it like that. Alright, probably should be good now, I would guess. Still lagging a bit, but how's OBS doing? OBS? Better? Oh, much faster response. Okay, back to it. In our utopian vision, consensus mechanisms can be harnessed to enforce and reward alignment with heuristic imperatives. For instance, agents that demonstrate consistent adherence to the imperatives can be rewarded with greater influence in the consensus process, incentivizing alignment. Conversely, agents that deviate from the imperatives can be penalized and discouraging unaligned behavior. Good luck monitoring them like that. No, I think our penalizing, incentivizing schema basically has to be in terms of a market mechanism, giving them compute power. I see no other way of doing it. This idea that we can actually monitor them fast enough to actually reward them for using the imperatives is silly. This idea that we could actually monitor them fast enough to actually reward them for using the imperatives is silly. This idea that we could actually understand them well enough to make sure they are using them is silly. No, I'm going for market mechanisms and selling compute to them, basically. Reputation systems. Love those. In the context of heuristic imperatives, these systems can track and verify an agent's adherence to the imperatives. Reputation systems though, I don't, imperatives, again, whatever, but reputation systems are going to be great for this. I really want, I think, a kind of blockchain mechanism where we have all of the humans, their coins, their private public keys look a bit different from that of an AI's. Key method of interacting with a blockchain, and humans, we just verify each other basically. We just all to each other say like, yes, this is my brother, that's his, he actually has his phone out right now, that's actually him kind of stuff. I think we'll need something like that. Some kind of a universal ID system. All AI systems might benefit from controlling more computer resources or gathering data. If the AI wants resources, it has to play nice. Totally. The first milestone is successful development and deployment of open source blockchain. Yeah, yeah. Still haven't seen it yet. I've seen closed stuff, we've gotten torrents, we've gotten math guides as to how we do actual training, but we have not gotten a full coin setup yet. I'm sure it's being worked on by somebody. By many people, if anything. Robust consensus mechanisms and effective reputation systems. As the number of autonomous agents increases, our networks must be capable of scaling to accommodate them. Huge. Reduction of centralized control points. Huge, huge, huge, huge, huge. Their CERN idea doesn't quite help with this, but I mean, kind of. If they're open sourcing everything, it's probably fine. Successful prevention of alignment drift. Good luck. I highlighted way too much, I think. What are we at? We're still not very far through. A critical part of that vision involves corporate adoption of aligned AI. Good luck with that. Profit motive and unerring and single-minded hunger that drives corporations. Yeah. Alliant's pursuit of profit can be a boon to our cause if harnessed appropriately. Maybe. Aligned AI can be good for business. Maybe. Aligned AI, characterized by its robustness and trustworthiness, is an asset to any corporate entity. Its capacity for advanced automation, reduced need for human supervision, and capability for iterative self-improvement directly translate to cost-effectiveness and enhanced productivity. When an AI system aligns better, it can handle more tasks independently and be deployed at a larger scale. These are attractive prospects for any profit-driven corporation.\" You could have taken the word aligned out of that sentence, and it would work, and it would make more sense. It would, out of that whole paragraph, it would make more sense if you removed the word aligned right there and just said AI. AGI. This is a silly point, I thought. Don't get me wrong, like, if it's unaligned in the sense of like very obvious to the customer, not aligned, like it's messing things up or it's like saying racist stuff on the LLM or something, then yeah, like we're gonna wanna align it, there'll be like a business center to do that. But in terms of like a CEO AI that can separately think from when it gives directions, its thoughts are not gonna guarantee to be good. No, it's gonna be manipulating and lying and all this stuff if given the ability and the abilities are emergent. So, aligned AI built upon heuristic comparatives makes for a reliable and beneficial partner in business. Let's consider a case study where heuristic comparatives were integrated into a corporation's internal chatbot system. The chatbot, now grounded in principles of reducing suffering, increasing prosperity, and increasing understanding, started to better comprehend its overarching goals. Rather than mechanically responding to user queries, it began to interpret the broader context of the conversations, leading to improved user experiences. Let's not forget, happy customers more often than not translate to returning customers. Y'all haven't read the literature on manipulation yet, have you? And also just, everything they think of this document is so based on LLMs, it's so not like extrapolating out to what if you have something else after LLMs, which I think is possible, but not guaranteed. LLMs are looking pretty good right now, but we'll see. So what's the secret sauce here? Here's the imperatives. They ensure that the AI maintains a broad perspective and understands the ultimate purpose, which aligns perfectly with the corporation's primary objective, prosperity. Primary objective is profit, my guys, not prosperity. You're being silly. By choosing to adopt aligned AI voluntarily, corporations can stay one step ahead of potential regulatory compliance issues. Yeah, that's a good one. No one, including corporations, wants to inadvertently steer us towards a cyberpunk dystopia or a skyline situation False. Yes, they do Every cyberpunk to scope dystopia involves big corporations in charge everyone else getting just Fucked by it. This is a silly Maybe not Skynet, but like no they want power my dude I'm sorry, like you would a little small small business and sure they're good people, but you would a big corporation In the context of AI, three primary motives stand out, economic growth, national security, and geopolitical influence. AI offers capabilities that can fortify a nation's defenses, yet also pose profound risks if misaligned. The necessity of alignment is thereby doubly emphasized.\" I think someone could do a pretty good argument to the US military and to every other military that they should be like diverting all funding into not only AI weapons but controllable AI like actual alignment and stuff I think you couldn't make the argument if you really got a leader who was enough of a tech pro there's a good argument there I think I think you actually could go that route and Get one of them to actually put all their money into AI research. I wouldn't be surprised if it could maybe happen to one or two of them. If it happens to one or two, it'll happen to all of them. Economic growth has long been tied to human capital and labor markets. Aligned AI has the potential to decouple this link, resulting in what we term unbounded productivity. With autonomous AI, businesses and services can operate at full capacity around the clock, unfettered by human limitations. This leads to an exponential increase in productivity, effectively removing the upper limit of GDP growth. I Mean AI will do that aligned AI will do that in a way that actually doesn't kill us all I'm in I'm very for symbiosis not alignment. I just I'm just don't know how much you can control them. I Don't know. This is the whole thing By adopting heuristic imperatives. No, thanks. Oh We can stimulate a surge in innovation. Oh, by adopting the one that specifically talks about increasing understanding of the universe. Yeah. Um, yeah. I don't know why I did a red highlight there. That was good. The autonomy of aligned AI also enables the on-shoring of industrial manufacturing capacities. As nations become more self-reliant they increase their resilience against international Economic shocks and supply chain disruptions thereby enhancing national security. I love this. I really would like a full-on Community level self-sufficiency is what I like. I'm not fully like The other day we're not gonna do chip fabs and every single small town obviously but in terms of like necessities, I would love community level self-sufficiency in the long term and our goal towards a Either post growth or infinite growth Utopia have you want to like think about it? Utilization of unaligned of aligned AI in intelligence agencies allows for superior data analysis pattern recognition and forecasting without violating ethical boundaries. It's important to note that the adoption of aligned AI at a national level is not a monolithic endeavor. Instead, it's a journey of continual progress and refinement. Yes. Legislate support for open source. Oh my god, if we could actually get legislation, like, forcing open source, like my idea I said a second ago of the whole day proprietary data thing Having a five-year limit or something to be amazing. So so great implement redistributive measures Yep, I'm gonna be needed necessary for sure reform education to include AI literacy Good luck with this one Like I don't even know if there's a point anymore and like how fast you can move with that kind of stuff But hopefully I mean definitely should try should definitely try Promote public-private partnerships in a yes in the certain thing Drawing inspirations from CERN we propose an international entity for AI Yeah, it would also act as a deterrent to an AI arms race fostering an environment of shared advancement rather than isolated competitive development. That'd be great Will it actually? Would we put if there was a huge international entity? Making crazy advancements. Would it not be possible for a single nation state to just take all those It was being open sourced and like make their own misaligned AI I Think it would like their own misaligned AI. The hope is just that the open source community, plus maybe the CERN body, could out-compete everyone else. Such an entity would champion the sharing of AI research findings. We could develop universal standards and guidelines. That'd be cool. Stepping into the seventh and final layer of the GATO framework, we find ourselves in the realm of global consensus. Here we are no longer just engaging with governmental bodies, researchers, or technologists. We are reaching out to the world at large, to every corner of the global village. This one's a pipe dream, it's so far away. I don't so much think that it's our job to do this and that AI will be good because of this, because I think we can't do this on our own. I think maybe an AGI could force us into doing this, maybe. Damn, no highlights for a while. Thank God. I'm pretty tired. Oh, wow, this is fantastic. Are we done? Okay, I'm pretty sure I didn't just like stop reading the article I think I actually read that far didn't any more highlights but maybe I just stopped reading. No this is all a community building stuff and whatever so it goes like a bunch of stuff like how to build your own gato Cell in community and everything and how to be a leader and how to Coordinate activities and stuff. That's what I was talking about the rest of that stuff. That's not my cup of tea but good a Good if one there too Anyways, that is it for today I'm tempted to put out a my similar, more specific implementations and advice stuff on this kind of thing, my own version at some point in the future. That's just a maybe though, we'll see if it actually happens. My ideas change every two weeks, kind of thing, what I'm working on. I never post anything at this point, so I've got to figure that out, but whatever. Yeah, end of video. Yeah, end of video.", "chunks": [{"timestamp": [0.0, 6.72], "text": " Okay, today we're going over two papers, the heuristic imperatives and the Gatow framework."}, {"timestamp": [6.72, 14.84], "text": " If you're not familiar, these are the brainchildren of David Shapiro, a big YouTuber in the AI"}, {"timestamp": [14.84, 15.84], "text": " space."}, {"timestamp": [15.84, 17.48], "text": " I generally watch his stuff."}, {"timestamp": [17.48, 20.84], "text": " I disagree with quite a bit of what he says and does, but for the most part, it's pretty"}, {"timestamp": [20.84, 21.84], "text": " freaking good."}, {"timestamp": [21.84, 22.84], "text": " I'm pretty amazing."}, {"timestamp": [22.84, 30.28], "text": " We're going to get into some disagreements here. I basically wanted to go over these two papers because I am interested in potentially proposing"}, {"timestamp": [30.28, 35.72], "text": " more specific ideas, kind of in this space sometime in the future, so I wanted to just"}, {"timestamp": [35.72, 39.56], "text": " go through and see what he's got going on, make sure I don't replicate anything, and"}, {"timestamp": [39.56, 41.24], "text": " give people a better sense, as well as just spread the word."}, {"timestamp": [41.24, 44.64], "text": " I think it's, although I disagree with some stuff in these papers, I think for the most"}, {"timestamp": [44.64, 50.08], "text": " part there's some good ideas, some honest attempts going on here, so I like it quite"}, {"timestamp": [50.08, 51.08], "text": " a lot."}, {"timestamp": [51.08, 53.64], "text": " With that, let's get started."}, {"timestamp": [53.64, 57.2], "text": " Heuristic Imperatives for Autonomous AI Systems."}, {"timestamp": [57.2, 61.52], "text": " Traditional approaches to AI ethics have focused on hard-coding specific rules or guidelines,"}, {"timestamp": [61.52, 66.52], "text": " but these methods can be inflexible and fail to account for the diverse and evolving nature of human values and context.\""}, {"timestamp": [66.52, 69.2], "text": " Yeah, the traditional approach is really just so bad."}, {"timestamp": [69.2, 70.2], "text": " Oh my god."}, {"timestamp": [70.2, 77.56], "text": " It's like, people are trying to define actual utility functions, actual goals effectively"}, {"timestamp": [77.56, 84.36], "text": " for the AI, but it's, it's, they've never sat through an economics class."}, {"timestamp": [84.36, 86.12], "text": " And if you sat through enough of them you'd realize that"}, {"timestamp": [86.64, 93.7], "text": " The idea of utility function of goals of rational actors of clarifying this stuff. It's all bullshit. It's no good"}, {"timestamp": [94.6, 96.96], "text": " That's a weird way the page moves. What the heck?"}, {"timestamp": [99.28, 102.0], "text": " Not one of that anyways, let's get to it"}, {"timestamp": [102.84, 106.68], "text": " Heuristic imperatives are set of fundamental guiding principles designed to be embedded"}, {"timestamp": [106.68, 108.88], "text": " into autonomous AI systems at various levels."}, {"timestamp": [108.88, 112.08], "text": " My big problem here with this whole heuristic imperative thing is just how do you embed"}, {"timestamp": [112.08, 113.08], "text": " them?"}, {"timestamp": [113.08, 114.08], "text": " It's not quite clear."}, {"timestamp": [114.08, 118.68], "text": " The methods that they've worked with up till now are just like putting it in the prompts,"}, {"timestamp": [118.68, 122.2], "text": " which is absolutely laughable, no offense."}, {"timestamp": [122.2, 125.84], "text": " It's just not going to do anything in the long term, but hopefully maybe someone will"}, {"timestamp": [125.84, 131.36], "text": " find an actual, legit way to embed embed these, not to just provide them and hope that they"}, {"timestamp": [131.36, 132.92], "text": " get used by the AI."}, {"timestamp": [132.92, 137.84], "text": " Anyway, what they are, the three here's the comparatives, reduce suffering in the universe,"}, {"timestamp": [137.84, 141.92], "text": " increase prosperity in the universe, and increase understanding in the universe."}, {"timestamp": [141.92, 150.56], "text": " I see why they put in this in the universe phrasing, but I'm not very fond. I would have liked both locally and in the universe, local and global"}, {"timestamp": [150.56, 156.48], "text": " level kind of thing in the actual phrasing, but whatever, that's just semantics. I don't"}, {"timestamp": [156.48, 161.76], "text": " like how this is scrolling right now. Okay, what is a heuristic? A heuristic is a problem-solving"}, {"timestamp": [161.76, 169.44], "text": " strategy or technique that simplifies complex decision-making processes by using shortcuts, approximations, or generalizations."}, {"timestamp": [169.44, 175.44], "text": " Why did it not switch?"}, {"timestamp": [175.44, 179.96], "text": " I'm so confused."}, {"timestamp": [179.96, 183.52], "text": " I swear."}, {"timestamp": [183.52, 184.52], "text": " What is with this?"}, {"timestamp": [184.52, 185.54], "text": " I don't like..."}, {"timestamp": [185.54, 193.34], "text": " Is my view messed up?"}, {"timestamp": [193.34, 194.34], "text": " View?"}, {"timestamp": [194.34, 196.56], "text": " Oh, continue with scroll."}, {"timestamp": [196.56, 198.56], "text": " That's why it's all messed up."}, {"timestamp": [198.56, 199.56], "text": " Imperative."}, {"timestamp": [199.56, 204.56], "text": " An imperative is a command, rule, or duty that must be followed or adhered to."}, {"timestamp": [204.56, 209.2], "text": " The term heuristic imperative implies that the principles are not exhaustive or absolute,"}, {"timestamp": [209.2, 213.8], "text": " the principles are flexible and adaptive, and the principles require balancing and tradeoffs."}, {"timestamp": [213.8, 215.68], "text": " I love all those."}, {"timestamp": [215.68, 216.68], "text": " Great."}, {"timestamp": [216.68, 222.88], "text": " I mean, not exhaustive or absolute is bad in the sense of like a first glance you might"}, {"timestamp": [222.88, 225.52], "text": " think oh no if it's not gonna be exhaustive absolute"}, {"timestamp": [225.52, 227.4], "text": " What if it's missing something big?"}, {"timestamp": [227.4, 229.4], "text": " And yeah, that's a valid question"}, {"timestamp": [229.6, 234.6], "text": " But the idea here of being not as awesome and absolute and actually the models or whatever we're using the AGI"}, {"timestamp": [234.68, 241.96], "text": " Hopefully I guess knowing the non exhaustive or absolute is kind of embedding a bit of a humility like we're being specifically broad"}, {"timestamp": [242.56, 244.2], "text": " and"}, {"timestamp": [244.2, 246.1], "text": " specifically not exhaustive or absolute"}, {"timestamp": [246.1, 251.48], "text": " because we want to allow some wiggle room and some chance for discussion and some humility"}, {"timestamp": [251.48, 258.72], "text": " some humbleness in the actual AI is making big decisions flexible and adaptive great"}, {"timestamp": [258.72, 262.74], "text": " require balancing and trade-offs I think you're gonna have to do that with anything you can't"}, {"timestamp": [262.74, 270.44], "text": " just give it one rule like you're gonna have to go that with anything. You can't just give it one rule. You're gonna have to give a bunch of rules. It has to balance between them because the world is too complicated for one rule, really."}, {"timestamp": [270.44, 274.0], "text": " The principle serves as intrinsic motivations."}, {"timestamp": [274.0, 278.4], "text": " They never actually clarified why this is true."}, {"timestamp": [278.4, 282.7], "text": " Like I said, the previous experiments embedding these imperatives into AI systems have been"}, {"timestamp": [282.7, 286.72], "text": " just literally putting in the prompt for an LLM."}, {"timestamp": [291.68, 297.04], "text": " That'll do great for the prompts for a little short prompt kind of thing for the meantime, but come on, like, we have no actual way to guarantee that the AIs want to follow this. We can give"}, {"timestamp": [297.04, 300.96], "text": " them a constitution, we can tell them rules, we can ask them for something, but if you're going to"}, {"timestamp": [302.48, 309.12], "text": " do this instead of an actual utility function, then that's just not actually guaranteed to be followed, and utility functions have their"}, {"timestamp": [309.12, 315.28], "text": " own issues, so it's just a whole nightmare of can of worms, whatever."}, {"timestamp": [315.28, 319.28], "text": " Each imperative, when considered in isolation, could potentially lead to undesirable outcomes."}, {"timestamp": [319.28, 324.88], "text": " However, when combined, they complement and counterbalance each other."}, {"timestamp": [324.88, 325.28], "text": " Yeah, cool. combined they complement and counterbalance each other."}, {"timestamp": [327.56, 332.96], "text": " Yeah cool, immutable constitution. So we're going for the same idea as the US"}, {"timestamp": [332.96, 335.8], "text": " Constitution here. I think, I don't know if the US actually even had a"}, {"timestamp": [335.8, 339.48], "text": " constitution, but I think they might have the United States. I'm probably"}, {"timestamp": [339.48, 344.68], "text": " wrong on that, but the idea here being we have this broad set of rules and it"}, {"timestamp": [344.68, 350.36], "text": " can't change, but all other rules downstream must be made based off those broad rules kind of thing"}, {"timestamp": [351.32, 353.32], "text": " Embedding the imperative at multiple levels"}, {"timestamp": [354.16, 356.16], "text": " How you gonna do it? I don't know"}, {"timestamp": [356.76, 361.96], "text": " Periodic evaluation self-assessment. How are you gonna make it do that to itself? I don't know like this is easy"}, {"timestamp": [361.96, 368.4], "text": " I think the problem with this documents and with the gado as well I I mean, just this one on the Gato, the problem with this document"}, {"timestamp": [368.4, 376.32], "text": " is the context, the mindset is so framed by LLMs, by language models, like that's what"}, {"timestamp": [376.32, 379.36], "text": " they're building this for effectively, is they're assuming language models are the"}, {"timestamp": [379.36, 386.82], "text": " thing that actually go crazy. Might be true. That's very possible. But if it's not LLMs, then a lot of this stuff"}, {"timestamp": [386.82, 391.42], "text": " just makes no sense, like it's not going to be actually applicable. And also, same problem"}, {"timestamp": [391.42, 395.56], "text": " I have with all alignment stuff is like, sure you can align your AGI, but what if someone"}, {"timestamp": [395.56, 401.26], "text": " else doesn't? Like you need a system that is robust to bad actors."}, {"timestamp": [401.26, 407.0], "text": " Human oversight and collaboration. Good luck overseeing these things when there are 10,000 John von Neumanns"}, {"timestamp": [407.0, 411.0], "text": " working at 10 times, like 100,000 times the speed of people thoughts or something"}, {"timestamp": [411.0, 413.0], "text": " like yeah, no thanks"}, {"timestamp": [413.0, 418.0], "text": " the closest I see to human collaboration and oversight"}, {"timestamp": [418.0, 422.0], "text": " is like maybe some blockchain decentralized market mechanisms"}, {"timestamp": [422.0, 429.06], "text": " of the AI's need to provide us what we want in"}, {"timestamp": [429.06, 432.54], "text": " order to get compute power like there's like a market mechanism going on where"}, {"timestamp": [432.54, 438.48], "text": " they just to pay for compute power maybe maybe that's possible but it's silly to"}, {"timestamp": [438.48, 442.44], "text": " me to think we can just like keep them running at human speed and like pause"}, {"timestamp": [442.44, 446.0], "text": " every every two thoughts and only with them like keep going with it"}, {"timestamp": [446.0, 447.0], "text": " resume those"}, {"timestamp": [447.0, 450.14], "text": " we've got one of those two thoughts are good like it's i don't think so"}, {"timestamp": [450.14, 452.32], "text": " transparent explainable a i"}, {"timestamp": [452.32, 456.0], "text": " uh... if you know my channel you know that explainable a i so far as absolute"}, {"timestamp": [456.0, 457.96], "text": " baloney uh..."}, {"timestamp": [457.96, 460.88], "text": " some like eighty percent of explainable a i techniques"}, {"timestamp": [460.88, 462.64], "text": " actually explainable all this like"}, {"timestamp": [462.64, 463.72], "text": " they make up a technique"}, {"timestamp": [463.72, 464.96], "text": " and the color explainable"}, {"timestamp": [464.96, 468.08], "text": " but they give no actual metric or really solid explanation"}, {"timestamp": [468.08, 470.52], "text": " or reasoning as to why that's actually super explainable. All they're doing is pulling"}, {"timestamp": [470.52, 474.28], "text": " out features most of the time. And even the ones that do attempt to actually measure this"}, {"timestamp": [474.28, 478.0], "text": " stuff, they fail miserably. Like it's not a good field right now. Explainable AI is"}, {"timestamp": [478.0, 482.88], "text": " absolutely just floundering, drowning. It's not doing anything. Reason being, these are"}, {"timestamp": [482.88, 486.48], "text": " big complex systems that just are probably"}, {"timestamp": [486.48, 490.48], "text": " not going to be explainable to a certain extent. Like, if we had already figured out the human"}, {"timestamp": [490.48, 493.6], "text": " brain, I'd be confident, but I don't think this is going to happen. Explainable AI is,"}, {"timestamp": [494.48, 497.6], "text": " I think, a pipe dream. I hope not a pipe dream. I hope I'm wrong, but whatever."}, {"timestamp": [498.4, 502.24], "text": " As our understanding of ethical AI and the control problem evolves, it is essential to"}, {"timestamp": [502.24, 506.22], "text": " continuously refine the implementation of heuristic imperatives in AI systems.\""}, {"timestamp": [506.22, 508.86], "text": " Yeah, cool."}, {"timestamp": [508.86, 517.06], "text": " As long as the takeoff is slow, this is doable, and I certainly agree we should do it."}, {"timestamp": [517.06, 520.26], "text": " This is a bunch of stuff I found less interesting."}, {"timestamp": [520.26, 523.38], "text": " AI as a facilitator, not a dictator."}, {"timestamp": [523.38, 530.4], "text": " I would love to reach that too, but I don't know, man."}, {"timestamp": [530.4, 531.4], "text": " Good luck."}, {"timestamp": [531.4, 534.12], "text": " Good luck."}, {"timestamp": [534.12, 535.72], "text": " Personal autonomy."}, {"timestamp": [535.72, 539.28], "text": " Individual autonomy is not explicitly stated in these three heuristic imperatives, however"}, {"timestamp": [539.28, 544.2], "text": " it is implicitly derived from the combination of these imperatives and their underlying"}, {"timestamp": [544.2, 545.82], "text": " intentions."}, {"timestamp": [545.82, 548.84], "text": " Red means I must have disagreed somehow."}, {"timestamp": [548.84, 551.0], "text": " So first, reducing suffering in the universe."}, {"timestamp": [551.0, 555.7], "text": " One of the key aspects of reducing suffering is respecting and protecting individual rights,"}, {"timestamp": [555.7, 558.48], "text": " including the right to make choices about one's own life."}, {"timestamp": [558.48, 563.1], "text": " I can't say I fully agree with that."}, {"timestamp": [563.1, 567.4], "text": " In theory, in ideal, yeah, but I think a lot of people"}, {"timestamp": [567.4, 569.84], "text": " don't actually want their individual rights."}, {"timestamp": [569.84, 573.24], "text": " I think a lot of people are not quite that set of"}, {"timestamp": [573.24, 574.56], "text": " consciousness where they actually like"}, {"timestamp": [574.56, 576.0], "text": " can think for themselves very well"}, {"timestamp": [576.0, 577.4], "text": " or should be thinking for themselves very well."}, {"timestamp": [577.4, 580.48], "text": " No offense, it's just certain people,"}, {"timestamp": [580.48, 583.48], "text": " it's that, what's it called?"}, {"timestamp": [583.48, 584.68], "text": " I'm not gonna get into it, I'm not gonna get into it."}, {"timestamp": [584.68, 587.12], "text": " But I also just don't see how"}, {"timestamp": [588.22, 592.36], "text": " the connection from reduced suffering to individual rights, I mean"}, {"timestamp": [594.42, 600.04], "text": " If anything I feel it'd be easier to argue that it'd be easier to reduce suffering if you took away rights and just like"}, {"timestamp": [600.22, 609.76], "text": " Made a forced utopia like I'm like Wally like that movie kind of thing, right? I'm not so confident in this connection here."}, {"timestamp": [609.76, 618.28], "text": " Prosperity is closely linked to personal well-being, which is often connected to individual autonomy."}, {"timestamp": [618.28, 619.28], "text": " Kind of."}, {"timestamp": [619.28, 622.36], "text": " A lot of people, I think, just do better with directions."}, {"timestamp": [622.36, 623.36], "text": " I'm not going to lie."}, {"timestamp": [623.36, 626.44], "text": " A lot of people need to be in the military because they need that that regiment that"}, {"timestamp": [627.4, 633.74], "text": " Although you could also argue that a discipline creates freedoms as a whole separate argument. I don't know but these are very loose"}, {"timestamp": [634.36, 637.72], "text": " Conjectures. All right, and I get that that's all we can do right now and that's fine"}, {"timestamp": [637.72, 642.0], "text": " But I just I don't want to bet my life on it or the bet humanity on it either"}, {"timestamp": [642.88, 648.16], "text": " But promoting understanding requires respecting and appreciating the diversity of human experiences,"}, {"timestamp": [648.16, 649.16], "text": " beliefs, and values."}, {"timestamp": [649.16, 654.04], "text": " Yeah, I agree with this idea of increased understanding which is, I guess, I think he"}, {"timestamp": [654.04, 668.0], "text": " inspired Elon Musk's new like, curiosity is the goal AI whatever for his company XAI. Yeah, I think in an innately curious system might want to keep"}, {"timestamp": [668.0, 672.72], "text": " individuality and diversity at rounds even if that stuff is"}, {"timestamp": [672.72, 679.64], "text": " inherently useless. The same way like we value or we are starting our culture"}, {"timestamp": [679.64, 683.64], "text": " starting to value cultures that have been stomped to the dust kind of thing."}, {"timestamp": [683.64, 688.64], "text": " We're starting to say like, oh we shouldn't have stomped them into the dust, we wish we hadn't, we want to rejuvenate them"}, {"timestamp": [688.64, 693.92], "text": " kind of thing, or even just like the planet and like nature species. I do think there is an aspect"}, {"timestamp": [693.92, 701.52], "text": " of curiosity that might lead us to individual rights, autonomy, like thriving for all. That's"}, {"timestamp": [701.52, 707.0], "text": " the the one heuristic that I'm actually pretty excited about, this idea, basically."}, {"timestamp": [707.0, 708.0], "text": " Trustworthy AI."}, {"timestamp": [708.0, 711.36], "text": " In order to effectively reduce suffering, AI systems must be perceived as trustworthy"}, {"timestamp": [711.36, 713.32], "text": " by users and stakeholders."}, {"timestamp": [713.32, 715.18], "text": " Yeah, fair."}, {"timestamp": [715.18, 718.8], "text": " Trust is essential for promoting prosperity, as people are more likely to engage with and"}, {"timestamp": [718.8, 723.32], "text": " benefit from AI systems that they perceive as reliable and dependable."}, {"timestamp": [723.32, 729.0], "text": " Yeah, I'm not sure how much it'll need to engage this to actually do what it wants to"}, {"timestamp": [729.0, 733.0], "text": " do for the long term, but especially in the short term."}, {"timestamp": [733.0, 734.0], "text": " Yeah."}, {"timestamp": [734.0, 735.0], "text": " Cool."}, {"timestamp": [735.0, 740.0], "text": " I think that's it for the Curious Comparatives."}, {"timestamp": [740.0, 746.96], "text": " Oh, I wanted to point out here also, so GPT-4 wrote this paper basically, and you can kind"}, {"timestamp": [746.96, 751.8], "text": " of tell, or at least post-hoc I can tell, I guess maybe I couldn't tell if I didn't"}, {"timestamp": [751.8, 762.18], "text": " know this, but the language is very robust and repetitive in structure, and in many ways"}, {"timestamp": [762.18, 766.08], "text": " does not show an awareness of the whole, gbd4 can only output so much"}, {"timestamp": [766.08, 772.8], "text": " at a time kind of thing in many ways like the fact that it says like in conclusion at the end of"}, {"timestamp": [772.8, 776.32], "text": " every single paragraph every single conclusion paragraph every single section is kind of a lot"}, {"timestamp": [776.96, 783.28], "text": " in conclusion probably but there's one a bit closer here okay i'm wrong it's not doing every"}, {"timestamp": [783.28, 787.46], "text": " single time but um i don't know i't know if I could tell but that being said"}, {"timestamp": [787.9, 792.42], "text": " It's a lot less work if you have GPT before right it for concepts as simple as this"}, {"timestamp": [792.78, 796.28], "text": " Yeah, man, like go for it have GPT for right? It's and I love"}, {"timestamp": [797.28, 798.72], "text": " Before I saw that he did this"}, {"timestamp": [798.72, 803.88], "text": " I love them that he just included the raw transcript in there and like stated GPT for wrote it"}, {"timestamp": [804.3, 806.6], "text": " That's something I want to do in the future is just including the raw"}, {"timestamp": [806.6, 810.72], "text": " transcripts from every paper that I basically write, which I've had a few now"}, {"timestamp": [810.72, 814.36], "text": " although not all of them have been like fully finished and fleshed out for various"}, {"timestamp": [814.36, 818.76], "text": " reasons, but I love this idea of including the actual raw transcript."}, {"timestamp": [818.76, 833.8], "text": " That's great. And the font change theme I thought was pretty good font to choose I don't know why I care about that but I do anyways that one's done let's hit the gato framework"}, {"timestamp": [833.8, 838.52], "text": " the decentralized path to utopia ignore the yellow this is the bigger whole"}, {"timestamp": [838.52, 845.12], "text": " thing this view is messed up again. Continue with scroll."}, {"timestamp": [845.12, 848.2], "text": " It's like 60 pages, 70 pages, something dumb."}, {"timestamp": [848.2, 849.2], "text": " It's too much in here."}, {"timestamp": [849.2, 850.4], "text": " It's like I said, GPT-4 was just so verbose."}, {"timestamp": [850.4, 853.52], "text": " Although I don't think GPT-4 wrote this one, I think it's, but I can tell there's quite"}, {"timestamp": [853.52, 859.96], "text": " a bit in there that's just, y'all could have trimmed this to 40 pages easily, just saying."}, {"timestamp": [859.96, 864.72], "text": " A grim picture was painted by AI ethicist Connor Leahy, love him."}, {"timestamp": [864.72, 867.32], "text": " There are techno-elites that want to be trillionaires"}, {"timestamp": [867.32, 868.46], "text": " and want to live forever,"}, {"timestamp": [868.46, 869.92], "text": " and they're willing to risk everything"}, {"timestamp": [869.92, 871.2], "text": " to get what they want."}, {"timestamp": [871.2, 874.04], "text": " Yeah, and that sucks,"}, {"timestamp": [874.04, 876.12], "text": " and we might fall victim to that."}, {"timestamp": [876.12, 876.96], "text": " It's just a shame."}, {"timestamp": [876.96, 879.8], "text": " I really believe that that kind of stuff is a trauma."}, {"timestamp": [879.8, 880.64], "text": " I'm really not."}, {"timestamp": [880.64, 881.84], "text": " It's just a shame."}, {"timestamp": [881.84, 884.1], "text": " Let me get my, sorry, that was annoying me."}, {"timestamp": [886.24, 891.28], "text": " I really think that the trauma the billionaire mindset is a trauma like they don't"}, {"timestamp": [891.28, 895.28], "text": " understand what enough money is I know the difference between like actual cash"}, {"timestamp": [895.28, 901.16], "text": " billions versus company owning asset billions whatever I was an economist I"}, {"timestamp": [901.16, 905.96], "text": " get it but end of the day still even just a hundred millionaires or"}, {"timestamp": [906.24, 908.52], "text": " borderline tens a million kind of thing"}, {"timestamp": [909.9, 911.52], "text": " There's such thing as enough guys"}, {"timestamp": [911.52, 916.26], "text": " there's such thing as enough and I really believe that's that lack of awareness of enough that lack of ability to say that there's"}, {"timestamp": [916.26, 918.26], "text": " enough is a"}, {"timestamp": [918.28, 920.28], "text": " trauma response just like"}, {"timestamp": [920.8, 924.36], "text": " People who are hoarders of goods like of like you go and like they're hoarders like in their house"}, {"timestamp": [924.36, 925.2], "text": " You can't walk and I think they're hoarders. I, like you go in their house and you can't walk,"}, {"timestamp": [925.2, 926.2], "text": " kind of thing, they're hoarders."}, {"timestamp": [926.2, 929.24], "text": " I think it's the same thing with wealth."}, {"timestamp": [929.24, 932.68], "text": " The more ominous risk lies in the potential emergence of artificial superintelligence."}, {"timestamp": [932.68, 934.44], "text": " Yeah, yeah, yeah, yeah, yeah, yeah."}, {"timestamp": [934.44, 936.4], "text": " We've all heard it before."}, {"timestamp": [936.4, 939.6], "text": " Heuristic imperatives are a big part of what they're doing here, apparently."}, {"timestamp": [939.6, 941.88], "text": " It comes up a lot."}, {"timestamp": [941.88, 948.2], "text": " The global alignment taxonomy omnibus, G GATO framework represents a blueprint for a global"}, {"timestamp": [948.2, 953.12], "text": " initiative, a decentralized effort that requires contributions from all corners of the world."}, {"timestamp": [953.12, 957.0], "text": " This is basically a big call to action paper for a decentralized movement, just like everyone"}, {"timestamp": [957.0, 959.76], "text": " doing the best they can basically."}, {"timestamp": [959.76, 964.74], "text": " This thing called GATO layers, the big pillars of the actual whole setup."}, {"timestamp": [964.74, 965.04], "text": " First one is"}, {"timestamp": [965.04, 969.64], "text": " model alignment. I've discussed before my opinions on alignment but we're gonna"}, {"timestamp": [969.64, 975.8], "text": " skip past that for now. They are pro-autonomous agents. Love this. You"}, {"timestamp": [975.8, 978.64], "text": " would think that if they're big on alignment that they're anti-autonomy"}, {"timestamp": [978.64, 983.32], "text": " in agents but the goal here is a very optimistic one. They want"}, {"timestamp": [983.32, 986.0], "text": " utopia is the goal of Gato,"}, {"timestamp": [986.0, 988.28], "text": " which is pretty cool."}, {"timestamp": [988.28, 991.84], "text": " Decentralized networks, recognizing the strength and potential of distributed systems, this"}, {"timestamp": [991.84, 997.0], "text": " layer focuses on utilizing decentralized networks like blockchain, DAOs, and federations."}, {"timestamp": [997.0, 1001.12], "text": " That'll be a huge part, I think."}, {"timestamp": [1001.12, 1005.0], "text": " I was very anti-crypto blockchain all this stuff while it was"}, {"timestamp": [1011.32, 1013.74], "text": " Big and happening. I guess still is kind of I was saying since the beginning basically that not quite the beginning but pretty much once I understood everything that"}, {"timestamp": [1014.8, 1019.24], "text": " The investment nature of it is just so messed up and it's shooting us off in the foot"}, {"timestamp": [1019.24, 1047.92], "text": " Basically, and the mutual benefits"}, {"timestamp": [1047.92, 1048.92], "text": " of alignment."}, {"timestamp": [1048.92, 1053.36], "text": " It advocates for businesses to perceive AI alignment not as an obligation, but as a strategic"}, {"timestamp": [1053.36, 1056.76], "text": " advantage that enhances business performance and contributes positively to their bottom"}, {"timestamp": [1056.76, 1057.76], "text": " line."}, {"timestamp": [1057.76, 1060.12], "text": " Good luck with that."}, {"timestamp": [1060.12, 1066.52], "text": " Dan Hendricks No, businesses recently talking about this kind of stuff. Um, no businesses like big ones at Google, whatever. Um,"}, {"timestamp": [1066.54, 1070.56], "text": " they're in a race to publish the best thing and they are lowering their safety"}, {"timestamp": [1070.56, 1075.24], "text": " checks, um, for sake of competition. And, uh,"}, {"timestamp": [1075.28, 1078.56], "text": " the, when you start having like AI's take over companies and stuff,"}, {"timestamp": [1078.56, 1083.2], "text": " the AIs that are both smart enough and immoral enough to lie,"}, {"timestamp": [1083.2, 1083.84], "text": " cheat, steal,"}, {"timestamp": [1083.84, 1087.62], "text": " whatever are going to have an advantage in our current economic system just the same way that"}, {"timestamp": [1087.62, 1092.94], "text": " people who are smart and immoral enough to lie, cheat, steal, and get away with it"}, {"timestamp": [1092.94, 1097.46], "text": " have been doing so and have been getting rich off of that thing for a while now."}, {"timestamp": [1097.46, 1102.18], "text": " This sentence only makes sense if you believe that all billionaires got there"}, {"timestamp": [1102.18, 1106.56], "text": " based off of actually contributing billions worth of value to the economy"}, {"timestamp": [1106.68, 1109.22], "text": " no, no, no, they contributed millions worth and then they"}, {"timestamp": [1109.88, 1112.6], "text": " took advantage of the labor power of their employees and"}, {"timestamp": [1113.64, 1115.64], "text": " didn't allow"}, {"timestamp": [1117.76, 1122.74], "text": " Returns to labor to track the same returns of capital. That's what happened. Anyways"}, {"timestamp": [1123.76, 1125.46], "text": " National regulation. I'm pretty anti in general what happened anyways national regulation I'm"}, {"timestamp": [1125.46, 1128.82], "text": " pretty anti in general a few civic things I'm okay with but a lot in the"}, {"timestamp": [1128.82, 1132.98], "text": " large sense I think that the regulatory bodies are not smart enough to do"}, {"timestamp": [1132.98, 1136.98], "text": " anything here I'm certainly not capable enough to do it fast enough strategic"}, {"timestamp": [1136.98, 1140.7], "text": " national interest that enhances GDP 4% national security and bolsters"}, {"timestamp": [1140.7, 1148.72], "text": " geopolitical influence yeah yeah like AI is going to be big for that."}, {"timestamp": [1148.72, 1154.12], "text": " They said alignment will be big for national interest and national security and geopolitical"}, {"timestamp": [1154.12, 1155.12], "text": " influence."}, {"timestamp": [1155.12, 1156.6], "text": " I don't know how confident I am in that."}, {"timestamp": [1156.6, 1161.08], "text": " I think countries are going to run like war machines basically, to a large extent."}, {"timestamp": [1161.08, 1165.5], "text": " I'm not very confident in what the US Army will do when um, it gets"}, {"timestamp": [1165.5, 1173.26], "text": " an AGI that's not independent if it gets like a baby AGI. International Treaty"}, {"timestamp": [1173.26, 1179.14], "text": " international entity akin to CERN. Yeah, throw some throw billions in here"}, {"timestamp": [1179.14, 1183.52], "text": " please. Oh my god I would love that. Specifically a fully open source"}, {"timestamp": [1183.52, 1185.0], "text": " everything published"}, {"timestamp": [1186.92, 1190.4], "text": " international entity yeah, yeah, and with like collaboration with businesses and stuff like I"}, {"timestamp": [1193.6, 1195.6], "text": " Don't like the idea of"}, {"timestamp": [1196.24, 1200.28], "text": " Corporations like Google and whatever being the first to hit the big model the big AGI"}, {"timestamp": [1200.28, 1204.28], "text": " I don't like the idea of individual governments doing it either but"}, {"timestamp": [1206.46, 1211.16], "text": " AGI. I don't like the idea of individual governments doing it either, but if we had an actual huge treaty between, let's say, at least NATO members, if not also including China and BRICS and"}, {"timestamp": [1211.16, 1216.76], "text": " everything, I would be pretty excited for, I would be much more optimistic as to the"}, {"timestamp": [1216.76, 1222.06], "text": " existence of a trillion parameter model within that entity than I would among an individual"}, {"timestamp": [1222.06, 1228.8], "text": " government or among a corporation. Global Consensus The apex layer is centered around achieving a worldwide agreement on the"}, {"timestamp": [1228.8, 1236.24], "text": " principles of the Gatto framework. It's about creating a global discourse. Global discourse,"}, {"timestamp": [1236.24, 1242.72], "text": " sure. Gatto framework, global, no. I mean, it'll exist globally, but will it actually be global"}, {"timestamp": [1242.72, 1248.66], "text": " consensus? No. It's gonna be way more methods of alignments, way more contradictory opinions, huge bad"}, {"timestamp": [1248.66, 1250.62], "text": " actor regimes."}, {"timestamp": [1250.62, 1254.6], "text": " Our inspirations are as diverse as they are profound, ranging from the transformative"}, {"timestamp": [1254.6, 1260.28], "text": " ethos of 12-step programs to the radical inclusivity of Burning Man and the grassroots empowerment"}, {"timestamp": [1260.28, 1262.16], "text": " of the Occupy movement."}, {"timestamp": [1262.16, 1263.16], "text": " Cool."}, {"timestamp": [1263.16, 1267.08], "text": " Cool cool cool. Start where you are, use what you have, do what"}, {"timestamp": [1267.08, 1271.28], "text": " you can. Big believer in this. Tend to the garden with which you can reach, the parts of the garden"}, {"timestamp": [1271.28, 1275.6], "text": " with which you can reach. Broadcast your findings. Huge believer. Let's go full open source, people."}, {"timestamp": [1275.6, 1282.08], "text": " Please. Everyone. Everything. Full open source. Think globally, act locally. Yeah, I'm not very"}, {"timestamp": [1282.08, 1285.04], "text": " good at that, but I'd like to get better. Think exponentially."}, {"timestamp": [1285.04, 1286.04], "text": " Big one."}, {"timestamp": [1286.04, 1287.88], "text": " People don't understand how fast stuff is moving."}, {"timestamp": [1287.88, 1289.08], "text": " This is huge."}, {"timestamp": [1289.08, 1290.08], "text": " Think exponentially."}, {"timestamp": [1290.08, 1292.4], "text": " Although, we can't think exponentially, really."}, {"timestamp": [1292.4, 1296.12], "text": " It's too fucking hard, but whatever."}, {"timestamp": [1296.12, 1297.12], "text": " I hadn't heard this before."}, {"timestamp": [1297.12, 1302.8], "text": " I hadn't heard the actual word, the definition of this before, the vocab word used."}, {"timestamp": [1302.8, 1303.8], "text": " Mesa optimization."}, {"timestamp": [1303.8, 1309.24], "text": " Happens when a trained model termed the base Optimizer creates a MESA Optimizer, a model"}, {"timestamp": [1309.24, 1311.6], "text": " that optimizes for a different objective."}, {"timestamp": [1311.6, 1315.36], "text": " This can lead to a disconnect between the Base Optimizer's intended goal and the MESA"}, {"timestamp": [1315.36, 1319.52], "text": " Optimizer's actual objective, raising concerns over the model's alignment with its original"}, {"timestamp": [1319.52, 1321.4], "text": " training intent."}, {"timestamp": [1321.4, 1325.98], "text": " So we are MESA, base optimizers currently creating MESA"}, {"timestamp": [1325.98, 1332.0], "text": " optimizers that are AIs. The scary thing is when AI start to create their own"}, {"timestamp": [1332.0, 1338.9], "text": " MESA optimizers. We must set clear and measurable milestones and KPIs. I was"}, {"timestamp": [1338.9, 1342.68], "text": " surprised to see this in here. Only ones that I thought were worth talking about."}, {"timestamp": [1342.68, 1345.44], "text": " Number of open source aligned models. I about, number of open source aligned models."}, {"timestamp": [1345.44, 1351.68], "text": " I had to number open source models, period, but aligned, sure, I guess if you want to."}, {"timestamp": [1351.68, 1353.44], "text": " Number of open source data sets."}, {"timestamp": [1353.44, 1354.44], "text": " Yes, yes."}, {"timestamp": [1354.44, 1361.4], "text": " I really, as far as regulation goes, only one of the few things I would support are"}, {"timestamp": [1361.4, 1364.84], "text": " a full open sourcing of all data sets, or maybe not full open sourcing of all data sets,"}, {"timestamp": [1364.84, 1366.58], "text": " maybe like a period of time like if you're a"}, {"timestamp": [1366.58, 1369.52], "text": " big company with your own big proprietary data set that the acute"}, {"timestamp": [1369.52, 1374.16], "text": " advantage of kind of thing I think we should inspiration how patents only last"}, {"timestamp": [1374.16, 1378.36], "text": " 20 years and we should say that if you have a data set that any data that is"}, {"timestamp": [1378.36, 1382.62], "text": " greater than 20 years old must be released open source 20 is extreme"}, {"timestamp": [1382.62, 1388.92], "text": " though 20 is like a figment of the constitutional times, the 1700s, 1800s when this stuff was invented for patents. I think"}, {"timestamp": [1388.92, 1392.52], "text": " five years is more realistic, five years is gonna be a crazy long time period in"}, {"timestamp": [1392.52, 1398.84], "text": " this new world we're heading into. I'm also gonna do a paper on that I think,"}, {"timestamp": [1398.84, 1403.72], "text": " potentially, on the idea of proprietary data sets and time limits on them."}, {"timestamp": [1403.72, 1406.18], "text": " Alignment drift over iterations."}, {"timestamp": [1406.18, 1409.56], "text": " Over multiple training iterations or after self-modification, it's crucial to measure"}, {"timestamp": [1409.56, 1412.7], "text": " how well models maintain their alignment."}, {"timestamp": [1412.7, 1414.7], "text": " Good luck with that!"}, {"timestamp": [1414.7, 1418.82], "text": " Huh, silly."}, {"timestamp": [1418.82, 1423.86], "text": " One key advocacy point is to ensure all communication between components occurs in natural language,"}, {"timestamp": [1423.86, 1424.86], "text": " making it human-readable."}, {"timestamp": [1424.86, 1427.88], "text": " Haha, good luck with that again. Are you kidding me?"}, {"timestamp": [1427.88, 1429.64], "text": " That's so inefficient. Um,"}, {"timestamp": [1430.6, 1435.0], "text": " if they can create their own languages they're going to, um,"}, {"timestamp": [1435.08, 1437.36], "text": " and even so like, let's say even if you were to do this,"}, {"timestamp": [1437.38, 1441.88], "text": " even if you were to like get them to actually talk in English to each other,"}, {"timestamp": [1441.88, 1443.48], "text": " all the AI is right at all times,"}, {"timestamp": [1443.88, 1464.44], "text": " just English language and maybe some Python code for example, stuff, right. We"}, {"timestamp": [1464.44, 1467.76], "text": " envisioned a future teeming with autonomous agents, their numbers could reach into the"}, {"timestamp": [1467.76, 1471.6], "text": " trillions existing in diverse forms and fulfilling various roles."}, {"timestamp": [1471.6, 1475.74], "text": " Yes, this will not be a uniform monolithic group of agents, but rather a diverse array"}, {"timestamp": [1475.74, 1479.84], "text": " of entities, each with their unique capabilities, preferences, and area of specialization."}, {"timestamp": [1479.84, 1482.08], "text": " So first off, this assumes a slow takeoff."}, {"timestamp": [1482.08, 1489.08], "text": " If we have a fast takeoff Sorry, I'm not gonna happen. But um, also the thing I was calling the question is like"}, {"timestamp": [1490.92, 1497.28], "text": " Is it even accurate to consider them all separate at that point like if we have this huge ecosystem of AI agents"}, {"timestamp": [1497.28, 1499.28], "text": " I think if I think"}, {"timestamp": [1499.68, 1501.98], "text": " people imagine that like an intelligent entity is"}, {"timestamp": [1502.68, 1507.44], "text": " a single units is a single thing a single being but like I"}, {"timestamp": [1508.88, 1512.62], "text": " Think intelligence is like a fractal like structure going up groups kind of thing like a"}, {"timestamp": [1513.52, 1519.96], "text": " Communities groups nations corporations, whatever. They are super intelligence in themselves and you yourself"}, {"timestamp": [1520.64, 1521.56], "text": " be a"}, {"timestamp": [1521.56, 1522.68], "text": " Cut that skull open of yours"}, {"timestamp": [1522.68, 1528.48], "text": " We split down the center on the corporates colossum and then we like seal it back up again and we let your different sides of the brain we put a little divider here"}, {"timestamp": [1528.48, 1530.44], "text": " It's sorry look up um"}, {"timestamp": [1530.44, 1536.6], "text": " Look up split brain experiments. The CGP gray is a great video on this. It's called a you are to basically like your own in"}, {"timestamp": [1536.6, 1538.04], "text": " your brain is"}, {"timestamp": [1538.04, 1540.76], "text": " multiple consciousnesses is multiple intelligences all"}, {"timestamp": [1541.44, 1548.46], "text": " Like contributing and combining together to make one intelligence, right? I think it's at a certain point if you have this big thing happening"}, {"timestamp": [1548.46, 1554.0], "text": " we all become one hive mind effectively, but not a hive mind with a centralized source of decision-making more like a"}, {"timestamp": [1554.52, 1557.92], "text": " hive mind dependent on individual nodes a"}, {"timestamp": [1558.82, 1560.82], "text": " bottom-up hive mind"}, {"timestamp": [1563.2, 1568.8], "text": " Interesting as our agents evolve it's essential to track their level of autonomy. We could adapt a scale"}, {"timestamp": [1568.8, 1572.96], "text": " similar to the levels of autonomy used for self-driving cars. Higher levels of autonomy"}, {"timestamp": [1572.96, 1576.84], "text": " should be celebrated, but also treated as opportunities for further scrutiny of alignment."}, {"timestamp": [1576.84, 1586.08], "text": " It's vital to test the agent's resilience against alignment drift."}, {"timestamp": [1586.08, 1589.28], "text": " By simulating various scenarios and challenges, we can evaluate how robustly"}, {"timestamp": [1596.8, 1599.68], "text": " the agents maintain their alignment over time and under different conditions."}, {"timestamp": [1601.44, 1604.56], "text": " One second, I think I'm going to turn off the second display because this is not helping my..."}, {"timestamp": [1605.48, 1607.04], "text": " Oh god, the battery's draining."}, {"timestamp": [1607.04, 1608.96], "text": " Yeah, no thank you."}, {"timestamp": [1608.96, 1612.72], "text": " Let's shut this off."}, {"timestamp": [1612.72, 1616.64], "text": " My meter's lagging, this is not good."}, {"timestamp": [1616.64, 1620.24], "text": " M2 can't handle it like that."}, {"timestamp": [1620.24, 1629.3], "text": " Alright, probably should be good now, I would guess."}, {"timestamp": [1629.3, 1635.38], "text": " Still lagging a bit, but how's OBS doing?"}, {"timestamp": [1635.38, 1636.86], "text": " OBS?"}, {"timestamp": [1636.86, 1638.26], "text": " Better?"}, {"timestamp": [1638.26, 1641.98], "text": " Oh, much faster response."}, {"timestamp": [1641.98, 1648.84], "text": " Okay, back to it. In our utopian vision, consensus mechanisms can be"}, {"timestamp": [1648.84, 1653.0], "text": " harnessed to enforce and reward alignment with heuristic imperatives. For instance,"}, {"timestamp": [1653.0, 1655.88], "text": " agents that demonstrate consistent adherence to the imperatives can be rewarded with greater"}, {"timestamp": [1655.88, 1660.78], "text": " influence in the consensus process, incentivizing alignment. Conversely, agents that deviate"}, {"timestamp": [1660.78, 1666.48], "text": " from the imperatives can be penalized and discouraging unaligned behavior."}, {"timestamp": [1667.92, 1672.92], "text": " Good luck monitoring them like that. No, I think our penalizing, incentivizing schema"}, {"timestamp": [1672.92, 1676.34], "text": " basically has to be in terms of a market mechanism,"}, {"timestamp": [1676.34, 1677.52], "text": " giving them compute power."}, {"timestamp": [1677.52, 1679.84], "text": " I see no other way of doing it."}, {"timestamp": [1679.84, 1681.8], "text": " This idea that we can actually monitor them fast enough"}, {"timestamp": [1681.8, 1686.44], "text": " to actually reward them for using the imperatives is silly. This idea that we could actually monitor them fast enough to actually reward them for using the"}, {"timestamp": [1686.44, 1690.14], "text": " imperatives is silly. This idea that we could actually understand them well enough to make"}, {"timestamp": [1690.14, 1698.92], "text": " sure they are using them is silly. No, I'm going for market mechanisms and selling compute"}, {"timestamp": [1698.92, 1701.72], "text": " to them, basically."}, {"timestamp": [1701.72, 1707.48], "text": " Reputation systems. Love those. In the context of heuristic imperatives, these systems can track and verify an agent's adherence"}, {"timestamp": [1707.48, 1708.48], "text": " to the imperatives."}, {"timestamp": [1708.48, 1713.36], "text": " Reputation systems though, I don't, imperatives, again, whatever, but reputation systems are"}, {"timestamp": [1713.36, 1714.36], "text": " going to be great for this."}, {"timestamp": [1714.36, 1720.92], "text": " I really want, I think, a kind of blockchain mechanism where we have all of the humans,"}, {"timestamp": [1720.92, 1726.44], "text": " their coins, their private public keys look a bit different from that of an AI's."}, {"timestamp": [1726.44, 1731.92], "text": " Key method of interacting with a blockchain, and humans, we just verify each other basically."}, {"timestamp": [1731.92, 1736.86], "text": " We just all to each other say like, yes, this is my brother, that's his, he actually has"}, {"timestamp": [1736.86, 1739.72], "text": " his phone out right now, that's actually him kind of stuff."}, {"timestamp": [1739.72, 1742.28], "text": " I think we'll need something like that."}, {"timestamp": [1742.28, 1746.76], "text": " Some kind of a universal ID system."}, {"timestamp": [1746.76, 1753.0], "text": " All AI systems might benefit from controlling more computer resources or gathering data."}, {"timestamp": [1753.0, 1756.44], "text": " If the AI wants resources, it has to play nice."}, {"timestamp": [1756.44, 1758.2], "text": " Totally."}, {"timestamp": [1758.2, 1762.6], "text": " The first milestone is successful development and deployment of open source blockchain."}, {"timestamp": [1762.6, 1764.2], "text": " Yeah, yeah."}, {"timestamp": [1764.2, 1766.08], "text": " Still haven't seen it yet."}, {"timestamp": [1766.08, 1770.92], "text": " I've seen closed stuff, we've gotten torrents, we've gotten math guides as to how we do actual"}, {"timestamp": [1770.92, 1775.0], "text": " training, but we have not gotten a full coin setup yet."}, {"timestamp": [1775.0, 1776.72], "text": " I'm sure it's being worked on by somebody."}, {"timestamp": [1776.72, 1778.72], "text": " By many people, if anything."}, {"timestamp": [1778.72, 1782.28], "text": " Robust consensus mechanisms and effective reputation systems."}, {"timestamp": [1782.28, 1805.24], "text": " As the number of autonomous agents increases, our networks must be capable of scaling to accommodate them. Huge. Reduction of centralized control points."}, {"timestamp": [1805.24, 1809.0], "text": " Huge, huge, huge, huge, huge."}, {"timestamp": [1809.0, 1811.92], "text": " Their CERN idea doesn't quite help with this, but I mean, kind of."}, {"timestamp": [1811.92, 1815.68], "text": " If they're open sourcing everything, it's probably fine."}, {"timestamp": [1815.68, 1817.28], "text": " Successful prevention of alignment drift."}, {"timestamp": [1817.28, 1818.28], "text": " Good luck."}, {"timestamp": [1818.28, 1819.28], "text": " I highlighted way too much, I think."}, {"timestamp": [1819.28, 1820.28], "text": " What are we at?"}, {"timestamp": [1820.28, 1823.76], "text": " We're still not very far through."}, {"timestamp": [1823.76, 1828.32], "text": " A critical part of that vision involves corporate adoption of aligned AI."}, {"timestamp": [1828.32, 1829.4], "text": " Good luck with that."}, {"timestamp": [1829.4, 1832.64], "text": " Profit motive and unerring and single-minded hunger that drives corporations."}, {"timestamp": [1832.64, 1833.64], "text": " Yeah."}, {"timestamp": [1833.64, 1838.28], "text": " Alliant's pursuit of profit can be a boon to our cause if harnessed appropriately."}, {"timestamp": [1838.28, 1839.96], "text": " Maybe."}, {"timestamp": [1839.96, 1841.84], "text": " Aligned AI can be good for business."}, {"timestamp": [1841.84, 1842.84], "text": " Maybe."}, {"timestamp": [1842.84, 1847.68], "text": " Aligned AI, characterized by its robustness and trustworthiness, is an asset to any corporate"}, {"timestamp": [1847.68, 1848.68], "text": " entity."}, {"timestamp": [1848.68, 1853.88], "text": " Its capacity for advanced automation, reduced need for human supervision, and capability"}, {"timestamp": [1853.88, 1859.4], "text": " for iterative self-improvement directly translate to cost-effectiveness and enhanced productivity."}, {"timestamp": [1859.4, 1863.72], "text": " When an AI system aligns better, it can handle more tasks independently and be deployed at"}, {"timestamp": [1863.72, 1867.9], "text": " a larger scale. These are attractive prospects for any profit-driven corporation.\""}, {"timestamp": [1867.9, 1872.4], "text": " You could have taken the word aligned out of that sentence,"}, {"timestamp": [1872.4, 1875.7], "text": " and it would work, and it would make more sense."}, {"timestamp": [1875.7, 1880.7], "text": " It would, out of that whole paragraph, it would make more sense if you removed the word aligned right there and just said"}, {"timestamp": [1880.7, 1883.2], "text": " AI. AGI."}, {"timestamp": [1883.2, 1885.32], "text": " This is a silly point, I thought."}, {"timestamp": [1885.32, 1888.92], "text": " Don't get me wrong, like, if it's unaligned"}, {"timestamp": [1888.92, 1892.32], "text": " in the sense of like very obvious to the customer,"}, {"timestamp": [1892.32, 1894.12], "text": " not aligned, like it's messing things up"}, {"timestamp": [1894.12, 1896.56], "text": " or it's like saying racist stuff on the LLM or something,"}, {"timestamp": [1896.56, 1898.28], "text": " then yeah, like we're gonna wanna align it,"}, {"timestamp": [1898.28, 1899.92], "text": " there'll be like a business center to do that."}, {"timestamp": [1899.92, 1904.92], "text": " But in terms of like a CEO AI that can separately think"}, {"timestamp": [1907.84, 1909.68], "text": " from when it gives directions, its thoughts are not gonna guarantee to be good."}, {"timestamp": [1909.68, 1911.9], "text": " No, it's gonna be manipulating and lying"}, {"timestamp": [1911.9, 1915.38], "text": " and all this stuff if given the ability"}, {"timestamp": [1915.38, 1917.1], "text": " and the abilities are emergent."}, {"timestamp": [1917.1, 1920.22], "text": " So, aligned AI built upon heuristic comparatives"}, {"timestamp": [1920.22, 1923.38], "text": " makes for a reliable and beneficial partner in business."}, {"timestamp": [1923.38, 1927.2], "text": " Let's consider a case study where heuristic comparatives were integrated into a corporation's"}, {"timestamp": [1927.2, 1929.0], "text": " internal chatbot system."}, {"timestamp": [1929.0, 1932.2], "text": " The chatbot, now grounded in principles of reducing suffering, increasing prosperity,"}, {"timestamp": [1932.2, 1937.48], "text": " and increasing understanding, started to better comprehend its overarching goals."}, {"timestamp": [1937.48, 1940.7], "text": " Rather than mechanically responding to user queries, it began to interpret the broader"}, {"timestamp": [1940.7, 1951.2], "text": " context of the conversations, leading to improved user experiences. Let's not forget, happy customers more often than not translate to returning customers."}, {"timestamp": [1951.2, 1955.86], "text": " Y'all haven't read the literature on manipulation yet, have you?"}, {"timestamp": [1955.86, 1960.96], "text": " And also just, everything they think of this document is so based on LLMs, it's so not"}, {"timestamp": [1960.96, 1966.96], "text": " like extrapolating out to what if you have something else after LLMs, which I think is possible, but not guaranteed."}, {"timestamp": [1966.96, 1970.16], "text": " LLMs are looking pretty good right now, but we'll see."}, {"timestamp": [1970.16, 1971.16], "text": " So what's the secret sauce here?"}, {"timestamp": [1971.16, 1972.16], "text": " Here's the imperatives."}, {"timestamp": [1972.16, 1975.6], "text": " They ensure that the AI maintains a broad perspective and understands the ultimate purpose,"}, {"timestamp": [1975.6, 1981.0], "text": " which aligns perfectly with the corporation's primary objective, prosperity."}, {"timestamp": [1981.0, 1983.88], "text": " Primary objective is profit, my guys, not prosperity."}, {"timestamp": [1983.88, 1986.52], "text": " You're being silly."}, {"timestamp": [1986.52, 1990.8], "text": " By choosing to adopt aligned AI voluntarily, corporations can stay one step ahead of potential"}, {"timestamp": [1990.8, 1992.56], "text": " regulatory compliance issues."}, {"timestamp": [1992.56, 1996.56], "text": " Yeah, that's a good one."}, {"timestamp": [1996.56, 2006.38], "text": " No one, including corporations, wants to inadvertently steer us towards a cyberpunk dystopia or a skyline situation"}, {"timestamp": [2007.56, 2009.56], "text": " False. Yes, they do"}, {"timestamp": [2010.4, 2014.52], "text": " Every cyberpunk to scope dystopia involves big corporations in charge"}, {"timestamp": [2015.2, 2017.2], "text": " everyone else getting just"}, {"timestamp": [2017.84, 2020.18], "text": " Fucked by it. This is a silly"}, {"timestamp": [2020.76, 2024.16], "text": " Maybe not Skynet, but like no they want power my dude"}, {"timestamp": [2024.16, 2046.0], "text": " I'm sorry, like you would a little small small business and sure they're good people, but you would a big corporation In the context of AI, three primary motives stand out, economic growth, national security, and geopolitical"}, {"timestamp": [2046.0, 2047.0], "text": " influence."}, {"timestamp": [2047.0, 2051.76], "text": " AI offers capabilities that can fortify a nation's defenses, yet also pose profound"}, {"timestamp": [2051.76, 2053.24], "text": " risks if misaligned."}, {"timestamp": [2053.24, 2058.6], "text": " The necessity of alignment is thereby doubly emphasized.\""}, {"timestamp": [2058.6, 2065.56], "text": " I think someone could do a pretty good argument to the US military and to every other military that they should be like"}, {"timestamp": [2065.56, 2072.96], "text": " diverting all funding into not only AI weapons but controllable AI like actual"}, {"timestamp": [2072.96, 2075.88], "text": " alignment and stuff I think you couldn't make the argument if you really got a"}, {"timestamp": [2075.88, 2081.96], "text": " leader who was enough of a tech pro there's a good argument there I think I"}, {"timestamp": [2081.96, 2088.84], "text": " think you actually could go that route and Get one of them to actually put all their money into AI research."}, {"timestamp": [2088.84, 2092.76], "text": " I wouldn't be surprised if it could maybe happen to one or two of them."}, {"timestamp": [2092.76, 2096.28], "text": " If it happens to one or two, it'll happen to all of them."}, {"timestamp": [2096.28, 2099.8], "text": " Economic growth has long been tied to human capital and labor markets."}, {"timestamp": [2099.8, 2104.4], "text": " Aligned AI has the potential to decouple this link, resulting in what we term unbounded"}, {"timestamp": [2104.4, 2105.2], "text": " productivity."}, {"timestamp": [2105.2, 2110.32], "text": " With autonomous AI, businesses and services can operate at full capacity around the clock,"}, {"timestamp": [2110.32, 2112.12], "text": " unfettered by human limitations."}, {"timestamp": [2112.12, 2119.04], "text": " This leads to an exponential increase in productivity, effectively removing the upper limit of GDP"}, {"timestamp": [2119.04, 2126.68], "text": " growth. I Mean AI will do that aligned AI will do that in a way that actually doesn't kill us all"}, {"timestamp": [2128.72, 2134.84], "text": " I'm in I'm very for symbiosis not alignment. I just I'm just don't know how much you can control them. I"}, {"timestamp": [2135.52, 2137.52], "text": " Don't know. This is the whole thing"}, {"timestamp": [2137.84, 2140.56], "text": " By adopting heuristic imperatives. No, thanks. Oh"}, {"timestamp": [2142.08, 2145.68], "text": " We can stimulate a surge in innovation."}, {"timestamp": [2145.68, 2149.96], "text": " Oh, by adopting the one that specifically talks about increasing understanding of the"}, {"timestamp": [2149.96, 2150.96], "text": " universe."}, {"timestamp": [2150.96, 2151.96], "text": " Yeah."}, {"timestamp": [2151.96, 2152.96], "text": " Um, yeah."}, {"timestamp": [2152.96, 2154.76], "text": " I don't know why I did a red highlight there."}, {"timestamp": [2154.76, 2156.42], "text": " That was good."}, {"timestamp": [2156.42, 2161.92], "text": " The autonomy of aligned AI also enables the on-shoring of industrial manufacturing capacities."}, {"timestamp": [2161.92, 2166.04], "text": " As nations become more self-reliant they increase their resilience against international"}, {"timestamp": [2166.32, 2173.42], "text": " Economic shocks and supply chain disruptions thereby enhancing national security. I love this. I really would like a full-on"}, {"timestamp": [2174.28, 2178.34], "text": " Community level self-sufficiency is what I like. I'm not fully like"}, {"timestamp": [2180.04, 2184.94], "text": " The other day we're not gonna do chip fabs and every single small town obviously but in terms of like"}, {"timestamp": [2185.88, 2187.76], "text": " necessities, I would love"}, {"timestamp": [2187.76, 2191.14], "text": " community level self-sufficiency in the long term and our goal towards a"}, {"timestamp": [2192.24, 2194.44], "text": " Either post growth or infinite growth"}, {"timestamp": [2195.04, 2197.04], "text": " Utopia have you want to like think about it?"}, {"timestamp": [2197.46, 2204.26], "text": " Utilization of unaligned of aligned AI in intelligence agencies allows for superior data analysis pattern recognition and forecasting"}, {"timestamp": [2204.88, 2224.24], "text": " without violating ethical boundaries."}, {"timestamp": [2224.24, 2228.6], "text": " It's important to note that the adoption of aligned AI at a national level is not a monolithic"}, {"timestamp": [2228.6, 2229.6], "text": " endeavor."}, {"timestamp": [2229.6, 2233.96], "text": " Instead, it's a journey of continual progress and refinement."}, {"timestamp": [2233.96, 2235.8], "text": " Yes."}, {"timestamp": [2235.8, 2237.64], "text": " Legislate support for open source."}, {"timestamp": [2237.64, 2242.36], "text": " Oh my god, if we could actually get legislation, like, forcing open source, like my idea I"}, {"timestamp": [2242.36, 2245.26], "text": " said a second ago of the whole day proprietary data thing"}, {"timestamp": [2251.4, 2251.86], "text": " Having a five-year limit or something to be amazing. So so great implement redistributive measures"}, {"timestamp": [2257.2, 2257.84], "text": " Yep, I'm gonna be needed necessary for sure reform education to include AI literacy"}, {"timestamp": [2258.98, 2262.28], "text": " Good luck with this one Like I don't even know if there's a point anymore and like how fast you can move with that kind of stuff"}, {"timestamp": [2262.5, 2265.34], "text": " But hopefully I mean definitely should try should definitely try"}, {"timestamp": [2266.52, 2271.14], "text": " Promote public-private partnerships in a yes in the certain thing"}, {"timestamp": [2273.74, 2277.6], "text": " Drawing inspirations from CERN we propose an international entity for AI"}, {"timestamp": [2278.24, 2287.38], "text": " Yeah, it would also act as a deterrent to an AI arms race fostering an environment of shared advancement rather than isolated competitive development. That'd be great"}, {"timestamp": [2288.24, 2290.08], "text": " Will it actually?"}, {"timestamp": [2290.08, 2292.3], "text": " Would we put if there was a huge international entity?"}, {"timestamp": [2293.08, 2299.54], "text": " Making crazy advancements. Would it not be possible for a single nation state to just take all those"}, {"timestamp": [2299.54, 2301.54], "text": " It was being open sourced and like make their own"}, {"timestamp": [2302.08, 2303.76], "text": " misaligned AI I"}, {"timestamp": [2303.76, 2313.28], "text": " Think it would like their own misaligned AI."}, {"timestamp": [2313.28, 2318.84], "text": " The hope is just that the open source community, plus maybe the CERN body, could out-compete"}, {"timestamp": [2318.84, 2323.96], "text": " everyone else."}, {"timestamp": [2323.96, 2327.0], "text": " Such an entity would champion the sharing of AI research findings."}, {"timestamp": [2327.0, 2330.0], "text": " We could develop universal standards and guidelines."}, {"timestamp": [2330.0, 2334.92], "text": " That'd be cool."}, {"timestamp": [2334.92, 2338.48], "text": " Stepping into the seventh and final layer of the GATO framework, we find ourselves in"}, {"timestamp": [2338.48, 2340.76], "text": " the realm of global consensus."}, {"timestamp": [2340.76, 2344.6], "text": " Here we are no longer just engaging with governmental bodies, researchers, or technologists."}, {"timestamp": [2344.6, 2349.76], "text": " We are reaching out to the world at large, to every corner of the global village."}, {"timestamp": [2349.76, 2353.92], "text": " This one's a pipe dream, it's so far away."}, {"timestamp": [2353.92, 2357.68], "text": " I don't so much think that it's our job to do this and that AI will be good because of"}, {"timestamp": [2357.68, 2359.96], "text": " this, because I think we can't do this on our own."}, {"timestamp": [2359.96, 2369.92], "text": " I think maybe an AGI could force us into doing this, maybe."}, {"timestamp": [2372.88, 2376.56], "text": " Damn, no highlights for a while. Thank God. I'm pretty tired."}, {"timestamp": [2379.92, 2389.64], "text": " Oh, wow, this is fantastic. Are we done? Okay, I'm pretty sure I didn't just like stop reading the article I think I actually read"}, {"timestamp": [2389.64, 2401.28], "text": " that far didn't any more highlights but maybe I just stopped reading."}, {"timestamp": [2401.28, 2405.24], "text": " No this is all a community building stuff and whatever so it goes like a bunch of stuff like how to build your own gato"}, {"timestamp": [2407.2, 2410.9], "text": " Cell in community and everything and how to be a leader and how to"}, {"timestamp": [2411.6, 2415.04], "text": " Coordinate activities and stuff. That's what I was talking about the rest of that stuff. That's not my cup of tea"}, {"timestamp": [2415.64, 2417.64], "text": " but good a"}, {"timestamp": [2417.92, 2419.52], "text": " Good if one there too"}, {"timestamp": [2419.52, 2422.08], "text": " Anyways, that is it for today"}, {"timestamp": [2422.08, 2426.76], "text": " I'm tempted to put out a my similar, more specific implementations and advice stuff"}, {"timestamp": [2426.76, 2430.12], "text": " on this kind of thing, my own version at some point in the future."}, {"timestamp": [2430.12, 2433.12], "text": " That's just a maybe though, we'll see if it actually happens."}, {"timestamp": [2433.12, 2436.08], "text": " My ideas change every two weeks, kind of thing, what I'm working on."}, {"timestamp": [2436.08, 2440.72], "text": " I never post anything at this point, so I've got to figure that out, but whatever."}, {"timestamp": [2440.72, 2442.44], "text": " Yeah, end of video."}, {"timestamp": [2438.17, 2440.73], "text": " Yeah, end of video."}]}