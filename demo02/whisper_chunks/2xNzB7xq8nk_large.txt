{"text": " All right, guys, this is really exciting. So today's video is sponsored by Pinecone. And if you know me, you know that I have been like super resistant to sponsorships, but when Pinecone reached out, I was like, yes, this makes sense because I was going to use Pinecone anyways. Let's do it. So what are we doing today? First, need to do a little bit of preamble. So come and support me on Patreon. I'm almost at my goals, at which point I will take down all ads forever. I might continue with sponsorships, but that's fine. So anyways, my Patreon is patreon.com slash Dave Schapp. Also, if you sign up for a higher tier, you get to interact with me directly. So that has less to do with the videos and more to do with just like, hey, if you have a question, I'm happy to answer. Okay. So moving right along, a lot of you also know that I'm working on an open source AGI project or what I call artificial cognitive entity because artificial general intelligence is a useless term. I actually prefer artificial global intelligence because that's really the goal. Let's do Ultron, but safe. And so the architecture, the underpinning architecture for my project is what I call Meragi, microservices architecture for robotics and artificial general intelligence. So the primary thing that you need to know about Meragi is that it is all organized around a nexus. This is not a COVID virus, by the way. It kind of looks like one. All right, so the nexus is, as the name implies, the center of it. It is a hub and spoke model, which means it is a very simple model. Now what is the Nexus? The Nexus is primarily an information store, a database. Now you could use SQL. In my original experiments, I actually used SQLite. SQLite works perfectly fine. It is indexable. However, you're limited to SQL searches. You're not, you don't have good semantic search. So we need to upgrade to something that does have semantic search. Enter Pinecone. If you're not familiar with Pinecone, we're gonna go through from the beginning because I, even though I've intended to use Pinecone for more than a year now, I hadn't gotten around to it. So now I'm gonna get around to it. So we're gonna learn Pinecone together. Oh, you know what I just noticed? Oh, my OCD is gonna kill me. Do you see this one right here? One of the spokes is bent. Okay, anyways. All right, so this is the highest level overview functional diagram for Meragi is where basically you have a nexus, which is this is the data. And the reason that Meragi is a data-first or data-centric model is because I realized that all learning, all experiences, all knowledge are all data, right? Everything that makes us human, everything that makes us intelligent is about the information we carry and that we can then use. So information first. It is also, it's a data first model. So that's kind of like the bits and bytes. It's also a cognition first model. And so what I mean by cognition first is this can think even without being connected to the outside world in any way. It doesn't need sensor input. It doesn't need motor output. It can, but it is a cognition-first and data-first model of artificial intelligence. Okay, if you're lost, that's fine. Scrolling down a little bit, this is my Raven project, so github.com slash Dave Schapp slash Raven if you want to get involved. So basically, there's a few layers of abstraction. Here, let me make sure I'm not talking too loud. Okay, cool. There's a few layers of abstraction to think about artificial cognition. So at the very foundation is data, data first. So this is episodic and declarative memory, which, oh, by the way, a vector database is the optimal thing to store this in. Now, that being said, if you combine vector search or semantic search with things like knowledge graph, that could be even better. We're figuring this in. Now, that being said, if you combine vector search or semantic search with things like Knowledge Graph, that could be even better. We're figuring this out. This is a large open source project. It's about three weeks old and we're getting organized. We're getting funding. We're having talks about funding, so on and so forth. But the architecture is here. Oh man, you know what I just realized? There's an uneven gap here too. My OCD is just like way turned up today. Maybe it's all the coffee. No, I haven't had my coffee yet. Maybe that's the problem. All right, so anyways, what we're doing today is we're going to create a store for episodic and declarative memory, and then we're gonna do semantic search or vector database search. So we're basically just covering the bottom two layers of this. Now, technically, we're also introducing the core objective functions or the here's to comparatives. covering the bottom two layers of this. Now technically we're also introducing the core objective functions or the heuristic comparatives and that is just by virtue of the fact that it is embedded into the prompts. You can embed the core objective functions at many many layers, at inference time, at evaluation time, at planning time, there's all kinds of places, but the point being is that the core objective functions are literally the first type of cognition that's applied to the data going up and down the stack, which means that they're core, they're central to the operation of the thing. Whatever else it's doing, problem solving, executive function, cognitive control, memory consolidation, all of that still is above the core objective functions, which mean the core objective functions apply to all of that. And then finally, here's a much more complicated diagram. This is kind of what it's gonna look like eventually, but we're basically just doing the bottom three layers here. And we're essentially gonna be doing just the Nexus today. We're not gonna be doing any other microservices. Okay, so before we dive off, I did also want to ask you to consider, if you don't donate to me to support my life, consider donating to UNHCR. So the UNHCR is the UN's organization for helping refugees. I've given to UNHCR for many, many years. I started during actually the civil war in Syria many years ago because it was just awful and it broke my heart and I was like, I wanna do something, but like, you know, I'm just a dude. It's not like I was gonna go stop the war single-handedly. So I looked around and I was like, you know, I'm just I'm just a dude. It's not like I was going to go stop the war single-handedly. So I looked around and I was like, okay, well, you know, the it's a it's a huge refugee crisis. So how can I help that? And I looked around and actually have a friend an internet friend who was in aid coordination for many years. I don't know if he still does that. But anyways, I asked him and he said, yeah, UNHCR, like, they're, you know, every non-profit organization has its issues. He's like, but UNHCR is good because one, it is accountable. It's a global entity, but also it has global reach, which also makes it pretty good. So I give to UNHCR every month for more than 10 years now and I certainly don't regret it. So given everything going on all over the world, you know, just give it a thought. Yeah. All right. So moving right along. Let's dive into Pinecone. Like I said, I haven't used it before. I know people that have. I've looked at it. I know the underlying technology, and I said, this is the way. Oh, also before we get started, the repo for this is Pinecone Infinite Memory Chatbot. Infinite memory. Once you see the numbers, you will understand what I mean. Like, we are about to build a chatbot that has a longer memory than, well, I'm not gonna say than God because that's just too hyperbolic, but longer memory than any single human. The base, like tier for Pinecone is free. And I think, I could be wrong, but I think that they said like you get up to 5 million inferences or 5 million records. So imagine that you have a database, a chatbot, that remembers the last 5 million messages. That is a long conversation. So let's see if we can do this. Oh, and also you go to their homepage, long-term memory for AI. Gee, that's kind of what we want, isn't it? All right, so I'm not gonna bore you to death with how this works. We're just gonna jump in and use it. So I'm gonna go ahead and sign up. Here, I'll go ahead and pause this for a second. Okay, well, it was supposed to ask me like how I got, how I figured out that I wanted to use Pinecone, but it did not. So create your first index. All right, this will be raven MVP, lowercase. Okay, let's see. Let's see, raven MVP. Dimensions, okay, So here's the thing is we need to know OpenAI, not that. Docs.OpenA. This is a mess. All right, documentation. Because here's what we're gonna do. We're going to use OpenAI, their embeddings, right here, there we go. And we need, let's see, embedding models. Where is Ada02? Pages per dollar, 3,000. Text embedding 802. Output dimensions, so max input is 8,100. Output is 1536. Okay. So, we are going to do 1536 dimensions. Let's see, lowest latency and highest throughput, best storage capacity, faster queries. Let's do, we'll do P1, sure, whatever. We'll be terminated after 14 days, create index, okay. Yep. All right, so here's our index, it's initializing, that's cool. So if I recall correctly, and I apologize to anyone at Pinecone if this is wrong, I think what it's doing is it's actually spinning up an individual container for me on the cloud. Yeah, because the environment is US East 1 GCP. So this tells me that it is running my container on the Google Cloud. Yes. So, and then rather than uploading stuff individually, what I'm going to do is blah, not that. All right, we're going to do, we're gonna go back to, where did it go? Pinecone.io. All right, so we're gonna go back to the docs and we're gonna do quick start. And also I apologize that this video is a total unmitigated mess, but you know what? All of you watch it on like 2x speed anyways. So you're probably just laughing your butt off. All right, so let's do a pip install pine cone client. Ta-da! So TLDR, you need the pine cone client in order to run Python. Now, that being said, a lot of you expressed interest in how do I do this stuff if I don't know the first thing about coding. So I'm going to try and do as little coding as possible. So we're going to jump over here and use ChatGPT. I upgraded to ChatGPT Pro. Man, is it much faster? I definitely recommend it because one, the speed, and two, I don't know if it's actually smarter or if it just feels smarter because it's faster, but definitely ChatGPT Pro, I think is totally worth it, especially for 20 bucks a month. It's like, it saves me so much time, and my time is so valuable, right? It's a no brainer. It's definitely worth it. Okay, so what are we trying to achieve here? We're trying to get a chatbot that will use up to 5,000 or 5 million memories. So what I want to do is I want to save every chat message that I do with this chatbot. And I want to recall stuff based on recent conversations. So let's make a ultra simple chatbot using OpenAI TextAda02 embeddings for searchability. We will index each message, chat message, back and forth, and then we will store all messages in a pine cone index. I want you to write a Python script that will do this for me. Okay, so if you don't know code, that's okay, because chat GPT does. All you need to do is know the English language to describe what you want. And so let's see if this works. All right, it doesn't seem to know some of this stuff yet. It's creating the index. I don't need it to do that. It also doesn't, it chose 512. So all you need to do is be able to look at it and say like, is this really right? Okay, I'm sorry, I don't understand Dave. But you see how much faster this is? Okay. I'm sorry, I don't understand, Dave. But you see how much faster this is? It just spit all this out in a couple seconds. And so I'm like, this is right. I don't know if this is right. All right. So close close but no cigar. Text DaVinci 02, okay, so it's this out of date. It doesn't even know about Text DaVinci 03. All right, so we're probably going to need to do a little bit of cutting and pasting, let's say. All right, so this was the script that I used for the last long-term chatbot memory. So basically what we're gonna do is we're gonna throw all that out because here's what happens. With iterative development, if you're not familiar with Agile, basically you get as far as you can. This is iterative development on an individual level. You get as far as you can, and then when you get stuck, you basically refactor everything that you've learned and start over. Okay, so actually my door is open. I'm going to pause it and close the door real quick. And also a quick wardrobe change because I was getting too toasty. Imagine that, you're super animated and drinking coffee and you get too hot. It's getting hot and no, I'm not going to do that. I'm not going to subject you people to that. I actually do have a good singing voice. It's just, I'm super not warmed up and I have awful timing. All right, where was I? Right, Pinecone. Yeah, this is close, but it's not quite. So let's break this down into the functions that we're gonna need. So I've got this function or this script here, 172 lines of code. We can actually simplify this a lot, but we need to keep the stuff that we like. So I'm going to go ahead and keep my completion. So this is the actual chat function. So we're going to leave this script as is just so that I have it for reference. Because if you know me, you know that I am horrible at just recycling code, which sometimes gets me in trouble, but whatever. Whoops, come back. So we're gonna call this chat2.py. Because that's super creative. Okay, so what this does is it chooses TextDaVinci 03, which is the latest and greatest in terms of fine-tuned models. The temperature is set to zero, because I actually don't like entropy injected unless I want it to be injected. In most cases, a deterministic output is fine because it's just a thinking machine and I achieve what I want with prompt engineering. So I'm a chatbot named Raven. My goals are to reduce suffering, increase prosperity, and increase understanding. I will read the conversation notes, recent messages, and I'll provide a long, verbose, detailed response. I'll then end my response with a follow up or leading question. All right. So instead we are going to do we don't need notes because we're just going to do a super long term memory. And then we are going to actually, yeah, here, we're just have the conversation here. Let's see, previous conversation. All right, that's fine. And then we will say, yeah, that's fine. All right, so we update this. And basically, what I'm going to do is I'm going to populate this with what we retrieve from Pinecone. populate this with With what we retrieve from From pine cone So we come let's take a look at the repo. So the repo I've got pine cone text and I Thought I copied my open AI API key. I did not We'll copy this from another repo. Okay, so we got open AI and then Pinecone. Here, I'm just going to shorten this so it's OpenAI.txt. So there's, these are my, these are my keys. Here, I'll, I'll do this. I'll do a key underscore so that they stay together. Repo hygiene is important. Key underscore. So we've got two API keys. We're going to be talking to OpenAI for the large language model and the embedding, and then we're going to be talking to Pinecone for storage. So we've got two API keys. We're gonna be talking to OpenAI for the large language model and the embedding. And then we're gonna be talking to Pinecone for storage. So we're not really gonna be doing any storage locally. All right, so first things first, let's come back here and figure out what we need. Let's, oops, I closed the chat. Let's go to this one. So we need the main loop. So if you're not familiar with coding, and I do apologize, I promised that we would do like, let's call this low code, not no code, but low code. While true, so pretty much all chatbots start with a while true loop. And actually a lot of programs start with a while true loop. So this is basically just an infinite loop. And let's shorten this, openai.text. Okay, so while true, what do we do? I don't know. I think over here, I had, yep, get the user input first. That's all good. Let's see. So then we get the vector. So GPT-3 embedding. Did I use... I might have already switched to Ada. Oops. Text embedding Ada02. Oh, I was using it. Great. Okay, cool. So I've already written this function. Fantastic. So we can copy... Whoops, we don't need the prompt. So we can go ahead and copy this function too. So we get the embedding, which is great. And also some of you asked why I do this here where I do content and code to ASCII, ignore errors and then decode. This is because for whatever reason, some Unicode breaks GPT-3. I don't know what it was, but I just found out that forcing it to ASCII, because ASCII is a smaller code base than Unicode, and then converting it back forces it into a correct format that GPT-3 already accepts, or always accepts. So fix any Unicode errors. Because there's like Unicode bomb, which is not bill of materials, but something to do with the way that it's encoding, whatever. Anyways, so we get our embedding. And then we come back down here. All right, so let's do this. And then the message, oh, yeah, so user, time string, et cetera, great. So that's do this. And then the message. Oh yeah. So user, time string, etc. Great. So that's all fine. And then maybe we need the info too because I think Pinecone allows us to save some metadata. All right. So let's do this. All right. And then we will come over to Pinecone documentation. Nope. It's do this. All right. Then we will come over to Pinecone documentation. Nope, it's over here. Quickstart. All right. So Pinecone in it, we need this guy here, environment, your environment. Let's see. No, not create index. Let's come over here. Import Pinecone. And then we also need to set Pinecone in it, your API key. So this will be open file pinecone.txt. And if you didn't catch this, what I did earlier that I didn't show you was I got my API key and put it in here. So there you have it. Oh, actually, no, I need to update the name so that it is correct. Key pine cone and then key open AI. And then environment, I think, is going to be this guy. Is that the environment? So API values. I'm not sure which environment. It could be the index name? I don't know. Usually, sometimes what they have is, they'll give you like, here's the Python version of this, but we will go back to the docs. RTFM, read the fun manual. All right. Index.upsert. Oh, here we go. So Pinecone. Let's see, what is I don't know, not applicable, okay. I really don't know what this is. This also, note both your API key. Okay, just says note your environment. All right, Pinecone people, if you're watching, I don't know what you mean by environment because I go to my console and I don't see anything named environment. Index name. Oh, environment here. Okay. Region. Region. Okay. So for the IT infrastructure parlance, this would be called a region, not an environment. Environment is like, are you running in a Docker container or on a Linux machine or in VMware but the cloud region. Okay got it figured it out. All right so basically I need to tell the API talk to this region. Region figured it out. I'm not going to let this go the way of the future. So I recently rewatched The Aviator with Leonardo DiCaprio and I initially watched it in theaters many years ago and I was ultra disturbed because the way that he repeats things, I used to do that in my head and turns out that's just a really common feature of OCD and it's because it doesn't feel right. And if you know what I mean, you know what I mean. And that is basically like it's a sensory processing issue and so if something doesn't feel right, you have to keep doing it until it does feel right. Fortunately, I got over that mostly just by one, accepting that like, okay, this is a thing and then I kind of got used to it over time. Also fixing your diet and sleep really helps with OCD. Okay, quick aside. All right, we got that figured out. Let's go back to our documentation. Okay, so we initialize it. So this is just initialize. So this init thing, so Pinecone init is about the same as just setting our API key. Okay, I think that that's fine. is just setting our API key. Okay. I think that that's fine. Yes. All right. So that's nested correctly, I think. And then we get down, we don't need to create the index, we just need to grab an index. So let's come back over here. So index equals pine cone Pinecone index and this will be raven-mvp. I know how to type, I promise. Raven-mvp. And I don't like index, I'm gonna do vdb. Vector database. Whoops. All right, so this is our vector database. And then we, okay, so we do an upsert. So we give it a string and then that. Okay, so if we give it a string and then a vector, if that's, I know that they said that we can do, oh wait, here we go. IDC score, oh wait, no, that's just the query. Okay. Well, okay, so we'll keep it simple. We'll just do the string, that's fine. So index.upsert, or in this case, vdb.upsert. So the message string will include the user, the time, and the message. So it's all gonna be included in the message, so that's fine. But the vector, we should actually do the vector later so that the vector includes all should actually do the vector later so that the vector includes all of that information, I think. Right? Or should we only search what the user actually put in? You know what? Screw it. I'm going to index the entire thing. So the vector equals GPT-3 premature tabbing tab completion. Not all men have this problem, but if you do have this problem, there's nothing to be ashamed of. Okay. You're getting a little bit more unfiltered Dave lately. I hope that's okay. Some people say like you need to be like more humble and laid back and it's like no. This is this just my true self. So I apologize. If you don't like it, tough. All right. So timestamp equals this. So we get a timestamp. So timestamp is a Unix epoch. So it's just a number. It's a floating point number. We need timestamp to date time. So let's grab this function. OK, all it does is it returns a specifically formatted, whatchamacallit, English readable timestamp. That's fine. So let's grab this, put this here, and then, so, all right, we can get rid of that guy, we can get rid of that guy, vectorize it, save to Pinecone. All right. Now, so we've got our message and we've got our vector. So let's go look at the Pinecone documentation. And so what we do is we have to upsert, and we have string. Or so first it's a list and then within the list, there are tuples and the tuples have at index zero is a string and index two is a vector. So this should be pretty straightforward. You know, we can actually probably save some time if we just do the upsert at the end. So let's do that. Let's add our message and Raven's at the end so that way we don't accidentally pull our most recent messages. Oh, I like that. Yeah, okay. So let's see, this will be, upsearch, actually no, we'll just, we'll call info, info list, and so then we will do info.append. And then, so we need, the info is the list container, so now all we need to do is add a tuple that has message and vector. And so then, once we get our chatb add a tuple that has message and vector. And so then once we get our chatbots message, we'll append it as well, and then we'll just upsert them both at the same time. Bingo. All right. So next thing we do is, and this is actually good because we will use this vector to search for Raven's response, which means that we won't pull, yeah, yeah, it's order of operations, man. And if it doesn't make sense, sit tight, it will in a minute. Let me do a time check. I'm probably just going to take this all the way through. We're at 30 minutes. Heck with it. I want to drive this across the finish line. I don't care if it takes two hours. Probably won't take two hours. And I know that all of your eyes are bleeding already. Eyes and ears. You're just like, Dave, shut up, write some code. Whatever. OK. All right so now search for relevant messages and generate a response. Cool. So this is this is where it gets fun. Not that it wasn't fun already. We need to do a query. I don't know what top k is. What does top k mean in this context? Include values. include values. The following example for the top three vectors that are most similar. Oh, okay, so top k is just how many similar do you want? Simple enough. Alright, so we will do results equals vdb.query. This is straightforward. Then vector equals vector. Basically, we're going to say whatever it is we just said to Raven, get all messages that are most similar. We'll do a topK. Butterfingers, topK butterfingers. Top K equals, let's say 15. So the 15 most relevant messages. And then I don't actually care about the values. So that's fine. So we'll just do, what is it? Include values, include values equals false. I just want the messages, don't really care. So it should return, okay, it'll return a dictionary with matches and then matches will be a list and it may or may not have values or it might just be a list of strings. OK so results. OK so results matches. So this is this is the this is the list of stuff so now we need to format it. Let's see. Let's see. Let's see. Let's see. Let's see. We're not going to delete anything. Are we really almost done? That's fine. All right, so if this is a list, so let's see how I formatted it over here. Let's go back in time. All right, so if this is a list, so let's see how I formatted it over here. Let's go back in time. All right, so conversation, load conversation, generate a response, fetch declarative memories. We're not gonna do that. Memories equals fetch memories. Okay, we did all that fun stuff and it's just a big giant mess. So actually, here's what we're gonna do. We're going to have conversation equals n.joinResults.matches and actually, wait, why? Oh, oh, right, hang on. So what this returns is it doesn't return the string or does it? Hang on. And the indexer updates a vector with the same ID already present. Wait, this isn't a string, this is an ID. How do I actually fetch a string? Let's see if there is something better because if I have to keep track of an ID in my own index, I mean, it's not the worst thing in the world, but it would be slightly less convenient. Okay. So here is some information. Oh, here we go, here we go, here we go. Okay. Yes. So I give it an ID and then a vector and then some metadata. Okay. So then if I want to give it that, I can filter. So you can pull those IDs. So ID values. So the value is that and then the metadata is something like this. So I can set some metadata. Got it. Got metadata. Got it. Got it. Got it. All right. That's straightforward enough, I think. I guess. Okay. So the message is not this. The message is something else. But this is actually fine because now I can set the timestamp, time string, the speaker, and the message itself as the metadata. So let me go back and grab this, because this is basically, so, because if you look at this, set metadata, right? So when we do this, all right, at this set metadata, right? So when we do this, all right, so here's the upsert. So we give it an ID and it looks like it can be alphanumeric. I'm gonna try a UUID because I don't really care about the ID, but I get, maybe I should, but I just like using UUIDs. So instead we will use this here. So we will just give it an arbitrary UUID four. Okay. And then, so then there's the vector. So it's ID, vector, and then metadata. So the metadata is this guy here that I'm working on. So we'll just call this payload instead. So our payload, because this is what we're actually going to upload eventually. All right. So the speaker is user, the time is timestamp, the vector, we don't need the vector in the metadata. We do need the message. We don't need a UUID, and then a timestring. So speaker, time, message, timestring. So that will all be in the metadata. And actually, let's just go ahead and call this metadata so that we don't hurt our brains later. And then metadata. All right. So now, hypothetically, maybe we actually do need to include values true. It should include the metadata on fetch. Query, so we can use that to filter, but can it return the metadata? Fetch vectors, update vectors, list collections, describe a collection, delete a collection. It might be, this might not be the way, we might have to keep the messages locally and all that this is gonna do is return us an ID. Now, if we have a UUID, that makes it really easy to metadata, let's see. All right, so we can set the metadata, metadata, let's see. All right, so we can set the metadata. Metadata, let's see. All right. All metadata is indexed. OK, that's interesting. That will slow it down in the long run, but I kind of don't care. Here, let's go way back up to the top. Metadata. Okay. Without a metadata config. Okay. So it just, it just, that's setting the metadata config. Include metadata. Okay. Okay. Here it is. Here it is. This is what I was looking for. If we say include metadata this will probably return. Our metadata. I think. We'll give it a shot. OK. All right. So include values include metadata. Let's see if this works. So if this is the case, then results matches. So results will be, that was the absurd, where is the query? So index.query here. Then wait, there was an example of what was the output? What did the output look like? I think that was under Quickstart. Index.query, there we go. Okay. So it'll give me matches, which is itself a list, and then the list will include the ID, so that we're going to use a UUID. It will give us a score, which is the similarity, and then the values, but then also it should include metadata, hypothetically. I think. So that'll have a list, metadata, hypothetically. I think. So that'll have a list, and each item in the list will be itself a dictionary. And the dictionary should include metadata, and the metadata should include a message, as well as a timestamp and a few other things. This is all very confusing. But with all that said, that needs to be in that correct order. Does it? I don't know. It might be actually simpler just to save our stuff locally. So what we'll do actually is we'll call... I was afraid of... I kind of knew that this would be the result. Let's see, unique ID equals string four. And the reason that I knew this was a thing is because I know that the underlying technology is the Facebook AI semantic search, which all it does is return an ID. So what we'll do is we'll save the metadata separately. Yeah, yeah. And yeah, we will go ahead and save. We won't save the vector, because that's a huge burden. But we will save the UUID. All right, so we'll do unique ID. That's fine. So the metadata, this will actually be saved as a JSON file the same way that we did over here. Okay, so we'll do save JSON ne. Actually, no, we just do.json. Then we'll use the unique ID for the placeholder. Then metadata here, and we'll remove metadata here. So Pinecone will just serve as instant search, and then it'll tell us which UUIDs to grab later. Okay, cool. This is a hot mess. All right, so we don't need values or metadata. All we need is the ID. Yeah, all we need is the ID. So it should return a list of dictionaries with IDs when we do the search. Okay. And so then what we do is we need to we need to, we need to recompose the conversation. So let's do load conversation and then we'll say results. Okay, so results should So, load conversation, and then we'll say results. Okay, so results should be, actually no, yeah. We'll say, yeah, results should be, should be a dict with matches, dict with matches, which is a list of dicts with id. OK. Lowercase this, because that is the actual name of the element. So that should be what we need there. So now we need to do a function def, no, that's not how that works. Def, what did I do? Load conversation, okay. So then results. And I know a lot of people say, why don't you use VS Code and Copilot and stuff? It's like, I hate IDEs. I particularly hate VS Code. But also that kind of defeats the point of like showing you how to code. Also, I realized that I am increasingly a liar because this is not a low code experiment. Let's see if we can get this to write it, though. OK. Let's write a function that accepts a results object from a Pinecone query. This returns a list of IDs which are used for the file names of the objects we are trying to load from local storage. These files are in a folder called nexus and they are appended with .json. So now write a very simple Python function that accepts the results and then loads the JSON files with matching ID file name. Does that make sense? with matching ID file name. Does that make sense? All right, let's see what it does. Let that run. Yep. All right. Load objects from Nexus. Object IDs equals results.ids. I don't think that that's how that works. Unless maybe it does. Does results.ids, is that part of the payload? Like .ids, that doesn't seem to be part of it. So index query, ID string optional example. Yeah, so like here's a filter. It doesn't give me an example of the response. So Pinecone, if you're watching, it would be helpful if you gave me an example of the response object. Because I don't know that chat GPT understands this. Yeah. But the logic here is sound enough because we know that it's a list of objects. So we'll do result equals list and then we'll do for let's see m in results that matches. I think that was the go back up to quick start. Yep so matches so So for that, what we need then is the ID. So then we'll have file name equals M ID plus, actually here, we'll just do, I'll grab my load JSON. Let's see, open file, save file, load JSON, save JSON. Grab these functions. Okay. So we'll do info equals load JSON, and all I need to do is pass it the file path. So the file path will be nexus. And then we'll backfill that, .json. And then this will be MID. And I think that should load the message. And the message will have this metadata. Right, so we'll know the UUID, it was in the file name already, but now it's a handy-dandy object, time string, message, time stamp, and user. So now what we can do is we can actually recall, we can recall all the most relevant things and sort it by time. Okay. So info equals load JSON, we get that information. Then I think I already had this function here, loadConvo. Yeah. So we load it, load it, we sort it. Okay. Then we do result.appendInfo. Ordered equals sorted result key lambda d time. Yep. Reverse equals false. Sort them all chronologically. Yes, that should be good. And then, so after we load the convo, we need to format it. And then recent get last messages. Okay, I don't think that that's the correct way to do it. But, so if we look here, the message saved has all this. So all we need to do is then reconstruct. All we need to do is then, sorry, brains are a little bit scattered. So ordered equals this, so rather than return ordered, all we need to do then is say, message block equals backslash n dot join. Oh, actually here. Let's do some list So then we'll just do messages equals I message. Oh, that's why my keyboard is slid over. That's why I keep fudging it. It's like I'm leaning a little bit. All right. I message for I in ordered. So this is what's called a list comprehension, which basically just says, let's make a simple list instead of a nested list. All right, so then we'll do join i messages dot strip at the end, and then return message block. And you could just do,ify this, return this. Okay. So in theory, this returns, given the results, this returns just the most recent conversation that we had or not most recent, most relevant conversation that we had with Raven. So now we can populate the prompt. So prompt response, we don't need any notes. We're just doing infinite memory. So prompt equals open file prompt response replace. Here we can get rid of all this. Replace conversation with conversation. And so what this is going to do is it's going to jump over here and we're just going to replace this. So it will be sorted chronologically, but it will also only have the most relevant conversation elements. So this will go back infinitely far, regardless of how long the conversation is, it's only going to pull the 15 most relevant things. So we should actually probably parameterize this. So we'll do like, let's see, convo length equals 15. And then we'll do top k equals convo length. And so then, because we do have a really large window, so we can actually probably up this to like 30. Cause we have, unless the messages are super long, and also since we're offloading a big chunk of the search, it's all fine. Okay, so then we do, we basically have to do the same thing. Generate response vectorize, et cetera, et cetera. So output. So this will be Raven's response. Time stamp, time string, message. Yep, we get the message. Yep, we get the message and then the vector. We're going to do this this way, so the vector is that. We're going to pick another unique ID, do this the same way. All right, so instead the metadata or the speaker will be Raven. The rest will be the same because we've already got that. Save JSON. We give it the same unique ID. We give it the metadata, payload append, unique ID, and vector. So that's all there. So now it's searchable and I think the last bit is we just need to give the output. Oh yeah, we did save JSON so we saved it to the Nexus. Let's go ahead and make that folder. Folder, Nexus and then print the output and it's that simple. And then print the output. And it's that simple. And my friends, if I'm not mistaken, we are ready to test this. Oh, boy. This is going to, this will either work or it will explode. Python chat 2. User, hello, Raven, are you awake? List append takes exactly one argument and two given. Oh, right, line 102, append, this is supposed to be a tuple. I knew that. Payload append. Oh, you know what else I forgot to do? I forgot to actually do the upsert. So now we'll do vdb.upsert payload. Hello, Raven. How are you today? Yay, cool. Okay, so we got a response. So let's check. Okay, previous conversation, there's nothing here obviously of any society in order to... Cool, so Raven's already waxing philosophical about the core objective functions. Yes, remember they are very low in the stack, that is on purpose. What do you think are the primary causes of suffering and poverty? Well suffering is intrinsic to life according to Buddhism. So then what we'll do is once we get a response we will make sure. Let's see. Yep there we go. We're getting we're getting responses so we know that the search is working all this is good. This is good. This is good. Some of the primary causes include inequality. All right. So Raven is kind of talking to himself. I also misspelled previous previous conversation. Right. Do you think there are any solutions to reduce suffering and poverty? Actually that's kind of your purpose, isn't it? You are a chatbot and prototype AGI meant to help us with these goals. So let me turn that around. How do you think you can help reduce suffering and increase prosperity? So let's see what Raven thinks about this. Now also one thing to keep in mind is because... okay so this is a brand new conversation. It's super short. is OK so this is a brand new conversation it's super short. Let's see. Yeah gives me some stuff let's see do you think there are any other solutions to this so on and so forth. So let me show you what's going on in the Nexus. So this is the raw data right. There we go. Time string. So one thing that I noticed is that Raven spit out a timeString here. So that's not exactly what we want. So let's see. And I'll now provide a long detailed response. Oh, right. This happened last time because I'm including the timeSt string here. So I think what we need to do is, I think we actually do need to go back and remove, we need to remove, yeah, I actually did this exact thing last time where the message that I vectorized and saved included all this, and it's not supposed to. This is only supposed to be there on the output, I think. Because basically what chat-gpt is doing, or not chat-gpt, gpt3 is doing is it's noticing the pattern and it's saying, oh, well, I'm supposed to regurgitate this pattern. So that's not correct, actually. What I did was I added this information so that when Raven is reading it, when it's put in, Raven can see the timestamps. But I think that that is actually just superfluous information. So what I'm going to do is I'm going to duplicate this and then just say message equals. A. Whoops. And then we'll do the same thing down here. Message equals output. OK so that's fine. All right so suffering and poverty are complex issues that are multifaceted solutions blah blah blah. This can be done through initiatives such as this. OK. Raven do you know what you are? Let's see what Raven says. Okay, so Raven's basically just talking to himself. So that's no good. So if we go back and look at, let's look at the last log. Oh, Raven's not responding to my question because I don't have my thing. So this is why Raven is just talking to himself. Okay, so we need to add, we do need to insert at least my most recent response. OK, so we need to go into our prompt. And so then we'll do user, and we'll do message. And so then what we need to do is for the replace, let's see, output, there we go. And then we'll do replace message with A, because that's my user input. Because otherwise, this is why Raven's talking to himself. Hello, Raven. What were we talking about? Yeah, we were discussing this, we were discussing this. We were discussing do you think there are any other solutions? Let's change topic. I want to know. I want to I want to discuss what you are. Do you know what you are, Raven? And so what I do want to say, I'm a chatbot and prototype AGI. Okay, cool. So he remembered when I said that. My primary goal is to provide a platform for people to discuss and understand underlying causes of suffering and poverty, to explore potential solutions. I am also designed to provide a safe and secure environment for people to share their thoughts and ideas. My capabilities include natural language processing, machine learning, and artificial intelligence.\" Yeah, that's, those are some good inferences. We're actually still exploring what you are and how to deploy you. So let's see what he says to that. So anyways, you get the picture, it's working. There's a million little bugs to figure out, but we just set up and we are using Pinecone as a Nexus service. So that's pretty cool. Raven's final output, as a chatbot and prototype AGI, I am designed to reduce suffering and increase prosperity. Forgot about the increase understanding. Just kind of regurgitating what he said. I'm also designed to... okay blah blah blah blah blah. Okay so not the smartest thing but we do have a very basic Nexus service. I think we'll call it there because this was a raging success. service, I think we'll call it there because this was a raging success.", "chunks": [{"timestamp": [0.0, 2.42], "text": " All right, guys, this is really exciting."}, {"timestamp": [2.42, 5.58], "text": " So today's video is sponsored by Pinecone."}, {"timestamp": [5.58, 9.62], "text": " And if you know me, you know that I have been like super resistant to sponsorships, but"}, {"timestamp": [9.62, 12.6], "text": " when Pinecone reached out, I was like, yes, this makes sense because I was going to use"}, {"timestamp": [12.6, 13.82], "text": " Pinecone anyways."}, {"timestamp": [13.82, 14.82], "text": " Let's do it."}, {"timestamp": [14.82, 17.02], "text": " So what are we doing today?"}, {"timestamp": [17.02, 19.6], "text": " First, need to do a little bit of preamble."}, {"timestamp": [19.6, 22.06], "text": " So come and support me on Patreon."}, {"timestamp": [22.06, 27.16], "text": " I'm almost at my goals, at which point I will take down all ads forever."}, {"timestamp": [27.16, 29.6], "text": " I might continue with sponsorships, but that's fine."}, {"timestamp": [29.6, 32.8], "text": " So anyways, my Patreon is patreon.com slash Dave Schapp."}, {"timestamp": [32.8, 36.16], "text": " Also, if you sign up for a higher tier, you get to interact with me directly."}, {"timestamp": [36.16, 39.16], "text": " So that has less to do with the videos and more to do with just like, hey, if you have"}, {"timestamp": [39.16, 41.48], "text": " a question, I'm happy to answer."}, {"timestamp": [41.48, 42.48], "text": " Okay."}, {"timestamp": [42.48, 46.42], "text": " So moving right along, a lot of you also know that I'm working"}, {"timestamp": [46.42, 48.12], "text": " on an open source AGI project"}, {"timestamp": [48.12, 50.2], "text": " or what I call artificial cognitive entity"}, {"timestamp": [50.2, 53.0], "text": " because artificial general intelligence is a useless term."}, {"timestamp": [53.0, 55.0], "text": " I actually prefer artificial global intelligence"}, {"timestamp": [55.0, 56.6], "text": " because that's really the goal."}, {"timestamp": [57.84, 59.64], "text": " Let's do Ultron, but safe."}, {"timestamp": [59.64, 62.2], "text": " And so the architecture,"}, {"timestamp": [62.2, 64.3], "text": " the underpinning architecture for my project"}, {"timestamp": [64.3, 65.92], "text": " is what I call Meragi,"}, {"timestamp": [65.92, 68.16], "text": " microservices architecture for robotics"}, {"timestamp": [68.16, 70.28], "text": " and artificial general intelligence."}, {"timestamp": [70.28, 73.84], "text": " So the primary thing that you need to know about Meragi"}, {"timestamp": [73.84, 76.32], "text": " is that it is all organized around a nexus."}, {"timestamp": [76.32, 78.78], "text": " This is not a COVID virus, by the way."}, {"timestamp": [79.88, 80.96], "text": " It kind of looks like one."}, {"timestamp": [80.96, 83.48], "text": " All right, so the nexus is,"}, {"timestamp": [83.48, 85.12], "text": " as the name implies, the center of it."}, {"timestamp": [85.12, 89.38], "text": " It is a hub and spoke model, which means it is a very simple model."}, {"timestamp": [89.38, 90.46], "text": " Now what is the Nexus?"}, {"timestamp": [90.46, 95.6], "text": " The Nexus is primarily an information store, a database."}, {"timestamp": [95.6, 97.48], "text": " Now you could use SQL."}, {"timestamp": [97.48, 100.2], "text": " In my original experiments, I actually used SQLite."}, {"timestamp": [100.2, 102.38], "text": " SQLite works perfectly fine."}, {"timestamp": [102.38, 103.64], "text": " It is indexable."}, {"timestamp": [103.64, 106.16], "text": " However, you're limited to SQL searches."}, {"timestamp": [106.16, 108.82], "text": " You're not, you don't have good semantic search."}, {"timestamp": [108.82, 110.86], "text": " So we need to upgrade to something"}, {"timestamp": [110.86, 112.48], "text": " that does have semantic search."}, {"timestamp": [112.48, 113.52], "text": " Enter Pinecone."}, {"timestamp": [113.52, 114.84], "text": " If you're not familiar with Pinecone,"}, {"timestamp": [114.84, 116.72], "text": " we're gonna go through from the beginning"}, {"timestamp": [116.72, 119.4], "text": " because I, even though I've intended to use Pinecone"}, {"timestamp": [119.4, 122.8], "text": " for more than a year now, I hadn't gotten around to it."}, {"timestamp": [122.8, 123.64], "text": " So now I'm gonna get around to it."}, {"timestamp": [123.64, 124.92], "text": " So we're gonna learn Pinecone together."}, {"timestamp": [124.92, 126.78], "text": " Oh, you know what I just noticed?"}, {"timestamp": [126.78, 128.78], "text": " Oh, my OCD is gonna kill me."}, {"timestamp": [128.78, 130.64], "text": " Do you see this one right here?"}, {"timestamp": [130.64, 132.98], "text": " One of the spokes is bent."}, {"timestamp": [132.98, 133.82], "text": " Okay, anyways."}, {"timestamp": [135.22, 139.02], "text": " All right, so this is the highest level overview"}, {"timestamp": [139.02, 140.7], "text": " functional diagram for Meragi"}, {"timestamp": [140.7, 142.22], "text": " is where basically you have a nexus,"}, {"timestamp": [142.22, 143.74], "text": " which is this is the data."}, {"timestamp": [143.74, 147.48], "text": " And the reason that Meragi is a data-first"}, {"timestamp": [147.48, 149.84], "text": " or data-centric model is because I realized"}, {"timestamp": [149.84, 151.64], "text": " that all learning, all experiences,"}, {"timestamp": [151.64, 153.96], "text": " all knowledge are all data, right?"}, {"timestamp": [153.96, 155.62], "text": " Everything that makes us human,"}, {"timestamp": [155.62, 157.32], "text": " everything that makes us intelligent"}, {"timestamp": [157.32, 159.44], "text": " is about the information we carry"}, {"timestamp": [159.44, 162.52], "text": " and that we can then use."}, {"timestamp": [162.52, 164.6], "text": " So information first."}, {"timestamp": [164.6, 167.98], "text": " It is also, it's a data first model."}, {"timestamp": [167.98, 170.04], "text": " So that's kind of like the bits and bytes."}, {"timestamp": [170.04, 171.98], "text": " It's also a cognition first model."}, {"timestamp": [171.98, 176.4], "text": " And so what I mean by cognition first is this can think"}, {"timestamp": [176.4, 178.56], "text": " even without being connected to the outside world"}, {"timestamp": [178.56, 179.38], "text": " in any way."}, {"timestamp": [179.38, 180.72], "text": " It doesn't need sensor input."}, {"timestamp": [180.72, 182.68], "text": " It doesn't need motor output."}, {"timestamp": [182.68, 187.2], "text": " It can, but it is a cognition-first and data-first"}, {"timestamp": [187.2, 193.2], "text": " model of artificial intelligence. Okay, if you're lost, that's fine. Scrolling down a little bit,"}, {"timestamp": [193.2, 198.72], "text": " this is my Raven project, so github.com slash Dave Schapp slash Raven if you want to get involved."}, {"timestamp": [199.36, 203.76], "text": " So basically, there's a few layers of abstraction. Here, let me make sure I'm not talking too loud."}, {"timestamp": [203.76, 210.02], "text": " Okay, cool. There's a few layers of abstraction to think about artificial cognition."}, {"timestamp": [210.02, 214.44], "text": " So at the very foundation is data, data first."}, {"timestamp": [214.44, 218.76], "text": " So this is episodic and declarative memory, which, oh, by the way, a vector database is"}, {"timestamp": [218.76, 220.64], "text": " the optimal thing to store this in."}, {"timestamp": [220.64, 225.92], "text": " Now, that being said, if you combine vector search or semantic search with things like knowledge graph, that could be even better. We're figuring this in. Now, that being said, if you combine vector search or semantic search with"}, {"timestamp": [225.92, 229.4], "text": " things like Knowledge Graph, that could be even better. We're figuring this out. This"}, {"timestamp": [229.4, 234.04], "text": " is a large open source project. It's about three weeks old and we're getting organized."}, {"timestamp": [234.04, 238.0], "text": " We're getting funding. We're having talks about funding, so on and so forth. But the"}, {"timestamp": [238.0, 242.32], "text": " architecture is here. Oh man, you know what I just realized? There's an uneven gap here"}, {"timestamp": [242.32, 245.84], "text": " too. My OCD is just like way turned up today."}, {"timestamp": [245.84, 247.22], "text": " Maybe it's all the coffee."}, {"timestamp": [247.22, 248.44], "text": " No, I haven't had my coffee yet."}, {"timestamp": [248.44, 249.28], "text": " Maybe that's the problem."}, {"timestamp": [249.28, 251.64], "text": " All right, so anyways, what we're doing today"}, {"timestamp": [251.64, 254.08], "text": " is we're going to create a store"}, {"timestamp": [254.08, 255.96], "text": " for episodic and declarative memory,"}, {"timestamp": [255.96, 257.68], "text": " and then we're gonna do semantic search"}, {"timestamp": [257.68, 259.16], "text": " or vector database search."}, {"timestamp": [259.16, 260.48], "text": " So we're basically just covering"}, {"timestamp": [260.48, 263.76], "text": " the bottom two layers of this."}, {"timestamp": [263.76, 266.0], "text": " Now, technically, we're also introducing the core objective functions or the here's to comparatives. covering the bottom two layers"}, {"timestamp": [262.24, 267.52], "text": " of this. Now technically we're also"}, {"timestamp": [266.0, 269.84], "text": " introducing the core objective functions"}, {"timestamp": [267.52, 271.36], "text": " or the heuristic comparatives"}, {"timestamp": [269.84, 273.12], "text": " and that is just by virtue of the fact"}, {"timestamp": [271.36, 274.48], "text": " that it is embedded into the prompts."}, {"timestamp": [273.12, 276.8], "text": " You can embed the core objective"}, {"timestamp": [274.48, 279.52], "text": " functions at many many layers,"}, {"timestamp": [276.8, 280.4], "text": " at inference time, at evaluation time, at"}, {"timestamp": [279.52, 282.8], "text": " planning time,"}, {"timestamp": [280.4, 283.84], "text": " there's all kinds of places, but the point"}, {"timestamp": [282.8, 285.36], "text": " being is that"}, {"timestamp": [283.84, 285.2], "text": " the core objective functions"}, {"timestamp": [285.2, 291.8], "text": " are literally the first type of cognition that's applied to the data going up and down"}, {"timestamp": [291.8, 296.72], "text": " the stack, which means that they're core, they're central to the operation of the thing."}, {"timestamp": [296.72, 301.88], "text": " Whatever else it's doing, problem solving, executive function, cognitive control, memory"}, {"timestamp": [301.88, 306.6], "text": " consolidation, all of that still is above the core objective functions,"}, {"timestamp": [306.6, 307.84], "text": " which mean the core objective functions"}, {"timestamp": [307.84, 309.16], "text": " apply to all of that."}, {"timestamp": [310.68, 313.6], "text": " And then finally, here's a much more complicated diagram."}, {"timestamp": [313.6, 315.76], "text": " This is kind of what it's gonna look like eventually,"}, {"timestamp": [315.76, 318.6], "text": " but we're basically just doing the bottom three layers here."}, {"timestamp": [318.6, 322.12], "text": " And we're essentially gonna be doing just the Nexus today."}, {"timestamp": [322.12, 324.94], "text": " We're not gonna be doing any other microservices."}, {"timestamp": [324.94, 325.28], "text": " Okay,"}, {"timestamp": [325.28, 332.64], "text": " so before we dive off, I did also want to ask you to consider, if you don't donate to me to"}, {"timestamp": [332.64, 345.0], "text": " support my life, consider donating to UNHCR. So the UNHCR is the UN's organization for helping refugees."}, {"timestamp": [346.48, 349.88], "text": " I've given to UNHCR for many, many years."}, {"timestamp": [349.88, 354.72], "text": " I started during actually the civil war in Syria"}, {"timestamp": [354.72, 357.24], "text": " many years ago because it was just awful"}, {"timestamp": [357.24, 358.92], "text": " and it broke my heart and I was like,"}, {"timestamp": [358.92, 362.36], "text": " I wanna do something, but like, you know,"}, {"timestamp": [362.36, 364.16], "text": " I'm just a dude."}, {"timestamp": [364.16, 366.7], "text": " It's not like I was gonna go stop the war single-handedly. So I looked around and I was like, you know, I'm just I'm just a dude. It's not like I was going to go stop the war single-handedly."}, {"timestamp": [366.7, 370.7], "text": " So I looked around and I was like, okay, well, you know,"}, {"timestamp": [370.7, 373.0], "text": " the it's a it's a huge refugee crisis."}, {"timestamp": [373.0, 373.7], "text": " So how can I help that?"}, {"timestamp": [373.7, 377.0], "text": " And I looked around and actually have a friend an internet friend"}, {"timestamp": [377.0, 380.8], "text": " who was in aid coordination for many years."}, {"timestamp": [380.8, 381.7], "text": " I don't know if he still does that."}, {"timestamp": [383.2, 390.64], "text": " But anyways, I asked him and he said, yeah, UNHCR, like, they're, you know, every non-profit organization has its issues."}, {"timestamp": [391.28, 398.0], "text": " He's like, but UNHCR is good because one, it is accountable. It's a global entity,"}, {"timestamp": [398.0, 409.24], "text": " but also it has global reach, which also makes it pretty good. So I give to UNHCR every month for more than 10 years now and I certainly don't regret"}, {"timestamp": [409.24, 410.24], "text": " it."}, {"timestamp": [410.24, 417.88], "text": " So given everything going on all over the world, you know, just give it a thought."}, {"timestamp": [417.88, 418.88], "text": " Yeah."}, {"timestamp": [418.88, 419.88], "text": " All right."}, {"timestamp": [419.88, 420.92], "text": " So moving right along."}, {"timestamp": [420.92, 424.36], "text": " Let's dive into Pinecone."}, {"timestamp": [424.36, 425.0], "text": " Like I said, I haven't used it before."}, {"timestamp": [425.0, 426.0], "text": " I know people that have."}, {"timestamp": [426.0, 427.0], "text": " I've looked at it."}, {"timestamp": [427.0, 429.0], "text": " I know the underlying technology, and I said,"}, {"timestamp": [429.0, 431.0], "text": " this is the way."}, {"timestamp": [431.0, 433.0], "text": " Oh, also before we get started,"}, {"timestamp": [433.0, 436.0], "text": " the repo for this is Pinecone Infinite Memory Chatbot."}, {"timestamp": [436.0, 438.0], "text": " Infinite memory."}, {"timestamp": [438.0, 441.0], "text": " Once you see the numbers, you will understand what I mean."}, {"timestamp": [441.0, 446.04], "text": " Like, we are about to build a chatbot that has a longer memory than,"}, {"timestamp": [446.04, 447.26], "text": " well, I'm not gonna say than God"}, {"timestamp": [447.26, 448.58], "text": " because that's just too hyperbolic,"}, {"timestamp": [448.58, 450.9], "text": " but longer memory than any single human."}, {"timestamp": [450.9, 455.9], "text": " The base, like tier for Pinecone is free."}, {"timestamp": [458.92, 461.46], "text": " And I think, I could be wrong,"}, {"timestamp": [461.46, 463.84], "text": " but I think that they said like you get up"}, {"timestamp": [463.84, 467.2], "text": " to 5 million inferences or 5 million"}, {"timestamp": [467.2, 468.2], "text": " records."}, {"timestamp": [468.2, 474.64], "text": " So imagine that you have a database, a chatbot, that remembers the last 5 million messages."}, {"timestamp": [474.64, 477.88], "text": " That is a long conversation."}, {"timestamp": [477.88, 479.92], "text": " So let's see if we can do this."}, {"timestamp": [479.92, 483.56], "text": " Oh, and also you go to their homepage, long-term memory for AI."}, {"timestamp": [483.56, 485.86], "text": " Gee, that's kind of what we want, isn't it?"}, {"timestamp": [485.86, 487.56], "text": " All right, so I'm not gonna bore you to death"}, {"timestamp": [487.56, 488.4], "text": " with how this works."}, {"timestamp": [488.4, 489.76], "text": " We're just gonna jump in and use it."}, {"timestamp": [489.76, 491.46], "text": " So I'm gonna go ahead and sign up."}, {"timestamp": [496.32, 499.24], "text": " Here, I'll go ahead and pause this for a second."}, {"timestamp": [499.24, 501.44], "text": " Okay, well, it was supposed to ask me"}, {"timestamp": [502.64, 504.12], "text": " like how I got, how I figured out"}, {"timestamp": [504.12, 508.1], "text": " that I wanted to use Pinecone, but it did not."}, {"timestamp": [508.1, 510.2], "text": " So create your first index."}, {"timestamp": [510.2, 515.2], "text": " All right, this will be raven MVP, lowercase."}, {"timestamp": [516.36, 517.92], "text": " Okay, let's see."}, {"timestamp": [517.92, 522.54], "text": " Let's see, raven MVP."}, {"timestamp": [522.54, 542.64], "text": " Dimensions, okay, So here's the thing is we need to know OpenAI, not that. Docs.OpenA."}, {"timestamp": [542.64, 546.04], "text": " This is a mess."}, {"timestamp": [547.72, 549.04], "text": " All right, documentation. Because here's what we're gonna do."}, {"timestamp": [549.04, 553.6], "text": " We're going to use OpenAI, their embeddings,"}, {"timestamp": [553.6, 554.84], "text": " right here, there we go."}, {"timestamp": [555.92, 560.14], "text": " And we need, let's see, embedding models."}, {"timestamp": [561.48, 563.2], "text": " Where is Ada02?"}, {"timestamp": [565.92, 567.76], "text": " Pages per dollar, 3,000."}, {"timestamp": [567.76, 572.4], "text": " Text embedding 802."}, {"timestamp": [572.4, 575.92], "text": " Output dimensions, so max input is 8,100."}, {"timestamp": [575.92, 578.12], "text": " Output is 1536."}, {"timestamp": [578.12, 579.56], "text": " Okay."}, {"timestamp": [579.56, 585.58], "text": " So, we are going to do 1536 dimensions."}, {"timestamp": [589.1, 591.64], "text": " Let's see, lowest latency and highest throughput, best storage capacity, faster queries."}, {"timestamp": [591.64, 596.18], "text": " Let's do, we'll do P1, sure, whatever."}, {"timestamp": [598.56, 602.62], "text": " We'll be terminated after 14 days, create index, okay."}, {"timestamp": [603.68, 604.52], "text": " Yep."}, {"timestamp": [605.52, 607.46], "text": " All right, so here's our index, it's initializing,"}, {"timestamp": [607.46, 608.4], "text": " that's cool."}, {"timestamp": [608.4, 611.0], "text": " So if I recall correctly, and I apologize to anyone"}, {"timestamp": [611.0, 613.2], "text": " at Pinecone if this is wrong, I think what it's doing"}, {"timestamp": [613.2, 615.44], "text": " is it's actually spinning up an individual container"}, {"timestamp": [615.44, 617.48], "text": " for me on the cloud."}, {"timestamp": [617.48, 620.64], "text": " Yeah, because the environment is US East 1 GCP."}, {"timestamp": [620.64, 624.3], "text": " So this tells me that it is running my container"}, {"timestamp": [624.3, 626.4], "text": " on the Google Cloud."}, {"timestamp": [626.4, 628.32], "text": " Yes."}, {"timestamp": [628.32, 643.84], "text": " So, and then rather than uploading stuff individually, what I'm going to do is blah, not that."}, {"timestamp": [643.84, 646.78], "text": " All right, we're going to do,"}, {"timestamp": [646.78, 650.78], "text": " we're gonna go back to, where did it go?"}, {"timestamp": [651.9, 654.32], "text": " Pinecone.io."}, {"timestamp": [655.5, 657.02], "text": " All right, so we're gonna go back to the docs"}, {"timestamp": [657.02, 658.98], "text": " and we're gonna do quick start."}, {"timestamp": [658.98, 660.5], "text": " And also I apologize that this video"}, {"timestamp": [660.5, 664.42], "text": " is a total unmitigated mess, but you know what?"}, {"timestamp": [666.06, 668.48], "text": " All of you watch it on like 2x speed anyways."}, {"timestamp": [668.48, 671.84], "text": " So you're probably just laughing your butt off."}, {"timestamp": [671.84, 675.06], "text": " All right, so let's do a pip install pine cone client."}, {"timestamp": [675.06, 676.5], "text": " Ta-da!"}, {"timestamp": [676.5, 678.88], "text": " So TLDR, you need the pine cone client"}, {"timestamp": [678.88, 680.42], "text": " in order to run Python."}, {"timestamp": [680.42, 682.86], "text": " Now, that being said, a lot of you expressed interest"}, {"timestamp": [682.86, 684.26], "text": " in how do I do this stuff if I don't know"}, {"timestamp": [684.26, 686.48], "text": " the first thing about coding."}, {"timestamp": [686.48, 689.9], "text": " So I'm going to try and do as little coding as possible."}, {"timestamp": [689.9, 692.64], "text": " So we're going to jump over here and use ChatGPT."}, {"timestamp": [692.64, 695.0], "text": " I upgraded to ChatGPT Pro."}, {"timestamp": [695.0, 697.72], "text": " Man, is it much faster?"}, {"timestamp": [697.72, 702.22], "text": " I definitely recommend it because one, the speed, and two, I don't know if it's actually"}, {"timestamp": [702.22, 705.76], "text": " smarter or if it just feels smarter because it's faster,"}, {"timestamp": [705.76, 711.52], "text": " but definitely ChatGPT Pro, I think is totally worth it, especially for 20 bucks a month."}, {"timestamp": [711.52, 717.76], "text": " It's like, it saves me so much time, and my time is so valuable, right?"}, {"timestamp": [717.76, 718.76], "text": " It's a no brainer."}, {"timestamp": [718.76, 719.76], "text": " It's definitely worth it."}, {"timestamp": [719.76, 723.48], "text": " Okay, so what are we trying to achieve here?"}, {"timestamp": [723.48, 726.12], "text": " We're trying to get a chatbot that will use up to 5,000"}, {"timestamp": [726.12, 728.04], "text": " or 5 million memories."}, {"timestamp": [728.04, 731.88], "text": " So what I want to do is I want to save every chat message"}, {"timestamp": [731.88, 735.84], "text": " that I do with this chatbot."}, {"timestamp": [735.84, 758.0], "text": " And I want to recall stuff based on recent conversations. So let's make a ultra simple chatbot using OpenAI TextAda02"}, {"timestamp": [758.0, 763.16], "text": " embeddings for searchability."}, {"timestamp": [763.16, 772.24], "text": " We will index each message, chat message, back and forth, and then we will store"}, {"timestamp": [773.68, 787.58], "text": " all messages in a pine cone index. I want you to write a Python script that will do this for me."}, {"timestamp": [787.58, 790.24], "text": " Okay, so if you don't know code,"}, {"timestamp": [791.2, 792.58], "text": " that's okay, because chat GPT does."}, {"timestamp": [792.58, 796.5], "text": " All you need to do is know the English language"}, {"timestamp": [800.36, 802.92], "text": " to describe what you want."}, {"timestamp": [802.92, 804.54], "text": " And so let's see if this works."}, {"timestamp": [804.54, 808.36], "text": " All right, it doesn't seem to know some of this stuff yet."}, {"timestamp": [808.36, 809.64], "text": " It's creating the index."}, {"timestamp": [809.64, 810.76], "text": " I don't need it to do that."}, {"timestamp": [810.76, 813.68], "text": " It also doesn't, it chose 512."}, {"timestamp": [813.68, 815.28], "text": " So all you need to do is be able to look at it"}, {"timestamp": [815.28, 817.38], "text": " and say like, is this really right?"}, {"timestamp": [824.8, 825.96], "text": " Okay, I'm sorry, I don't understand Dave. But you see how much faster this is? Okay."}, {"timestamp": [825.96, 828.8], "text": " I'm sorry, I don't understand, Dave."}, {"timestamp": [828.8, 830.12], "text": " But you see how much faster this is?"}, {"timestamp": [830.12, 832.6], "text": " It just spit all this out in a couple seconds."}, {"timestamp": [832.6, 834.96], "text": " And so I'm like, this is right."}, {"timestamp": [834.96, 844.0], "text": " I don't know if this is right."}, {"timestamp": [844.0, 849.68], "text": " All right. So close close but no cigar."}, {"timestamp": [849.68, 852.16], "text": " Text DaVinci 02, okay, so it's this out of date."}, {"timestamp": [852.16, 855.48], "text": " It doesn't even know about Text DaVinci 03."}, {"timestamp": [855.48, 860.12], "text": " All right, so we're probably going to need to do a little bit of cutting and pasting,"}, {"timestamp": [860.12, 861.12], "text": " let's say."}, {"timestamp": [861.12, 867.32], "text": " All right, so this was the script that I used for the last long-term chatbot memory."}, {"timestamp": [867.32, 868.4], "text": " So basically what we're gonna do"}, {"timestamp": [868.4, 869.6], "text": " is we're gonna throw all that out"}, {"timestamp": [869.6, 871.04], "text": " because here's what happens."}, {"timestamp": [871.04, 872.44], "text": " With iterative development,"}, {"timestamp": [872.44, 874.08], "text": " if you're not familiar with Agile,"}, {"timestamp": [874.08, 876.4], "text": " basically you get as far as you can."}, {"timestamp": [876.4, 879.72], "text": " This is iterative development on an individual level."}, {"timestamp": [879.72, 881.12], "text": " You get as far as you can,"}, {"timestamp": [881.12, 882.26], "text": " and then when you get stuck,"}, {"timestamp": [882.26, 886.88], "text": " you basically refactor everything that you've learned and start over."}, {"timestamp": [886.88, 889.64], "text": " Okay, so actually my door is open."}, {"timestamp": [889.64, 892.32], "text": " I'm going to pause it and close the door real quick."}, {"timestamp": [892.32, 895.68], "text": " And also a quick wardrobe change because I was getting too toasty."}, {"timestamp": [895.68, 900.08], "text": " Imagine that, you're super animated and drinking coffee and you get too hot."}, {"timestamp": [900.08, 901.92], "text": " It's getting hot and no, I'm not going to do that."}, {"timestamp": [901.92, 904.48], "text": " I'm not going to subject you people to that."}, {"timestamp": [904.48, 906.0], "text": " I actually do have a good singing voice."}, {"timestamp": [906.0, 907.32], "text": " It's just, I'm super not warmed up"}, {"timestamp": [907.32, 908.64], "text": " and I have awful timing."}, {"timestamp": [910.32, 911.88], "text": " All right, where was I?"}, {"timestamp": [911.88, 913.04], "text": " Right, Pinecone."}, {"timestamp": [913.04, 915.6], "text": " Yeah, this is close, but it's not quite."}, {"timestamp": [915.6, 919.76], "text": " So let's break this down into the functions"}, {"timestamp": [919.76, 920.88], "text": " that we're gonna need."}, {"timestamp": [921.82, 924.88], "text": " So I've got this function or this script here,"}, {"timestamp": [924.88, 926.32], "text": " 172 lines of code."}, {"timestamp": [926.32, 928.04], "text": " We can actually simplify this a lot,"}, {"timestamp": [928.04, 931.4], "text": " but we need to keep the stuff that we like."}, {"timestamp": [931.4, 936.64], "text": " So I'm going to go ahead and keep my completion."}, {"timestamp": [936.64, 939.4], "text": " So this is the actual chat function."}, {"timestamp": [939.4, 945.0], "text": " So we're going to leave this script as is"}, {"timestamp": [945.0, 948.36], "text": " just so that I have it for reference."}, {"timestamp": [948.36, 949.64], "text": " Because if you know me,"}, {"timestamp": [949.64, 953.24], "text": " you know that I am horrible at just recycling code,"}, {"timestamp": [953.24, 955.54], "text": " which sometimes gets me in trouble, but whatever."}, {"timestamp": [955.54, 958.2], "text": " Whoops, come back."}, {"timestamp": [958.2, 960.72], "text": " So we're gonna call this chat2.py."}, {"timestamp": [961.86, 963.44], "text": " Because that's super creative."}, {"timestamp": [963.44, 969.72], "text": " Okay, so what this does is it chooses TextDaVinci 03, which is the latest and greatest in terms"}, {"timestamp": [969.72, 972.04], "text": " of fine-tuned models."}, {"timestamp": [972.04, 978.8], "text": " The temperature is set to zero, because I actually don't like entropy injected unless"}, {"timestamp": [978.8, 980.56], "text": " I want it to be injected."}, {"timestamp": [980.56, 985.76], "text": " In most cases, a deterministic output is fine because it's just a thinking machine and I"}, {"timestamp": [985.76, 989.52], "text": " achieve what I want with prompt engineering."}, {"timestamp": [989.52, 990.84], "text": " So I'm a chatbot named Raven."}, {"timestamp": [990.84, 999.2], "text": " My goals are to reduce suffering, increase prosperity, and increase understanding."}, {"timestamp": [999.2, 1003.8], "text": " I will read the conversation notes, recent messages, and I'll provide a long, verbose,"}, {"timestamp": [1003.8, 1004.8], "text": " detailed response."}, {"timestamp": [1004.8, 1007.44], "text": " I'll then end my response with a follow up or leading question."}, {"timestamp": [1007.44, 1029.32], "text": " All right. So instead we are going to do we don't need notes because we're just going to do a super long term memory. And then we are going to actually, yeah, here, we're just have the conversation here."}, {"timestamp": [1029.32, 1031.32], "text": " Let's see, previous conversation."}, {"timestamp": [1031.32, 1034.0], "text": " All right, that's fine."}, {"timestamp": [1034.0, 1036.72], "text": " And then we will say, yeah, that's fine."}, {"timestamp": [1036.72, 1038.58], "text": " All right, so we update this."}, {"timestamp": [1038.58, 1044.76], "text": " And basically, what I'm going to do is I'm going to populate this with what we retrieve"}, {"timestamp": [1044.76, 1045.0], "text": " from Pinecone. populate this with With what we retrieve from"}, {"timestamp": [1046.36, 1048.08], "text": " From pine cone"}, {"timestamp": [1048.08, 1054.24], "text": " So we come let's take a look at the repo. So the repo I've got pine cone text and I"}, {"timestamp": [1054.76, 1057.66], "text": " Thought I copied my open AI API key. I did not"}, {"timestamp": [1058.52, 1066.08], "text": " We'll copy this from another repo. Okay, so we got open AI and then Pinecone. Here, I'm just going to shorten this"}, {"timestamp": [1066.08, 1072.48], "text": " so it's OpenAI.txt. So there's, these are my, these are my keys. Here, I'll, I'll do this. I'll do"}, {"timestamp": [1072.48, 1079.2], "text": " a key underscore so that they stay together. Repo hygiene is important. Key underscore."}, {"timestamp": [1079.84, 1086.18], "text": " So we've got two API keys. We're going to be talking to OpenAI for the large language model and the embedding, and then we're going to be talking to Pinecone for storage. So we've got two API keys. We're gonna be talking to OpenAI for the large language model and the embedding."}, {"timestamp": [1086.18, 1088.82], "text": " And then we're gonna be talking to Pinecone for storage."}, {"timestamp": [1088.82, 1092.12], "text": " So we're not really gonna be doing any storage locally."}, {"timestamp": [1092.12, 1093.72], "text": " All right, so first things first,"}, {"timestamp": [1093.72, 1096.62], "text": " let's come back here and figure out what we need."}, {"timestamp": [1096.62, 1099.4], "text": " Let's, oops, I closed the chat."}, {"timestamp": [1099.4, 1100.26], "text": " Let's go to this one."}, {"timestamp": [1100.26, 1102.26], "text": " So we need the main loop."}, {"timestamp": [1102.26, 1105.32], "text": " So if you're not familiar with coding,"}, {"timestamp": [1105.32, 1108.68], "text": " and I do apologize, I promised that we would do like,"}, {"timestamp": [1108.68, 1111.44], "text": " let's call this low code, not no code, but low code."}, {"timestamp": [1113.08, 1118.08], "text": " While true, so pretty much all chatbots"}, {"timestamp": [1118.12, 1119.8], "text": " start with a while true loop."}, {"timestamp": [1119.8, 1123.8], "text": " And actually a lot of programs start with a while true loop."}, {"timestamp": [1123.8, 1125.92], "text": " So this is basically just an infinite loop."}, {"timestamp": [1125.92, 1128.76], "text": " And let's shorten this, openai.text."}, {"timestamp": [1128.76, 1131.76], "text": " Okay, so while true, what do we do?"}, {"timestamp": [1131.76, 1132.6], "text": " I don't know."}, {"timestamp": [1133.44, 1137.64], "text": " I think over here, I had, yep, get the user input first."}, {"timestamp": [1137.64, 1139.48], "text": " That's all good."}, {"timestamp": [1139.48, 1140.4], "text": " Let's see."}, {"timestamp": [1141.7, 1143.96], "text": " So then we get the vector."}, {"timestamp": [1143.96, 1146.0], "text": " So GPT-3 embedding."}, {"timestamp": [1146.0, 1148.72], "text": " Did I use..."}, {"timestamp": [1148.72, 1152.52], "text": " I might have already switched to Ada."}, {"timestamp": [1152.52, 1154.92], "text": " Oops."}, {"timestamp": [1154.92, 1155.92], "text": " Text embedding Ada02."}, {"timestamp": [1155.92, 1156.92], "text": " Oh, I was using it."}, {"timestamp": [1156.92, 1157.92], "text": " Great."}, {"timestamp": [1157.92, 1158.92], "text": " Okay, cool."}, {"timestamp": [1158.92, 1161.0], "text": " So I've already written this function."}, {"timestamp": [1161.0, 1163.0], "text": " Fantastic."}, {"timestamp": [1163.0, 1164.0], "text": " So we can copy..."}, {"timestamp": [1164.0, 1167.88], "text": " Whoops, we don't need the prompt. So we can go ahead and copy this function too."}, {"timestamp": [1167.88, 1171.7], "text": " So we get the embedding, which is great."}, {"timestamp": [1171.7, 1177.32], "text": " And also some of you asked why I do this here where I do content and code to ASCII, ignore"}, {"timestamp": [1177.32, 1179.08], "text": " errors and then decode."}, {"timestamp": [1179.08, 1184.44], "text": " This is because for whatever reason, some Unicode breaks GPT-3."}, {"timestamp": [1184.44, 1189.12], "text": " I don't know what it was, but I just found out that forcing it to ASCII, because ASCII"}, {"timestamp": [1189.12, 1194.56], "text": " is a smaller code base than Unicode, and then converting it back forces it into a correct"}, {"timestamp": [1194.56, 1199.44], "text": " format that GPT-3 already accepts, or always accepts."}, {"timestamp": [1199.44, 1203.12], "text": " So fix any Unicode errors."}, {"timestamp": [1203.12, 1205.88], "text": " Because there's like Unicode bomb,"}, {"timestamp": [1205.88, 1209.2], "text": " which is not bill of materials, but something"}, {"timestamp": [1209.2, 1211.88], "text": " to do with the way that it's encoding, whatever."}, {"timestamp": [1211.88, 1215.8], "text": " Anyways, so we get our embedding."}, {"timestamp": [1215.8, 1219.32], "text": " And then we come back down here."}, {"timestamp": [1219.32, 1222.48], "text": " All right, so let's do this."}, {"timestamp": [1222.48, 1227.24], "text": " And then the message, oh, yeah, so user, time string, et cetera, great. So that's do this. And then the message. Oh yeah. So user, time string, etc."}, {"timestamp": [1227.24, 1228.24], "text": " Great."}, {"timestamp": [1228.24, 1229.44], "text": " So that's all fine."}, {"timestamp": [1229.44, 1235.44], "text": " And then maybe we need the info too because I think Pinecone allows us to save some metadata."}, {"timestamp": [1235.44, 1238.2], "text": " All right."}, {"timestamp": [1238.2, 1241.48], "text": " So let's do this."}, {"timestamp": [1241.48, 1244.0], "text": " All right."}, {"timestamp": [1244.0, 1248.0], "text": " And then we will come over to Pinecone documentation. Nope. It's do this. All right. Then we will come over to Pinecone documentation."}, {"timestamp": [1248.0, 1250.8], "text": " Nope, it's over here. Quickstart."}, {"timestamp": [1250.8, 1252.76], "text": " All right. So Pinecone in it,"}, {"timestamp": [1252.76, 1254.28], "text": " we need this guy here,"}, {"timestamp": [1254.28, 1257.08], "text": " environment, your environment."}, {"timestamp": [1289.0, 1295.44], "text": " Let's see. No, not create index. Let's come over here. Import Pinecone. And then we also need to set Pinecone in it, your API key. So this will be open file pinecone.txt."}, {"timestamp": [1295.44, 1298.52], "text": " And if you didn't catch this, what I did earlier"}, {"timestamp": [1298.52, 1303.76], "text": " that I didn't show you was I got my API key and put it in here."}, {"timestamp": [1303.76, 1305.96], "text": " So there you have it."}, {"timestamp": [1308.96, 1310.8], "text": " Oh, actually, no, I need to update the name"}, {"timestamp": [1310.8, 1312.88], "text": " so that it is correct."}, {"timestamp": [1312.88, 1317.12], "text": " Key pine cone and then key open AI."}, {"timestamp": [1317.12, 1324.76], "text": " And then environment, I think, is going to be this guy."}, {"timestamp": [1324.76, 1327.44], "text": " Is that the environment?"}, {"timestamp": [1327.44, 1331.28], "text": " So API values."}, {"timestamp": [1337.84, 1341.16], "text": " I'm not sure which environment."}, {"timestamp": [1341.16, 1342.48], "text": " It could be the index name?"}, {"timestamp": [1345.68, 1346.88], "text": " I don't know."}, {"timestamp": [1346.88, 1350.02], "text": " Usually, sometimes what they have is,"}, {"timestamp": [1350.02, 1353.44], "text": " they'll give you like, here's the Python version of this,"}, {"timestamp": [1353.44, 1358.32], "text": " but we will go back to the docs."}, {"timestamp": [1358.32, 1361.78], "text": " RTFM, read the fun manual."}, {"timestamp": [1361.78, 1362.62], "text": " All right."}, {"timestamp": [1385.4, 1387.76], "text": " Index.upsert. Oh, here we go. So Pinecone. Let's see, what is I don't know, not applicable, okay. I really don't know what this is."}, {"timestamp": [1392.04, 1395.76], "text": " This also, note both your API key."}, {"timestamp": [1397.6, 1399.4], "text": " Okay, just says note your environment."}, {"timestamp": [1399.4, 1401.34], "text": " All right, Pinecone people, if you're watching,"}, {"timestamp": [1401.34, 1403.4], "text": " I don't know what you mean by environment"}, {"timestamp": [1403.4, 1404.44], "text": " because I go to my console"}, {"timestamp": [1404.44, 1406.0], "text": " and I don't see anything named environment."}, {"timestamp": [1408.88, 1417.76], "text": " Index name. Oh, environment here. Okay. Region. Region. Okay. So for the IT infrastructure"}, {"timestamp": [1417.76, 1422.24], "text": " parlance, this would be called a region, not an environment. Environment is like,"}, {"timestamp": [1422.24, 1426.4], "text": " are you running in a Docker container or on a Linux machine or in"}, {"timestamp": [1426.4, 1433.2], "text": " VMware but the cloud region. Okay got it figured it out. All right so basically I need to tell"}, {"timestamp": [1433.76, 1451.16], "text": " the API talk to this region. Region figured it out. I'm not going to let this go the way of the future. So I recently rewatched The Aviator with Leonardo DiCaprio and I initially watched it in theaters"}, {"timestamp": [1451.16, 1457.72], "text": " many years ago and I was ultra disturbed because the way that he repeats things, I used to"}, {"timestamp": [1457.72, 1462.06], "text": " do that in my head and turns out that's just a really common feature of OCD and it's because"}, {"timestamp": [1462.06, 1464.42], "text": " it doesn't feel right."}, {"timestamp": [1464.42, 1466.92], "text": " And if you know what I mean, you know what I mean."}, {"timestamp": [1466.92, 1471.82], "text": " And that is basically like it's a sensory processing issue and so if something doesn't"}, {"timestamp": [1471.82, 1474.84], "text": " feel right, you have to keep doing it until it does feel right."}, {"timestamp": [1474.84, 1480.2], "text": " Fortunately, I got over that mostly just by one, accepting that like, okay, this is a"}, {"timestamp": [1480.2, 1482.92], "text": " thing and then I kind of got used to it over time."}, {"timestamp": [1482.92, 1489.84], "text": " Also fixing your diet and sleep really helps with OCD. Okay, quick aside. All right, we got that figured out. Let's go back"}, {"timestamp": [1489.84, 1497.28], "text": " to our documentation. Okay, so we initialize it. So this is just initialize. So this init thing,"}, {"timestamp": [1498.08, 1503.52], "text": " so Pinecone init is about the same as just setting our API key. Okay, I think that that's fine."}, {"timestamp": [1507.4, 1512.32], "text": " is just setting our API key. Okay. I think that that's fine. Yes. All right. So that's nested correctly, I think. And then we get down, we don't need to create the index, we"}, {"timestamp": [1512.32, 1525.0], "text": " just need to grab an index. So let's come back over here. So index equals pine cone Pinecone index and this will be raven-mvp."}, {"timestamp": [1526.24, 1527.68], "text": " I know how to type, I promise."}, {"timestamp": [1527.68, 1529.16], "text": " Raven-mvp."}, {"timestamp": [1531.0, 1533.48], "text": " And I don't like index, I'm gonna do vdb."}, {"timestamp": [1533.48, 1534.72], "text": " Vector database."}, {"timestamp": [1534.72, 1535.76], "text": " Whoops."}, {"timestamp": [1535.76, 1537.44], "text": " All right, so this is our vector database."}, {"timestamp": [1537.44, 1540.72], "text": " And then we, okay, so we do an upsert."}, {"timestamp": [1541.68, 1545.0], "text": " So we give it a string and then that."}, {"timestamp": [1545.82, 1549.94], "text": " Okay, so if we give it a string and then a vector,"}, {"timestamp": [1551.82, 1553.92], "text": " if that's, I know that they said that we can do,"}, {"timestamp": [1553.92, 1555.4], "text": " oh wait, here we go."}, {"timestamp": [1555.4, 1559.1], "text": " IDC score, oh wait, no, that's just the query."}, {"timestamp": [1559.1, 1559.94], "text": " Okay."}, {"timestamp": [1563.14, 1565.0], "text": " Well, okay, so we'll keep it simple."}, {"timestamp": [1565.3, 1567.46], "text": " We'll just do the string, that's fine."}, {"timestamp": [1567.46, 1571.18], "text": " So index.upsert, or in this case, vdb.upsert."}, {"timestamp": [1571.18, 1575.86], "text": " So the message string will include the user,"}, {"timestamp": [1575.86, 1577.74], "text": " the time, and the message."}, {"timestamp": [1577.74, 1580.3], "text": " So it's all gonna be included in the message,"}, {"timestamp": [1580.3, 1581.24], "text": " so that's fine."}, {"timestamp": [1582.62, 1584.98], "text": " But the vector, we should actually do the vector later"}, {"timestamp": [1584.98, 1585.24], "text": " so that the vector includes all should actually do the vector later so that"}, {"timestamp": [1585.24, 1594.12], "text": " the vector includes all of that information, I think. Right? Or should we only search what"}, {"timestamp": [1594.12, 1608.28], "text": " the user actually put in? You know what? Screw it. I'm going to index the entire thing. So the vector equals GPT-3 premature tabbing"}, {"timestamp": [1608.28, 1611.56], "text": " tab completion. Not all men have this problem, but if you do"}, {"timestamp": [1611.56, 1616.52], "text": " have this problem, there's nothing to be ashamed of. Okay."}, {"timestamp": [1617.72, 1620.7], "text": " You're getting a little bit more unfiltered Dave lately. I"}, {"timestamp": [1620.7, 1623.42], "text": " hope that's okay. Some people say like you need to be like"}, {"timestamp": [1623.42, 1630.4], "text": " more humble and laid back and it's like no. This is this just my true self. So I apologize. If you don't like it,"}, {"timestamp": [1630.4, 1631.8], "text": " tough."}, {"timestamp": [1631.8, 1640.02], "text": " All right. So timestamp equals this. So we get a timestamp. So timestamp is a Unix epoch."}, {"timestamp": [1640.02, 1648.02], "text": " So it's just a number. It's a floating point number. We need timestamp to date time."}, {"timestamp": [1648.02, 1649.84], "text": " So let's grab this function."}, {"timestamp": [1649.84, 1657.92], "text": " OK, all it does is it returns a specifically formatted, whatchamacallit, English readable"}, {"timestamp": [1657.92, 1658.92], "text": " timestamp."}, {"timestamp": [1658.92, 1660.56], "text": " That's fine."}, {"timestamp": [1660.56, 1668.26], "text": " So let's grab this, put this here, and then, so, all right, we can get rid of that guy,"}, {"timestamp": [1668.26, 1673.16], "text": " we can get rid of that guy, vectorize it, save to Pinecone."}, {"timestamp": [1673.16, 1674.32], "text": " All right."}, {"timestamp": [1674.32, 1677.38], "text": " Now, so we've got our message and we've got our vector."}, {"timestamp": [1677.38, 1679.44], "text": " So let's go look at the Pinecone documentation."}, {"timestamp": [1679.44, 1688.06], "text": " And so what we do is we have to upsert, and we have string. Or so first it's a list and then within the list,"}, {"timestamp": [1688.06, 1693.06], "text": " there are tuples and the tuples have at index zero"}, {"timestamp": [1693.78, 1697.28], "text": " is a string and index two is a vector."}, {"timestamp": [1697.28, 1700.86], "text": " So this should be pretty straightforward."}, {"timestamp": [1703.86, 1708.16], "text": " You know, we can actually probably save some time if we just do the upsert at the end."}, {"timestamp": [1708.16, 1710.36], "text": " So let's do that."}, {"timestamp": [1710.36, 1716.14], "text": " Let's add our message and Raven's at the end so that way we don't accidentally pull our"}, {"timestamp": [1716.14, 1717.14], "text": " most recent messages."}, {"timestamp": [1717.14, 1718.64], "text": " Oh, I like that."}, {"timestamp": [1718.64, 1721.64], "text": " Yeah, okay."}, {"timestamp": [1721.64, 1727.3], "text": " So let's see, this will be,"}, {"timestamp": [1731.5, 1736.5], "text": " upsearch, actually no, we'll just, we'll call info, info list, and so then we will do info.append."}, {"timestamp": [1736.5, 1740.66], "text": " And then, so we need, the info is the list container,"}, {"timestamp": [1740.66, 1742.8], "text": " so now all we need to do is add a tuple"}, {"timestamp": [1742.8, 1744.92], "text": " that has message and vector."}, {"timestamp": [1744.92, 1745.08], "text": " And so then, once we get our chatb add a tuple that has message and vector. And"}, {"timestamp": [1745.08, 1750.4], "text": " so then once we get our chatbots message, we'll append it as well, and then we'll just"}, {"timestamp": [1750.4, 1758.84], "text": " upsert them both at the same time. Bingo. All right. So next thing we do is, and this"}, {"timestamp": [1758.84, 1763.2], "text": " is actually good because we will use this vector to search for Raven's response, which"}, {"timestamp": [1763.2, 1766.96], "text": " means that we won't pull, yeah, yeah, it's order of operations, man."}, {"timestamp": [1766.96, 1769.28], "text": " And if it doesn't make sense, sit tight, it will in a minute."}, {"timestamp": [1769.28, 1770.16], "text": " Let me do a time check."}, {"timestamp": [1770.16, 1771.92], "text": " I'm probably just going to take this all the way through."}, {"timestamp": [1771.92, 1772.96], "text": " We're at 30 minutes."}, {"timestamp": [1772.96, 1773.44], "text": " Heck with it."}, {"timestamp": [1773.44, 1775.84], "text": " I want to drive this across the finish line."}, {"timestamp": [1775.84, 1777.36], "text": " I don't care if it takes two hours."}, {"timestamp": [1777.36, 1778.8], "text": " Probably won't take two hours."}, {"timestamp": [1778.8, 1781.2], "text": " And I know that all of your eyes are bleeding already."}, {"timestamp": [1781.2, 1782.08], "text": " Eyes and ears."}, {"timestamp": [1782.08, 1784.56], "text": " You're just like, Dave, shut up, write some code."}, {"timestamp": [1786.72, 1787.28], "text": " Whatever."}, {"timestamp": [1790.36, 1792.48], "text": " OK. All right so now"}, {"timestamp": [1793.44, 1796.08], "text": " search for relevant messages"}, {"timestamp": [1796.64, 1798.96], "text": " and generate a response."}, {"timestamp": [1799.48, 1800.0], "text": " Cool."}, {"timestamp": [1800.92, 1803.32], "text": " So this is this is where it gets fun."}, {"timestamp": [1803.8, 1805.88], "text": " Not that it wasn't fun already."}, {"timestamp": [1806.56, 1809.4], "text": " We need to do a query."}, {"timestamp": [1809.4, 1813.32], "text": " I don't know what top k is."}, {"timestamp": [1813.32, 1817.28], "text": " What does top k mean in this context?"}, {"timestamp": [1819.6, 1823.0], "text": " Include values."}, {"timestamp": [1822.0, 1826.5], "text": " include values."}, {"timestamp": [1830.5, 1834.5], "text": " The following example for the top three vectors that are most similar. Oh, okay, so top k is just how many similar do you want?"}, {"timestamp": [1834.5, 1836.5], "text": " Simple enough."}, {"timestamp": [1836.5, 1840.5], "text": " Alright, so we will do"}, {"timestamp": [1840.5, 1845.56], "text": " results equals vdb.query."}, {"timestamp": [1845.56, 1847.36], "text": " This is straightforward."}, {"timestamp": [1847.36, 1850.12], "text": " Then vector equals vector."}, {"timestamp": [1850.12, 1854.36], "text": " Basically, we're going to say whatever it is we just said to Raven,"}, {"timestamp": [1854.36, 1857.68], "text": " get all messages that are most similar."}, {"timestamp": [1857.68, 1860.84], "text": " We'll do a topK."}, {"timestamp": [1864.28, 1866.56], "text": " Butterfingers, topK butterfingers."}, {"timestamp": [1866.56, 1869.68], "text": " Top K equals, let's say 15."}, {"timestamp": [1869.68, 1873.4], "text": " So the 15 most relevant messages."}, {"timestamp": [1873.4, 1880.28], "text": " And then I don't actually care about the values."}, {"timestamp": [1880.28, 1885.68], "text": " So that's fine. So we'll just do,"}, {"timestamp": [1887.6, 1888.44], "text": " what is it?"}, {"timestamp": [1888.44, 1889.54], "text": " Include values,"}, {"timestamp": [1890.88, 1893.78], "text": " include values equals false."}, {"timestamp": [1893.78, 1896.0], "text": " I just want the messages, don't really care."}, {"timestamp": [1896.0, 1897.98], "text": " So it should return,"}, {"timestamp": [1899.94, 1904.1], "text": " okay, it'll return a dictionary with matches"}, {"timestamp": [1904.1, 1906.4], "text": " and then matches will be a list"}, {"timestamp": [1906.56, 1908.12], "text": " and it may or may not have values"}, {"timestamp": [1908.12, 1910.0], "text": " or it might just be a list of strings."}, {"timestamp": [1910.52, 1912.0], "text": " OK so results."}, {"timestamp": [1912.12, 1915.44], "text": " OK so results"}, {"timestamp": [1915.88, 1916.96], "text": " matches."}, {"timestamp": [1918.88, 1920.76], "text": " So this is this"}, {"timestamp": [1920.76, 1922.92], "text": " is the this is the list of stuff"}, {"timestamp": [1922.92, 1924.52], "text": " so now we need to format it."}, {"timestamp": [1927.52, 1928.48], "text": " Let's see."}, {"timestamp": [1928.48, 1928.96], "text": " Let's see."}, {"timestamp": [1928.96, 1929.32], "text": " Let's see."}, {"timestamp": [1929.32, 1930.2], "text": " Let's see."}, {"timestamp": [1930.2, 1930.48], "text": " Let's see."}, {"timestamp": [1930.48, 1933.48], "text": " We're not going to delete anything."}, {"timestamp": [1933.48, 1934.6], "text": " Are we really almost done?"}, {"timestamp": [1938.72, 1940.56], "text": " That's fine."}, {"timestamp": [1940.56, 1944.88], "text": " All right, so if this is a list, so let's"}, {"timestamp": [1944.88, 1947.14], "text": " see how I formatted it over here. Let's go back in time. All right, so if this is a list, so let's see how I formatted it over here."}, {"timestamp": [1947.14, 1948.46], "text": " Let's go back in time."}, {"timestamp": [1948.46, 1951.14], "text": " All right, so conversation, load conversation,"}, {"timestamp": [1953.38, 1956.66], "text": " generate a response, fetch declarative memories."}, {"timestamp": [1956.66, 1957.5], "text": " We're not gonna do that."}, {"timestamp": [1957.5, 1958.78], "text": " Memories equals fetch memories."}, {"timestamp": [1958.78, 1960.78], "text": " Okay, we did all that fun stuff"}, {"timestamp": [1960.78, 1962.98], "text": " and it's just a big giant mess."}, {"timestamp": [1962.98, 1964.98], "text": " So actually, here's what we're gonna do."}, {"timestamp": [1964.98, 1971.24], "text": " We're going to have conversation equals"}, {"timestamp": [1971.24, 1985.0], "text": " n.joinResults.matches and actually, wait, why?"}, {"timestamp": [1989.7, 1992.54], "text": " Oh, oh, right, hang on."}, {"timestamp": [1993.62, 1998.62], "text": " So what this returns is it doesn't return"}, {"timestamp": [2000.22, 2003.5], "text": " the string or does it?"}, {"timestamp": [2012.8, 2017.72], "text": " Hang on. And the indexer updates a vector with the same ID already present. Wait, this isn't a string, this is an ID. How do I actually fetch a string?"}, {"timestamp": [2032.4, 2040.24], "text": " Let's see if there is something better because if I have to keep track of an ID in my own index, I mean, it's not the worst thing in the world, but it would be slightly less convenient."}, {"timestamp": [2040.24, 2041.94], "text": " Okay."}, {"timestamp": [2041.94, 2045.0], "text": " So here is some information."}, {"timestamp": [2045.0, 2050.0], "text": " Oh, here we go, here we go, here we go."}, {"timestamp": [2050.0, 2052.0], "text": " Okay."}, {"timestamp": [2052.0, 2054.0], "text": " Yes."}, {"timestamp": [2054.0, 2062.0], "text": " So I give it an ID and then a vector and then some metadata."}, {"timestamp": [2062.0, 2065.0], "text": " Okay."}, {"timestamp": [2065.0, 2069.0], "text": " So then if I want to give it that, I can filter."}, {"timestamp": [2069.0, 2071.0], "text": " So you can pull those IDs."}, {"timestamp": [2076.0, 2078.0], "text": " So ID values."}, {"timestamp": [2078.0, 2082.0], "text": " So the value is that and then the metadata is something like this."}, {"timestamp": [2082.0, 2084.0], "text": " So I can set some metadata."}, {"timestamp": [2084.0, 2085.28], "text": " Got it. Got metadata. Got it."}, {"timestamp": [2085.28, 2086.28], "text": " Got it."}, {"timestamp": [2086.28, 2087.28], "text": " Got it."}, {"timestamp": [2087.28, 2088.28], "text": " All right."}, {"timestamp": [2088.28, 2092.2], "text": " That's straightforward enough, I think."}, {"timestamp": [2092.2, 2093.2], "text": " I guess."}, {"timestamp": [2093.2, 2094.2], "text": " Okay."}, {"timestamp": [2094.2, 2095.94], "text": " So the message is not this."}, {"timestamp": [2095.94, 2099.88], "text": " The message is something else."}, {"timestamp": [2099.88, 2106.56], "text": " But this is actually fine because now I can set the timestamp, time string, the speaker,"}, {"timestamp": [2106.56, 2108.9], "text": " and the message itself as the metadata."}, {"timestamp": [2108.9, 2112.4], "text": " So let me go back and grab this,"}, {"timestamp": [2113.24, 2114.74], "text": " because this is basically,"}, {"timestamp": [2116.96, 2121.96], "text": " so, because if you look at this, set metadata, right?"}, {"timestamp": [2125.0, 2125.24], "text": " So when we do this, all right, at this set metadata, right?"}, {"timestamp": [2130.24, 2130.72], "text": " So when we do this, all right, so here's the upsert."}, {"timestamp": [2134.8, 2136.32], "text": " So we give it an ID and it looks like it can be alphanumeric. I'm gonna try a UUID"}, {"timestamp": [2136.32, 2138.36], "text": " because I don't really care about the ID,"}, {"timestamp": [2139.96, 2143.08], "text": " but I get, maybe I should, but I just like using UUIDs."}, {"timestamp": [2143.08, 2146.98], "text": " So instead we will use this here."}, {"timestamp": [2149.8, 2152.96], "text": " So we will just give it an arbitrary UUID four."}, {"timestamp": [2155.8, 2157.36], "text": " Okay."}, {"timestamp": [2157.36, 2159.56], "text": " And then, so then there's the vector."}, {"timestamp": [2159.56, 2163.28], "text": " So it's ID, vector, and then metadata."}, {"timestamp": [2163.28, 2169.4], "text": " So the metadata is this guy here that I'm working on."}, {"timestamp": [2169.4, 2172.78], "text": " So we'll just call this payload instead."}, {"timestamp": [2172.78, 2176.96], "text": " So our payload, because this is what we're actually going to upload eventually."}, {"timestamp": [2176.96, 2178.28], "text": " All right."}, {"timestamp": [2178.28, 2182.68], "text": " So the speaker is user, the time is timestamp, the vector, we don't need the vector in the"}, {"timestamp": [2182.68, 2184.2], "text": " metadata."}, {"timestamp": [2184.2, 2188.88], "text": " We do need the message. We don't need a UUID, and then a timestring."}, {"timestamp": [2188.88, 2192.28], "text": " So speaker, time, message, timestring."}, {"timestamp": [2192.28, 2194.38], "text": " So that will all be in the metadata."}, {"timestamp": [2194.38, 2198.28], "text": " And actually, let's just go ahead and call this metadata so that we don't hurt our brains"}, {"timestamp": [2198.28, 2200.0], "text": " later."}, {"timestamp": [2200.0, 2201.0], "text": " And then metadata."}, {"timestamp": [2201.0, 2202.0], "text": " All right."}, {"timestamp": [2202.0, 2208.82], "text": " So now, hypothetically, maybe we actually do need to include values true."}, {"timestamp": [2208.82, 2213.46], "text": " It should include the metadata on fetch."}, {"timestamp": [2213.46, 2216.56], "text": " Query, so we can use that to filter,"}, {"timestamp": [2216.56, 2219.98], "text": " but can it return the metadata?"}, {"timestamp": [2219.98, 2223.7], "text": " Fetch vectors, update vectors, list collections,"}, {"timestamp": [2223.7, 2226.2], "text": " describe a collection, delete a collection."}, {"timestamp": [2226.2, 2229.32], "text": " It might be, this might not be the way,"}, {"timestamp": [2229.32, 2233.64], "text": " we might have to keep the messages locally"}, {"timestamp": [2233.64, 2237.08], "text": " and all that this is gonna do is return us an ID."}, {"timestamp": [2237.08, 2239.88], "text": " Now, if we have a UUID, that makes it really easy to"}, {"timestamp": [2242.52, 2245.0], "text": " metadata, let's see. All right, so we can set the metadata, metadata, let's see."}, {"timestamp": [2245.0, 2246.72], "text": " All right, so we can set the metadata."}, {"timestamp": [2246.72, 2247.68], "text": " Metadata, let's see."}, {"timestamp": [2247.68, 2249.96], "text": " All right."}, {"timestamp": [2249.96, 2251.6], "text": " All metadata is indexed."}, {"timestamp": [2251.6, 2253.84], "text": " OK, that's interesting."}, {"timestamp": [2253.84, 2255.48], "text": " That will slow it down in the long run,"}, {"timestamp": [2255.48, 2258.56], "text": " but I kind of don't care."}, {"timestamp": [2258.56, 2260.28], "text": " Here, let's go way back up to the top."}, {"timestamp": [2264.12, 2275.64], "text": " Metadata. Okay. Without a metadata config. Okay. So it just, it just, that's setting"}, {"timestamp": [2275.64, 2286.34], "text": " the metadata config. Include metadata. Okay. Okay. Here it is. Here it is. This is what I was looking for. If we say include metadata this will"}, {"timestamp": [2286.54, 2288.16], "text": " probably return."}, {"timestamp": [2291.34, 2292.34], "text": " Our metadata."}, {"timestamp": [2294.66, 2295.66], "text": " I think."}, {"timestamp": [2296.22, 2297.22], "text": " We'll give it a shot."}, {"timestamp": [2297.46, 2298.46], "text": " OK."}, {"timestamp": [2298.82, 2299.82], "text": " All right."}, {"timestamp": [2300.82, 2303.06], "text": " So include values include metadata."}, {"timestamp": [2307.76, 2312.04], "text": " Let's see if this works."}, {"timestamp": [2314.4, 2316.88], "text": " So if this is the case, then results matches."}, {"timestamp": [2316.88, 2319.84], "text": " So results will be,"}, {"timestamp": [2319.84, 2323.08], "text": " that was the absurd, where is the query?"}, {"timestamp": [2323.08, 2329.6], "text": " So index.query here."}, {"timestamp": [2332.88, 2337.08], "text": " Then wait, there was an example of what was the output?"}, {"timestamp": [2337.08, 2338.52], "text": " What did the output look like?"}, {"timestamp": [2338.52, 2340.28], "text": " I think that was under Quickstart."}, {"timestamp": [2340.28, 2342.56], "text": " Index.query, there we go."}, {"timestamp": [2342.56, 2344.4], "text": " Okay. So it'll give me matches,"}, {"timestamp": [2344.4, 2346.08], "text": " which is itself a list,"}, {"timestamp": [2346.08, 2350.04], "text": " and then the list will include the ID,"}, {"timestamp": [2350.04, 2352.04], "text": " so that we're going to use a UUID."}, {"timestamp": [2352.04, 2353.88], "text": " It will give us a score,"}, {"timestamp": [2353.88, 2355.96], "text": " which is the similarity,"}, {"timestamp": [2355.96, 2357.54], "text": " and then the values,"}, {"timestamp": [2357.54, 2362.52], "text": " but then also it should include metadata, hypothetically."}, {"timestamp": [2364.44, 2365.48], "text": " I think. So that'll have a list, metadata, hypothetically."}, {"timestamp": [2366.76, 2371.28], "text": " I think. So that'll have a list, and each item in the list"}, {"timestamp": [2371.28, 2373.08], "text": " will be itself a dictionary."}, {"timestamp": [2373.08, 2374.88], "text": " And the dictionary should include metadata,"}, {"timestamp": [2374.88, 2376.8], "text": " and the metadata should include a message,"}, {"timestamp": [2376.8, 2378.76], "text": " as well as a timestamp and a few other things."}, {"timestamp": [2381.12, 2383.36], "text": " This is all very confusing."}, {"timestamp": [2383.36, 2398.84], "text": " But with all that said, that needs to be in that correct order."}, {"timestamp": [2398.84, 2399.84], "text": " Does it?"}, {"timestamp": [2399.84, 2402.78], "text": " I don't know."}, {"timestamp": [2402.78, 2409.6], "text": " It might be actually simpler just to save our stuff locally."}, {"timestamp": [2409.6, 2417.08], "text": " So what we'll do actually is we'll call..."}, {"timestamp": [2417.08, 2418.08], "text": " I was afraid of..."}, {"timestamp": [2418.08, 2421.0], "text": " I kind of knew that this would be the result."}, {"timestamp": [2421.0, 2428.44], "text": " Let's see, unique ID equals string four."}, {"timestamp": [2428.44, 2431.66], "text": " And the reason that I knew this was a thing is because I know that the underlying technology"}, {"timestamp": [2431.66, 2438.98], "text": " is the Facebook AI semantic search, which all it does is return an ID."}, {"timestamp": [2438.98, 2442.38], "text": " So what we'll do is we'll save the metadata separately."}, {"timestamp": [2442.38, 2446.24], "text": " Yeah, yeah."}, {"timestamp": [2446.24, 2450.96], "text": " And yeah, we will go ahead and save."}, {"timestamp": [2450.96, 2456.16], "text": " We won't save the vector, because that's a huge burden."}, {"timestamp": [2456.16, 2458.88], "text": " But we will save the UUID."}, {"timestamp": [2458.88, 2461.36], "text": " All right, so we'll do unique ID."}, {"timestamp": [2461.36, 2462.36], "text": " That's fine."}, {"timestamp": [2462.36, 2464.2], "text": " So the metadata, this will actually"}, {"timestamp": [2464.2, 2465.84], "text": " be saved as a JSON file the same"}, {"timestamp": [2465.84, 2488.42], "text": " way that we did over here. Okay, so we'll do save JSON ne. Actually, no, we just do.json."}, {"timestamp": [2490.5, 2494.66], "text": " Then we'll use the unique ID for the placeholder."}, {"timestamp": [2494.66, 2498.14], "text": " Then metadata here,"}, {"timestamp": [2498.14, 2500.02], "text": " and we'll remove metadata here."}, {"timestamp": [2500.02, 2503.98], "text": " So Pinecone will just serve as"}, {"timestamp": [2503.98, 2505.44], "text": " instant search,"}, {"timestamp": [2505.44, 2509.3], "text": " and then it'll tell us which UUIDs to grab later."}, {"timestamp": [2509.3, 2510.52], "text": " Okay, cool."}, {"timestamp": [2512.52, 2513.76], "text": " This is a hot mess."}, {"timestamp": [2515.88, 2518.92], "text": " All right, so we don't need values or metadata."}, {"timestamp": [2518.92, 2521.8], "text": " All we need is the ID."}, {"timestamp": [2523.34, 2524.8], "text": " Yeah, all we need is the ID."}, {"timestamp": [2524.8, 2526.0], "text": " So it should return a list"}, {"timestamp": [2526.0, 2528.0], "text": " of dictionaries with IDs"}, {"timestamp": [2528.0, 2530.0], "text": " when we do the search."}, {"timestamp": [2530.0, 2532.0], "text": " Okay."}, {"timestamp": [2532.0, 2534.0], "text": " And so then what we do is we need to"}, {"timestamp": [2534.0, 2536.0], "text": " we need to, we need to"}, {"timestamp": [2536.0, 2538.0], "text": " recompose the conversation."}, {"timestamp": [2538.0, 2540.0], "text": " So let's do"}, {"timestamp": [2540.0, 2542.0], "text": " load conversation"}, {"timestamp": [2542.0, 2544.0], "text": " and then we'll say results."}, {"timestamp": [2544.0, 2545.0], "text": " Okay, so results should So, load conversation, and then we'll say results."}, {"timestamp": [2545.0, 2550.0], "text": " Okay, so results should be, actually no, yeah."}, {"timestamp": [2551.0, 2554.4], "text": " We'll say, yeah, results should be,"}, {"timestamp": [2559.56, 2563.44], "text": " should be a dict with matches,"}, {"timestamp": [2573.0, 2573.5], "text": " dict with matches, which is a list of dicts with id. OK."}, {"timestamp": [2577.36, 2579.36], "text": " Lowercase this, because that is the actual name"}, {"timestamp": [2579.36, 2580.08], "text": " of the element."}, {"timestamp": [2582.84, 2585.0], "text": " So that should be what we need there."}, {"timestamp": [2587.5, 2591.3], "text": " So now we need to do a function def, no,"}, {"timestamp": [2591.3, 2592.6], "text": " that's not how that works."}, {"timestamp": [2593.76, 2594.72], "text": " Def, what did I do?"}, {"timestamp": [2594.72, 2596.92], "text": " Load conversation, okay."}, {"timestamp": [2596.92, 2598.76], "text": " So then results."}, {"timestamp": [2601.76, 2602.96], "text": " And I know a lot of people say,"}, {"timestamp": [2602.96, 2605.12], "text": " why don't you use VS Code and Copilot and stuff?"}, {"timestamp": [2605.12, 2606.92], "text": " It's like, I hate IDEs."}, {"timestamp": [2606.92, 2609.56], "text": " I particularly hate VS Code."}, {"timestamp": [2609.56, 2614.24], "text": " But also that kind of defeats the point of like showing you how to code."}, {"timestamp": [2614.24, 2620.16], "text": " Also, I realized that I am increasingly a liar because this is not a low code experiment."}, {"timestamp": [2620.16, 2625.68], "text": " Let's see if we can get this to write it, though."}, {"timestamp": [2625.68, 2628.68], "text": " OK."}, {"timestamp": [2628.68, 2640.72], "text": " Let's write a function that accepts a results object"}, {"timestamp": [2640.72, 2657.12], "text": " from a Pinecone query. This returns a list of IDs which are used for the file names"}, {"timestamp": [2660.32, 2665.0], "text": " of the objects we are trying to load from local storage."}, {"timestamp": [2670.6, 2675.6], "text": " These files are in a folder called nexus"}, {"timestamp": [2675.92, 2680.92], "text": " and they are appended with .json."}, {"timestamp": [2682.6, 2694.32], "text": " So now write a very simple Python function that accepts the results and then loads"}, {"timestamp": [2696.0, 2704.8], "text": " the JSON files with matching ID file name. Does that make sense?"}, {"timestamp": [2704.0, 2705.16], "text": " with matching ID file name. Does that make sense?"}, {"timestamp": [2706.06, 2708.2], "text": " All right, let's see what it does."}, {"timestamp": [2708.2, 2709.58], "text": " Let that run."}, {"timestamp": [2710.7, 2711.88], "text": " Yep."}, {"timestamp": [2711.88, 2712.72], "text": " All right."}, {"timestamp": [2713.64, 2715.48], "text": " Load objects from Nexus."}, {"timestamp": [2715.48, 2717.56], "text": " Object IDs equals results.ids."}, {"timestamp": [2717.56, 2719.8], "text": " I don't think that that's how that works."}, {"timestamp": [2720.84, 2722.56], "text": " Unless maybe it does."}, {"timestamp": [2722.56, 2729.8], "text": " Does results.ids, is that part of the payload?"}, {"timestamp": [2730.44, 2742.4], "text": " Like .ids, that doesn't seem to be part of it."}, {"timestamp": [2742.4, 2748.18], "text": " So index query, ID string optional example."}, {"timestamp": [2751.74, 2753.3], "text": " Yeah, so like here's a filter."}, {"timestamp": [2753.3, 2758.3], "text": " It doesn't give me an example of the response."}, {"timestamp": [2758.42, 2760.02], "text": " So Pinecone, if you're watching,"}, {"timestamp": [2760.02, 2762.26], "text": " it would be helpful if you gave me an example"}, {"timestamp": [2763.14, 2765.28], "text": " of the response object."}, {"timestamp": [2766.16, 2768.8], "text": " Because I don't know that chat GPT understands this."}, {"timestamp": [2773.06, 2773.9], "text": " Yeah."}, {"timestamp": [2774.78, 2777.96], "text": " But the logic here is sound enough"}, {"timestamp": [2777.96, 2781.78], "text": " because we know that it's a list of objects."}, {"timestamp": [2781.78, 2791.2], "text": " So we'll do result equals list and then we'll do for let's see m in results that"}, {"timestamp": [2791.2, 2806.28], "text": " matches. I think that was the go back up to quick start. Yep so matches so So for that, what we need then is the ID."}, {"timestamp": [2808.92, 2813.92], "text": " So then we'll have file name equals M ID plus,"}, {"timestamp": [2817.56, 2819.06], "text": " actually here, we'll just do,"}, {"timestamp": [2821.9, 2824.14], "text": " I'll grab my load JSON."}, {"timestamp": [2824.14, 2829.54], "text": " Let's see, open file, save file, load JSON, save JSON."}, {"timestamp": [2829.54, 2832.94], "text": " Grab these functions."}, {"timestamp": [2832.94, 2835.22], "text": " Okay."}, {"timestamp": [2835.22, 2848.6], "text": " So we'll do info equals load JSON, and all I need to do is pass it the file path."}, {"timestamp": [2848.6, 2852.92], "text": " So the file path will be nexus."}, {"timestamp": [2852.92, 2855.44], "text": " And then we'll backfill that, .json."}, {"timestamp": [2855.44, 2860.2], "text": " And then this will be MID."}, {"timestamp": [2860.2, 2863.04], "text": " And I think that should load the message."}, {"timestamp": [2863.04, 2865.68], "text": " And the message will have this metadata."}, {"timestamp": [2872.24, 2872.88], "text": " Right, so we'll know the UUID, it was in the file name already, but now it's a handy-dandy object,"}, {"timestamp": [2879.28, 2889.24], "text": " time string, message, time stamp, and user. So now what we can do is we can actually recall, we can recall all the most relevant things and sort it by time. Okay. So info equals load JSON,"}, {"timestamp": [2889.24, 2891.72], "text": " we get that information."}, {"timestamp": [2891.72, 2897.64], "text": " Then I think I already had this function here, loadConvo."}, {"timestamp": [2897.64, 2899.32], "text": " Yeah. So we load it,"}, {"timestamp": [2899.32, 2900.64], "text": " load it, we sort it."}, {"timestamp": [2900.64, 2913.44], "text": " Okay. Then we do result.appendInfo. Ordered equals sorted result key lambda d time."}, {"timestamp": [2915.2, 2920.08], "text": " Yep. Reverse equals false. Sort them all chronologically. Yes, that should be good."}, {"timestamp": [2922.56, 2925.0], "text": " And then, so after we load the convo, we need to format it."}, {"timestamp": [2932.64, 2935.76], "text": " And then recent get last messages."}, {"timestamp": [2935.76, 2938.72], "text": " Okay, I don't think that that's the correct way to do it."}, {"timestamp": [2939.88, 2944.56], "text": " But, so if we look here, the message saved has all this."}, {"timestamp": [2944.56, 2946.88], "text": " So all we need to do is then reconstruct."}, {"timestamp": [2951.52, 2956.24], "text": " All we need to do is then, sorry, brains are a little bit scattered."}, {"timestamp": [2956.24, 2962.16], "text": " So ordered equals this, so rather than return ordered,"}, {"timestamp": [2962.16, 2988.32], "text": " all we need to do then is say, message block equals backslash n dot join. Oh, actually here. Let's do some list So then we'll just do messages equals I message."}, {"timestamp": [2988.32, 2992.64], "text": " Oh, that's why my keyboard is slid over."}, {"timestamp": [2992.64, 2993.92], "text": " That's why I keep fudging it."}, {"timestamp": [2993.92, 2996.2], "text": " It's like I'm leaning a little bit."}, {"timestamp": [2996.2, 2996.56], "text": " All right."}, {"timestamp": [2996.56, 2999.76], "text": " I message for I in ordered."}, {"timestamp": [2999.76, 3003.28], "text": " So this is what's called a list comprehension, which basically just says,"}, {"timestamp": [3003.28, 3009.76], "text": " let's make a simple list instead of a nested list. All right, so then we'll do join i messages"}, {"timestamp": [3011.92, 3025.96], "text": " dot strip at the end, and then return message block. And you could just do,ify this, return this. Okay. So in theory,"}, {"timestamp": [3025.96, 3028.96], "text": " this returns, given the results,"}, {"timestamp": [3028.96, 3033.16], "text": " this returns just the most recent conversation that we"}, {"timestamp": [3033.16, 3035.16], "text": " had or not most recent,"}, {"timestamp": [3035.16, 3038.36], "text": " most relevant conversation that we had with Raven."}, {"timestamp": [3038.36, 3042.16], "text": " So now we can populate the prompt."}, {"timestamp": [3042.16, 3045.72], "text": " So prompt response, we don't need any notes."}, {"timestamp": [3045.72, 3051.36], "text": " We're just doing infinite memory."}, {"timestamp": [3051.36, 3055.48], "text": " So prompt equals open file prompt response replace."}, {"timestamp": [3055.48, 3058.84], "text": " Here we can get rid of all this."}, {"timestamp": [3058.84, 3063.24], "text": " Replace conversation with conversation."}, {"timestamp": [3063.24, 3066.0], "text": " And so what this is going to do is it's going to jump over here"}, {"timestamp": [3066.0, 3069.0], "text": " and we're just going to replace this."}, {"timestamp": [3069.0, 3079.0], "text": " So it will be sorted chronologically, but it will also only have the most relevant conversation elements."}, {"timestamp": [3079.0, 3084.0], "text": " So this will go back infinitely far, regardless of how long the conversation is,"}, {"timestamp": [3084.0, 3088.26], "text": " it's only going to pull the 15 most relevant things."}, {"timestamp": [3088.26, 3092.56], "text": " So we should actually probably parameterize this."}, {"timestamp": [3092.56, 3098.42], "text": " So we'll do like, let's see, convo length equals 15."}, {"timestamp": [3098.42, 3102.44], "text": " And then we'll do top k equals convo length."}, {"timestamp": [3102.44, 3104.86], "text": " And so then, because we do have a really large window,"}, {"timestamp": [3104.86, 3107.22], "text": " so we can actually probably up this to like 30."}, {"timestamp": [3108.38, 3113.02], "text": " Cause we have, unless the messages are super long,"}, {"timestamp": [3113.02, 3116.34], "text": " and also since we're offloading a big chunk of the search,"}, {"timestamp": [3116.34, 3117.5], "text": " it's all fine."}, {"timestamp": [3117.5, 3120.26], "text": " Okay, so then we do,"}, {"timestamp": [3121.82, 3131.24], "text": " we basically have to do the same thing."}, {"timestamp": [3133.4, 3134.36], "text": " Generate response vectorize, et cetera, et cetera. So output."}, {"timestamp": [3134.36, 3136.48], "text": " So this will be Raven's response."}, {"timestamp": [3136.48, 3144.04], "text": " Time stamp, time string, message."}, {"timestamp": [3144.04, 3150.4], "text": " Yep, we get the message. Yep, we get the message and then the vector. We're going to do this this way,"}, {"timestamp": [3150.4, 3156.6], "text": " so the vector is that. We're going to pick another unique ID, do this the same way."}, {"timestamp": [3163.52, 3167.68], "text": " All right, so instead the metadata or the speaker will be Raven."}, {"timestamp": [3167.68, 3169.92], "text": " The rest will be the same because we've already got that."}, {"timestamp": [3169.92, 3171.4], "text": " Save JSON."}, {"timestamp": [3171.4, 3172.8], "text": " We give it the same unique ID."}, {"timestamp": [3172.8, 3178.52], "text": " We give it the metadata, payload append, unique ID, and vector."}, {"timestamp": [3178.52, 3180.22], "text": " So that's all there."}, {"timestamp": [3180.22, 3186.72], "text": " So now it's searchable and I think the last bit is we just need to give the output."}, {"timestamp": [3190.64, 3195.68], "text": " Oh yeah, we did save JSON so we saved it to the Nexus. Let's go ahead and make that folder."}, {"timestamp": [3197.52, 3204.16], "text": " Folder, Nexus and then print the output and it's that simple."}, {"timestamp": [3203.48, 3204.36], "text": " And then print the output. And it's that simple."}, {"timestamp": [3210.0, 3211.6], "text": " And my friends, if I'm not mistaken,"}, {"timestamp": [3211.6, 3214.4], "text": " we are ready to test this."}, {"timestamp": [3214.4, 3216.68], "text": " Oh, boy."}, {"timestamp": [3216.68, 3220.36], "text": " This is going to, this will either work or it will explode."}, {"timestamp": [3220.36, 3221.56], "text": " Python chat 2."}, {"timestamp": [3224.52, 3227.42], "text": " User, hello, Raven, are you awake?"}, {"timestamp": [3230.72, 3235.06], "text": " List append takes exactly one argument and two given."}, {"timestamp": [3235.06, 3238.02], "text": " Oh, right, line 102, append,"}, {"timestamp": [3238.02, 3240.74], "text": " this is supposed to be a tuple."}, {"timestamp": [3240.74, 3241.74], "text": " I knew that."}, {"timestamp": [3246.3, 3247.22], "text": " Payload append."}, {"timestamp": [3247.22, 3248.4], "text": " Oh, you know what else I forgot to do?"}, {"timestamp": [3248.4, 3250.16], "text": " I forgot to actually do the upsert."}, {"timestamp": [3251.7, 3256.36], "text": " So now we'll do vdb.upsert payload."}, {"timestamp": [3269.68, 3275.16], "text": " Hello, Raven. How are you today? Yay, cool."}, {"timestamp": [3275.16, 3279.28], "text": " Okay, so we got a response."}, {"timestamp": [3279.28, 3280.28], "text": " So let's check."}, {"timestamp": [3280.28, 3286.76], "text": " Okay, previous conversation, there's nothing here obviously of any society in order to..."}, {"timestamp": [3286.76, 3290.96], "text": " Cool, so Raven's already waxing philosophical about the core objective functions."}, {"timestamp": [3290.96, 3295.48], "text": " Yes, remember they are very low in the stack, that is on purpose."}, {"timestamp": [3295.48, 3300.12], "text": " What do you think are the primary causes of suffering and poverty?"}, {"timestamp": [3300.12, 3309.12], "text": " Well suffering is intrinsic to life according to Buddhism."}, {"timestamp": [3309.12, 3315.6], "text": " So then what we'll do is once we get a response we will make sure."}, {"timestamp": [3315.6, 3317.32], "text": " Let's see."}, {"timestamp": [3317.32, 3319.84], "text": " Yep there we go."}, {"timestamp": [3319.84, 3324.92], "text": " We're getting we're getting responses so we know that the search is working all this is good."}, {"timestamp": [3324.92, 3325.0], "text": " This is good."}, {"timestamp": [3325.0, 3327.0], "text": " This is good."}, {"timestamp": [3327.0, 3329.6], "text": " Some of the primary causes include inequality."}, {"timestamp": [3329.6, 3333.0], "text": " All right. So Raven is kind of talking to himself."}, {"timestamp": [3333.0, 3342.6], "text": " I also misspelled previous previous conversation."}, {"timestamp": [3342.6, 3348.32], "text": " Right. Do you think there are any solutions to reduce suffering and poverty?"}, {"timestamp": [3348.32, 3353.4], "text": " Actually that's kind of your purpose, isn't it?"}, {"timestamp": [3353.4, 3362.9], "text": " You are a chatbot and prototype AGI meant to help us with these goals."}, {"timestamp": [3362.9, 3371.68], "text": " So let me turn that around. How do you think you can help reduce suffering and increase"}, {"timestamp": [3373.04, 3379.28], "text": " prosperity? So let's see what Raven thinks about this. Now also one thing to keep in mind is"}, {"timestamp": [3379.28, 3382.48], "text": " because... okay so this is a brand new conversation. It's super short."}, {"timestamp": [3382.6, 3387.36], "text": " is OK so this is a brand new conversation it's super short."}, {"timestamp": [3389.44, 3391.24], "text": " Let's see. Yeah gives me some stuff let's see do you"}, {"timestamp": [3391.24, 3393.92], "text": " think there are any other solutions to this so on"}, {"timestamp": [3393.92, 3395.2], "text": " and so forth."}, {"timestamp": [3395.2, 3397.2], "text": " So let me show you what's going on in the Nexus."}, {"timestamp": [3397.2, 3400.88], "text": " So this is the raw data right."}, {"timestamp": [3400.88, 3403.24], "text": " There we go."}, {"timestamp": [3403.24, 3404.28], "text": " Time string."}, {"timestamp": [3404.28, 3409.02], "text": " So one thing that I noticed is that Raven spit out a timeString here."}, {"timestamp": [3409.02, 3411.76], "text": " So that's not exactly what we want."}, {"timestamp": [3411.76, 3412.76], "text": " So let's see."}, {"timestamp": [3412.76, 3416.36], "text": " And I'll now provide a long detailed response."}, {"timestamp": [3416.36, 3417.86], "text": " Oh, right."}, {"timestamp": [3417.86, 3425.6], "text": " This happened last time because I'm including the timeSt string here. So I think what we need to do is,"}, {"timestamp": [3425.6, 3428.7], "text": " I think we actually do need to go back and remove,"}, {"timestamp": [3432.24, 3433.54], "text": " we need to remove,"}, {"timestamp": [3435.52, 3437.86], "text": " yeah, I actually did this exact thing last time"}, {"timestamp": [3437.86, 3440.68], "text": " where the message that I vectorized and saved"}, {"timestamp": [3440.68, 3443.46], "text": " included all this, and it's not supposed to."}, {"timestamp": [3443.46, 3446.44], "text": " This is only supposed to be there on the output, I think."}, {"timestamp": [3447.78, 3451.92], "text": " Because basically what chat-gpt is doing,"}, {"timestamp": [3451.92, 3455.86], "text": " or not chat-gpt, gpt3 is doing is it's noticing the pattern"}, {"timestamp": [3455.86, 3458.1], "text": " and it's saying, oh, well, I'm supposed to"}, {"timestamp": [3458.1, 3459.5], "text": " regurgitate this pattern."}, {"timestamp": [3462.62, 3464.22], "text": " So that's not correct, actually."}, {"timestamp": [3465.64, 3474.28], "text": " What I did was I added this information so that when Raven is reading it, when it's put"}, {"timestamp": [3474.28, 3476.8], "text": " in, Raven can see the timestamps."}, {"timestamp": [3476.8, 3479.78], "text": " But I think that that is actually just superfluous information."}, {"timestamp": [3479.78, 3486.0], "text": " So what I'm going to do is I'm going to duplicate this and then just say message equals. A."}, {"timestamp": [3486.0, 3487.04], "text": " Whoops."}, {"timestamp": [3487.04, 3490.4], "text": " And then we'll do the same thing down here."}, {"timestamp": [3490.4, 3493.16], "text": " Message equals output."}, {"timestamp": [3493.16, 3495.32], "text": " OK so that's fine."}, {"timestamp": [3495.32, 3497.36], "text": " All right so suffering and poverty are complex issues"}, {"timestamp": [3497.36, 3500.8], "text": " that are multifaceted solutions blah blah blah."}, {"timestamp": [3500.8, 3502.92], "text": " This can be done through initiatives such as this."}, {"timestamp": [3502.92, 3504.8], "text": " OK."}, {"timestamp": [3504.8, 3507.72], "text": " Raven do you know what you are?"}, {"timestamp": [3514.56, 3515.86], "text": " Let's see what Raven says."}, {"timestamp": [3517.56, 3520.12], "text": " Okay, so Raven's basically just talking to himself."}, {"timestamp": [3520.12, 3521.08], "text": " So that's no good."}, {"timestamp": [3521.98, 3524.24], "text": " So if we go back and look at,"}, {"timestamp": [3526.74, 3531.28], "text": " let's look at the last log."}, {"timestamp": [3535.22, 3537.8], "text": " Oh, Raven's not responding to my question because I don't have my thing."}, {"timestamp": [3537.8, 3540.48], "text": " So this is why Raven is just talking to himself."}, {"timestamp": [3540.48, 3543.22], "text": " Okay, so we need to add,"}, {"timestamp": [3543.22, 3546.48], "text": " we do need to insert at least my most"}, {"timestamp": [3546.48, 3548.68], "text": " recent response."}, {"timestamp": [3548.68, 3553.24], "text": " OK, so we need to go into our prompt."}, {"timestamp": [3553.24, 3559.36], "text": " And so then we'll do user, and we'll do message."}, {"timestamp": [3575.5, 3583.56], "text": " And so then what we need to do is for the replace, let's see, output, there we go. And then we'll do replace message with A, because that's my user input."}, {"timestamp": [3583.56, 3586.8], "text": " Because otherwise, this is why Raven's talking to himself."}, {"timestamp": [3590.8, 3596.16], "text": " Hello, Raven. What were we talking about?"}, {"timestamp": [3603.28, 3605.74], "text": " Yeah, we were discussing this, we were discussing this."}, {"timestamp": [3605.74, 3609.88], "text": " We were discussing do you think there are any other solutions?"}, {"timestamp": [3609.88, 3611.12], "text": " Let's change topic."}, {"timestamp": [3611.12, 3614.74], "text": " I want to know."}, {"timestamp": [3614.74, 3619.96], "text": " I want to I want to discuss what you are."}, {"timestamp": [3619.96, 3628.4], "text": " Do you know what you are, Raven?"}, {"timestamp": [3628.4, 3631.12], "text": " And so what I do want to say, I'm a chatbot and prototype AGI."}, {"timestamp": [3631.12, 3632.12], "text": " Okay, cool."}, {"timestamp": [3632.12, 3636.04], "text": " So he remembered when I said that."}, {"timestamp": [3636.04, 3639.16], "text": " My primary goal is to provide a platform for people to discuss and understand underlying"}, {"timestamp": [3639.16, 3642.2], "text": " causes of suffering and poverty, to explore potential solutions."}, {"timestamp": [3642.2, 3647.52], "text": " I am also designed to provide a safe and secure environment for people to share their thoughts and ideas. My capabilities include natural"}, {"timestamp": [3647.52, 3653.38], "text": " language processing, machine learning, and artificial intelligence.\" Yeah, that's, those"}, {"timestamp": [3653.38, 3665.0], "text": " are some good inferences. We're actually still exploring what you are and how to deploy you."}, {"timestamp": [3667.28, 3668.24], "text": " So let's see what he says to that."}, {"timestamp": [3668.24, 3670.92], "text": " So anyways, you get the picture, it's working."}, {"timestamp": [3670.92, 3673.78], "text": " There's a million little bugs to figure out,"}, {"timestamp": [3673.78, 3677.92], "text": " but we just set up and we are using Pinecone"}, {"timestamp": [3677.92, 3680.8], "text": " as a Nexus service."}, {"timestamp": [3680.8, 3682.16], "text": " So that's pretty cool."}, {"timestamp": [3682.16, 3687.56], "text": " Raven's final output, as a chatbot and prototype AGI, I am designed to reduce suffering and increase prosperity. Forgot about the"}, {"timestamp": [3687.56, 3692.08], "text": " increase understanding. Just kind of regurgitating what he said. I'm also"}, {"timestamp": [3692.08, 3698.36], "text": " designed to... okay blah blah blah blah blah. Okay so not the smartest thing but we do have a"}, {"timestamp": [3698.36, 3703.94], "text": " very basic Nexus service. I think we'll call it there because this was a raging"}, {"timestamp": [3703.94, 3706.28], "text": " success."}, {"timestamp": [3700.74, 3705.22], "text": " service, I think we'll call it there because this was a raging success."}]}