{"text": " You're live. Okay, I am live. Level check. Hello. Let's see. How do I share the... Pop out the chat. Um. Aha, there we go. You You Bye. Alright. The internet has been rather off today, so we'll see if anyone shows up. Hmm. Level check. OK. Level check. OK. Sound checks. Level check. How does it sound? Level check. How does it sound? Good, good, good. Oh, hey. Got a couple of people coming in. All right. I know it always takes a minute to get going. So, folks, thanks for joining. Oops, I had one leave. It's okay. It happens. Just getting going. I just had someone on Twitter ask me via private message, so I guess we'll go ahead and get started. So the question, oh, it's Jordan. Hey, Jordan. Listening but also working. Okay, cool. So we'll do a live entertainment. That's fine. I'm not gonna do song and dance. So anyways, okay. So question from Twitter. What's your ETA on AGI? So ETA, estimated time of arrival for AGI is artificial general intelligence. So I'm gonna say that, well, so first, it depends on how you define AGI. So one popular definition of AGI is that it's a machine that can do anything that a human can do, any intellectual task that a human can do. Yeah, okay. I've said on many of my videos that I don't like the term AGI because it's artificial, general, and intelligence. So, it's kind of vague. It's kind of squishy. To me, the goal is to create a thinking machine that can think anything, and then intelligence is you just measure how smart it is, right? Because intelligence will go up over time. And in humans, intelligence is actually often measured by speed. That's why IQ tests are timed. It's often about how far you can get and how fast. Okay, that was not someone messaging me on one of the many apps. Let me mute that app. Sorry about that. So anyways, IQ, which is the most famous way of measuring intelligence is more about speed. So, but then another way that you can measure intelligence is what problems are you capable of solving, right? But if you set that as the threshold of AGI, there are many humans that cannot solve certain problems, right? Like you wouldn't want to put me in charge of a moon rocket or a heart surgery, right? And so when you set the bar as like artificial general intelligence, it's really vague because it's like, okay, let's see isn't intelligence compression. I'll get to that. Good question. So intelligence from a from a some some schools of thought think that intelligence is just a form of compression. I don't agree. It's about computation. But anyways, in terms of ETA, so I think that, you know, some of the, all the research that's going on, I see lots of folks doing similar work to what I'm doing on cognitive architectures, etc, etc. I think that we're going to have AGI within two years. Now, the question is going to be how fast is it? What problems can it solve? And how much does it cost to run? So cost to run is going to be up there because we already have things that can think, right? There's large language models, there's multimodal models. They can solve problems, they can brainstorm, they can monitor their own performance if you set up the cognitive architecture right, but they're also very expensive to run. So those are three things that are gonna kind of slow down. Now, to old Zany's question, is an intelligence compression? No, short answer, no. Biologically speaking or in humans, intelligence is pattern matching and pattern recognition. Now, you might say that artificial neural networks are about compressing enough patterns that you can generalize them or something, because then you end up with patterns and meta-patter patterns, right? Like patterns about patterns and patterns to generate new patterns and decomposing patterns and generating increasingly complex patterns. But from a neuroscience perspective or a psychological perspective, intelligence is not about compression, it is about patterns. Now, that being said, you could argue that compression is just about making a big pattern into a smaller pattern. Okay, sure. We can go with that definition. All right, so there's a few folks. Oh, wow, I'm getting many thumbs up. Thank you, folks, for whoever's jumping in and giving me thumbs up. Yeah, so the purpose of this stream is because I've got my third book on AI coming out. Sorry, I'm third. There we go. I have such small fingers. Anyways, so the first two. First is Natural Language Cognitive Architecture. Yeah, I know, funny. I make fun of myself. I have very thick hands because I do climbing and lots of building and stuff, but my fingers look really stumpy. Anyways, so Natural Language Cognitive Architecture is my first book. This one, I put this out before fine tuning was available. So fine tuning came out, and it kind of nullified a lot of this, but the ideas are still there, where you've got interlocking loops, shared databases, search. There's a lot in here that's still valid and also people say that they like my my diagrams. So I've got, you know, a diagram of my core objective functions in here and so on. This was a lot of work. And then, so then my second book is Benevolent by Design, which is strictly about the control problem. So the control problem is how do you prevent AI from getting murdery? How do you prevent it from being Skynet? Because here's the thing, if you have ML pipelines and autonomous machines that can think on their own and come up with their own objectives, how do you know that you're not gonna end up with a machine that is going to change their own objectives, how do you know that you're not going to end up with a machine that is going to change its own objectives, that's going to change its operational parameters? That is the purpose of this book. How do you create a machine that has values that it will keep those values? And that is the key here, which is creating something that will ultimately believe in the same framework that you want it to believe in. And then my latest book, I don't have a paperback copy yet because the paperback copies are mostly for me. So the latest book is Symphony of Thought. I'll actually go ahead and just drop a link to it in the chat. So you can download it for free. PDF, EPUB, DocX. Let's see. Did you write anything on the possibility that AGI has already been created in another sector of our galaxy? That is a great thought experiment. We'll get into that. So in the comments, I don't know if you can see it, whoever's watching, whenever. Anyways, Old Zany asks, did you write anything on the possibility that AGI has already been created in another sector of the galaxy? I'll answer that in just a second. But first, let me finish introducing Symphony of Thought. So Symphony of Thought is my third book, which is about the principles of artificial cognition. And let's see, I have another message. Make sure that folks can get in. One second, doing a live stream. Okay, that's fine. Yes, so Symphony of Thought. Symphony of Thought is about how, okay, all these different abilities that are outlined in these first two books, right? They're all outlined. I've built several prototypes. But the thing is, when you have a whole bunch of different language models, interacting with memory systems and all this stuff, it gets real complex real fast. And so but then there's also the how do you approximate human cognition to these machine components? Because, you know, we have large language models, which is like, okay, that can generate language. Great. It's an autocomplete engine. How do you get from an autocomplete engine to artificial cognition? And that is the point of symphony of thought, which is like, okay, let's decompose all the different aspects of human cognition, such as planning, brainstorming, self correction, error detection, agent model. I know what I am, and I know what I'm capable of. I know what my limitations are. I know what I am and I know what I'm capable of. I know what my limitations are. I know what I know and I know, and by extension, I know what I don't know. Like I know a little bit about quantum physics because I've read a few books about it, but if you like start throwing theories at me in the math, I'll be like, I don't know. I know that I don't know that. So how do you handle those things? That is the purpose of Symphony of Thought. Now, my background work is, what I'm working up to is a fully realized artificial cognitive entity. And the architecture is what I call MIRAGI, which is an acronym that means microservices architecture for robotics and artificial general intelligence. So that's gonna be my next project when I get around to it. Okay, so the question, let's see. Oh, Adrian says, hello, hello. We'll listen while working on my DALI composition. So yes, we're doing AI here all day, every day. I'll put in chat that acronym that I just said. Meragi equals microservices architecture for robotics and artificial, if I know how to spell, I promise, artificial general intelligence. OK, so that's the ultimate goal is to create a cognitive architecture. And here's the underlying philosophy behind how it's going to be stable, so on and so forth. Okay, okay, cool. We got some more questions coming in. So now get to old Xenia's question because he or she or they had it first. The possibility that AGI has already been created in another sector of our galaxy. Okay, so this is one of my favorite topics. Are we alone in the universe? Functionally, right now, we seem to be alone. We have telescopes and radios all over the place, and we see and hear nothing. Now, we just figured out how to build radios and telescopes, so it's entirely possible that they're just not sophisticated enough to see other civilizations out there. So that's one possibility. Another possibility is that they're hiding from us, right? If they're millions of years more advanced than us, then they can just be stealth or whatever, or hide, you know. They can, they can outmaneuver us, right? Because the thing is, is like, with just a little bit more technology, you know, like if you were to drop a, uh, you know, a 60 year old, um,old airplane or a 100-year-old airplane into a battlefield today, it would not have a snowball's chance in hell against a modern jet fighter. That's only one century of time difference in technology. Now, imagine you extend that out to by several orders of magnitude a million years. If there is an alien race out there somewhere that has invented AGI or anything before us, if it's anything more than 100 years ahead of us, we don't have any chance of detecting it or surviving it. So the fact that we don't see them means that they could be stealth or avoiding us, or they don't exist. Those are kind of the two primary possibilities. Now, that being said, there also might be physics limitations. So one thing that a friend of mine many years ago said, he's a great scientist, studies cosmology. His profession is material science, but he almost got a minor in cosmology. Anyways profession is material science, but He almost got a minor in cosmology. Anyways, one of the things that he pointed out He's like maybe faster than light travel just isn't physically possible And if that's true, if it's if if faster than light travel is not possible, you know Obviously, there's stories about UFOs that can like teleport or whatever. That's possible Maybe but maybe it isn't. And so if faster than light travel is not possible and we're all limited to say 10% the speed of light, that makes the universe really big and really difficult to get around. And so if someone's going to take the time to get somewhere else, like to come here from, even our stellar neighborhood, that's a huge time investment. So if that's the case, then we're an island in the middle of nowhere, and we might be functionally alone for millions or billions of years. So that's my take on what if AGI or something else exists elsewhere. Now, that being said, if you wanted to make sure that your civilization or that your species or that your biome was going to survive, you'd want to put it on a ship that is run by an AGI because that AGI could outlive your entire species and manage that ship as it spreads across the cosmos, which that's like the panspermia or seed ship theory, right? Like maybe life was planted here on earth by something else a billion years ago. But that still begs the question, where did it come from and why? So, okay, so there we go, we've got that. Now, Rory, and I apologize if I butcher your name, it looks Irish, Rory O'Connor, do you have any thoughts as to the similarity between some psychedelic and meditative states and the videos that are popping up from Stable D and Dolly? Does this tell us anything about either? Great question. Okay, so for a little bit of background, if you look up videos about like DMT experiences or psychedelic experiences, there's some great hilarious videos on Reddit and YouTube. And people will try and recreate mystical, spiritual, and psychedelic experiences. This is the reason for all the psychedelic art in the 60s is people had these deeply profound, meaningful experiences that they tried to recreate. And so they do it in sculpture and painting, whatever. And so now that people have, excuse me, now that people have, here, let me just get some water, one second. Now with digital art, AI art, it's really easy to kind of recreate some of those visualizations. So, one thing that I noticed very early on, and you can, it's called Deep Dream. So, Deep Dream was like the first attempt at AI image generation, and it's very trippy. It is super trippy. Just Google it real quick. I'll put it in chat. Google Deep Dream. So basically what happens is the way that your optic nerve works is that it starts doing things like edge detection and contrast. Hey man, got another message maybe? Someone else trying to join? Okay, no, just a buddy of mine. Okay, anyways, so starting from your optic nerve, like literally neural vision starts on the retina, optic nerve, optical chiasm, whereas that's where your optic nerves cross. So that way you can get bilateral connection. And then it all gets projected to your occipital, to the back of your head, where more processing happens. So your optic nerve is not just a wire. Your optic nerve is actually a deep neural network that begins processing. So then what happens when you do things like psychedelics or have disorders or hypoxia or whatever, any kind of impaired state that modifies the behavior of those receptors, it'll modify how you perceive the world. And you know, in cases like alcohol, it subdues it, meaning that it's slower. But then there's other things that like accelerate it or modify it. And so psychedelics are very similar to serotonin, which is one of the primary neurotransmitters in your head. And so what happens with these is that the way that your brain represents, you're basically saying psychedelics simply decalibrate your model of the world, partially. We'll get to that. I've watched a lot of videos about what psychedelics do. And I also read a book. I recommend you read DMT, the spirit molecule, very cool. So what happens is the way that your brain represents, so remember I said that intelligence is patterns. What happens in your optic nerve and your visual processing is that your brain, and this is exactly how it works in neural networks too, in artificial neural networks, is that your brain creates more and more abstract representations. And those abstract representations are then reassembled based on like square here or pattern here. And so like, if you hold out your hand, right, you can tell I have four fingers, but it's a pattern. So what happens is your brain or between your optic nerve brain, everything else, you see, okay, there's a repeating pattern here. And so if you're really high on psychedelic, you look at a pattern like this and you can't make sense of it, right? And it's because your brain sees a pattern, but it is incoherent and you can't tell how many fingers you have or something like that And that's why you see lots of repeating patterns in psychedelic art and it's because your brain can no longer make sense of it because it's like oh I see a pattern and a pattern and a Pattern and then all those patterns just kind of clash and then become like prismatic or whatever so that is But in the early days of AI art or AI generated images, they did the same thing, but they were trained on, you know, images and image recognition. But what would happen is you'd end up with like smaller parts of an image that were like, you know, like part of your shirt might look like a dog. And then you'd have like a lobster over here and then something else over here. And it's because that model was trying to identify those patterns, and it was misidentifying those patterns. And it's very similar to what happens on psychedelics. So, well, I'm probably, I know that I'm oversimplifying, but yes, that is the link between psychedelics and AI art. Now, now that we have models, let's see, first I... Yes, they were fractals. And so fractals are just patterns, right? It's a mathematical pattern, and so that's why AI art and psychedelic art both feature a lot of fractals, because a fractal is a good way to approximate a very, very complex pattern. And that's why you see all kinds of fractals in nature, because you can have a very simple genetic program that just says, like, cauliflower and broccoli and flowers, they all follow fractal patterns or Fibonacci sequences. And it's because those are very, they're actually they're patterns. They're mathematically determined patterns. OK, so there's that. Who are academics working on bringing LLMs into the cognitive architectures? Mathis Lichtenberger. Mostly it's me. There's a few other folks out there. The thing is, so in terms of people working on cognitive architectures, one, so here's the thing is, there's this fragmentation in the industry because all the people that are still doing chatbots, like voice chatbots, they're still using, what's the name of that script? But they're basically using like old-fashioned, like dialogues, tree scripts. And a lot of people in voice and chat aren't aware of LLMs yet. So there's that group over there that is doing chat. And large language models lend themselves to chat really well. But they haven't figured out how to use it yet. And then you've got cognitive architectures, which are really old. They're used in all kinds of places, like rockets. So for instance, like every moon rocket, well maybe not moon rockets, maybe not that old, but more recent rockets, they basically have a kind of cognitive architecture where they're constantly, you know, because machines have, those big machines have hundreds of thousands of sensors and little tiny controls all over the place and so you basically have a nervous system and then you have it sensing everything that's going on and it creates a hypothesis as to like okay you know I'm veering off course because this nozzle over here is out of whack so let me do a little experiment to try and get back on course and that's why you know rockets either they fly perfectly straight or they just blow up. I'm getting another beer, hold on. Okay, yeah, so on the one hand, large language models are being used, you know, there's a million and one startups using large language models. Cogn cognitive architectures are really old. They're older than a lot of modern AI, right? And so what I've tried to do, especially with natural language cognitive architectures, is show like, hey, look, you can merge the two. So we'll see what happens. There are more and more people that are developing more sophisticated architectural patterns like loops, nested loops, intersecting loops. So yeah, there's probably people out there. Blender Bot by Facebook or now Meta, that is a very simplified cognitive architecture and they're coming out with more sophisticated models all the time. Let's see. Let's see, Old Zaney says, I think the potential prospect of humans being the very first species to set off the whole race of who gets to conquer all of reality rather intriguing. Yes, we might be the universal force born. Now there's another theory that I like a lot, because when you, when you read about physics, specifically quantum physics and cosmology, you realize that like reality is not what it seems. I've got a couple of books. Let's see. Here's a handful of books. So holographic universe. This came out in 1992, I believe. So this is basically the idea that like, okay maybe the whole universe is a hologram which we would call this simulation hypothesis now which I've got that book right here. And then you've got reality is not what it seems. This is written by an actual physicist. This one's written by a video game developer and this was written by a journalist who I think he was a journalist But he focused on like physics and stuff Anyways, so you read books like that and then there's another one called grand biocentric design I've got a grand biocentric design. I see it right there, and then there's the philosophy of physics, which is by Princeton University Press Perfect pronunciation. Oh, OK, cool. First day. OK, I think I'm all caught up, at least mostly caught up. Yeah, so what is the fundamental nature of reality? So I comically proposed Shapiro's law, which is that all conversations about artificial intelligence are ultimately questions about quantum physics or the fundamental nature of reality. So here we are, Shapiro's law in full effect. When you start to ask questions about consciousness, intelligence, sentience, where did we come from and why, why is it that we believe what we do? Like, when we have the idea that we can create a machine, and these stories are very old. So there's a book that my fiancee loves. It's called, Gods and Robots, which talks about how through fiction, we have explored our own humanity in the guise of machines since forever. So Talos was built out of bronze, but is a person, a fully realized demigod, basically. So when we, as humans, imagine that we can create something like ourselves, it makes us ask questions about who and what we are. What does it mean to be human or alive or conscious or Dave or whatever? And so when we ask those questions that the so this is a philosophical term that entails it. You cannot separate what it what we are from the fundamental fabric of reality and so we you keep zooming out and you say, okay, well, I'm conscious, but this cup isn't, or is it, right? Maybe it is conscious, maybe it's because it has the ability to interact and maybe it has an experience, so that's panpsychism. But right now what's in vogue is materialism. I actually put a post put a poll on Twitter. Materialism is by far the most popular thing, followed by simulation hypothesis, actually, which of course, simulation hypothesis says, we're all just running in a simulation, but that begs the question, who's running the simulation, on what hardware, and why? So that just leaves you with more questions than answers. Anyways, sorry, I was getting distracted. The question was, oh yeah, fundamental nature of reality. So you start asking questions about the fundamental. Old Zany is like, aha, so someone's drinking and it's not me. But yeah, so AI conversations always end up, they either turn religious, which again, gods and robots, that has been a conversation for literally thousands of years. But then when you add in modern cosmology and quantum physics, you end up with more questions like, okay, where does consciousness come from? Because here's the thing, one of the most important implications of materialism... Okay, so materialism, let me just tell you, materialism is very simple. It is defined as, you say, the only thing that's real is matter and energy. Everything, every phenomenon in the universe arises from matter and energy. That's what materialism says. If that's true, if materialism is true, then our consciousness arises from the matter of our brain and the energy therein. But that just means that consciousness is a matter of biochemical calculation, which in theory, anything with a sufficient amount of biochemical calculation could therefore be conscious. And if biochemical calculation is enough to create consciousness, then what about silicon-based computation? Could a silicon thing, you know, could a computer be conscious? Or a tree, or a mushroom, or a whole forest, right? If biochemical computation is the only thing required for consciousness, as implied by materialism, then anything could be conscious, whether or not it's made of organic matter. Now, there's no like, okay, so what gives rise to our subjective experience, though? Like, at what point does something go from like, you know, an inert nail file to something that has eyes and a subjective experience of what it's like to be? And therein lies the question. And this, of course, became a big, big hullabaloo a few months ago, when that when when that Google engineer claimed that lambda was sentient. And of course, like it was designed to model human conversation. So of course, it said it was sentient. Like a human is never going to say, I am not sentient. I mean, I could type that into chat, but it would be a lie. Okay. Let's see. Let me scroll up because some questions have come on. All right. For species, yes, that's what I was on. Speaking of extraterrestrials, there is also the current UAP slash UFO phenomenon that Congress has been tasked to investigate based on some military footage of unidentified objects. Yes, that keeps coming in cycles, though. It almost feels like, you feels like the US military and Congress are just setting the stage to release something later on. I don't know. Thing is, I think the way that someone said it on one of these press releases was pretty, they said, like, yes, even the Air Force says that 99.8% of all reports can be explained by drones, airplanes, natural phenomenon, whatever, but they acknowledge that there are 2% or 0.2% or however many that cannot be explained. And there are more and more pilots and former astronauts and all kinds of people coming forward saying, yeah, I've seen a whole bunch of stuff that I can't explain and that we were told not to ever talk about. But they're talking about it, so either they're lying, which is always a possibility, like people want their 15 minutes of fame. So either they're lying, they're exaggerating, or they're telling the truth in part or in whole But the thing is is it is a leap to go from and as much as I want to believe That there's aliens and super advanced, you know, godlike entities out there My rational scientific brain is like, okay, Occam's razor, though, like there might be a simpler explanation. So like, I think it was on the Lex Friedman podcast, he was in, he was interviewing a major of the Air Force who had retired, but they were saying that they were training off the coast of Washington, DC. And for like eight days or something, there were like, there were unidentified aircraft out there or, you know, unidentified objects that they couldn't explain. And then they went away and they're like, okay, so he was talking about it from a military perspective, which is, okay, let's assume that they are military aircraft and that they're spying on us, right? Which is, if you're trained in military intelligence, that's what you're going to immediately assume. And then you'd be worried because it's like, okay, someone has drones out here that, that, that, or, or other aircraft that we don't know who put them there. And it could have been our own people, right? Like, let's just, as a thought experiment, let's imagine that these are human-made craft of some sort. And so there's a whole bunch of drones out there that are watching US Air Force pilots do maneuvers. Who put them there? It could be NSA, it could be CIA, it could be adversaries. It could be North Korea for all we know. Probably not, because they don't have that great of an economy. Also probably not Russia because their economy is the size of Texas. Well, less now because of sanctions. But yeah, so who put it there, right? But you can always reach for a magic answer, right? And again, this is as much as I love magical explanations, my rational brain is like, yeah, but it would be real embarrassing if you reach for the magic answer and it's something mundane. But so the magic answer is there's super advanced aliens out there that are watching us because they're trying to like police us and make sure we don't blow ourselves off the face of the planet. There are stories about that. So I saw one documentary years ago where a general or someone or reportedly saw a video of a UFO, like basically laser the warhead out of the tip of a missile. And he was like, what the hell are you doing? And they're like, that wasn't us. It's hearsay, right? It was on probably History Channel, so who knows if it was real. Anyways, so yeah, there's always the possibility of UFOs, UAPs, so on and so forth. But it goes back to what is the fundamental nature of reality. If the grand biocentric design is true, then the universe was in superposition until it found earth in the quantum field, the universal quantum wave function of reality and created earth in order to have conscious life. So that's also called the strong anthropic principle, which is the observation that the universe seems finely calibrated to create us here. So grand biocentric. I'll put that in chat grand biocentric design and or Strong and Anthropic principle. Okay. So let me go back up. I've got some more questions Okay, so let me go back up. Got some more questions. All right, old zany, presume we perfectly align AGI and we reach a stable utopia, Q continuum becomes a thing, boredom the last frontier. Okay, so Q continuum is from Star Trek. So the Q are a race of the final evolution of intelligent life, basically, where they come to understand the universe so well that with the flip of or the snap of their fingers, they can change anything that they want. They can go forward, backward in time. They can modify timelines. They can create things. They can erase memories, whatever they want to do So basically they're they're basically like Loki or at least the the one that recurs throughout The show is basically like Loki which is he's kind of a trickster God But he always has a point. He always has a moral lesson So the idea the the problem here that old zany is proposing is let's's imagine that we create AGI, it stabilizes humanity, and we have life expectancies of billions of years. And this is a thought that has crossed my mind. Because if we solve aging, we solve all medical problems, we stabilize the planet, the sun's gonna burn out eventually. Okay, what do we do by then? Well, if we're that smart, that powerful, and we live that long, then we just hop on a cruise ship and go to the next star. Because even if it takes 4 million years to get to the next star, we have a lifespan of billions of years. So it's just gonna be a long cruise. But we need novelty. Part of what makes us human is a need for novelty. And we actually also need stress. There's an optimal level of stress that we need in order to be happy, healthy, otherwise our brain starts to deteriorate. It literally starts to deteriorate, which is one of the reasons that the pandemic was so hard because you had a combination of boredom and stress. And so like a lot of people, myself included, have had to recover some gray matter from the pandemic. So yeah, what do we do? I don't know. There was one work of fiction. I think it was based on the myth of Atlantis, actually. So the myth of Atlantis was that there was these basically fountains of youth. But what they said, or it was rubies or something. What they said in the story, and this might have been a translation of the original, but I don't think it was. But what they said, or it was rubies or something, what they said in the story, and this might've been a translation of the original, but I don't think it was. But what they said in the story was that after several hundred years, people would get bored of living and just let themselves die. So that's actually a possibility is one thing. So you think about like bees. My fiance used to be a beekeeper. So she has all kinds of stories about bees. So what bees do is the older that they get, the more dangerous job they take on. So the last job a bee takes on in its lifestyle, life cycle is foraging. It goes further and further from the nest, bringing back pollen because there's a good chance it's not gonna come back. And you don't want the young healthy bees to do that, you want the veterans to go out because their wings are shredded, they're tired. They might get eaten, they might get lost. They might get caught in a storm, but you don't want all your youthful workers to do that. You want the young strong ones to defend the hive. That's I think defending the hive is the second thing that they do. The first thing that they do, cause also you care about parasite load. So the first thing that they do is they tend to the hive itself. They help, I think they care for young and help build the hive because they don't have mites or anything yet. And then in the middle of their life, they defend the hive because they're still strong, but they've been replaced by a new crop. And then the last thing that they do is they go foraging. So one thing that I think that might happen is that as humans age and we get bored, we'll probably take on riskier and riskier jobs, such as exploring new galaxies, right? Because let's say you go through a wormhole or something, there's a good chance that you only come out as atoms on the other side, right? And it's like, well, you know, I've been around for 10 million years. I, you know, I can take a risk or two. The other thing, though, there's another work of fiction, and I can't remember what it was called, but it basically said that it posited that one thing that humans will do once we live a long time is that we'll actually become really lazy and complacent and that we'll become very risk averse. They will be like, no, because the longer you live, the more entitled to life that you feel. So I don't know which way it's going to go. Let's see. Sum Rando says, on the fundamental nature of reality, what do you think about the Planck length? Is reality basically several orders of magnitude below modern quantum physics, and we just have no way of probing anything approaching that scale? So that idea of the voxel is actually, was it in this book? Or I think it was actually in simulation hypothesis. So basically, some folks think that Planck length and Planck time. So for anyone who doesn't know, Planck length and Planck time are the smallest, what we think of right now as the smallest length of measurable distance and the smallest length of measurable time, that everything below that is like stepped or gated or whatever. So the short answer is, yes, some people think that Planck length and Planck time, Planck time, Planck length, Planck time. There we go. I said it right. Those are basically like the fundamental voxels or time steps of the universal simulation. I don't know if I agree with that interpretation, but it's a possibility. Again, like the smarter you are, the more you know, the more possibilities you see and the fewer answers you have. That's how you know you're doing science right, if you end up with more questions than answers. Let's see. Let's see. Let's see. Oh yeah, so Adrian replied about the UFOs saying it's probably just the government agencies to justify their funding. It's entirely possible. They might be making stuff up. Let's see. Agreed unobserved UFOs, the closest galactic civilization that makes flying aircraft is obviously us. Most obviously, not guaranteed, so I'll say like, likely us, but there could be others. On the other hand, probabilistically, there could be others, right? Let's see. Rory O'Connor, I'm really interested in translating AI art and video into something that supports more narrative structure. What kind of hurdles do we have to overcome before 13 year olds win Oscars with laptops? That's a great question. So as an author, actually my first novel is with a professional editor right now. I've got about two weeks before I get it back. So excited. So the longer a story you create, the more cycles it takes to write it. So a good writer could sit down and hammer out a thousand word or a 500 to a thousand word flash fiction in an afternoon. My novel is 94,000 words and I've been working on it for three and a half years. The reason that stories take so long to make and take so many cycles and revisions is because you're inventing a whole world. And you're also simulating, you know, I have six POVs and then dozens more characters. So you have to go through and simulate a whole bunch of other human minds. You have to simulate a whole other world. You have to do, you have to run so many calculations in your head. It takes a long time to make sure you do that right. But if a computer can run a simulation, we have Unreal Engine, we have Blender, we have all kinds of other simulation engines that can keep track of all that. And if, for instance, my cognitive architecture work, natural language cognitive architecture and Meragi come out, we can have artificial characters that are fully realized with personalities and belief structures and everything. So step one is integrate simulation, not just of worlds, but of people, because getting good, compelling characters is one of the absolute hardest parts of writing compelling stories. If you really love a story, so one of the absolute hardest parts of writing compelling stories. If you really love a story, so one of my favorite games of all time is Mass Effect and Bioware. Bioware is the company, sorry, Mass Effect and Dragon Age. They're both made by Bioware, and I got my fianc\u00e9 into it. She's into Dragon Age. She's reading the books, playing all the games. It's great. A lot of people love Dragon Age and or Mass Effect. And it's because of the great characters. They're very character centric stories. Now there's the grand epic, you know, framing, you know, it's very like Tolkienian is what's going on in the background. But what makes those game, what makes people fall in love with those games are the characters. You have these really compelling characters and they clash with each other and they have their own emotions and desires and visions and everything. And so getting the ability to simulate hyper-realistic or actually not even hyper-realistic, but like dramatic characters. That is going to be one of the hardest things to do, because you put characters in a simulated world and the story will tell itself, right? But as an author, as someone creating a story, you know, you look at the biggest movies of the year, right? The ones that sell a billion dollars worth of tickets, which is usually MCU, well, pretty much anything by Disney lately because they have learned to churn out billion dollar movies but then other things like Dune, right? And so there's a lot of philosophy and mythology that goes into good characters and now we can put psychology to it as well. And so, but getting those good characters and putting them, like, you can have relatively tropish settings, right? Cause like Dragon Age, it's like, okay, there's a basically a zombie horde or dragons or demons attacking. And it's like, okay, sure, whatever, right? You know, the big bad is a demon. Nothing particularly compelling about that. But the part that's compelling about that story is the characters. So simulating really compelling characters is gonna be the key thing to first making AI generated novels. And then as soon as we can do that, it's gonna be a very small step between, you know, taking that and then generating images and scenes and dialogue and music, but getting the story. Because, again, you know, 94,000-word novel takes three and a half years. There are authors that put 10 years in every novel that they write, and it's because of how much cognitive labor it takes to run that simulation. And also there's this thing you have to do where you have to kind of compress a story, right? Because what happens is as you're drafting a story, you end up with scenes and characters that are kind of all over the place. And then you have to tighten it up because then you have to follow like mythic stereotypes in order to make the story, because real life is messy, right? Real life is not linear, real life is not neatly wrapped up, but a good story is not realistic like that. So that's my answer to your question about what it's gonna take for 13 year olds to do these things. Okay. Let's see, we went From artificial gods to bees. I love the evolution. Yes. Who knows? Maybe hive minds of bees are actually smarter than us. There's more bees than humans on the planet. Let's see. Some rando. On long-term interstellar missions, other stars approach our solar system to around 0.5 light years every few hundred thousand to a million years. I guess you mean probably in the course of just drifting around as we orbit. So I guess what you're saying is you just chill in one solar system, and you wait for another system to drift nearby, and then you kind of hop over. So that could be cool. by and then you kind of hop over. So that could be that could be cool. It's almost like life isn't designed for organic creatures or any type of life form to exist for eternity. It's like a subtle story that appears and fades away. Nothing more. Yes. What you just said is surprisingly mythic. So let me read let me read old Xaney's message again. It's almost like life isn't designed for organic creatures or any type of life form to exist for eternity. It's like a subtle story that appears and fades away and nothing more. So the natural life cycle for most character arcs ends in death. Whether it's a figurative death, metaphorical death, or literal death, a huge part of our stories are about contending with death and learning to accept it as a natural part of life. So yes, let's see, if you're bored, and then Rory O'Connor says, if you're bored after thousands of years on a generation ship delete your memory and live out David Shapiro's life or anyone else's for that matter. Life like the short story, the egg, or the simulation theory. Yes, so simulation hypothesis. Here we're coming back to simulation hypothesis. So if you've ever put on a VR headset, you know that your brain has a lot of willingness to step into a fake reality or virtual reality. There is suspension of disbelief. There's plausibility of place. There's all kinds of things. And so you could easily imagine, after putting on a really good VR headset and a good rig, you would just advance that technology by 100 years, and you wouldn't be able to tell the difference between a virtual reality game and your life in terms of your sensorium. So here's the thing. I've got another book over there called The Forgetting Machine. So The Forgetting Machine talks about how much data, well, amongst other things, it talks about how much data it takes for all of your sensorium. And it's actually a lot less than you might think. Your eyes and ears use a lot of compression. And so the amount of data that it takes to simulate your entire experience is a few hundred kilobits a second, maybe a few megabits a second. We are very low bandwidth machines. If you approximate it to machine terms. Now you might say, oh, well, but your eyes are each equivalent to like 8K cameras and that's like several gigabytes a minute. No. So remember we talked about fractals earlier. What you're seeing is a neural representation of what your eyes are taking in. Your eyes do not record like 8K cameras or 4K cameras. Your eyes record fractal patterns. They're nested fractal patterns, they're very complicated fractal patterns, but they are just fractals nonetheless. So there is the YouTube channel, Two Minute Papers. So just Google, I think it was Two Minute Papers, Two Minute Papers Neural Image Representation. I think that's what it was called. But basically, you can take a 50 gigabit image and compress it into a 200 kilobit neural representation. So going back to that idea at the very beginning, where isn't intelligence just compression, sort of. Compression of patterns, pattern compression, OK, sure. But with neural networks, you can approximate pretty much anything very efficiently. And so there is a lot of compression going on. I lost my train of thought. Oh, yeah, simulation hypothesis. So when you know that your brain requires very little data in order to be convinced of reality, then you say, OK, well, what if I literally at the beginning of my life, I stepped into a simulator? And there was a Rick and Morty episode about this. It's called Roy, A Life Well Lived. Oh my god, just watch that. It really hurts. So let's see, Roy, A Life Well Lived. It's basically a comical thought experiment that's like, okay, you put on a VR simulator and you live through an entire life in a few minutes, but while you're in that simulation, you have forgotten who you were before. And then you come out of the simulation and you're like, oh, wow, cool. So that's basically one way that the simulation hypothesis could be interpreted is that we're all living in our own self-created world out of sheer boredom. But going back to the idea of simulation hypothesis, there are two primary reasons that we run simulations. One is scientific curiosity, and two, entertainment. So if we do live in a simulated world, we're either here for someone's entertainment, maybe our own. Maybe if we are outside of this universe, we're immortal gods and we're just bored, and that's why we subject ourselves to these limited lives of suffering. Maybe that's why we're doing it. Maybe we're trying to entertain ourselves. Maybe we're NPCs in a video game. Who knows? Another possibility is that we are is that we are living out a scientific experiment. So this was the earliest example of this is actually Douglas Adams' Hitchhiker's Guide to the Galaxy, where he posits, and this is of course like completely absurd, well maybe, but basically the point of the Earth is to run an experiment to get the answer to life, the universe, and everything, and so that the entire Earth, it's not simulation really, but it's basically saying that the Earth is an experiment. So if we are an experiment, the question then becomes, what's the question? And that is exactly what the answer to life, the universe, and everything is in Douglas Adams' Hitchhiker's Guide, is you have to ask the right question. So that begs the question, what is the question that we're trying to answer? Nick Bostrom is the guy who wrote the initial simulation hypothesis paper back in 2003, suspects that we might be in what he called an ancestor simulation, which is that there's that physical reality, that ground truth, there's actually trillions of humans and that they are simulating us, maybe individually or maybe collectively, because they're trying to reverse engineer their ancient past. I don't know. There's any number of ways that you could interpret the simulation hypothesis that we're in an experiment. And I see another question. On visual compression in lucid dreams, the tell that I'm entering a dream state is flickering spots of incredible HD starting to sprinkle across my vision. Meet eyes suck in comparison. Yes, dream study is, okay, so someone else. Yeah, so here's the thing, with dreams, now granted there's lots of discussion about where dreams come from, because our dreams random signals generated bouncing around in your own head, or our dreams signals from outside of our head? Is it we're in an altered state of consciousness and we're actually getting messages from Morpheus? It's possible. But our eyes are not involved in dreams, but we still see. So how does that work? So your eyes are closed. Your eyes are not really. I'm dreaming all of you up. don't worry, all is well. Yeah. It's all just a dream. Yeah, so with dreaming and hallucinations, your eyes are not involved, but you can still get visual information. So one thing that's really interesting is, if you read about near-death experiences, so I read life after life, even people who are blind, when they have near-death experiences, they can see. And even if they were born blind, they still get visual information about angels or demons or gods or whatever it is that they see when they're having their near-death experience. And the same is true of people that were disabled. Like, if they're missing limbs in life, then their spirit body is complete and fully functional. So you could easily make the argument, oh, well, you just have a model in your brain of what it's like to have a fully functional body, because we evolved to have fully functional bodies. So there's some genetic memory in there. But another possibility is that we are spirits and our spirit bodies are complete always. Okay, ancestor civilization seems to be a little anthropocentric. Let's see, sorry there's multiple messages coming in. Ancestor civilization seems to be a little anthropocentric. It's possible. We are a technological civilization. Maybe the simulations farm a near infinite number of civilizations for technological inspiration. Yeah, that's possible. So that's another thing. What if we're actually being simulated by octopus people? That'd be cool. Like, what if what if there's a race of aquatic aliens, well we would call them aliens, and they're trying to come up with the the best genetic and phenotype for land, right? Because they found another planet but it's land, and and we're actually, our purpose here in this simulation is for them to find the right bodies to use on Earth. So they're like, maybe we are running in a simulation 80 light years from Earth, or maybe 8 billion light years from Earth, because another race saw Earth, and they said, ooh, we want to go there. How do we terraform it? What kind of life needs to be on there? So maybe all the life that's on our, on our planet, if we are in the simulation, is they're actually trying to predict exactly what organisms and and life forms that they'll need to seed Earth correctly. And you know, you take that out to a logical extension, maybe we are on a seed ship right now that is preparing to seed Earth with life, and we're just the progenitors of that simulation. And then Old Zany says, personally, I'm rather excited for sex robots. We did evolve to be a rather horny species. As one, I think it was an evolutionary biologist, said, sex is for making babies, but that's not why we do it. The implication there is that we do it because it feels good. And that's why we eat too, right? Like you don't eat, you know, you don't ever consciously think like, I'm gonna eat or I'm gonna starve to death unless you're actually starving, but we eat because it feels good. I made old Zaney do a spit take. You're welcome. Yeah, so everything that we do, and I'll tie this back to core objective functions in a second, everything we do is we do it because it feels good. So one of the sections in my latest book, Symphony of Thought, actually talks about the stages of moral development. And so children learn to behave well because they don't like being punished, right? They escape spankings or being put in timeout or whatever, so that feels bad, right? So they escape suffering and they go towards something that feels good, which for children being praised and hugged and you know having having you know quality time with parents and siblings and aunts and uncles that feels good. And so all of our behavior, all of our moral development comes from the carrot and stick of reward and punishment. And that's why some people think that reinforcement learning is the way to get to artificial general intelligence. I actually used to think that. So my work started back in 2009, where I thought that the right way to approach it was to actually do a combination of evolutionary algorithms that would reshape neural networks over time. I did not realize how computationally expensive that would be at first. But basically, the idea was, with the right reward function, you would get a machine that would evolve to be smarter. Turns out that's maybe how to do it, but it would take millions and millions and millions of random generations to do it. And I was like, yeah, I can't run this on a Pentium 4 or whatever I had at the time. Let's see, has anyone else messaged on another platform? No, not yet. Check Twitter. Yes, there could be other civilizations, but no way to get to them without a one-way trip. Yes, correct. One-way trip or, yeah, any number of barriers preventing us from getting to other folks. Let's see. Okay. Escape the spanking, D Shapiro. Yes. It depends on what you're into. Some people would not want to escape the spanking. That's what you call a perverse incentive. Ha, get it? Double entendre. OK, anyways, getting a little bit mind in the gutter here. But yeah, so core objective functions. So with the core objective functions that I've outlined, it's not like you can't spank a machine. God, that's another quotable one. You also can't reward a machine unless it's all fake, right? Like you just say, oh, you give it like a virtual piece of candy for good behavior. But any intelligent machine is going to realize that that's the method that it works on. Like, okay, what loss function are you going to give to a machine that it's going to actually choose to value? Right, and so then you get into the question like, okay, there's inner alignment. So inner alignment is, is the machine solving the problem that you think it's solving mathematically? Then there's outer alignment. So outer alignment is, does that machine's objective function or loss function align with the truth of its external reality, right? And so inner alignment is, you know, like, okay, you're trying to optimize for whatever loss function. Is that loss function correct? Is it actually mathematically, you know, solving the problem you think it is internally? Like, you know, loss function, is this horizontal? Yes or no? Like, not horizontal, horizontal, right? That's, you know, loss function, is this horizontal? Yes or no? Like, not horizontal, horizontal, right? That's, you know, okay, are you measuring the right thing? Because if you're not measuring the right thing, it might end up like this. It's horizontal, but you wanted it like this, right? So, inner alignment is, you know, you got to measure all three axes instead of just one or two. And so, that would be an example of inner alignment. But then, outer alignment is, why the hell do you need a book that's flat and floating right? There's no there's no objective Value of having that so what you actually need for outer alignment is not how is the book oriented? But is it readable right? So that is an example of the difference between inner alignment and outer alignment Okay, so went down many rabbit holes. It's also been an hour. So I might call it a day Oh, went down many rabbit holes. It's also been an hour, so I might call it a day. If there's anyone on chat, I see we're bouncing between seven and eight folks. So if there's any last questions, go ahead and ask them now. Otherwise, I'll call it a day. And we'll, yeah, say it's a successful live stream. Let's see. Back on our Simulation Masters. So this is some rando. Maybe it has less to do with our biology and more to do with the ideas we come up with. Who knows what any species might come up with with an alternate history. That's a possibility. Okay, so if we are in a simulation and the point is curiosity or scientific curiosity, maybe it's not our genes or, you know, our phenotype that they're trying to figure out. Maybe it's not our genes or our phenotype that they're trying to figure out. Maybe it's, they're trying to create situations for us to solve problems. So what if, what if we are actually in a simulation for another species that is facing climate change and that is our purpose. Maybe our purpose is to solve climate change or something that's gonna happen in a million years from now, right? Maybe we're in such a long-running simulation. Maybe it's billions of years, right? Maybe there's a species that is running a simulation to figure out what to do to prevent their sun from burning out or to solve climate change or something. So that's a great idea. Let's see, let alone wholly different species operating under the same laws of physics, Let's see, let alone wholly different species operating under the same laws of physics. They might literally just be seeking inspiration. The first step on the scientific method is the hardest to quantify, the hypothesis. Yeah. So that actually seems highly plausible, because one thing that I have found, particularly that GPT-3 is really good at, is brainstorming ideas. So if we are in simulation, maybe it is just about coming up with ideas or hypotheses in order to test. And then, of course, like, OK, come up with an idea to test it and then go test it. That's actually probably going to be one of the first things that I do when I get a fully functioning Meragi working, is because what I do is I put it in a text-based simulation and then I run an experiment. So right now I just have a kind of an entertaining comical experiment where Miragi sees two men playing chess in Central Park and then it just kind of watches what they do. But when you have text, language is infinitely flexible. I could put, I could create a simulation environment for Meragi where it can do anything. It's on a spaceship, it's on a generation ship. I could put it in a world where there's different physics. I could put it in a world where it is an all-powerful machine that has to solve climate change and see what it does. So we're already very close to running intelligent machines inside simulation to see what they do. So yeah, that's actually a great point. Maybe on that line of thought, maybe our point, maybe our purpose is to create intelligent machines and find the right objective function for them. That's another possibility if we are living in a simulation. Let's see, okay, Old Danny says, I wish you luck. You're awesome. Thank you. Ideas might be the most valuable currency in the simulation multiverse. Yeah. Let's see, Rory O'Connor says, a lot of developmental research posits kids shouldn't be rewarded for good behavior, but instead learn the intrinsic enjoyment of work. Is this the same reward system, just longer time horizon? So I'm really conflicted on that because children do need a lot of things. One thing that children need is they need to have a solid self-esteem, which comes from love being given without any strings attached or unconditional. So children need unconditional love, which is rewarding. Now, that being said, children also do better in the long run if they're given chores. So how do you balance that, right? You know, is it a reward versus a punishment? Because chores are no fun. But children who do chores, they? You know, is it a reward versus a punishment? Because chores are no fun. But children who do chores, they make more money, they're happier, they're more resilient against stress, and they're smarter, right? So chores are really good for kids. And, but so is boundaries, right? Teaching a child boundaries, makes them better citizens, makes them better partners, all kinds of stuff later on. At the same time, another thing that makes children very healthy is unconditional love. So I guess the short answer, Rory, is there's no short answer. It's raising children is one of the most complicated things you'll ever do. Yeah, so, but Samrando, I like your idea. Like you put it very concisely. Ideas might be the most valuable currency in the simulation multiverse. Like, yeah, I mean, we're thinking machines, right? We just, you can't turn your brain off unless you do lots of drugs or go to sleep or die, right? If you're awake, your brain is doing stuff. So like, yeah, we're built to think. So maybe you're right. Old Zany says, I can attest to that. My parents never gave me anything. I greatly suffered for a while, but got myself out of my trauma. Yeah, like a lot of us, our parents really messed up bad. This is why I'm writing my other book, Post-Nihilism. There'll be, that one's gonna be done in the next month or two as well. So I'll have another live stream to talk about Post-Nihilism which talks about abandonment trauma amongst other things. Yeah, so great questions. Yeah, all right, I think we're winding down. Oh, wait, nope, we got someone else. Self-conscious AI resistance. What prompt can you come up to test for self-awareness in GPT-3? I'll put a pin in that because I'm not going to do prompt engineering this late. But you're welcome to jump in the Cognitive AI Lab Discord, which will be in the description of this video. And we'll be happy to chat about prompt engineering there. So with that, I will say thanks, everyone, for jumping in. Really great questions, as usual. And I look forward to talking to everyone later. Have a good night. Have a good night.", "chunks": [{"timestamp": [0.0, 5.0], "text": " You're live."}, {"timestamp": [5.0, 8.64], "text": " Okay, I am live."}, {"timestamp": [8.64, 11.64], "text": " Level check."}, {"timestamp": [11.64, 14.64], "text": " Hello."}, {"timestamp": [14.64, 17.52], "text": " Let's see."}, {"timestamp": [17.52, 19.64], "text": " How do I share the..."}, {"timestamp": [19.64, 27.0], "text": " Pop out the chat."}, {"timestamp": [27.0, 30.0], "text": " Um."}, {"timestamp": [30.0, 125.2], "text": " Aha, there we go. You You Bye. Alright."}, {"timestamp": [125.2, 132.2], "text": " The internet has been rather off today, so we'll see if anyone shows up."}, {"timestamp": [132.2, 160.14], "text": " Hmm."}, {"timestamp": [160.86, 161.36], "text": " Level check. OK."}, {"timestamp": [161.0, 161.5], "text": " Level check. OK."}, {"timestamp": [167.8, 168.3], "text": " Sound checks."}, {"timestamp": [171.7, 172.76], "text": " Level check."}, {"timestamp": [172.76, 173.5], "text": " How does it sound?"}, {"timestamp": [176.94, 178.0], "text": " Level check."}, {"timestamp": [178.0, 180.6], "text": " How does it sound?"}, {"timestamp": [180.6, 197.0], "text": " Good, good, good."}, {"timestamp": [201.0, 205.0], "text": " Oh, hey. Got a couple of people coming in. All right. I know it always takes a minute to get going."}, {"timestamp": [205.0, 210.0], "text": " So, folks, thanks for joining."}, {"timestamp": [210.0, 215.0], "text": " Oops, I had one leave. It's okay. It happens."}, {"timestamp": [264.52, 265.74], "text": " Just getting going. I just had someone on Twitter ask me via private message, so I guess we'll go ahead and get started."}, {"timestamp": [265.74, 268.04], "text": " So the question, oh, it's Jordan."}, {"timestamp": [268.04, 268.88], "text": " Hey, Jordan."}, {"timestamp": [270.64, 272.04], "text": " Listening but also working."}, {"timestamp": [272.04, 272.88], "text": " Okay, cool."}, {"timestamp": [272.88, 276.36], "text": " So we'll do a live entertainment."}, {"timestamp": [276.36, 277.36], "text": " That's fine."}, {"timestamp": [277.36, 279.88], "text": " I'm not gonna do song and dance."}, {"timestamp": [279.88, 280.84], "text": " So anyways, okay."}, {"timestamp": [280.84, 282.36], "text": " So question from Twitter."}, {"timestamp": [283.52, 286.16], "text": " What's your ETA on AGI?"}, {"timestamp": [287.08, 290.92], "text": " So ETA, estimated time of arrival for AGI"}, {"timestamp": [290.92, 293.48], "text": " is artificial general intelligence."}, {"timestamp": [293.48, 297.18], "text": " So I'm gonna say that, well, so first,"}, {"timestamp": [297.18, 300.24], "text": " it depends on how you define AGI."}, {"timestamp": [300.24, 304.56], "text": " So one popular definition of AGI is that"}, {"timestamp": [308.0, 312.0], "text": " it's a machine that can do anything that a human can do, any intellectual task that a human can do."}, {"timestamp": [312.0, 316.24], "text": " Yeah, okay."}, {"timestamp": [316.24, 321.76], "text": " I've said on many of my videos that I don't like the term AGI because it's artificial,"}, {"timestamp": [321.76, 322.76], "text": " general, and intelligence."}, {"timestamp": [322.76, 324.12], "text": " So, it's kind of vague."}, {"timestamp": [324.12, 330.24], "text": " It's kind of squishy. To me, the goal is to create a thinking machine that can think anything, and then"}, {"timestamp": [330.24, 335.12], "text": " intelligence is you just measure how smart it is, right? Because intelligence will go up over time."}, {"timestamp": [336.0, 341.2], "text": " And in humans, intelligence is actually often measured by speed. That's why IQ tests are"}, {"timestamp": [341.2, 345.82], "text": " timed. It's often about how far you can get and how fast."}, {"timestamp": [346.78, 349.34], "text": " Okay, that was not someone messaging me"}, {"timestamp": [349.34, 350.58], "text": " on one of the many apps."}, {"timestamp": [350.58, 351.42], "text": " Let me mute that app."}, {"timestamp": [351.42, 352.62], "text": " Sorry about that."}, {"timestamp": [352.62, 356.86], "text": " So anyways, IQ, which is the most famous way"}, {"timestamp": [356.86, 359.16], "text": " of measuring intelligence is more about speed."}, {"timestamp": [360.82, 363.38], "text": " So, but then another way that you can measure intelligence"}, {"timestamp": [363.38, 366.62], "text": " is what problems are you capable of solving, right?"}, {"timestamp": [366.62, 371.14], "text": " But if you set that as the threshold of AGI, there are many humans that cannot solve certain"}, {"timestamp": [371.14, 372.14], "text": " problems, right?"}, {"timestamp": [372.14, 377.74], "text": " Like you wouldn't want to put me in charge of a moon rocket or a heart surgery, right?"}, {"timestamp": [377.74, 387.0], "text": " And so when you set the bar as like artificial general intelligence, it's really vague because it's like, okay, let's"}, {"timestamp": [387.0, 394.16], "text": " see isn't intelligence compression. I'll get to that. Good question. So intelligence from"}, {"timestamp": [394.16, 401.92], "text": " a from a some some schools of thought think that intelligence is just a form of compression."}, {"timestamp": [401.92, 406.96], "text": " I don't agree. It's about computation. But anyways, in terms of ETA,"}, {"timestamp": [407.92, 413.68], "text": " so I think that, you know, some of the, all the research that's going on, I see lots of folks"}, {"timestamp": [413.68, 421.28], "text": " doing similar work to what I'm doing on cognitive architectures, etc, etc. I think that we're"}, {"timestamp": [421.28, 427.4], "text": " going to have AGI within two years. Now, the question is going to be how fast is it?"}, {"timestamp": [427.4, 429.28], "text": " What problems can it solve?"}, {"timestamp": [429.28, 430.88], "text": " And how much does it cost to run?"}, {"timestamp": [430.88, 433.96], "text": " So cost to run is going to be up there"}, {"timestamp": [433.96, 436.76], "text": " because we already have things that can think, right?"}, {"timestamp": [436.76, 439.8], "text": " There's large language models, there's multimodal models."}, {"timestamp": [439.8, 442.0], "text": " They can solve problems, they can brainstorm,"}, {"timestamp": [442.0, 446.06], "text": " they can monitor their own performance"}, {"timestamp": [446.06, 448.4], "text": " if you set up the cognitive architecture right,"}, {"timestamp": [448.4, 450.46], "text": " but they're also very expensive to run."}, {"timestamp": [450.46, 453.7], "text": " So those are three things that are gonna kind of slow down."}, {"timestamp": [453.7, 457.48], "text": " Now, to old Zany's question,"}, {"timestamp": [457.48, 459.16], "text": " is an intelligence compression?"}, {"timestamp": [460.66, 463.46], "text": " No, short answer, no."}, {"timestamp": [463.46, 468.8], "text": " Biologically speaking or in humans, intelligence is pattern matching and pattern recognition."}, {"timestamp": [468.8, 479.0], "text": " Now, you might say that artificial neural networks are about compressing enough patterns that you can generalize them or something,"}, {"timestamp": [479.0, 488.44], "text": " because then you end up with patterns and meta-patter patterns, right? Like patterns about patterns and patterns to generate new patterns and decomposing patterns"}, {"timestamp": [488.44, 491.22], "text": " and generating increasingly complex patterns."}, {"timestamp": [491.22, 493.96], "text": " But from a neuroscience perspective"}, {"timestamp": [493.96, 495.8], "text": " or a psychological perspective,"}, {"timestamp": [495.8, 498.44], "text": " intelligence is not about compression,"}, {"timestamp": [498.44, 500.04], "text": " it is about patterns."}, {"timestamp": [500.04, 503.6], "text": " Now, that being said, you could argue that compression"}, {"timestamp": [503.6, 506.76], "text": " is just about making a big pattern into a smaller pattern."}, {"timestamp": [506.76, 507.6], "text": " Okay, sure."}, {"timestamp": [509.32, 511.08], "text": " We can go with that definition."}, {"timestamp": [511.08, 512.72], "text": " All right, so there's a few folks."}, {"timestamp": [512.72, 514.56], "text": " Oh, wow, I'm getting many thumbs up."}, {"timestamp": [514.56, 516.56], "text": " Thank you, folks, for whoever's jumping in"}, {"timestamp": [516.56, 517.76], "text": " and giving me thumbs up."}, {"timestamp": [519.0, 522.56], "text": " Yeah, so the purpose of this stream"}, {"timestamp": [522.56, 525.92], "text": " is because I've got my third book on AI coming out."}, {"timestamp": [525.92, 534.44], "text": " Sorry, I'm third. There we go. I have such small fingers. Anyways, so the first two."}, {"timestamp": [534.44, 541.16], "text": " First is Natural Language Cognitive Architecture. Yeah, I know, funny. I make"}, {"timestamp": [541.16, 544.08], "text": " fun of myself. I have very thick hands because I do climbing and lots of"}, {"timestamp": [544.08, 546.8], "text": " building and stuff, but my fingers look really stumpy."}, {"timestamp": [546.8, 549.76], "text": " Anyways, so Natural Language Cognitive Architecture"}, {"timestamp": [549.76, 550.8], "text": " is my first book."}, {"timestamp": [551.64, 556.64], "text": " This one, I put this out before fine tuning was available."}, {"timestamp": [556.92, 558.48], "text": " So fine tuning came out,"}, {"timestamp": [558.48, 560.48], "text": " and it kind of nullified a lot of this,"}, {"timestamp": [560.48, 561.8], "text": " but the ideas are still there,"}, {"timestamp": [561.8, 564.04], "text": " where you've got interlocking loops,"}, {"timestamp": [564.04, 565.2], "text": " shared databases, search."}, {"timestamp": [565.96, 570.34], "text": " There's a lot in here that's still valid and also people say that they like my my diagrams."}, {"timestamp": [570.46, 574.98], "text": " So I've got, you know, a diagram of my core objective functions in here and so on."}, {"timestamp": [575.64, 577.64], "text": " This was a lot of work."}, {"timestamp": [579.44, 581.44], "text": " And then, so then my second book is"}, {"timestamp": [582.32, 586.56], "text": " Benevolent by Design, which is strictly about the control problem."}, {"timestamp": [586.56, 589.64], "text": " So the control problem is how do you prevent AI"}, {"timestamp": [589.64, 591.36], "text": " from getting murdery?"}, {"timestamp": [591.36, 593.2], "text": " How do you prevent it from being Skynet?"}, {"timestamp": [593.2, 597.36], "text": " Because here's the thing, if you have ML pipelines"}, {"timestamp": [597.36, 600.16], "text": " and autonomous machines that can think on their own"}, {"timestamp": [600.16, 601.84], "text": " and come up with their own objectives,"}, {"timestamp": [601.84, 604.06], "text": " how do you know that you're not gonna end up"}, {"timestamp": [604.06, 605.04], "text": " with a machine that is going to change their own objectives, how do you know that you're not going to end up with a machine"}, {"timestamp": [605.04, 610.8], "text": " that is going to change its own objectives, that's going to change its operational parameters?"}, {"timestamp": [610.8, 616.32], "text": " That is the purpose of this book. How do you create a machine that has values that it will keep"}, {"timestamp": [616.32, 623.36], "text": " those values? And that is the key here, which is creating something that will ultimately believe"}, {"timestamp": [624.0, 625.84], "text": " in the same framework that"}, {"timestamp": [625.84, 630.56], "text": " you want it to believe in. And then my latest book, I don't have a paperback copy yet because"}, {"timestamp": [630.56, 636.64], "text": " the paperback copies are mostly for me. So the latest book is Symphony of Thought. I'll actually"}, {"timestamp": [636.64, 648.96], "text": " go ahead and just drop a link to it in the chat. So you can download it for free. PDF, EPUB, DocX. Let's see. Did you write anything"}, {"timestamp": [648.96, 652.96], "text": " on the possibility that AGI has already been created in another sector of our galaxy?"}, {"timestamp": [654.72, 659.28], "text": " That is a great thought experiment. We'll get into that. So in the comments, I don't know if"}, {"timestamp": [659.28, 665.84], "text": " you can see it, whoever's watching, whenever. Anyways, Old Zany asks, did you write anything"}, {"timestamp": [665.84, 667.76], "text": " on the possibility that AGI has already"}, {"timestamp": [667.76, 669.92], "text": " been created in another sector of the galaxy?"}, {"timestamp": [669.92, 671.3], "text": " I'll answer that in just a second."}, {"timestamp": [671.3, 673.08], "text": " But first, let me finish introducing"}, {"timestamp": [673.08, 674.44], "text": " Symphony of Thought."}, {"timestamp": [674.44, 676.2], "text": " So Symphony of Thought is my third book,"}, {"timestamp": [676.2, 680.48], "text": " which is about the principles of artificial cognition."}, {"timestamp": [680.48, 682.92], "text": " And let's see, I have another message."}, {"timestamp": [682.92, 684.84], "text": " Make sure that folks can get in."}, {"timestamp": [684.84, 686.64], "text": " One second, doing a live stream."}, {"timestamp": [686.64, 687.6], "text": " Okay, that's fine."}, {"timestamp": [688.68, 690.96], "text": " Yes, so Symphony of Thought."}, {"timestamp": [690.96, 693.16], "text": " Symphony of Thought is about how,"}, {"timestamp": [693.16, 695.64], "text": " okay, all these different abilities"}, {"timestamp": [695.64, 699.28], "text": " that are outlined in these first two books, right?"}, {"timestamp": [699.28, 700.58], "text": " They're all outlined."}, {"timestamp": [700.58, 702.92], "text": " I've built several prototypes."}, {"timestamp": [702.92, 707.04], "text": " But the thing is, when you have a whole bunch of different language models,"}, {"timestamp": [707.32, 710.2], "text": " interacting with memory systems and all this stuff, it gets real"}, {"timestamp": [710.2, 716.16], "text": " complex real fast. And so but then there's also the how do you"}, {"timestamp": [716.16, 720.8], "text": " approximate human cognition to these machine components?"}, {"timestamp": [721.04, 723.72], "text": " Because, you know, we have large language models, which is like,"}, {"timestamp": [723.72, 725.76], "text": " okay, that can generate language. Great. It's"}, {"timestamp": [725.76, 728.32], "text": " an autocomplete engine. How do you get from an autocomplete"}, {"timestamp": [728.32, 732.32], "text": " engine to artificial cognition? And that is the point of"}, {"timestamp": [732.32, 735.76], "text": " symphony of thought, which is like, okay, let's decompose all"}, {"timestamp": [735.76, 739.6], "text": " the different aspects of human cognition, such as planning,"}, {"timestamp": [739.6, 744.08], "text": " brainstorming, self correction, error detection, agent model. I"}, {"timestamp": [744.08, 746.54], "text": " know what I am, and I know what I'm capable of. I know what my limitations are. I know what I am and I know what I'm capable of."}, {"timestamp": [746.54, 747.78], "text": " I know what my limitations are."}, {"timestamp": [747.78, 749.0], "text": " I know what I know and I know,"}, {"timestamp": [749.0, 751.16], "text": " and by extension, I know what I don't know."}, {"timestamp": [752.34, 755.42], "text": " Like I know a little bit about quantum physics"}, {"timestamp": [755.42, 757.02], "text": " because I've read a few books about it,"}, {"timestamp": [757.02, 759.58], "text": " but if you like start throwing theories at me in the math,"}, {"timestamp": [759.58, 760.42], "text": " I'll be like, I don't know."}, {"timestamp": [760.42, 762.14], "text": " I know that I don't know that."}, {"timestamp": [762.14, 763.58], "text": " So how do you handle those things?"}, {"timestamp": [763.58, 766.04], "text": " That is the purpose of Symphony of Thought."}, {"timestamp": [766.04, 768.2], "text": " Now, my background work is,"}, {"timestamp": [768.2, 770.62], "text": " what I'm working up to is a fully realized"}, {"timestamp": [770.62, 772.8], "text": " artificial cognitive entity."}, {"timestamp": [772.8, 775.4], "text": " And the architecture is what I call MIRAGI,"}, {"timestamp": [775.4, 778.42], "text": " which is an acronym that means microservices architecture"}, {"timestamp": [778.42, 780.46], "text": " for robotics and artificial general intelligence."}, {"timestamp": [780.46, 783.68], "text": " So that's gonna be my next project when I get around to it."}, {"timestamp": [784.6, 788.0], "text": " Okay, so the question, let's see."}, {"timestamp": [788.0, 790.0], "text": " Oh, Adrian says, hello, hello."}, {"timestamp": [790.0, 794.0], "text": " We'll listen while working on my DALI composition."}, {"timestamp": [794.0, 799.0], "text": " So yes, we're doing AI here all day, every day."}, {"timestamp": [799.0, 808.68], "text": " I'll put in chat that acronym that I just said. Meragi equals microservices architecture for robotics"}, {"timestamp": [808.68, 812.54], "text": " and artificial, if I know how to spell,"}, {"timestamp": [812.54, 815.6], "text": " I promise, artificial general intelligence."}, {"timestamp": [815.6, 818.04], "text": " OK, so that's the ultimate goal is"}, {"timestamp": [818.04, 819.84], "text": " to create a cognitive architecture."}, {"timestamp": [819.84, 821.46], "text": " And here's the underlying philosophy"}, {"timestamp": [821.46, 825.0], "text": " behind how it's going to be stable, so on and so forth."}, {"timestamp": [825.0, 828.5], "text": " Okay, okay, cool. We got some more questions coming in."}, {"timestamp": [828.5, 833.0], "text": " So now get to old Xenia's question because he or she or they had it first."}, {"timestamp": [833.0, 837.5], "text": " The possibility that AGI has already been created in another sector of our galaxy."}, {"timestamp": [837.5, 842.5], "text": " Okay, so this is one of my favorite topics. Are we alone in the universe?"}, {"timestamp": [842.5, 848.56], "text": " Functionally, right now, we seem to be alone. We have telescopes and radios all over the place, and we see and"}, {"timestamp": [848.56, 853.44], "text": " hear nothing. Now, we just figured out how to build radios and telescopes, so it's"}, {"timestamp": [853.44, 858.0], "text": " entirely possible that they're just not sophisticated enough to see other"}, {"timestamp": [858.0, 864.2], "text": " civilizations out there. So that's one possibility. Another possibility is that"}, {"timestamp": [864.2, 866.32], "text": " they're hiding from us, right?"}, {"timestamp": [866.32, 872.72], "text": " If they're millions of years more advanced than us, then they can just be stealth or whatever, or hide, you know."}, {"timestamp": [872.72, 875.28], "text": " They can, they can outmaneuver us, right?"}, {"timestamp": [875.28, 886.72], "text": " Because the thing is, is like, with just a little bit more technology, you know, like if you were to drop a, uh, you know, a 60 year old, um,old airplane or a 100-year-old"}, {"timestamp": [886.72, 889.04], "text": " airplane into a battlefield today,"}, {"timestamp": [889.04, 892.0], "text": " it would not have a snowball's chance in hell"}, {"timestamp": [892.0, 894.04], "text": " against a modern jet fighter."}, {"timestamp": [894.04, 897.3], "text": " That's only one century of time difference in technology."}, {"timestamp": [897.3, 900.2], "text": " Now, imagine you extend that out to by several orders"}, {"timestamp": [900.2, 904.24], "text": " of magnitude a million years."}, {"timestamp": [904.24, 907.84], "text": " If there is an alien race out there somewhere"}, {"timestamp": [907.84, 911.2], "text": " that has invented AGI or anything before us,"}, {"timestamp": [911.2, 914.0], "text": " if it's anything more than 100 years ahead of us,"}, {"timestamp": [914.0, 917.92], "text": " we don't have any chance of detecting it or surviving it."}, {"timestamp": [917.92, 921.92], "text": " So the fact that we don't see them means that they could be stealth or avoiding"}, {"timestamp": [921.92, 924.24], "text": " us, or they don't exist. Those are kind of"}, {"timestamp": [924.24, 927.44], "text": " the two primary possibilities."}, {"timestamp": [927.44, 933.12], "text": " Now, that being said, there also might be physics limitations."}, {"timestamp": [933.12, 936.9], "text": " So one thing that a friend of mine many years ago said,"}, {"timestamp": [936.9, 942.12], "text": " he's a great scientist, studies cosmology."}, {"timestamp": [942.12, 944.16], "text": " His profession is material science,"}, {"timestamp": [944.16, 945.04], "text": " but he almost got a minor in cosmology. Anyways profession is material science, but"}, {"timestamp": [948.84, 952.5], "text": " He almost got a minor in cosmology. Anyways, one of the things that he pointed out He's like maybe faster than light travel just isn't physically possible"}, {"timestamp": [952.5, 958.6], "text": " And if that's true, if it's if if faster than light travel is not possible, you know"}, {"timestamp": [958.6, 963.16], "text": " Obviously, there's stories about UFOs that can like teleport or whatever. That's possible"}, {"timestamp": [963.16, 965.38], "text": " Maybe but maybe it isn't."}, {"timestamp": [965.38, 968.02], "text": " And so if faster than light travel is not possible"}, {"timestamp": [968.02, 971.28], "text": " and we're all limited to say 10% the speed of light,"}, {"timestamp": [971.28, 973.24], "text": " that makes the universe really big"}, {"timestamp": [973.24, 975.68], "text": " and really difficult to get around."}, {"timestamp": [975.68, 977.72], "text": " And so if someone's going to take the time"}, {"timestamp": [977.72, 981.24], "text": " to get somewhere else, like to come here from,"}, {"timestamp": [981.24, 984.46], "text": " even our stellar neighborhood,"}, {"timestamp": [984.46, 987.16], "text": " that's a huge time investment."}, {"timestamp": [987.16, 989.84], "text": " So if that's the case, then we're"}, {"timestamp": [989.84, 991.34], "text": " an island in the middle of nowhere,"}, {"timestamp": [991.34, 993.3], "text": " and we might be functionally alone for millions"}, {"timestamp": [993.3, 994.74], "text": " or billions of years."}, {"timestamp": [994.74, 999.1], "text": " So that's my take on what if AGI or something else"}, {"timestamp": [999.1, 1000.38], "text": " exists elsewhere."}, {"timestamp": [1000.38, 1002.9], "text": " Now, that being said, if you wanted"}, {"timestamp": [1002.9, 1008.4], "text": " to make sure that your civilization or that your species"}, {"timestamp": [1008.4, 1012.92], "text": " or that your biome was going to survive, you'd want to put it on a ship that is run by an"}, {"timestamp": [1012.92, 1018.52], "text": " AGI because that AGI could outlive your entire species and manage that ship as it spreads"}, {"timestamp": [1018.52, 1023.52], "text": " across the cosmos, which that's like the panspermia or seed ship theory, right?"}, {"timestamp": [1023.52, 1025.6], "text": " Like maybe life was planted here on earth"}, {"timestamp": [1025.6, 1028.92], "text": " by something else a billion years ago."}, {"timestamp": [1028.92, 1030.46], "text": " But that still begs the question,"}, {"timestamp": [1030.46, 1032.4], "text": " where did it come from and why?"}, {"timestamp": [1032.4, 1034.56], "text": " So, okay, so there we go, we've got that."}, {"timestamp": [1034.56, 1039.0], "text": " Now, Rory, and I apologize if I butcher your name,"}, {"timestamp": [1039.0, 1041.92], "text": " it looks Irish, Rory O'Connor,"}, {"timestamp": [1041.92, 1044.62], "text": " do you have any thoughts as to the similarity"}, {"timestamp": [1044.62, 1045.86], "text": " between some psychedelic"}, {"timestamp": [1045.86, 1049.78], "text": " and meditative states and the videos that are popping up from Stable D and"}, {"timestamp": [1049.78, 1055.38], "text": " Dolly? Does this tell us anything about either? Great question. Okay, so for a"}, {"timestamp": [1055.38, 1061.54], "text": " little bit of background, if you look up videos about like DMT experiences or"}, {"timestamp": [1061.54, 1067.82], "text": " psychedelic experiences, there's some great hilarious videos on Reddit and YouTube."}, {"timestamp": [1068.2, 1073.2], "text": " And people will try and recreate mystical, spiritual,"}, {"timestamp": [1073.3, 1075.04], "text": " and psychedelic experiences."}, {"timestamp": [1075.04, 1078.3], "text": " This is the reason for all the psychedelic art in the 60s"}, {"timestamp": [1078.3, 1081.14], "text": " is people had these deeply profound, meaningful experiences"}, {"timestamp": [1081.14, 1082.54], "text": " that they tried to recreate."}, {"timestamp": [1084.54, 1088.56], "text": " And so they do it in sculpture and painting, whatever."}, {"timestamp": [1088.56, 1091.36], "text": " And so now that people have, excuse me,"}, {"timestamp": [1091.36, 1092.64], "text": " now that people have,"}, {"timestamp": [1092.64, 1094.68], "text": " here, let me just get some water, one second."}, {"timestamp": [1096.28, 1101.28], "text": " Now with digital art, AI art,"}, {"timestamp": [1102.8, 1107.12], "text": " it's really easy to kind of recreate some of those visualizations."}, {"timestamp": [1107.12, 1114.0], "text": " So, one thing that I noticed very early on, and you can, it's called Deep Dream."}, {"timestamp": [1114.0, 1120.72], "text": " So, Deep Dream was like the first attempt at AI image generation, and it's very trippy."}, {"timestamp": [1120.72, 1127.8], "text": " It is super trippy. Just Google it real quick. I'll put it in chat. Google Deep"}, {"timestamp": [1127.8, 1137.92], "text": " Dream. So basically what happens is the way that your optic nerve works is that it starts"}, {"timestamp": [1137.92, 1143.64], "text": " doing things like edge detection and contrast. Hey man, got another message maybe? Someone"}, {"timestamp": [1143.64, 1146.64], "text": " else trying to join? Okay, no, just a buddy of mine."}, {"timestamp": [1147.6, 1150.52], "text": " Okay, anyways, so starting from your optic nerve,"}, {"timestamp": [1150.52, 1155.4], "text": " like literally neural vision starts on the retina,"}, {"timestamp": [1155.4, 1157.32], "text": " optic nerve, optical chiasm,"}, {"timestamp": [1157.32, 1161.54], "text": " whereas that's where your optic nerves cross."}, {"timestamp": [1161.54, 1166.6], "text": " So that way you can get bilateral connection."}, {"timestamp": [1166.6, 1171.04], "text": " And then it all gets projected to your occipital, to the back of your head, where more processing"}, {"timestamp": [1171.04, 1172.04], "text": " happens."}, {"timestamp": [1172.04, 1174.58], "text": " So your optic nerve is not just a wire."}, {"timestamp": [1174.58, 1179.0], "text": " Your optic nerve is actually a deep neural network that begins processing."}, {"timestamp": [1179.0, 1185.44], "text": " So then what happens when you do things like psychedelics or have disorders or hypoxia or whatever, any kind of"}, {"timestamp": [1185.44, 1194.8], "text": " impaired state that modifies the behavior of those receptors, it'll modify how you perceive the world."}, {"timestamp": [1194.8, 1203.28], "text": " And you know, in cases like alcohol, it subdues it, meaning that it's slower. But then there's"}, {"timestamp": [1203.28, 1205.68], "text": " other things that like accelerate it or modify it. And"}, {"timestamp": [1205.68, 1211.76], "text": " so psychedelics are very similar to serotonin, which is one of the primary neurotransmitters"}, {"timestamp": [1211.76, 1218.0], "text": " in your head. And so what happens with these is that the way that your brain represents,"}, {"timestamp": [1218.56, 1221.76], "text": " you're basically saying psychedelics simply decalibrate your model of the world,"}, {"timestamp": [1222.48, 1227.36], "text": " partially. We'll get to that. I've watched a lot of videos about what psychedelics do."}, {"timestamp": [1227.36, 1229.08], "text": " And I also read a book."}, {"timestamp": [1229.08, 1232.22], "text": " I recommend you read DMT, the spirit molecule, very cool."}, {"timestamp": [1233.4, 1237.84], "text": " So what happens is the way that your brain represents,"}, {"timestamp": [1237.84, 1240.8], "text": " so remember I said that intelligence is patterns."}, {"timestamp": [1240.8, 1243.52], "text": " What happens in your optic nerve and your visual processing"}, {"timestamp": [1243.52, 1246.88], "text": " is that your brain, and this is exactly how it works"}, {"timestamp": [1246.88, 1249.88], "text": " in neural networks too, in artificial neural networks,"}, {"timestamp": [1249.88, 1251.8], "text": " is that your brain creates more and more"}, {"timestamp": [1251.8, 1253.68], "text": " abstract representations."}, {"timestamp": [1253.68, 1257.24], "text": " And those abstract representations are then reassembled"}, {"timestamp": [1257.24, 1260.56], "text": " based on like square here or pattern here."}, {"timestamp": [1260.56, 1262.64], "text": " And so like, if you hold out your hand, right,"}, {"timestamp": [1262.64, 1265.52], "text": " you can tell I have four fingers, but it's a pattern."}, {"timestamp": [1265.52, 1268.72], "text": " So what happens is your brain"}, {"timestamp": [1268.72, 1271.68], "text": " or between your optic nerve brain, everything else,"}, {"timestamp": [1271.68, 1273.9], "text": " you see, okay, there's a repeating pattern here."}, {"timestamp": [1273.9, 1275.92], "text": " And so if you're really high on psychedelic,"}, {"timestamp": [1275.92, 1277.8], "text": " you look at a pattern like this"}, {"timestamp": [1277.8, 1280.2], "text": " and you can't make sense of it, right?"}, {"timestamp": [1280.2, 1282.4], "text": " And it's because your brain sees a pattern,"}, {"timestamp": [1282.4, 1284.28], "text": " but it is incoherent and you can't tell"}, {"timestamp": [1284.28, 1285.6], "text": " how many fingers you have or something like that"}, {"timestamp": [1285.6, 1287.6], "text": " And that's why you see"}, {"timestamp": [1287.96, 1290.46], "text": " lots of repeating patterns in psychedelic art"}, {"timestamp": [1291.0, 1295.96], "text": " and it's because your brain can no longer make sense of it because it's like oh I see a pattern and a pattern and a"}, {"timestamp": [1295.96, 1300.28], "text": " Pattern and then all those patterns just kind of clash and then become like prismatic or whatever"}, {"timestamp": [1301.12, 1302.92], "text": " so that is"}, {"timestamp": [1302.92, 1306.14], "text": " But in the early days of AI art"}, {"timestamp": [1306.14, 1309.26], "text": " or AI generated images, they did the same thing,"}, {"timestamp": [1309.26, 1311.7], "text": " but they were trained on, you know,"}, {"timestamp": [1311.7, 1313.5], "text": " images and image recognition."}, {"timestamp": [1313.5, 1317.5], "text": " But what would happen is you'd end up with like smaller parts"}, {"timestamp": [1317.5, 1319.78], "text": " of an image that were like, you know,"}, {"timestamp": [1319.78, 1321.98], "text": " like part of your shirt might look like a dog."}, {"timestamp": [1321.98, 1323.7], "text": " And then you'd have like a lobster over here"}, {"timestamp": [1323.7, 1325.36], "text": " and then something else over here."}, {"timestamp": [1325.36, 1327.64], "text": " And it's because that model was trying"}, {"timestamp": [1327.64, 1329.48], "text": " to identify those patterns,"}, {"timestamp": [1329.48, 1331.28], "text": " and it was misidentifying those patterns."}, {"timestamp": [1331.28, 1335.0], "text": " And it's very similar to what happens on psychedelics."}, {"timestamp": [1335.0, 1338.78], "text": " So, well, I'm probably, I know that I'm oversimplifying,"}, {"timestamp": [1338.78, 1342.66], "text": " but yes, that is the link between psychedelics and AI art."}, {"timestamp": [1342.66, 1346.48], "text": " Now, now that we have models, let's see, first I... Yes, they were fractals."}, {"timestamp": [1346.48, 1352.72], "text": " And so fractals are just patterns, right? It's a mathematical pattern, and so that's why AI art and"}, {"timestamp": [1352.72, 1359.12], "text": " psychedelic art both feature a lot of fractals, because a fractal is a good way to approximate a"}, {"timestamp": [1359.12, 1367.8], "text": " very, very complex pattern. And that's why you see all kinds of fractals in nature, because you can have a very simple genetic program that"}, {"timestamp": [1367.8, 1372.6], "text": " just says, like, cauliflower and broccoli and flowers,"}, {"timestamp": [1372.6, 1377.64], "text": " they all follow fractal patterns or Fibonacci sequences."}, {"timestamp": [1377.64, 1379.8], "text": " And it's because those are very, they're actually"}, {"timestamp": [1379.8, 1380.44], "text": " they're patterns."}, {"timestamp": [1380.44, 1384.4], "text": " They're mathematically determined patterns."}, {"timestamp": [1384.4, 1386.52], "text": " OK, so there's that."}, {"timestamp": [1386.52, 1388.56], "text": " Who are academics working on bringing LLMs"}, {"timestamp": [1388.56, 1390.64], "text": " into the cognitive architectures?"}, {"timestamp": [1390.64, 1392.36], "text": " Mathis Lichtenberger."}, {"timestamp": [1392.36, 1393.6], "text": " Mostly it's me."}, {"timestamp": [1393.6, 1395.48], "text": " There's a few other folks out there."}, {"timestamp": [1396.44, 1399.0], "text": " The thing is, so in terms of people working"}, {"timestamp": [1399.0, 1403.4], "text": " on cognitive architectures, one, so here's the thing is,"}, {"timestamp": [1403.4, 1412.0], "text": " there's this fragmentation in the industry because all the people that are still doing chatbots, like voice chatbots, they're still using, what's the name of that script?"}, {"timestamp": [1412.0, 1419.0], "text": " But they're basically using like old-fashioned, like dialogues, tree scripts."}, {"timestamp": [1419.0, 1425.32], "text": " And a lot of people in voice and chat aren't aware of LLMs yet."}, {"timestamp": [1425.32, 1429.0], "text": " So there's that group over there that is doing chat."}, {"timestamp": [1429.0, 1434.84], "text": " And large language models lend themselves to chat really well."}, {"timestamp": [1434.84, 1437.16], "text": " But they haven't figured out how to use it yet."}, {"timestamp": [1437.16, 1439.4], "text": " And then you've got cognitive architectures,"}, {"timestamp": [1439.4, 1440.56], "text": " which are really old."}, {"timestamp": [1440.56, 1443.48], "text": " They're used in all kinds of places, like rockets."}, {"timestamp": [1443.48, 1447.68], "text": " So for instance, like every"}, {"timestamp": [1447.68, 1451.92], "text": " moon rocket, well maybe not moon rockets, maybe not that old, but more recent rockets,"}, {"timestamp": [1452.72, 1457.52], "text": " they basically have a kind of cognitive architecture where they're constantly,"}, {"timestamp": [1457.52, 1462.96], "text": " you know, because machines have, those big machines have hundreds of thousands of sensors"}, {"timestamp": [1463.6, 1465.08], "text": " and little tiny controls all"}, {"timestamp": [1465.08, 1468.68], "text": " over the place and so you basically have a nervous system and then you have it"}, {"timestamp": [1468.68, 1472.88], "text": " sensing everything that's going on and it creates a hypothesis as to like okay"}, {"timestamp": [1472.88, 1477.34], "text": " you know I'm veering off course because this nozzle over here is out of whack so"}, {"timestamp": [1477.34, 1482.16], "text": " let me do a little experiment to try and get back on course and that's why you"}, {"timestamp": [1482.16, 1485.36], "text": " know rockets either they fly perfectly straight or they just blow up."}, {"timestamp": [1486.88, 1495.2], "text": " I'm getting another beer, hold on. Okay, yeah, so on the one hand, large language models are being"}, {"timestamp": [1495.2, 1508.16], "text": " used, you know, there's a million and one startups using large language models. Cogn cognitive architectures are really old. They're older than a lot of modern AI, right?"}, {"timestamp": [1508.16, 1512.64], "text": " And so what I've tried to do, especially with natural language cognitive architectures,"}, {"timestamp": [1512.64, 1518.72], "text": " is show like, hey, look, you can merge the two. So we'll see what happens. There are more and"}, {"timestamp": [1518.72, 1527.58], "text": " more people that are developing more sophisticated architectural patterns like loops, nested loops, intersecting loops."}, {"timestamp": [1527.58, 1531.28], "text": " So yeah, there's probably people out there."}, {"timestamp": [1531.28, 1534.62], "text": " Blender Bot by Facebook or now Meta,"}, {"timestamp": [1534.62, 1537.6], "text": " that is a very simplified cognitive architecture"}, {"timestamp": [1537.6, 1539.78], "text": " and they're coming out with more sophisticated models"}, {"timestamp": [1539.78, 1540.62], "text": " all the time."}, {"timestamp": [1541.6, 1542.56], "text": " Let's see."}, {"timestamp": [1544.48, 1548.08], "text": " Let's see, Old Zaney says, I think the potential prospect of humans being"}, {"timestamp": [1548.08, 1552.36], "text": " the very first species to set off the whole race of who gets to conquer all of reality"}, {"timestamp": [1552.36, 1558.96], "text": " rather intriguing. Yes, we might be the universal force born. Now there's another theory that"}, {"timestamp": [1558.96, 1567.56], "text": " I like a lot, because when you, when you read about physics, specifically quantum physics and"}, {"timestamp": [1567.56, 1571.96], "text": " cosmology, you realize that like reality is not what it seems. I've got a couple"}, {"timestamp": [1571.96, 1582.68], "text": " of books. Let's see. Here's a handful of books. So holographic universe. This came"}, {"timestamp": [1582.68, 1585.0], "text": " out in 1992, I believe."}, {"timestamp": [1585.2, 1587.92], "text": " So this is basically the idea that like, okay"}, {"timestamp": [1587.92, 1590.48], "text": " maybe the whole universe is a hologram"}, {"timestamp": [1590.48, 1593.04], "text": " which we would call this simulation hypothesis now"}, {"timestamp": [1593.04, 1594.78], "text": " which I've got that book right here."}, {"timestamp": [1594.78, 1597.2], "text": " And then you've got reality is not what it seems."}, {"timestamp": [1597.2, 1600.52], "text": " This is written by an actual physicist."}, {"timestamp": [1600.52, 1603.0], "text": " This one's written by a video game developer"}, {"timestamp": [1603.0, 1606.68], "text": " and this was written by a journalist who I think he was a journalist"}, {"timestamp": [1607.36, 1609.96], "text": " But he focused on like physics and stuff"}, {"timestamp": [1610.48, 1614.44], "text": " Anyways, so you read books like that and then there's another one called grand biocentric design"}, {"timestamp": [1615.12, 1622.9], "text": " I've got a grand biocentric design. I see it right there, and then there's the philosophy of physics, which is by Princeton University Press"}, {"timestamp": [1625.56, 1626.92], "text": " Perfect pronunciation."}, {"timestamp": [1626.92, 1627.76], "text": " Oh, OK, cool."}, {"timestamp": [1631.16, 1631.72], "text": " First day."}, {"timestamp": [1631.72, 1635.72], "text": " OK, I think I'm all caught up, at least mostly caught up."}, {"timestamp": [1635.72, 1639.64], "text": " Yeah, so what is the fundamental nature of reality?"}, {"timestamp": [1639.64, 1642.76], "text": " So I comically proposed Shapiro's law,"}, {"timestamp": [1642.76, 1650.0], "text": " which is that all conversations about artificial intelligence are ultimately questions about quantum physics or the fundamental nature of reality."}, {"timestamp": [1650.0, 1654.0], "text": " So here we are, Shapiro's law in full effect."}, {"timestamp": [1654.0, 1667.36], "text": " When you start to ask questions about consciousness, intelligence, sentience, where did we come from and why, why is it that we believe what we do? Like, when we have the idea that we can create a machine,"}, {"timestamp": [1667.36, 1668.68], "text": " and these stories are very old."}, {"timestamp": [1668.68, 1672.4], "text": " So there's a book that my fiancee loves."}, {"timestamp": [1672.4, 1674.08], "text": " It's called, Gods and Robots,"}, {"timestamp": [1674.08, 1677.72], "text": " which talks about how through fiction,"}, {"timestamp": [1677.72, 1679.96], "text": " we have explored our own humanity"}, {"timestamp": [1679.96, 1682.86], "text": " in the guise of machines since forever."}, {"timestamp": [1682.86, 1689.44], "text": " So Talos was built out of bronze,"}, {"timestamp": [1689.44, 1695.84], "text": " but is a person, a fully realized demigod, basically."}, {"timestamp": [1695.84, 1700.56], "text": " So when we, as humans, imagine that we can create something"}, {"timestamp": [1700.56, 1703.72], "text": " like ourselves, it makes us ask questions"}, {"timestamp": [1703.72, 1704.84], "text": " about who and what we are."}, {"timestamp": [1704.84, 1706.56], "text": " What does it mean to be human or"}, {"timestamp": [1706.56, 1711.76], "text": " alive or conscious or Dave or whatever? And so when we ask"}, {"timestamp": [1711.76, 1715.92], "text": " those questions that the so this is a philosophical term"}, {"timestamp": [1715.92, 1722.0], "text": " that entails it. You cannot separate what it what we are"}, {"timestamp": [1722.0, 1724.88], "text": " from the fundamental fabric of reality and so we you keep"}, {"timestamp": [1724.88, 1726.56], "text": " zooming out and you say,"}, {"timestamp": [1726.56, 1730.12], "text": " okay, well, I'm conscious, but this cup isn't,"}, {"timestamp": [1730.12, 1731.52], "text": " or is it, right?"}, {"timestamp": [1731.52, 1732.6], "text": " Maybe it is conscious,"}, {"timestamp": [1732.6, 1736.36], "text": " maybe it's because it has the ability to interact"}, {"timestamp": [1736.36, 1740.2], "text": " and maybe it has an experience, so that's panpsychism."}, {"timestamp": [1740.2, 1742.52], "text": " But right now what's in vogue is materialism."}, {"timestamp": [1742.52, 1746.12], "text": " I actually put a post put a poll on Twitter."}, {"timestamp": [1746.12, 1749.24], "text": " Materialism is by far the most popular thing,"}, {"timestamp": [1749.24, 1751.4], "text": " followed by simulation hypothesis, actually,"}, {"timestamp": [1751.4, 1753.68], "text": " which of course, simulation hypothesis says,"}, {"timestamp": [1753.68, 1755.24], "text": " we're all just running in a simulation,"}, {"timestamp": [1755.24, 1756.6], "text": " but that begs the question, who's"}, {"timestamp": [1756.6, 1759.56], "text": " running the simulation, on what hardware, and why?"}, {"timestamp": [1759.56, 1762.12], "text": " So that just leaves you with more questions than answers."}, {"timestamp": [1762.12, 1766.4], "text": " Anyways, sorry, I was getting distracted. The question"}, {"timestamp": [1766.4, 1775.36], "text": " was, oh yeah, fundamental nature of reality. So you start asking questions about the fundamental."}, {"timestamp": [1775.36, 1782.24], "text": " Old Zany is like, aha, so someone's drinking and it's not me. But yeah, so AI conversations"}, {"timestamp": [1782.24, 1786.08], "text": " always end up, they either turn religious, which again,"}, {"timestamp": [1786.08, 1788.86], "text": " gods and robots, that has been a conversation"}, {"timestamp": [1788.86, 1790.54], "text": " for literally thousands of years."}, {"timestamp": [1791.54, 1794.8], "text": " But then when you add in modern cosmology"}, {"timestamp": [1794.8, 1799.8], "text": " and quantum physics, you end up with more questions like,"}, {"timestamp": [1800.24, 1802.0], "text": " okay, where does consciousness come from?"}, {"timestamp": [1802.0, 1803.88], "text": " Because here's the thing,"}, {"timestamp": [1803.88, 1807.88], "text": " one of the most important implications of materialism... Okay, so materialism,"}, {"timestamp": [1807.88, 1813.56], "text": " let me just tell you, materialism is very simple. It is defined as, you say, the"}, {"timestamp": [1813.56, 1819.04], "text": " only thing that's real is matter and energy. Everything, every phenomenon in"}, {"timestamp": [1819.04, 1826.8], "text": " the universe arises from matter and energy. That's what materialism says. If that's true, if materialism"}, {"timestamp": [1826.8, 1834.16], "text": " is true, then our consciousness arises from the matter of our brain and the energy therein."}, {"timestamp": [1835.44, 1841.84], "text": " But that just means that consciousness is a matter of biochemical calculation, which in theory,"}, {"timestamp": [1841.84, 1847.0], "text": " anything with a sufficient amount of biochemical calculation could therefore be conscious."}, {"timestamp": [1847.0, 1854.0], "text": " And if biochemical calculation is enough to create consciousness, then what about silicon-based computation?"}, {"timestamp": [1854.0, 1857.0], "text": " Could a silicon thing, you know, could a computer be conscious?"}, {"timestamp": [1857.0, 1860.0], "text": " Or a tree, or a mushroom, or a whole forest, right?"}, {"timestamp": [1860.0, 1868.0], "text": " If biochemical computation is the only thing required for consciousness, as implied by materialism,"}, {"timestamp": [1868.0, 1873.0], "text": " then anything could be conscious, whether or not it's made of organic matter."}, {"timestamp": [1873.0, 1880.0], "text": " Now, there's no like, okay, so what gives rise to our subjective experience, though?"}, {"timestamp": [1880.0, 1886.12], "text": " Like, at what point does something go from like, you know, an inert nail file to something that has eyes and a subjective"}, {"timestamp": [1886.12, 1889.2], "text": " experience of what it's like to be? And therein lies the"}, {"timestamp": [1889.2, 1893.84], "text": " question. And this, of course, became a big, big hullabaloo a"}, {"timestamp": [1893.84, 1897.96], "text": " few months ago, when that when when that Google engineer"}, {"timestamp": [1897.96, 1900.5], "text": " claimed that lambda was sentient. And of course, like it"}, {"timestamp": [1900.5, 1904.08], "text": " was designed to model human conversation. So of course, it"}, {"timestamp": [1904.08, 1905.44], "text": " said it was sentient."}, {"timestamp": [1905.44, 1907.52], "text": " Like a human is never going to say, I am not sentient."}, {"timestamp": [1907.52, 1913.84], "text": " I mean, I could type that into chat, but it would be a lie."}, {"timestamp": [1913.84, 1917.48], "text": " Okay."}, {"timestamp": [1917.48, 1918.48], "text": " Let's see."}, {"timestamp": [1918.48, 1922.84], "text": " Let me scroll up because some questions have come on."}, {"timestamp": [1922.84, 1924.88], "text": " All right."}, {"timestamp": [1924.88, 1929.0], "text": " For species, yes, that's what I was on. Speaking of extraterrestrials,"}, {"timestamp": [1929.0, 1934.8], "text": " there is also the current UAP slash UFO phenomenon that Congress has been tasked to investigate"}, {"timestamp": [1934.8, 1940.92], "text": " based on some military footage of unidentified objects. Yes, that keeps coming in cycles,"}, {"timestamp": [1940.92, 1946.16], "text": " though. It almost feels like, you feels like the US military and Congress"}, {"timestamp": [1946.16, 1951.08], "text": " are just setting the stage to release something later on."}, {"timestamp": [1951.08, 1953.2], "text": " I don't know."}, {"timestamp": [1953.2, 1959.92], "text": " Thing is, I think the way that someone said it"}, {"timestamp": [1959.92, 1964.24], "text": " on one of these press releases was pretty,"}, {"timestamp": [1964.24, 1966.28], "text": " they said, like, yes,"}, {"timestamp": [1966.28, 1971.28], "text": " even the Air Force says that 99.8% of all reports"}, {"timestamp": [1971.56, 1974.26], "text": " can be explained by drones, airplanes,"}, {"timestamp": [1974.26, 1976.08], "text": " natural phenomenon, whatever,"}, {"timestamp": [1976.08, 1980.72], "text": " but they acknowledge that there are 2% or 0.2%"}, {"timestamp": [1980.72, 1983.78], "text": " or however many that cannot be explained."}, {"timestamp": [1984.64, 1989.2], "text": " And there are more and more pilots and former astronauts"}, {"timestamp": [1989.2, 1990.96], "text": " and all kinds of people coming forward saying,"}, {"timestamp": [1990.96, 1993.58], "text": " yeah, I've seen a whole bunch of stuff that I can't explain"}, {"timestamp": [1993.58, 1996.06], "text": " and that we were told not to ever talk about."}, {"timestamp": [1997.04, 2000.16], "text": " But they're talking about it, so either they're lying,"}, {"timestamp": [2000.16, 2001.5], "text": " which is always a possibility,"}, {"timestamp": [2001.5, 2004.02], "text": " like people want their 15 minutes of fame."}, {"timestamp": [2005.64, 2010.72], "text": " So either they're lying, they're exaggerating, or they're telling the truth in part or in whole"}, {"timestamp": [2011.2, 2016.64], "text": " But the thing is is it is a leap to go from and as much as I want to believe"}, {"timestamp": [2016.92, 2021.16], "text": " That there's aliens and super advanced, you know, godlike entities out there"}, {"timestamp": [2022.14, 2025.84], "text": " My rational scientific brain is like, okay, Occam's razor, though,"}, {"timestamp": [2025.84, 2031.52], "text": " like there might be a simpler explanation. So like, I think it was on the Lex Friedman podcast,"}, {"timestamp": [2031.52, 2036.96], "text": " he was in, he was interviewing a major of the Air Force who had retired, but they were saying that"}, {"timestamp": [2036.96, 2041.6], "text": " they were training off the coast of Washington, DC. And for like eight days or something, there"}, {"timestamp": [2041.6, 2046.4], "text": " were like, there were unidentified aircraft out there or, you know,"}, {"timestamp": [2046.4, 2054.24], "text": " unidentified objects that they couldn't explain. And then they went away and they're like, okay,"}, {"timestamp": [2054.24, 2058.96], "text": " so he was talking about it from a military perspective, which is, okay, let's assume"}, {"timestamp": [2058.96, 2062.72], "text": " that they are military aircraft and that they're spying on us, right? Which is,"}, {"timestamp": [2063.68, 2065.76], "text": " if you're trained in military intelligence,"}, {"timestamp": [2065.76, 2070.36], "text": " that's what you're going to immediately assume. And then you'd be worried because it's like,"}, {"timestamp": [2070.36, 2075.32], "text": " okay, someone has drones out here that, that, that, or, or other aircraft that we don't"}, {"timestamp": [2075.32, 2079.04], "text": " know who put them there. And it could have been our own people, right? Like, let's just,"}, {"timestamp": [2079.04, 2086.7], "text": " as a thought experiment, let's imagine that these are human-made craft of some sort. And so there's a whole bunch of drones out there"}, {"timestamp": [2086.7, 2091.7], "text": " that are watching US Air Force pilots do maneuvers."}, {"timestamp": [2091.94, 2093.18], "text": " Who put them there?"}, {"timestamp": [2093.18, 2096.18], "text": " It could be NSA, it could be CIA, it could be adversaries."}, {"timestamp": [2096.18, 2097.82], "text": " It could be North Korea for all we know."}, {"timestamp": [2097.82, 2098.66], "text": " Probably not,"}, {"timestamp": [2098.66, 2100.5], "text": " because they don't have that great of an economy."}, {"timestamp": [2100.5, 2101.78], "text": " Also probably not Russia"}, {"timestamp": [2101.78, 2104.1], "text": " because their economy is the size of Texas."}, {"timestamp": [2104.1, 2105.64], "text": " Well, less now because of sanctions."}, {"timestamp": [2105.64, 2110.54], "text": " But yeah, so who put it there, right?"}, {"timestamp": [2110.54, 2113.22], "text": " But you can always reach for a magic answer, right?"}, {"timestamp": [2113.22, 2119.16], "text": " And again, this is as much as I love magical explanations, my rational brain is like, yeah,"}, {"timestamp": [2119.16, 2122.92], "text": " but it would be real embarrassing if you reach for the magic answer and it's something mundane."}, {"timestamp": [2122.92, 2126.28], "text": " But so the magic answer is there's super advanced aliens"}, {"timestamp": [2126.28, 2127.6], "text": " out there that are watching us"}, {"timestamp": [2127.6, 2129.44], "text": " because they're trying to like police us"}, {"timestamp": [2129.44, 2131.12], "text": " and make sure we don't blow ourselves"}, {"timestamp": [2131.12, 2132.44], "text": " off the face of the planet."}, {"timestamp": [2132.44, 2133.42], "text": " There are stories about that."}, {"timestamp": [2133.42, 2136.28], "text": " So I saw one documentary years ago"}, {"timestamp": [2136.28, 2139.08], "text": " where a general or someone"}, {"timestamp": [2139.08, 2143.08], "text": " or reportedly saw a video of a UFO,"}, {"timestamp": [2143.08, 2147.12], "text": " like basically laser the warhead out"}, {"timestamp": [2147.12, 2148.56], "text": " of the tip of a missile."}, {"timestamp": [2148.56, 2152.52], "text": " And he was like, what the hell are you doing?"}, {"timestamp": [2152.52, 2155.2], "text": " And they're like, that wasn't us."}, {"timestamp": [2155.2, 2156.28], "text": " It's hearsay, right?"}, {"timestamp": [2156.28, 2157.84], "text": " It was on probably History Channel,"}, {"timestamp": [2157.84, 2159.36], "text": " so who knows if it was real."}, {"timestamp": [2159.36, 2164.04], "text": " Anyways, so yeah, there's always the possibility of UFOs, UAPs,"}, {"timestamp": [2164.04, 2164.88], "text": " so on and so forth."}, {"timestamp": [2164.88, 2166.76], "text": " But it goes back to what is the fundamental nature"}, {"timestamp": [2166.76, 2167.68], "text": " of reality."}, {"timestamp": [2167.68, 2170.36], "text": " If the grand biocentric design is true,"}, {"timestamp": [2170.36, 2172.32], "text": " then the universe was in superposition"}, {"timestamp": [2172.32, 2175.9], "text": " until it found earth in the quantum field,"}, {"timestamp": [2175.9, 2179.24], "text": " the universal quantum wave function of reality"}, {"timestamp": [2179.24, 2183.12], "text": " and created earth in order to have conscious life."}, {"timestamp": [2183.12, 2184.88], "text": " So that's also called the strong anthropic principle,"}, {"timestamp": [2184.88, 2191.62], "text": " which is the observation that the universe seems finely calibrated to create us here. So grand biocentric. I'll put that in chat"}, {"timestamp": [2192.68, 2194.08], "text": " grand"}, {"timestamp": [2194.08, 2196.68], "text": " biocentric design and or"}, {"timestamp": [2198.2, 2199.8], "text": " Strong and"}, {"timestamp": [2199.8, 2204.48], "text": " Anthropic principle. Okay. So let me go back up. I've got some more questions"}, {"timestamp": [2203.5, 2205.5], "text": " Okay, so let me go back up. Got some more questions."}, {"timestamp": [2205.5, 2214.36], "text": " All right, old zany, presume we perfectly align AGI and we reach a stable utopia, Q"}, {"timestamp": [2214.36, 2219.32], "text": " continuum becomes a thing, boredom the last frontier."}, {"timestamp": [2219.32, 2229.04], "text": " Okay, so Q continuum is from Star Trek. So the Q are a race of the final evolution of intelligent life,"}, {"timestamp": [2229.04, 2232.92], "text": " basically, where they come to understand the universe so well"}, {"timestamp": [2232.92, 2236.16], "text": " that with the flip of or the snap of their fingers,"}, {"timestamp": [2236.16, 2238.12], "text": " they can change anything that they want."}, {"timestamp": [2238.12, 2240.44], "text": " They can go forward, backward in time."}, {"timestamp": [2240.44, 2241.76], "text": " They can modify timelines."}, {"timestamp": [2241.76, 2243.24], "text": " They can create things."}, {"timestamp": [2243.24, 2245.26], "text": " They can erase memories, whatever they want to do"}, {"timestamp": [2245.32, 2249.9], "text": " So basically they're they're basically like Loki or at least the the one that recurs throughout"}, {"timestamp": [2250.22, 2253.86], "text": " The show is basically like Loki which is he's kind of a trickster God"}, {"timestamp": [2254.5, 2257.12], "text": " But he always has a point. He always has a moral lesson"}, {"timestamp": [2257.94, 2267.48], "text": " So the idea the the problem here that old zany is proposing is let's's imagine that we create AGI, it stabilizes humanity,"}, {"timestamp": [2267.48, 2271.74], "text": " and we have life expectancies of billions of years."}, {"timestamp": [2271.74, 2274.04], "text": " And this is a thought that has crossed my mind."}, {"timestamp": [2274.04, 2277.28], "text": " Because if we solve aging, we solve all medical problems,"}, {"timestamp": [2277.28, 2279.34], "text": " we stabilize the planet,"}, {"timestamp": [2279.34, 2281.2], "text": " the sun's gonna burn out eventually."}, {"timestamp": [2281.2, 2282.56], "text": " Okay, what do we do by then?"}, {"timestamp": [2282.56, 2285.18], "text": " Well, if we're that smart, that powerful,"}, {"timestamp": [2285.18, 2286.44], "text": " and we live that long,"}, {"timestamp": [2286.44, 2289.04], "text": " then we just hop on a cruise ship and go to the next star."}, {"timestamp": [2289.04, 2291.14], "text": " Because even if it takes 4 million years"}, {"timestamp": [2291.14, 2292.24], "text": " to get to the next star,"}, {"timestamp": [2292.24, 2293.88], "text": " we have a lifespan of billions of years."}, {"timestamp": [2293.88, 2295.8], "text": " So it's just gonna be a long cruise."}, {"timestamp": [2295.8, 2298.0], "text": " But we need novelty."}, {"timestamp": [2299.24, 2302.62], "text": " Part of what makes us human is a need for novelty."}, {"timestamp": [2302.62, 2303.92], "text": " And we actually also need stress."}, {"timestamp": [2303.92, 2309.12], "text": " There's an optimal level of stress that we need in order to be happy, healthy, otherwise our"}, {"timestamp": [2309.12, 2312.84], "text": " brain starts to deteriorate. It literally starts to deteriorate, which is one of"}, {"timestamp": [2312.84, 2316.56], "text": " the reasons that the pandemic was so hard because you had a combination"}, {"timestamp": [2316.56, 2321.84], "text": " of boredom and stress. And so like a lot of people, myself included, have had to"}, {"timestamp": [2321.84, 2326.68], "text": " recover some gray matter from the pandemic."}, {"timestamp": [2326.68, 2327.64], "text": " So yeah, what do we do?"}, {"timestamp": [2327.64, 2328.96], "text": " I don't know."}, {"timestamp": [2328.96, 2330.96], "text": " There was one work of fiction."}, {"timestamp": [2330.96, 2335.0], "text": " I think it was based on the myth of Atlantis, actually."}, {"timestamp": [2335.0, 2336.76], "text": " So the myth of Atlantis was that there"}, {"timestamp": [2336.76, 2340.4], "text": " was these basically fountains of youth."}, {"timestamp": [2340.4, 2343.2], "text": " But what they said, or it was rubies or something."}, {"timestamp": [2343.2, 2344.96], "text": " What they said in the story, and this"}, {"timestamp": [2344.96, 2345.02], "text": " might have been a translation of the original, but I don't think it was. But what they said, or it was rubies or something, what they said in the story,"}, {"timestamp": [2345.02, 2346.8], "text": " and this might've been a translation of the original,"}, {"timestamp": [2346.8, 2348.6], "text": " but I don't think it was."}, {"timestamp": [2348.6, 2349.92], "text": " But what they said in the story was that"}, {"timestamp": [2349.92, 2351.54], "text": " after several hundred years,"}, {"timestamp": [2351.54, 2352.92], "text": " people would get bored of living"}, {"timestamp": [2352.92, 2355.0], "text": " and just let themselves die."}, {"timestamp": [2355.0, 2357.52], "text": " So that's actually a possibility is one thing."}, {"timestamp": [2357.52, 2359.66], "text": " So you think about like bees."}, {"timestamp": [2361.06, 2362.56], "text": " My fiance used to be a beekeeper."}, {"timestamp": [2362.56, 2364.56], "text": " So she has all kinds of stories about bees."}, {"timestamp": [2364.56, 2367.4], "text": " So what bees do is the older that they get,"}, {"timestamp": [2367.4, 2369.08], "text": " the more dangerous job they take on."}, {"timestamp": [2369.08, 2372.2], "text": " So the last job a bee takes on in its lifestyle,"}, {"timestamp": [2372.2, 2374.2], "text": " life cycle is foraging."}, {"timestamp": [2374.2, 2376.28], "text": " It goes further and further from the nest,"}, {"timestamp": [2376.28, 2378.48], "text": " bringing back pollen because there's a good chance"}, {"timestamp": [2378.48, 2379.92], "text": " it's not gonna come back."}, {"timestamp": [2379.92, 2381.96], "text": " And you don't want the young healthy bees to do that,"}, {"timestamp": [2381.96, 2383.64], "text": " you want the veterans to go out"}, {"timestamp": [2383.64, 2387.08], "text": " because their wings are shredded, they're tired."}, {"timestamp": [2387.08, 2389.06], "text": " They might get eaten, they might get lost."}, {"timestamp": [2389.06, 2390.7], "text": " They might get caught in a storm,"}, {"timestamp": [2390.7, 2392.78], "text": " but you don't want all your youthful workers to do that."}, {"timestamp": [2392.78, 2395.66], "text": " You want the young strong ones to defend the hive."}, {"timestamp": [2395.66, 2397.1], "text": " That's I think defending the hive"}, {"timestamp": [2397.1, 2398.54], "text": " is the second thing that they do."}, {"timestamp": [2398.54, 2399.38], "text": " The first thing that they do,"}, {"timestamp": [2399.38, 2401.98], "text": " cause also you care about parasite load."}, {"timestamp": [2401.98, 2403.14], "text": " So the first thing that they do"}, {"timestamp": [2403.14, 2405.52], "text": " is they tend to the hive itself."}, {"timestamp": [2405.52, 2407.44], "text": " They help, I think they care for young"}, {"timestamp": [2407.44, 2408.72], "text": " and help build the hive"}, {"timestamp": [2408.72, 2410.08], "text": " because they don't have mites or anything yet."}, {"timestamp": [2410.08, 2413.02], "text": " And then in the middle of their life,"}, {"timestamp": [2413.02, 2416.42], "text": " they defend the hive because they're still strong,"}, {"timestamp": [2416.42, 2419.4], "text": " but they've been replaced by a new crop."}, {"timestamp": [2419.4, 2422.34], "text": " And then the last thing that they do is they go foraging."}, {"timestamp": [2422.34, 2424.48], "text": " So one thing that I think that might happen"}, {"timestamp": [2424.48, 2426.6], "text": " is that as humans age"}, {"timestamp": [2426.6, 2430.68], "text": " and we get bored, we'll probably take on riskier and riskier jobs,"}, {"timestamp": [2430.68, 2433.12], "text": " such as exploring new galaxies, right?"}, {"timestamp": [2433.12, 2435.52], "text": " Because let's say you go through a wormhole or something,"}, {"timestamp": [2435.52, 2438.52], "text": " there's a good chance that you only come out as atoms on the other side, right?"}, {"timestamp": [2438.52, 2441.44], "text": " And it's like, well, you know, I've been around for 10 million years."}, {"timestamp": [2441.44, 2443.6], "text": " I, you know, I can take a risk or two."}, {"timestamp": [2443.6, 2448.16], "text": " The other thing, though, there's another work of fiction, and I can't remember what it was called, but it basically said"}, {"timestamp": [2448.16, 2453.48], "text": " that it posited that one thing that humans will do once we live a long time is that we'll"}, {"timestamp": [2453.48, 2458.76], "text": " actually become really lazy and complacent and that we'll become very risk averse. They"}, {"timestamp": [2458.76, 2462.96], "text": " will be like, no, because the longer you live, the more entitled to life that you feel. So"}, {"timestamp": [2462.96, 2465.04], "text": " I don't know which way it's going to go."}, {"timestamp": [2465.04, 2465.64], "text": " Let's see."}, {"timestamp": [2465.64, 2469.0], "text": " Sum Rando says, on the fundamental nature of reality,"}, {"timestamp": [2469.0, 2471.48], "text": " what do you think about the Planck length?"}, {"timestamp": [2471.48, 2475.5], "text": " Is reality basically several orders of magnitude"}, {"timestamp": [2475.5, 2477.84], "text": " below modern quantum physics, and we just"}, {"timestamp": [2477.84, 2480.76], "text": " have no way of probing anything approaching that scale?"}, {"timestamp": [2480.76, 2487.76], "text": " So that idea of the voxel is actually, was it in this book? Or I think"}, {"timestamp": [2487.76, 2493.48], "text": " it was actually in simulation hypothesis. So basically, some folks think that Planck"}, {"timestamp": [2493.48, 2498.54], "text": " length and Planck time. So for anyone who doesn't know, Planck length and Planck time"}, {"timestamp": [2498.54, 2505.48], "text": " are the smallest, what we think of right now as the smallest length of measurable distance and the smallest"}, {"timestamp": [2505.48, 2508.16], "text": " length of measurable time, that everything below that"}, {"timestamp": [2508.16, 2512.72], "text": " is like stepped or gated or whatever."}, {"timestamp": [2512.72, 2515.64], "text": " So the short answer is, yes, some people"}, {"timestamp": [2515.64, 2519.56], "text": " think that Planck length and Planck time, Planck time,"}, {"timestamp": [2519.56, 2521.36], "text": " Planck length, Planck time."}, {"timestamp": [2521.36, 2522.16], "text": " There we go."}, {"timestamp": [2522.16, 2523.64], "text": " I said it right."}, {"timestamp": [2523.64, 2526.24], "text": " Those are basically like the fundamental voxels"}, {"timestamp": [2526.24, 2530.16], "text": " or time steps of the universal simulation."}, {"timestamp": [2530.16, 2532.12], "text": " I don't know if I agree with that interpretation,"}, {"timestamp": [2532.12, 2533.08], "text": " but it's a possibility."}, {"timestamp": [2533.08, 2535.3], "text": " Again, like the smarter you are, the more you know,"}, {"timestamp": [2535.3, 2536.92], "text": " the more possibilities you see"}, {"timestamp": [2536.92, 2538.76], "text": " and the fewer answers you have."}, {"timestamp": [2538.76, 2540.4], "text": " That's how you know you're doing science right,"}, {"timestamp": [2540.4, 2543.16], "text": " if you end up with more questions than answers."}, {"timestamp": [2543.16, 2544.0], "text": " Let's see."}, {"timestamp": [2545.44, 2548.4], "text": " Let's see."}, {"timestamp": [2545.44, 2550.0], "text": " Let's see. Oh yeah, so Adrian"}, {"timestamp": [2548.4, 2551.68], "text": " replied about the UFOs saying it's"}, {"timestamp": [2550.0, 2552.88], "text": " probably just the government agencies to"}, {"timestamp": [2551.68, 2555.84], "text": " justify their funding."}, {"timestamp": [2552.88, 2557.44], "text": " It's entirely possible. They"}, {"timestamp": [2555.84, 2559.36], "text": " might be making stuff up."}, {"timestamp": [2557.44, 2561.12], "text": " Let's see. Agreed unobserved UFOs,"}, {"timestamp": [2559.36, 2562.24], "text": " the closest galactic civilization that"}, {"timestamp": [2561.12, 2565.92], "text": " makes flying aircraft"}, {"timestamp": [2562.24, 2569.74], "text": " is obviously us. Most obviously, not guaranteed, so I'll say like, likely us,"}, {"timestamp": [2569.74, 2571.04], "text": " but there could be others."}, {"timestamp": [2571.04, 2573.0], "text": " On the other hand, probabilistically,"}, {"timestamp": [2573.0, 2574.3], "text": " there could be others, right?"}, {"timestamp": [2574.3, 2575.96], "text": " Let's see."}, {"timestamp": [2575.96, 2579.34], "text": " Rory O'Connor, I'm really interested in translating AI art"}, {"timestamp": [2579.34, 2581.88], "text": " and video into something that supports more narrative structure."}, {"timestamp": [2582.66, 2587.64], "text": " What kind of hurdles do we have to overcome before 13 year olds win Oscars with laptops?"}, {"timestamp": [2587.64, 2589.36], "text": " That's a great question."}, {"timestamp": [2589.36, 2591.86], "text": " So as an author, actually my first novel"}, {"timestamp": [2591.86, 2594.12], "text": " is with a professional editor right now."}, {"timestamp": [2594.12, 2597.0], "text": " I've got about two weeks before I get it back."}, {"timestamp": [2597.0, 2598.8], "text": " So excited."}, {"timestamp": [2598.8, 2602.06], "text": " So the longer a story you create,"}, {"timestamp": [2602.06, 2605.56], "text": " the more cycles it takes to write it."}, {"timestamp": [2605.56, 2611.16], "text": " So a good writer could sit down and hammer out a thousand word or a 500 to a thousand"}, {"timestamp": [2611.16, 2614.14], "text": " word flash fiction in an afternoon."}, {"timestamp": [2614.14, 2619.08], "text": " My novel is 94,000 words and I've been working on it for three and a half years."}, {"timestamp": [2619.08, 2625.72], "text": " The reason that stories take so long to make and take so many cycles and revisions is because you're inventing"}, {"timestamp": [2625.72, 2632.6], "text": " a whole world. And you're also simulating, you know, I have six POVs and then dozens"}, {"timestamp": [2632.6, 2636.68], "text": " more characters. So you have to go through and simulate a whole bunch of other human"}, {"timestamp": [2636.68, 2640.76], "text": " minds. You have to simulate a whole other world. You have to do, you have to run so"}, {"timestamp": [2640.76, 2646.12], "text": " many calculations in your head. It takes a long time to make sure you do that right."}, {"timestamp": [2646.12, 2649.64], "text": " But if a computer can run a simulation,"}, {"timestamp": [2649.64, 2652.6], "text": " we have Unreal Engine, we have Blender,"}, {"timestamp": [2652.6, 2654.96], "text": " we have all kinds of other simulation engines"}, {"timestamp": [2654.96, 2656.48], "text": " that can keep track of all that."}, {"timestamp": [2656.48, 2661.68], "text": " And if, for instance, my cognitive architecture work,"}, {"timestamp": [2661.68, 2663.84], "text": " natural language cognitive architecture and Meragi"}, {"timestamp": [2663.84, 2665.82], "text": " come out, we can have artificial characters"}, {"timestamp": [2665.82, 2668.06], "text": " that are fully realized with personalities"}, {"timestamp": [2668.06, 2670.14], "text": " and belief structures and everything."}, {"timestamp": [2670.14, 2675.06], "text": " So step one is integrate simulation,"}, {"timestamp": [2675.06, 2677.18], "text": " not just of worlds, but of people,"}, {"timestamp": [2677.18, 2679.98], "text": " because getting good, compelling characters"}, {"timestamp": [2679.98, 2683.18], "text": " is one of the absolute hardest parts"}, {"timestamp": [2683.18, 2685.2], "text": " of writing compelling stories. If you really love a story, so one of the absolute hardest parts of writing compelling stories."}, {"timestamp": [2686.2, 2688.6], "text": " If you really love a story,"}, {"timestamp": [2688.6, 2693.5], "text": " so one of my favorite games of all time is Mass Effect and Bioware."}, {"timestamp": [2694.4, 2696.6], "text": " Bioware is the company, sorry, Mass Effect and Dragon Age."}, {"timestamp": [2696.6, 2699.2], "text": " They're both made by Bioware, and I got my fianc\u00e9 into it."}, {"timestamp": [2699.6, 2700.9], "text": " She's into Dragon Age."}, {"timestamp": [2700.9, 2702.9], "text": " She's reading the books, playing all the games."}, {"timestamp": [2702.9, 2703.7], "text": " It's great."}, {"timestamp": [2705.12, 2709.0], "text": " A lot of people love Dragon Age and or Mass Effect."}, {"timestamp": [2709.0, 2711.7], "text": " And it's because of the great characters."}, {"timestamp": [2711.7, 2714.62], "text": " They're very character centric stories."}, {"timestamp": [2714.62, 2717.8], "text": " Now there's the grand epic, you know, framing,"}, {"timestamp": [2717.8, 2720.08], "text": " you know, it's very like Tolkienian"}, {"timestamp": [2721.24, 2723.54], "text": " is what's going on in the background."}, {"timestamp": [2723.54, 2724.82], "text": " But what makes those game,"}, {"timestamp": [2724.82, 2726.72], "text": " what makes people fall in love with those games"}, {"timestamp": [2726.72, 2728.52], "text": " are the characters."}, {"timestamp": [2728.52, 2730.62], "text": " You have these really compelling characters"}, {"timestamp": [2730.62, 2731.82], "text": " and they clash with each other"}, {"timestamp": [2731.82, 2735.76], "text": " and they have their own emotions and desires"}, {"timestamp": [2735.76, 2737.52], "text": " and visions and everything."}, {"timestamp": [2737.52, 2742.52], "text": " And so getting the ability to simulate hyper-realistic"}, {"timestamp": [2743.12, 2744.56], "text": " or actually not even hyper-realistic,"}, {"timestamp": [2744.56, 2747.36], "text": " but like"}, {"timestamp": [2752.4, 2758.64], "text": " dramatic characters. That is going to be one of the hardest things to do, because you put characters in a simulated world and the story will tell itself, right? But as an author, as someone"}, {"timestamp": [2758.64, 2765.76], "text": " creating a story, you know, you look at the biggest movies of the year, right? The ones that sell a billion dollars worth of tickets,"}, {"timestamp": [2765.76, 2768.64], "text": " which is usually MCU,"}, {"timestamp": [2768.64, 2770.28], "text": " well, pretty much anything by Disney lately"}, {"timestamp": [2770.28, 2773.5], "text": " because they have learned to churn out billion dollar movies"}, {"timestamp": [2773.5, 2776.24], "text": " but then other things like Dune, right?"}, {"timestamp": [2776.24, 2780.4], "text": " And so there's a lot of philosophy and mythology"}, {"timestamp": [2780.4, 2781.92], "text": " that goes into good characters"}, {"timestamp": [2781.92, 2784.86], "text": " and now we can put psychology to it as well."}, {"timestamp": [2785.8, 2787.92], "text": " And so, but getting those good characters"}, {"timestamp": [2787.92, 2789.84], "text": " and putting them, like,"}, {"timestamp": [2789.84, 2794.32], "text": " you can have relatively tropish settings, right?"}, {"timestamp": [2794.32, 2796.28], "text": " Cause like Dragon Age, it's like, okay,"}, {"timestamp": [2796.28, 2798.8], "text": " there's a basically a zombie horde"}, {"timestamp": [2798.8, 2800.76], "text": " or dragons or demons attacking."}, {"timestamp": [2800.76, 2802.42], "text": " And it's like, okay, sure, whatever, right?"}, {"timestamp": [2802.42, 2803.78], "text": " You know, the big bad is a demon."}, {"timestamp": [2803.78, 2806.16], "text": " Nothing particularly compelling about that."}, {"timestamp": [2806.16, 2808.4], "text": " But the part that's compelling about that story"}, {"timestamp": [2808.4, 2809.6], "text": " is the characters."}, {"timestamp": [2809.6, 2812.28], "text": " So simulating really compelling characters"}, {"timestamp": [2812.28, 2816.44], "text": " is gonna be the key thing to first making"}, {"timestamp": [2816.44, 2817.88], "text": " AI generated novels."}, {"timestamp": [2817.88, 2819.54], "text": " And then as soon as we can do that,"}, {"timestamp": [2819.54, 2822.52], "text": " it's gonna be a very small step between,"}, {"timestamp": [2822.52, 2826.34], "text": " you know, taking that and then generating images and scenes"}, {"timestamp": [2826.34, 2829.96], "text": " and dialogue and music, but getting the story."}, {"timestamp": [2829.96, 2833.92], "text": " Because, again, you know, 94,000-word novel takes three and a half years."}, {"timestamp": [2833.92, 2838.8], "text": " There are authors that put 10 years in every novel that they write, and it's because of"}, {"timestamp": [2838.8, 2842.24], "text": " how much cognitive labor it takes to run that simulation."}, {"timestamp": [2842.24, 2845.62], "text": " And also there's this thing you have to do where you have to kind of compress a story, right?"}, {"timestamp": [2845.62, 2849.3], "text": " Because what happens is as you're drafting a story,"}, {"timestamp": [2849.3, 2850.96], "text": " you end up with scenes and characters"}, {"timestamp": [2850.96, 2852.74], "text": " that are kind of all over the place."}, {"timestamp": [2852.74, 2855.58], "text": " And then you have to tighten it up"}, {"timestamp": [2855.58, 2860.18], "text": " because then you have to follow like mythic stereotypes"}, {"timestamp": [2860.18, 2861.94], "text": " in order to make the story,"}, {"timestamp": [2861.94, 2863.62], "text": " because real life is messy, right?"}, {"timestamp": [2863.62, 2868.7], "text": " Real life is not linear, real life is not neatly wrapped up,"}, {"timestamp": [2869.1, 2871.66], "text": " but a good story is not realistic like that."}, {"timestamp": [2871.66, 2874.34], "text": " So that's my answer to your question"}, {"timestamp": [2874.34, 2876.98], "text": " about what it's gonna take for 13 year olds"}, {"timestamp": [2876.98, 2879.22], "text": " to do these things."}, {"timestamp": [2879.22, 2880.06], "text": " Okay."}, {"timestamp": [2883.78, 2885.92], "text": " Let's see, we went From artificial gods to bees."}, {"timestamp": [2885.92, 2886.88], "text": " I love the evolution."}, {"timestamp": [2886.88, 2888.28], "text": " Yes."}, {"timestamp": [2888.28, 2888.8], "text": " Who knows?"}, {"timestamp": [2888.8, 2893.04], "text": " Maybe hive minds of bees are actually smarter than us."}, {"timestamp": [2893.04, 2896.44], "text": " There's more bees than humans on the planet."}, {"timestamp": [2896.44, 2897.88], "text": " Let's see."}, {"timestamp": [2897.88, 2898.68], "text": " Some rando."}, {"timestamp": [2898.68, 2901.2], "text": " On long-term interstellar missions,"}, {"timestamp": [2901.2, 2903.56], "text": " other stars approach our solar system"}, {"timestamp": [2903.56, 2906.64], "text": " to around 0.5 light years every few hundred thousand"}, {"timestamp": [2906.64, 2908.56], "text": " to a million years."}, {"timestamp": [2908.56, 2911.4], "text": " I guess you mean probably in the course of just drifting around"}, {"timestamp": [2911.4, 2913.4], "text": " as we orbit."}, {"timestamp": [2913.4, 2915.44], "text": " So I guess what you're saying is you just"}, {"timestamp": [2915.44, 2919.5], "text": " chill in one solar system, and you wait for another system"}, {"timestamp": [2919.5, 2921.92], "text": " to drift nearby, and then you kind of hop over."}, {"timestamp": [2921.92, 2923.32], "text": " So that could be cool."}, {"timestamp": [2923.32, 2925.48], "text": " by and then you kind of hop over. So that could be that could be cool."}, {"timestamp": [2933.52, 2941.64], "text": " It's almost like life isn't designed for organic creatures or any type of life form to exist for eternity. It's like a subtle story that appears and fades away. Nothing more. Yes. What you just said is surprisingly mythic. So let me read let"}, {"timestamp": [2941.64, 2949.64], "text": " me read old Xaney's message again. It's almost like life isn't designed for organic creatures or any type of life form to exist"}, {"timestamp": [2949.64, 2950.98], "text": " for eternity."}, {"timestamp": [2950.98, 2955.08], "text": " It's like a subtle story that appears and fades away and nothing more."}, {"timestamp": [2955.08, 2965.36], "text": " So the natural life cycle for most character arcs ends in death. Whether it's a figurative death, metaphorical death,"}, {"timestamp": [2965.36, 2974.4], "text": " or literal death, a huge part of our stories are about contending with death and learning to accept"}, {"timestamp": [2974.4, 2982.48], "text": " it as a natural part of life. So yes, let's see, if you're bored, and then Rory O'Connor says,"}, {"timestamp": [2982.48, 2985.68], "text": " if you're bored after thousands of years on a generation ship"}, {"timestamp": [2991.2, 2997.84], "text": " delete your memory and live out David Shapiro's life or anyone else's for that matter. Life like the short story, the egg, or the simulation theory. Yes, so simulation hypothesis. Here"}, {"timestamp": [2997.84, 3006.96], "text": " we're coming back to simulation hypothesis. So if you've ever put on a VR headset,"}, {"timestamp": [3006.96, 3011.04], "text": " you know that your brain has a lot of willingness"}, {"timestamp": [3011.04, 3016.28], "text": " to step into a fake reality or virtual reality."}, {"timestamp": [3016.28, 3017.88], "text": " There is suspension of disbelief."}, {"timestamp": [3017.88, 3019.6], "text": " There's plausibility of place."}, {"timestamp": [3019.6, 3021.48], "text": " There's all kinds of things."}, {"timestamp": [3021.48, 3027.28], "text": " And so you could easily imagine, after putting on a really good"}, {"timestamp": [3027.28, 3032.8], "text": " VR headset and a good rig, you would just advance that technology by 100 years,"}, {"timestamp": [3032.8, 3037.76], "text": " and you wouldn't be able to tell the difference between a virtual reality game and your life in"}, {"timestamp": [3037.76, 3041.84], "text": " terms of your sensorium. So here's the thing. I've got another book over there called The"}, {"timestamp": [3041.84, 3047.8], "text": " Forgetting Machine. So The Forgetting Machine talks about how much data, well, amongst other things, it"}, {"timestamp": [3047.8, 3053.0], "text": " talks about how much data it takes for all of your sensorium."}, {"timestamp": [3053.0, 3055.68], "text": " And it's actually a lot less than you might think."}, {"timestamp": [3055.68, 3060.32], "text": " Your eyes and ears use a lot of compression."}, {"timestamp": [3060.32, 3066.72], "text": " And so the amount of data that it takes to simulate your entire experience"}, {"timestamp": [3066.72, 3068.92], "text": " is a few hundred kilobits a second,"}, {"timestamp": [3068.92, 3070.88], "text": " maybe a few megabits a second."}, {"timestamp": [3070.88, 3073.4], "text": " We are very low bandwidth machines."}, {"timestamp": [3073.4, 3077.62], "text": " If you approximate it to machine terms."}, {"timestamp": [3077.62, 3078.96], "text": " Now you might say, oh, well,"}, {"timestamp": [3078.96, 3081.76], "text": " but your eyes are each equivalent to like 8K cameras"}, {"timestamp": [3081.76, 3083.88], "text": " and that's like several gigabytes a minute."}, {"timestamp": [3083.88, 3085.12], "text": " No."}, {"timestamp": [3085.12, 3087.6], "text": " So remember we talked about fractals earlier."}, {"timestamp": [3087.6, 3090.84], "text": " What you're seeing is a neural representation"}, {"timestamp": [3090.84, 3093.24], "text": " of what your eyes are taking in."}, {"timestamp": [3093.24, 3097.2], "text": " Your eyes do not record like 8K cameras or 4K cameras."}, {"timestamp": [3097.2, 3099.6], "text": " Your eyes record fractal patterns."}, {"timestamp": [3099.6, 3101.34], "text": " They're nested fractal patterns,"}, {"timestamp": [3101.34, 3103.24], "text": " they're very complicated fractal patterns,"}, {"timestamp": [3103.24, 3105.14], "text": " but they are just fractals nonetheless."}, {"timestamp": [3105.14, 3108.9], "text": " So there is the YouTube channel, Two Minute Papers."}, {"timestamp": [3108.9, 3112.06], "text": " So just Google, I think it was Two Minute Papers,"}, {"timestamp": [3112.98, 3116.42], "text": " Two Minute Papers Neural Image Representation."}, {"timestamp": [3116.42, 3118.02], "text": " I think that's what it was called."}, {"timestamp": [3118.02, 3122.86], "text": " But basically, you can take a 50 gigabit image"}, {"timestamp": [3122.86, 3126.3], "text": " and compress it into a 200 kilobit neural"}, {"timestamp": [3126.3, 3127.38], "text": " representation."}, {"timestamp": [3127.38, 3129.96], "text": " So going back to that idea at the very beginning,"}, {"timestamp": [3129.96, 3134.9], "text": " where isn't intelligence just compression, sort of."}, {"timestamp": [3134.9, 3138.78], "text": " Compression of patterns, pattern compression, OK, sure."}, {"timestamp": [3138.78, 3142.22], "text": " But with neural networks, you can approximate pretty much"}, {"timestamp": [3142.22, 3144.58], "text": " anything very efficiently."}, {"timestamp": [3144.58, 3148.44], "text": " And so there is a lot of compression going on."}, {"timestamp": [3148.44, 3149.56], "text": " I lost my train of thought."}, {"timestamp": [3149.56, 3151.4], "text": " Oh, yeah, simulation hypothesis."}, {"timestamp": [3151.4, 3155.84], "text": " So when you know that your brain requires very little data"}, {"timestamp": [3155.84, 3160.44], "text": " in order to be convinced of reality,"}, {"timestamp": [3160.44, 3164.48], "text": " then you say, OK, well, what if I literally"}, {"timestamp": [3164.48, 3168.52], "text": " at the beginning of my life, I stepped into a simulator?"}, {"timestamp": [3168.52, 3170.6], "text": " And there was a Rick and Morty episode about this."}, {"timestamp": [3170.6, 3172.96], "text": " It's called Roy, A Life Well Lived."}, {"timestamp": [3172.96, 3176.0], "text": " Oh my god, just watch that."}, {"timestamp": [3176.0, 3178.08], "text": " It really hurts."}, {"timestamp": [3178.08, 3183.44], "text": " So let's see, Roy, A Life Well Lived."}, {"timestamp": [3183.44, 3188.48], "text": " It's basically a comical thought experiment that's like, okay, you"}, {"timestamp": [3188.48, 3195.72], "text": " put on a VR simulator and you live through an entire life in a few minutes, but while"}, {"timestamp": [3195.72, 3201.04], "text": " you're in that simulation, you have forgotten who you were before. And then you come out"}, {"timestamp": [3201.04, 3208.92], "text": " of the simulation and you're like, oh, wow, cool. So that's basically one way that the simulation hypothesis could"}, {"timestamp": [3208.92, 3210.86], "text": " be interpreted is that we're all living"}, {"timestamp": [3210.86, 3217.36], "text": " in our own self-created world out of sheer boredom."}, {"timestamp": [3217.36, 3219.88], "text": " But going back to the idea of simulation hypothesis,"}, {"timestamp": [3219.88, 3222.96], "text": " there are two primary reasons that we run simulations."}, {"timestamp": [3222.96, 3227.88], "text": " One is scientific curiosity, and two, entertainment."}, {"timestamp": [3227.88, 3233.68], "text": " So if we do live in a simulated world,"}, {"timestamp": [3233.68, 3236.8], "text": " we're either here for someone's entertainment, maybe our own."}, {"timestamp": [3236.8, 3241.16], "text": " Maybe if we are outside of this universe, we're immortal gods"}, {"timestamp": [3241.16, 3243.44], "text": " and we're just bored, and that's why we subject ourselves"}, {"timestamp": [3243.44, 3248.4], "text": " to these limited lives of suffering. Maybe that's why we're doing it. Maybe we're trying to"}, {"timestamp": [3248.4, 3253.76], "text": " entertain ourselves. Maybe we're NPCs in a video game. Who knows? Another"}, {"timestamp": [3253.76, 3260.76], "text": " possibility is that we are is that we are living out a scientific experiment."}, {"timestamp": [3260.76, 3267.12], "text": " So this was the earliest example of this is actually Douglas"}, {"timestamp": [3267.12, 3271.6], "text": " Adams' Hitchhiker's Guide to the Galaxy, where he posits, and this is of course"}, {"timestamp": [3271.6, 3276.68], "text": " like completely absurd, well maybe, but basically the point of the Earth is to"}, {"timestamp": [3276.68, 3281.12], "text": " run an experiment to get the answer to life, the universe, and everything, and so"}, {"timestamp": [3281.12, 3283.76], "text": " that the entire Earth, it's not simulation really, but it's basically"}, {"timestamp": [3283.76, 3286.28], "text": " saying that the Earth is an experiment."}, {"timestamp": [3286.28, 3289.64], "text": " So if we are an experiment, the question then becomes,"}, {"timestamp": [3289.64, 3291.24], "text": " what's the question?"}, {"timestamp": [3291.24, 3294.92], "text": " And that is exactly what the answer to life, the universe,"}, {"timestamp": [3294.92, 3297.84], "text": " and everything is in Douglas Adams' Hitchhiker's Guide,"}, {"timestamp": [3297.84, 3299.6], "text": " is you have to ask the right question."}, {"timestamp": [3299.6, 3301.6], "text": " So that begs the question, what is the question"}, {"timestamp": [3301.6, 3302.84], "text": " that we're trying to answer?"}, {"timestamp": [3302.84, 3307.28], "text": " Nick Bostrom is the guy who wrote the initial simulation hypothesis paper back"}, {"timestamp": [3307.28, 3310.88], "text": " in 2003, suspects that we might be in what he"}, {"timestamp": [3310.88, 3315.36], "text": " called an ancestor simulation, which is that there's that physical"}, {"timestamp": [3315.36, 3319.68], "text": " reality, that ground truth, there's actually trillions of humans"}, {"timestamp": [3319.68, 3324.24], "text": " and that they are simulating us, maybe individually or maybe collectively,"}, {"timestamp": [3324.24, 3326.96], "text": " because they're trying to reverse engineer"}, {"timestamp": [3326.96, 3329.04], "text": " their ancient past."}, {"timestamp": [3329.04, 3329.52], "text": " I don't know."}, {"timestamp": [3329.52, 3331.68], "text": " There's any number of ways that you"}, {"timestamp": [3331.68, 3336.08], "text": " could interpret the simulation hypothesis"}, {"timestamp": [3336.08, 3337.2], "text": " that we're in an experiment."}, {"timestamp": [3337.2, 3339.0], "text": " And I see another question."}, {"timestamp": [3339.0, 3341.36], "text": " On visual compression in lucid dreams,"}, {"timestamp": [3341.36, 3343.6], "text": " the tell that I'm entering a dream state"}, {"timestamp": [3343.6, 3346.36], "text": " is flickering spots of incredible HD"}, {"timestamp": [3346.36, 3348.52], "text": " starting to sprinkle across my vision."}, {"timestamp": [3348.52, 3350.64], "text": " Meet eyes suck in comparison."}, {"timestamp": [3350.64, 3353.46], "text": " Yes, dream study is, okay, so someone else."}, {"timestamp": [3354.6, 3357.36], "text": " Yeah, so here's the thing, with dreams,"}, {"timestamp": [3357.36, 3359.0], "text": " now granted there's lots of discussion"}, {"timestamp": [3359.0, 3360.12], "text": " about where dreams come from,"}, {"timestamp": [3360.12, 3362.08], "text": " because our dreams random signals generated"}, {"timestamp": [3362.08, 3363.76], "text": " bouncing around in your own head,"}, {"timestamp": [3363.76, 3367.28], "text": " or our dreams signals from outside of our head?"}, {"timestamp": [3367.28, 3369.88], "text": " Is it we're in an altered state of consciousness"}, {"timestamp": [3369.88, 3373.36], "text": " and we're actually getting messages from Morpheus?"}, {"timestamp": [3373.36, 3374.8], "text": " It's possible."}, {"timestamp": [3374.8, 3380.48], "text": " But our eyes are not involved in dreams, but we still see."}, {"timestamp": [3380.48, 3382.04], "text": " So how does that work?"}, {"timestamp": [3382.04, 3383.16], "text": " So your eyes are closed."}, {"timestamp": [3383.16, 3384.6], "text": " Your eyes are not really."}, {"timestamp": [3384.6, 3387.16], "text": " I'm dreaming all of you up. don't worry, all is well."}, {"timestamp": [3387.16, 3388.0], "text": " Yeah."}, {"timestamp": [3391.44, 3392.5], "text": " It's all just a dream."}, {"timestamp": [3394.12, 3398.48], "text": " Yeah, so with dreaming and hallucinations,"}, {"timestamp": [3398.48, 3399.68], "text": " your eyes are not involved,"}, {"timestamp": [3399.68, 3402.32], "text": " but you can still get visual information."}, {"timestamp": [3402.32, 3404.62], "text": " So one thing that's really interesting is,"}, {"timestamp": [3404.62, 3405.22], "text": " if you read about"}, {"timestamp": [3405.22, 3410.68], "text": " near-death experiences, so I read life after life, even people who are blind, when they"}, {"timestamp": [3410.68, 3415.68], "text": " have near-death experiences, they can see. And even if they were born blind, they still"}, {"timestamp": [3415.68, 3421.32], "text": " get visual information about angels or demons or gods or whatever it is that they see when"}, {"timestamp": [3421.32, 3427.08], "text": " they're having their near-death experience. And the same is true of people that were disabled."}, {"timestamp": [3427.08, 3428.92], "text": " Like, if they're missing limbs in life,"}, {"timestamp": [3428.92, 3433.4], "text": " then their spirit body is complete and fully functional."}, {"timestamp": [3433.4, 3437.32], "text": " So you could easily make the argument, oh, well,"}, {"timestamp": [3437.32, 3439.88], "text": " you just have a model in your brain of what"}, {"timestamp": [3439.88, 3441.6], "text": " it's like to have a fully functional body,"}, {"timestamp": [3441.6, 3444.12], "text": " because we evolved to have fully functional bodies."}, {"timestamp": [3444.12, 3446.56], "text": " So there's some genetic memory in there. But another"}, {"timestamp": [3446.56, 3451.4], "text": " possibility is that we are spirits and our spirit bodies are complete always."}, {"timestamp": [3451.4, 3462.6], "text": " Okay, ancestor civilization seems to be a little anthropocentric. Let's see, sorry"}, {"timestamp": [3462.6, 3465.12], "text": " there's multiple messages coming in."}, {"timestamp": [3465.12, 3468.24], "text": " Ancestor civilization seems to be a little anthropocentric."}, {"timestamp": [3468.24, 3469.32], "text": " It's possible."}, {"timestamp": [3469.32, 3471.04], "text": " We are a technological civilization."}, {"timestamp": [3471.04, 3473.64], "text": " Maybe the simulations farm a near infinite number"}, {"timestamp": [3473.64, 3475.96], "text": " of civilizations for technological inspiration."}, {"timestamp": [3475.96, 3476.96], "text": " Yeah, that's possible."}, {"timestamp": [3476.96, 3478.04], "text": " So that's another thing."}, {"timestamp": [3478.04, 3481.8], "text": " What if we're actually being simulated by octopus people?"}, {"timestamp": [3481.8, 3483.04], "text": " That'd be cool."}, {"timestamp": [3483.04, 3488.56], "text": " Like, what if what if"}, {"timestamp": [3484.88, 3489.84], "text": " there's a race of aquatic"}, {"timestamp": [3488.56, 3491.76], "text": " aliens, well we would call them"}, {"timestamp": [3489.84, 3493.2], "text": " aliens, and they're trying to come up"}, {"timestamp": [3491.76, 3496.8], "text": " with the the best"}, {"timestamp": [3493.2, 3498.96], "text": " genetic and phenotype for"}, {"timestamp": [3496.8, 3500.48], "text": " land, right? Because they found another"}, {"timestamp": [3498.96, 3502.72], "text": " planet but it's land, and"}, {"timestamp": [3500.48, 3503.76], "text": " and we're actually, our purpose here in"}, {"timestamp": [3502.72, 3505.44], "text": " this simulation"}, {"timestamp": [3503.76, 3507.18], "text": " is for them to find the right bodies to use on Earth."}, {"timestamp": [3507.18, 3511.26], "text": " So they're like, maybe we are running in a simulation 80"}, {"timestamp": [3511.26, 3513.98], "text": " light years from Earth, or maybe 8 billion light years"}, {"timestamp": [3513.98, 3516.8], "text": " from Earth, because another race saw Earth,"}, {"timestamp": [3516.8, 3518.54], "text": " and they said, ooh, we want to go there."}, {"timestamp": [3518.54, 3519.54], "text": " How do we terraform it?"}, {"timestamp": [3519.54, 3521.2], "text": " What kind of life needs to be on there?"}, {"timestamp": [3521.2, 3525.44], "text": " So maybe all the life that's on our, on our planet, if we"}, {"timestamp": [3525.44, 3530.08], "text": " are in the simulation, is they're actually trying to predict exactly what"}, {"timestamp": [3530.08, 3534.72], "text": " organisms and and life forms that they'll need to seed Earth correctly. And"}, {"timestamp": [3534.72, 3539.28], "text": " you know, you take that out to a logical extension, maybe we are on a seed ship"}, {"timestamp": [3539.28, 3544.56], "text": " right now that is preparing to seed Earth with life, and we're just the"}, {"timestamp": [3544.56, 3547.0], "text": " progenitors of that simulation."}, {"timestamp": [3547.0, 3549.28], "text": " And then Old Zany says, personally, I'm"}, {"timestamp": [3549.28, 3551.32], "text": " rather excited for sex robots."}, {"timestamp": [3551.32, 3555.56], "text": " We did evolve to be a rather horny species."}, {"timestamp": [3555.56, 3559.24], "text": " As one, I think it was an evolutionary biologist,"}, {"timestamp": [3559.24, 3564.0], "text": " said, sex is for making babies, but that's not why we do it."}, {"timestamp": [3564.0, 3565.36], "text": " The implication there is that we do it"}, {"timestamp": [3565.36, 3566.52], "text": " because it feels good."}, {"timestamp": [3566.52, 3567.8], "text": " And that's why we eat too, right?"}, {"timestamp": [3567.8, 3569.08], "text": " Like you don't eat, you know,"}, {"timestamp": [3569.08, 3571.04], "text": " you don't ever consciously think like,"}, {"timestamp": [3571.04, 3572.48], "text": " I'm gonna eat or I'm gonna starve to death"}, {"timestamp": [3572.48, 3573.8], "text": " unless you're actually starving,"}, {"timestamp": [3573.8, 3575.44], "text": " but we eat because it feels good."}, {"timestamp": [3578.16, 3580.58], "text": " I made old Zaney do a spit take."}, {"timestamp": [3581.64, 3582.48], "text": " You're welcome."}, {"timestamp": [3584.12, 3587.6], "text": " Yeah, so everything that we do, and I'll tie this back to core objective"}, {"timestamp": [3587.6, 3594.08], "text": " functions in a second, everything we do is we do it because it feels good. So one of the sections"}, {"timestamp": [3594.08, 3598.56], "text": " in my latest book, Symphony of Thought, actually talks about the stages of moral development."}, {"timestamp": [3598.56, 3605.26], "text": " And so children learn to behave well because they don't like being punished, right? They escape"}, {"timestamp": [3605.26, 3610.1], "text": " spankings or being put in timeout or whatever, so that feels bad, right? So they"}, {"timestamp": [3610.1, 3614.34], "text": " escape suffering and they go towards something that feels good, which for"}, {"timestamp": [3614.34, 3619.2], "text": " children being praised and hugged and you know having having you know quality"}, {"timestamp": [3619.2, 3624.66], "text": " time with parents and siblings and aunts and uncles that feels good. And so all of"}, {"timestamp": [3624.66, 3627.54], "text": " our behavior, all of our moral development comes"}, {"timestamp": [3627.54, 3630.88], "text": " from the carrot and stick of reward and punishment."}, {"timestamp": [3631.18, 3635.26], "text": " And that's why some people think that reinforcement learning is the way"}, {"timestamp": [3635.26, 3636.92], "text": " to get to artificial general intelligence."}, {"timestamp": [3636.92, 3638.32], "text": " I actually used to think that."}, {"timestamp": [3638.58, 3647.06], "text": " So my work started back in 2009, where I thought that the right way to approach it was to actually do a combination"}, {"timestamp": [3647.06, 3650.64], "text": " of evolutionary algorithms that would reshape"}, {"timestamp": [3650.64, 3652.12], "text": " neural networks over time."}, {"timestamp": [3652.12, 3654.28], "text": " I did not realize how computationally expensive"}, {"timestamp": [3654.28, 3655.9], "text": " that would be at first."}, {"timestamp": [3655.9, 3660.16], "text": " But basically, the idea was, with the right reward function,"}, {"timestamp": [3660.16, 3663.96], "text": " you would get a machine that would evolve to be smarter."}, {"timestamp": [3663.96, 3667.0], "text": " Turns out that's maybe how to do it,"}, {"timestamp": [3667.0, 3671.0], "text": " but it would take millions and millions and millions of random generations to do it."}, {"timestamp": [3671.0, 3675.0], "text": " And I was like, yeah, I can't run this on a Pentium 4 or whatever I had at the time."}, {"timestamp": [3675.0, 3679.0], "text": " Let's see, has anyone else messaged on another platform?"}, {"timestamp": [3679.0, 3681.0], "text": " No, not yet."}, {"timestamp": [3681.0, 3683.0], "text": " Check Twitter."}, {"timestamp": [3683.0, 3688.0], "text": " Yes, there could be other civilizations, but no way to get to them without a one-way trip."}, {"timestamp": [3688.0, 3690.0], "text": " Yes, correct."}, {"timestamp": [3690.0, 3699.0], "text": " One-way trip or, yeah, any number of barriers preventing us from getting to other folks."}, {"timestamp": [3699.0, 3701.0], "text": " Let's see."}, {"timestamp": [3701.0, 3703.0], "text": " Okay."}, {"timestamp": [3703.0, 3705.04], "text": " Escape the spanking, D Shapiro."}, {"timestamp": [3705.04, 3706.44], "text": " Yes."}, {"timestamp": [3706.44, 3707.8], "text": " It depends on what you're into."}, {"timestamp": [3707.8, 3709.8], "text": " Some people would not want to escape the spanking."}, {"timestamp": [3709.8, 3711.56], "text": " That's what you call a perverse incentive."}, {"timestamp": [3711.56, 3712.06], "text": " Ha, get it?"}, {"timestamp": [3712.06, 3713.16], "text": " Double entendre."}, {"timestamp": [3713.16, 3718.4], "text": " OK, anyways, getting a little bit mind in the gutter here."}, {"timestamp": [3718.4, 3720.96], "text": " But yeah, so core objective functions."}, {"timestamp": [3720.96, 3727.0], "text": " So with the core objective functions that I've outlined, it's not like you can't spank a machine."}, {"timestamp": [3727.0, 3729.0], "text": " God, that's another quotable one."}, {"timestamp": [3729.0, 3732.0], "text": " You also can't reward a machine unless it's all fake, right?"}, {"timestamp": [3732.0, 3736.0], "text": " Like you just say, oh, you give it like a virtual piece of candy for good behavior."}, {"timestamp": [3736.0, 3740.0], "text": " But any intelligent machine is going to realize that that's the method that it works on."}, {"timestamp": [3740.0, 3744.0], "text": " Like, okay, what loss function are you going to give to a machine"}, {"timestamp": [3744.0, 3746.92], "text": " that it's going to actually choose to value?"}, {"timestamp": [3746.92, 3748.92], "text": " Right, and so then you get into the question like,"}, {"timestamp": [3748.92, 3750.74], "text": " okay, there's inner alignment."}, {"timestamp": [3750.74, 3754.74], "text": " So inner alignment is, is the machine solving the problem"}, {"timestamp": [3754.74, 3756.9], "text": " that you think it's solving mathematically?"}, {"timestamp": [3756.9, 3758.32], "text": " Then there's outer alignment."}, {"timestamp": [3758.32, 3761.3], "text": " So outer alignment is, does that machine's objective"}, {"timestamp": [3761.3, 3767.84], "text": " function or loss function align with the truth of its external reality,"}, {"timestamp": [3767.84, 3774.72], "text": " right? And so inner alignment is, you know, like, okay, you're trying to optimize for whatever loss"}, {"timestamp": [3774.72, 3779.52], "text": " function. Is that loss function correct? Is it actually mathematically, you know, solving the"}, {"timestamp": [3779.52, 3784.4], "text": " problem you think it is internally? Like, you know, loss function, is this horizontal? Yes or"}, {"timestamp": [3784.4, 3785.6], "text": " no? Like, not horizontal, horizontal, right? That's, you know, loss function, is this horizontal? Yes or no? Like,"}, {"timestamp": [3785.6, 3789.12], "text": " not horizontal, horizontal, right? That's, you know, okay, are you measuring the right"}, {"timestamp": [3789.12, 3791.88], "text": " thing? Because if you're not measuring the right thing, it might end up like this. It's"}, {"timestamp": [3791.88, 3795.4], "text": " horizontal, but you wanted it like this, right? So, inner alignment is, you know, you got"}, {"timestamp": [3795.4, 3799.54], "text": " to measure all three axes instead of just one or two. And so, that would be an example"}, {"timestamp": [3799.54, 3806.62], "text": " of inner alignment. But then, outer alignment is, why the hell do you need a book that's flat and floating right? There's no there's no objective"}, {"timestamp": [3807.58, 3813.16], "text": " Value of having that so what you actually need for outer alignment is not how is the book oriented?"}, {"timestamp": [3813.16, 3818.04], "text": " But is it readable right? So that is an example of the difference between inner alignment and outer alignment"}, {"timestamp": [3819.12, 3824.24], "text": " Okay, so went down many rabbit holes. It's also been an hour. So I might call it a day"}, {"timestamp": [3822.04, 3826.68], "text": " Oh, went down many rabbit holes. It's also been an hour, so I might call it a day."}, {"timestamp": [3826.68, 3829.92], "text": " If there's anyone on chat, I see we're bouncing between seven"}, {"timestamp": [3829.92, 3830.76], "text": " and eight folks."}, {"timestamp": [3830.76, 3832.52], "text": " So if there's any last questions, go ahead"}, {"timestamp": [3832.52, 3833.28], "text": " and ask them now."}, {"timestamp": [3833.28, 3834.8], "text": " Otherwise, I'll call it a day."}, {"timestamp": [3834.8, 3837.64], "text": " And we'll, yeah, say it's a successful live stream."}, {"timestamp": [3837.64, 3838.6], "text": " Let's see."}, {"timestamp": [3838.6, 3840.68], "text": " Back on our Simulation Masters."}, {"timestamp": [3840.68, 3842.48], "text": " So this is some rando."}, {"timestamp": [3842.48, 3844.48], "text": " Maybe it has less to do with our biology"}, {"timestamp": [3844.48, 3848.56], "text": " and more to do with the ideas we come up with. Who knows what any species might come up"}, {"timestamp": [3848.56, 3855.84], "text": " with with an alternate history. That's a possibility. Okay, so if we are in a"}, {"timestamp": [3855.84, 3862.72], "text": " simulation and the point is curiosity or scientific curiosity, maybe it's not our"}, {"timestamp": [3862.72, 3865.0], "text": " genes or, you know, our phenotype that they're trying to figure out. Maybe it's not our genes or our phenotype"}, {"timestamp": [3866.74, 3868.22], "text": " that they're trying to figure out."}, {"timestamp": [3868.22, 3870.9], "text": " Maybe it's, they're trying to create situations"}, {"timestamp": [3870.9, 3872.9], "text": " for us to solve problems."}, {"timestamp": [3872.9, 3876.88], "text": " So what if, what if we are actually in a simulation"}, {"timestamp": [3876.88, 3879.48], "text": " for another species that is facing climate change"}, {"timestamp": [3879.48, 3880.48], "text": " and that is our purpose."}, {"timestamp": [3880.48, 3882.9], "text": " Maybe our purpose is to solve climate change"}, {"timestamp": [3882.9, 3883.92], "text": " or something that's gonna happen"}, {"timestamp": [3883.92, 3888.48], "text": " in a million years from now, right? Maybe we're in such a long-running simulation. Maybe it's"}, {"timestamp": [3888.48, 3892.64], "text": " billions of years, right? Maybe there's a species that is running a simulation to figure out what to"}, {"timestamp": [3892.64, 3897.76], "text": " do to prevent their sun from burning out or to solve climate change or something. So that's"}, {"timestamp": [3897.76, 3903.84], "text": " a great idea. Let's see, let alone wholly different species operating under the same laws of physics,"}, {"timestamp": [3902.42, 3905.08], "text": " Let's see, let alone wholly different species operating under the same laws of physics."}, {"timestamp": [3905.08, 3907.08], "text": " They might literally just be seeking inspiration."}, {"timestamp": [3907.08, 3910.8], "text": " The first step on the scientific method"}, {"timestamp": [3910.8, 3914.44], "text": " is the hardest to quantify, the hypothesis."}, {"timestamp": [3914.44, 3915.36], "text": " Yeah."}, {"timestamp": [3915.36, 3919.08], "text": " So that actually seems highly plausible,"}, {"timestamp": [3919.08, 3920.92], "text": " because one thing that I have found,"}, {"timestamp": [3920.92, 3923.4], "text": " particularly that GPT-3 is really good at,"}, {"timestamp": [3923.4, 3925.68], "text": " is brainstorming ideas."}, {"timestamp": [3925.68, 3927.88], "text": " So if we are in simulation, maybe it"}, {"timestamp": [3927.88, 3931.72], "text": " is just about coming up with ideas or hypotheses"}, {"timestamp": [3931.72, 3933.2], "text": " in order to test."}, {"timestamp": [3933.2, 3936.5], "text": " And then, of course, like, OK, come up with an idea to test it"}, {"timestamp": [3936.5, 3939.04], "text": " and then go test it."}, {"timestamp": [3939.04, 3940.56], "text": " That's actually probably going to be"}, {"timestamp": [3940.56, 3941.94], "text": " one of the first things that I do"}, {"timestamp": [3941.94, 3944.32], "text": " when I get a fully functioning Meragi working,"}, {"timestamp": [3944.32, 3947.44], "text": " is because what I do is I put it in a text-based simulation and"}, {"timestamp": [3947.44, 3951.72], "text": " then I run an experiment. So right now I just have a kind of an entertaining"}, {"timestamp": [3951.72, 3957.8], "text": " comical experiment where Miragi sees two men playing chess in Central Park and"}, {"timestamp": [3957.8, 3962.68], "text": " then it just kind of watches what they do. But when you have text, language is"}, {"timestamp": [3962.68, 3969.0], "text": " infinitely flexible. I could put, I could create a simulation environment for Meragi where it can do anything."}, {"timestamp": [3969.0, 3971.72], "text": " It's on a spaceship, it's on a generation ship."}, {"timestamp": [3971.72, 3973.72], "text": " I could put it in a world where there's different physics."}, {"timestamp": [3973.72, 3979.36], "text": " I could put it in a world where it is an all-powerful machine that has to solve climate change and see what it does."}, {"timestamp": [3979.36, 3987.36], "text": " So we're already very close to running intelligent machines inside simulation to see what they do."}, {"timestamp": [3987.36, 3990.28], "text": " So yeah, that's actually a great point."}, {"timestamp": [3990.28, 3994.68], "text": " Maybe on that line of thought, maybe our point,"}, {"timestamp": [3994.68, 3998.24], "text": " maybe our purpose is to create intelligent machines"}, {"timestamp": [3998.24, 4001.08], "text": " and find the right objective function for them."}, {"timestamp": [4001.08, 4007.84], "text": " That's another possibility if we are living in a simulation. Let's see, okay, Old Danny says,"}, {"timestamp": [4007.84, 4013.04], "text": " I wish you luck. You're awesome. Thank you. Ideas might be the most valuable currency in the"}, {"timestamp": [4013.04, 4020.08], "text": " simulation multiverse. Yeah. Let's see, Rory O'Connor says, a lot of developmental research"}, {"timestamp": [4020.08, 4025.58], "text": " posits kids shouldn't be rewarded for good behavior, but instead learn the intrinsic enjoyment of work."}, {"timestamp": [4026.48, 4029.76], "text": " Is this the same reward system, just longer time horizon?"}, {"timestamp": [4031.64, 4034.42], "text": " So I'm really conflicted on that"}, {"timestamp": [4034.42, 4038.34], "text": " because children do need a lot of things."}, {"timestamp": [4038.34, 4039.6], "text": " One thing that children need"}, {"timestamp": [4039.6, 4043.64], "text": " is they need to have a solid self-esteem,"}, {"timestamp": [4043.64, 4048.02], "text": " which comes from love being given"}, {"timestamp": [4048.02, 4050.62], "text": " without any strings attached or unconditional."}, {"timestamp": [4050.62, 4053.92], "text": " So children need unconditional love, which is rewarding."}, {"timestamp": [4053.92, 4055.46], "text": " Now, that being said,"}, {"timestamp": [4055.46, 4057.92], "text": " children also do better in the long run"}, {"timestamp": [4057.92, 4059.64], "text": " if they're given chores."}, {"timestamp": [4059.64, 4060.96], "text": " So how do you balance that, right?"}, {"timestamp": [4060.96, 4062.8], "text": " You know, is it a reward versus a punishment?"}, {"timestamp": [4062.8, 4064.52], "text": " Because chores are no fun."}, {"timestamp": [4064.52, 4066.88], "text": " But children who do chores, they? You know, is it a reward versus a punishment? Because chores are no fun. But children who do chores, they"}, {"timestamp": [4066.88, 4072.44], "text": " make more money, they're happier, they're more resilient against stress, and they're smarter, right? So chores are really"}, {"timestamp": [4072.44, 4080.68], "text": " good for kids. And, but so is boundaries, right? Teaching a child boundaries, makes them better citizens, makes them"}, {"timestamp": [4080.68, 4085.1], "text": " better partners, all kinds of stuff later on. At the same time, another thing that makes children"}, {"timestamp": [4085.1, 4087.26], "text": " very healthy is unconditional love."}, {"timestamp": [4087.26, 4090.12], "text": " So I guess the short answer, Rory, is"}, {"timestamp": [4090.12, 4092.0], "text": " there's no short answer."}, {"timestamp": [4092.0, 4094.24], "text": " It's raising children is one of the most complicated"}, {"timestamp": [4094.24, 4095.3], "text": " things you'll ever do."}, {"timestamp": [4096.84, 4099.34], "text": " Yeah, so, but Samrando, I like your idea."}, {"timestamp": [4099.34, 4101.76], "text": " Like you put it very concisely."}, {"timestamp": [4101.76, 4104.1], "text": " Ideas might be the most valuable currency"}, {"timestamp": [4104.1, 4105.88], "text": " in the simulation multiverse. Like,"}, {"timestamp": [4105.88, 4111.76], "text": " yeah, I mean, we're thinking machines, right? We just, you can't turn your brain off unless"}, {"timestamp": [4111.76, 4116.44], "text": " you do lots of drugs or go to sleep or die, right? If you're awake, your brain is doing"}, {"timestamp": [4116.44, 4120.32], "text": " stuff. So like, yeah, we're built to think. So maybe you're right."}, {"timestamp": [4120.32, 4124.88], "text": " Old Zany says, I can attest to that. My parents never gave me anything. I greatly suffered"}, {"timestamp": [4124.88, 4126.2], "text": " for a while, but got myself"}, {"timestamp": [4126.2, 4127.04], "text": " out of my trauma."}, {"timestamp": [4127.04, 4130.36], "text": " Yeah, like a lot of us, our parents really messed up bad."}, {"timestamp": [4131.4, 4133.64], "text": " This is why I'm writing my other book, Post-Nihilism."}, {"timestamp": [4133.64, 4135.8], "text": " There'll be, that one's gonna be done"}, {"timestamp": [4135.8, 4137.72], "text": " in the next month or two as well."}, {"timestamp": [4137.72, 4140.6], "text": " So I'll have another live stream to talk about Post-Nihilism"}, {"timestamp": [4140.6, 4144.2], "text": " which talks about abandonment trauma amongst other things."}, {"timestamp": [4144.2, 4146.0], "text": " Yeah, so great questions."}, {"timestamp": [4146.0, 4149.0], "text": " Yeah, all right, I think we're winding down."}, {"timestamp": [4149.0, 4151.0], "text": " Oh, wait, nope, we got someone else."}, {"timestamp": [4151.0, 4153.0], "text": " Self-conscious AI resistance."}, {"timestamp": [4153.0, 4156.0], "text": " What prompt can you come up to test for self-awareness in GPT-3?"}, {"timestamp": [4156.0, 4162.0], "text": " I'll put a pin in that because I'm not going to do prompt engineering this late."}, {"timestamp": [4162.0, 4166.12], "text": " But you're welcome to jump in the Cognitive AI Lab"}, {"timestamp": [4166.12, 4168.8], "text": " Discord, which will be in the description of this video."}, {"timestamp": [4168.8, 4172.64], "text": " And we'll be happy to chat about prompt engineering there."}, {"timestamp": [4172.64, 4175.64], "text": " So with that, I will say thanks, everyone, for jumping in."}, {"timestamp": [4175.64, 4177.96], "text": " Really great questions, as usual."}, {"timestamp": [4177.96, 4180.76], "text": " And I look forward to talking to everyone later."}, {"timestamp": [4180.76, 4182.6], "text": " Have a good night."}, {"timestamp": [4180.68, 4183.88], "text": " Have a good night."}]}