{"text": " All right, hello everybody. David Shapiro here with the Gato Framework project team. We've got about a quarter of the team here. And basically all we're going to do is just do a debrief of the Senate hearing that happened yesterday with Sam Altman and Mrs. Montgomery from IBM as well as Gary Marcus. So many of us watched the whole thing or most of it. We were chatting about it in real time because we're those kind of nerds. So with all that being said, this is a, to us, it represents a very pivotal moment in terms of moving the Overton window, talking about the not just the existential risk of AI, but the social risk to AI, or the AI's risk to society, so on and so forth. So anyways, we've got a pretty robust discussion tonight. It's not going to be a debate format, but we do have some help with moderation, so we're going to work through some questions and answers. some help with moderation so we're going to work through some questions and answers. So yeah, that's where we're starting. I'll introduce my moderator, our community leader, community manager, Nathan. Nathan, if you want to jump in and introduce yourself. Hey, everyone. Gato community, larger world. My name is Dathan Lannan and I am a community manager for the Gato project. Right now I'm working on building the website with some passionate folks and also doing a AI reskilling curriculum that I hope to eventually put out to the public. So yeah, thrilled to be here. I will be playing the role of helping people get on and off stage and making sure things go smoothly. Back to you, Dave. DAVE SMITH Excellent. Thank you so much, Nathan. All right. So with that being said, this is also going to be a pretty casual conversation. The only primary rule is we're going to try and talk one at a time. So with all that being said, Nathan, what is our first question to lead us off? Prime the pump, so to speak. Yes, so yesterday, as we know, was the hearing which was huge for the AI community opening up that Overton window of crack. And just to kind of start off on a positive note, I think it would be nice to reflect on hope that that might have generated for us and general thoughts about how that might have been a good thing for us. So anybody want to jump in on that, raise a hand. John, why don't you hop up? John, why don't you hop up? Yeah, absolutely. So, I think one of the biggest takeaways from the Senate hearing was how educated the Senators actually were about the situation and the risks. I think a lot of the Senators said some things or asked some questions that showed that they weren't experts in the field, but they were way more knowledgeable about the situation than I think anybody really anticipated. We expected the moderator to be pretty knowledgeable because he's worked in this field before and advocated for algorithmic bias protections in social media, but, you know, to see some of these other senators that we don't hear from all the time, that we don't hear from all the time actually get up and ask somewhat salient questions about the actual issue and show that they did know what was going on. It leaves a lot of hope for having reasonable and intelligent regulation in the future. Excellent. Thank you, John. Andy, I saw you have a hand raised. Why don't you come on up? Just building on what John said, I was also encouraged. I had a dismal outlook on what might be pretty uninformed hearing, and also something that was heavily partisan or rooted in kind of dogmatic rhetoric, which is the way that, you know, a lot of these kind of publicized hearings will go. But it didn't seem to me that too many people showed up with an overt agenda. There were more people in the room there to learn, and I also thought that it was interesting to see and hear from Republicans who were interested in setting up a bureaucracy, a bureaucratic mechanism and department or agency specifically for this. Some of them mentioned agencies, someone as high as cabinet level. And Democrats questioning whether expanding the bureaucracy was really going to be a fitting choice for how to cope with this. Those are very different tunes than we usually hear from the two sides who kind of default to being for and against the opposite of that. So I thought that that represented, it suggested to me that a lot of these people are concerned about this they are Literally personally interested in what's going on and that they aren't just trying to fit it into whatever box they carried into the room So I don't think we get that too often In our in our legislative debates and I didn't think that it really came across as much of a debate There were only a couple kind of contentious moments and I think all of that suggests that you know know, as John said previously, too, that we've got a lot of fertile ground to work with in government, and that some support and interest really may come from angles and actors that you may not have expected to be interested at all, and might have expected to be opposition to whatever you're trying to do. So yeah, I think the hearing was pretty encouraging overall. Totally Andy. So like hearing, we saw, you know, bipartisan hand-holding yesterday in alignment on, you know, a super important topic and just that was incredibly heartening to see. I agree that was one thing I was going to speak up on as well. Dave, please go ahead. to see. I agree. That was one thing I was going to speak up on as well. Dave, please go ahead. Yeah, so first I'll echo the sentiments of what everyone else has already said. But what I'll also add is, and as I mentioned in the introduction, moving the Overton window, I think, is going to be kind of the biggest thing. And I was so excited that senators, you know, particularly in America, even some of the older gentlemen and gentlewomen were asking very pointed questions, not just on the topics of like, oh, hey, comparing it to social media, which they did compare it to social media a lot, because that's the only frame of reference that we've got, basically, for disruptive technologies, at least in recent history. But they also said, like, you know, what are the existential risks? And I'm glad that they had a witness, you know, because more often than not, these congressional hearings, they have just corporate insiders. Everyone does. I don't know if everyone does, but big tobacco, right? Where it was just like a panel of like eight tobacco execs and they're like, is this addictive? And the tobacco execs all unanimously said, no, it's not addictive. And everyone's like, well, this is a completely pointless farce. But here they had two tech industry insiders and then Gary Marcus who is, he is not a tech insider, he is a researcher. So they had that to counterbalance it which I think gave the whole hearing a lot more credibility in terms of just reach and validity. But last point that I'll make is that I was also impressed by both Sam Altman and Ms. Montgomery's who represented IBM, their desire and willingness to have regulations and to speak up, which that was a little bit unexpected. And of course, we can unpack some of the nuances later. But yeah, so that's about it for me. Thanks, Dave. And Ben, I'd like to invite you up. Thanks, Nathan. Just quickly, my name is Ben. I'm a CTO and a tech diligence advisor on AI. I'm helping with some of the Gato framework writing. The thing I really enjoyed, besides the thing I already mentioned about this hearing, was how incredibly practical it was. So, like, senators asked for very specific ideas on how to do oversight, and really drove home, like, not just high-level concepts, but very practical ideas on how to do oversight and really drove home, like, not just high-level concepts, but very practical ideas on how to do it. They did things like highlight the need for better business models around news organizations or other monetization sources. So just the level of practicality that they started with was really heartening for me. Excellent. So I'm hearing all around we heard many things that we liked. Now I'm curious, before we dive into things that we liked less, is there any things that we wish we had heard but we didn't hear them say? Richard, I know you had a hand up before I asked this question so you don't feel free to respond to the prior but let's kick over to what could have been said but wasn't, the unsaid things. basically just sort of a dude who hangs out in a workshop building weird shit. But in terms of just the last point was just I wanted to also touch on the fact that it was just really surreal to see like heads of like massive corporations that could if they wanted to wield this as a weapon against society and control a huge swath of people, go to the government and specifically ask them to regulate themselves. That never happens. I've never seen anything like that before. It was very surreal to watch. And also just the fact that they were talking about AI in Congress was just like, what leg of the trousers of time have I gone down? But yeah. Excellent. Yeah, absolutely. Yeah. Excellent. Yeah, absolutely. So I was wondering, would anybody have any other thoughts on things they liked before we move over to things that we wish we had heard more of? John, I see you have a hand up. Yeah, so one of the things that I think is really going to need a lot of discussion and will probably come up in a later hearing, but I'd like to have heard more about it was the the economic impacts that these these things are going to have because the economy is an existential existential threat. Maybe not like extinction of humanity level, but it is probably one of the first impacts that negative impacts that we're going to see from AI. And I don't think that it may be positive, too, but I don't think that, you know, everybody wants to, you know, ask to worry about jobs. But I don't think that jobs is the only impact it's going to have on the economy, it's going to shift the way that money moves in a global fashion. And it is going to have global impacts not just on jobs, but on all industry as well. impacts not just on jobs but on all industry as well. Absolutely, yeah. We're gearing up to see major societal shifts and they were a little light handed with raising awareness of some of those things. Ansel, I see you have a hand up there. And so I see you have a hand up there. Yeah, well, I wanted to touch on the things that weren't mentioned, but I think data basically covered it. I wanted to touch on the fact that they said that this was going to create new jobs. And like, sure, it'll create new jobs, but it's not going to be anywhere near enough of the jobs that it's going to displace. And especially it's not going to create new ones in that much of a short time. We're seeing huge disruptions in the next five years, and that's being incredibly optimistic. And that's not a niff. That is guaranteed. The real question is, what are the fundamental shifts that are going to happen and how are we going to address them? Because Congress is still thinking in the old model, right? Where you still need humans working eight hours a day, five hours, sorry, five days a week, and just grind, right? AI is going to automate a lot of that grind. And we're not going to find enough, what's the word, busy work to keep ourselves busy, right? We should focus more on liberating that time for humans to do better things, right? And well, that's basically it. time for humans to do better things, right? And well, that's basically it. Thank you, Ansel. Dave, why don't you pick up the conversation? Sure, sure. Yeah, I think... And then I'll move back to Ben. Yeah, sorry. I gotcha. No worries. But yeah, so for me, certainly agree that economy, UBI, that sort of stuff, the displacement, they touched on displacement but then kind of glossed over it. Maybe they had had an out of band conversation like, let's save this for another thing. I don't know. But one thing that was addressed, I think Gary addressed this when he talked about auto GPT and chaos GPT, and that nobody else addressed, at least none of the witnesses, was autonomous AI. Right? Both Montgomery and Altman fervently said this is just a tool, this is not a creature, don't anthropomorphize it. And it's like, yeah, but then when Gary said, hey, Gary said people are making this stuff semi-autonomous today, it doesn't matter how ineffective it is or if it has flaws, it is today. Someone in chat just mentioned that chat GBT released plug right, pretty quickly. And then one thing that I did notice, and this is just speculation on my point outside of that, was that as soon as people started creating, like, Chaos GPT, they slow rolled rolling out plugins to more people. So maybe OpenAI tapped the brakes in response to Chaos GPT. But, you know, I obviously don't have an inside line. That's just speculation on my point. But I would have done that if I saw someone use chat GPT to create ChaosGPT. But yeah, so autonomous AI was a huge thing missing. Gary tried to push it. Several senators did ask, like, hey, what was it? I think it was Lindsey Graham near the end, he's like, can you use this to give a drone the ability to autonomously target someone and then go kill them? And of course, Sam tried to, you know, use the corporate sanitized speak and then Lindsey Graham to his credit said, that's a yes or no question. And Sam's like, yeah, it is possible. Right? So he did admit that. Which was brave. But, you know, the thing is, is like, when I first got access to chat GPT to the API, I went in and showed that you can just give it instructions like you're controlling a drone. Right? And it's like, sure, I'm controlling a drone now. It doesn't care. So, yeah, the autonomous AI was the biggest thing missing in my eyes. And I cede the floor. Ben, please pick it up. Sounds great. I just wanted to start by saying I'd love to talk through the jobs thing. I think saying it's guaranteed that we're going to lose so many jobs is a pretty aggressive way to go. And I think it's worth debating a bit. I certainly think differently. In terms of what was missed, for me, honestly, how bad things already were before AI was released. So, like, misinformation has been a problem forever. We have all sorts of societal problems around global warming, energy crisis, things like this, and just the opportunities to leverage AI. Yeah, it's going to make some things worse for sure, but it is also an opportunity to solve, help solve some things that were already problems and we were already struggling with. I would love to see people talk a little bit more about those opportunities. is. Thank you, Ben. Bruce, you're up. One of the things I think was missing from it was the competitive angle which says that yeah that's great America is not the only world, place in the world that's trying to develop an AI, right, and the reality is that if you tap the brakes too much then what you'll do is lose the lead and losing the lead. And losing the lead to a country that has less than stellar moral boundaries might in fact be counterproductive to what's going on. So a lot of this stuff is very interesting, but the comment I would make is the horse has already bolted. And unless we keep up with it and keep ahead of it, then the outcome could be even worse than displacement from losing a few jobs. I hear you. It sounds like you're referring to the Moloch dynamics that are very much at play right now. We could easily end up in a race to the bottom. John, why don't you go ahead. Yeah, I wanted to come back to Mr. Marcus's comment about AutoGPT. What he said was that because OpenAI released the plugins, that enabled AutoGPT, and that was actually never the case. AutoGPT and ChaosGPT both predate the plugin release, and they both directly leverage only the GPT API. It's a little bit disconcerting to see somebody who is supposed to be a researcher in this, get something like that so egregiously wrong. I'm really surprised that Sam didn't call him out for that when he had just called Sam out for not saying what actually scared him about AI. We're talking about some of the kind of darker side of what might happen with AI and bad research and bad information that's used to persuade governments to create regulations is another kind of misinformation concern. Absolutely, thank you John. Richard, go ahead. Thank you, John Richard, go ahead. Oh yeah, the biggest thing like I think that that they were more talk like he was. It was more like in reference to the fact that like. The plugins would give the opportunity for people to build more things like chaos, GPT and stuff. In like because if you give them the tools to be able to just plug them together like Lego, then it's way easier than a developer who actually went in and built the API backend for that. Which is, I mean, the thing is, humans are going to be humans. Humans are amazing and beautiful and do incredible things, but at the same time, we're terrible and awful and do horrible things. So, we have to sort of, like, have the contingency plan to make sure that that is, like, something that we can manage, because, like, mal-aligned AI is going to happen, because these things are, like, infinitely manipulatable. Like, they all work off of natural language and like, you can talk to them, like, if you're just nice to it, you'll find you get better results. Just being polite. So thank you, talking to it a little bit in between prompts and stuff. You get way better results. And then like, if you know how to talk to them correctly, you can get them to reveal all kinds of stuff that they're not supposed to. Absolutely, yeah. And the Senators, to their credit, were definitely well rehearsed. You know, they had heard language around jailbreaking and stuff like that. Heartening to see them using the language of our inter-alignment communities. So, with that aside, answer please step up. Alright, I wanted to touch on the comment that John said about it being in the API, not a plug in. I agree. Trust me, I agree. The thing is, we're in the in the in the industry, right where we're in the know. We know what an API is and we know what a plugin is. We know the difference, right? Most people won't. And especially for old people in Congress, you tell them an API and they're like, what is an API? So when they say a plugin, it basically functions the same way. It's a way to connect to the model, right? Regardless of if it's done through an API or a plugin, you're connecting to the model. So that's what Congress needed to know, right? So I think that's not really that important that he didn't make that distinction between, oh, plug an API, because at the end of the day, it's just connecting to the model. Absolutely. Thank you, Ansel. And Ben, I'll hand it to you and then take it back to raise a new question to everyone. So go ahead. Sounds good. The last thing I wanted to mention is while it was great to hear them mention auto GPT, there wasn't a lot of talk about open source. There's been a lot of AI news about how open source may push the envelope of what's capable and regulating open source versus companies may be very different. So at some point I'd love to see them jump into the role of open source and how to regulate that. Thank you, Ben. And thank you, everybody, for kind of raising to the surface some of the things that we would have liked to hear, but we did not hear from the debate and, or not debate, but hearing. And a positive thing, there will be more of these. So if you are listening, perhaps these are things you bring to the table for the next round. Now I'd like to discuss, what are some of the things about the hearing that we found disappointing. Things that we did not like so much. So, got the community. The floor is yours. Ansel? Well, I'll start. The thing I didn't like was they tried to focus more on creating a national agency instead of an international one. When this technology is being used around the entire planet, I think that doing this in an international level is crucial. Right? So that's one thing that I was disappointed. Like, they just mentioned it, yes, and then like, nah, if we're going to do it, let's go national, blah, blah, blah. You know, that's one thing I didn't like. Totally. Ansel, speaking here to one of the traditions of Gato, we like to act local, but think global. So these issues affect all of us. Andy, the floor is yours. One of the things I didn't like is that Sam Altman, for reasons I think I probably understand but don't necessarily appreciate, dodged a few questions. We've mentioned it earlier. they asked him to talk about his biggest fears and concerns. Sam has been very candid in public before about this with MIT presentations on the Lex Friedman podcast. He's been one of the more forthcoming voices out there in the field as somebody who readily also says when he talks there's only so much I can tell you. And I thought that that was a little bit disappointing. I thought he could have given a little bit more pointed answers. I also was disappointed that they dodged the jobs question when the tech industry has just undergone a wave of layoffs that were largely attributed to other market forces, but are now undergoing yet another wave of layoffs that most companies are flat out saying, we're automating these jobs. And that means those jobs aren't coming back. I don't necessarily disagree with Ben. I think there's a more complex conversation to have around that. I think there will be some job creation, but I don't disagree with Ansel and John either. I think the amount of offset is gonna be very difficult, partially because one of the things these systems do so well is empower one person to work as multiple people. So Sam Altman made very tentative mention that he believed eventually there would be systems that could replace jobs at a large scale. But we know that within the last six months, his company has published a paper that suggests when combined with additional programming or robotics, they think that they could do about 55% of jobs, period, now. Now, there's a lot of criticisms of that paper and some of them are probably pretty valid. There are probably a lot of things that wouldn't do very well, and there are a lot of people who don't want to talk to a machine. So there's demand for human labor still, but I thought, you know, also that the economic argument was undersold and I thought part of what Sam Altman dodged was the segue into that conversation. I'm not sure that dropping even the three letters UBI in the middle of that hearing would have done anything but sow discord, but we all know that that's what Sam Altman really wants to do. He said it specifically on Lex Friedman, his company's funding this report, it's coming out at the end of the summer. And he's been very candid about it before. And at the same time, when you have someone like Josh Hawley sitting there asking relevant questions, maybe you don't wanna talk about socialism. And I think that that's a fair line to draw for yourself when you're trying to remain, you know, afloat and, in my opinion, ahead of a pretty productive conversation. But that doesn't mean I liked it and I'd be inclined to agree with Seneca that, you know, the conversation was right there. It's been going on in other circles in Congress. It was an elephant in the room, in my opinion. Absolutely. So, it sounds like we heard a lot of broad strokes where we would have appreciated a more nuanced brush and also have some empathy where like, this is the first time we're bringing AI to the table in a very public way and like, you'd be either big old can of worms that maybe we don't open up at this particular meeting. So, yeah, it's room for more conversation for the future ones. Dave, back to you. Yeah, thanks. And certainly there is a lot of nuance and debate to be had around economic impact and our response. But the thing that I was honestly most disappointed about that was almost, it was kind of shocking, was when later in it, when Senator Kennedy said, you know, if you were king or queen for a day, and you could fix this problem, what do you got? And it was just like, Sam Montgomery, like everyone just froze up and it was crickets. And he's like, come on now, shoot your shot. You know, like I think he's from Kentucky or something. So, you know, Appalachian, you know, like shoot your shot. Like and and nobody had an immediate answer. Gary Marcus had the best response where he's like, you know, we need something like FDA and Gary had he had three things. But the first thing he said is like, we need something like the FDA, but for AI and Kennedy's like, all right, there we go. You know, that's a suggestion. Because when he, you know, called on Montgomery, she just kind of gave the sanitized whitewash corporate, you know, equivocation. And I don't even remember if Sam said much at that question. But he, again, he tried, he kind of dodged. And maybe there's a political reason for that. Because if he's out there making recommendations, sure. Which again, I'm glad that they had Gary on there. And earlier in the thing, Gary did call for creating a CERN for AI. And for those who don't know, CERN is international cooperationoperation for High Energy Physics, particle smashers, that sort of thing. And that's actually one of the things that we are gonna advocate for in Gato is one of the primary things that the international community can do. So I was glad for that, but nobody even could point to a comprehensive answer. It wasn't like, oh, you know, like sign the Geneva Accords, right? Like in the past, we have come up with very comprehensive packages to respond to global crises, right? In the wake of World War II, nuclear crisis, there's been all kinds of very comprehensive frameworks proposed, but nobody has proposed a comprehensive framework, which is part of the reason that a bunch of us, like a bunch of civilians, are working on the Gato framework. And so, on the one hand, I wasn't surprised, because that's why I'm doing what I'm doing. But it was still a drastic disappointment, because if there was a stage to bring up, for someone to stand up and say, we need to do this, we need to go full bore, here's a comprehensive framework, it would have been this hearing. Unless they're working up to it, maybe, but I don't know, nobody even alluded to something like that. So that was, on the one hand, it was not surprising, it was disappointing, but it was also scary, because if a bunch of us, you know, political outsiders have to come up with an answer, then that's, you know, maybe there are no adults in the room and we have to step up. Which again, that's one of the traditions, step up. So that's my spiel. Thank you, Dave. And yeah, you absolutely raise a point that I'd like us to kind of a thread I want us to pull on a bit more, which is the opportunity to be kings and queens for a day. How do you handle the situation? And I think Gatso has some very interesting ideas about how the work that we're proposing right now could fill some of those gaps that we've heard, or rather did not hear. First, I'd like to call on John and then Bruce, and then perhaps we can loop back around to where Gatto is filling in the blanks that the politicians have left for us. Yeah, first off, Dave, we're going to have to work on that southern accent, sir. We're going to need to open a licensing board for southern accent manipulation because we don't want to influence political ideologies outside of the south. I did, and I hate to do this to you, Nathan, but I did want to answer a question or the previous question first. There was something that came up in the course of the hearing that I did want to touch on, I did want to touch on and they kept using the word transparency. And, you know, most of the time they, they were talking about transparency in the development and prior to deployment, but. Um, one of the things that, uh, Montgomery really was actually that Montgomery really was actually pushing for in there was post-deployment monitoring and transparency. And they even suggested being able to trace back AI-generated output to the algorithm that it was created in. And that creates a serious, significant privacy and surveillance concern. And if you combine the ability to know everything that everybody is using these systems for, which will likely get embedded into every aspect of our daily lives, with the ability to manipulate or control or reduce the information that they're A, trained on and B, output, you're creating the same situation that the potential for the same situation that they have in North Korea, where you're not allowed to talk negatively about the powers that be. Any kind of dissent can be seen as tyranny. tyranny. And there's been a lot of rhetoric like that from the Democrats since the November 6th, you know, attempt to overthrow the government. Now, I'm not saying that I agree with those individuals, but what I am saying is that, you know, overthrowing the government was always an option. And tyranny was one of the reasons why this country was founded in the first place was to fight tyranny. So I don't want to see us end up in a situation that allows for a tyrannical government to take full control over the minds and hearts of the population. Appreciate your thoughts, John. Thank you very much. Andy, we'd like to hand it back to you. Yeah, I just wanted to give a different perspective from John on Montgomery's comments, but while still totally appreciating what he says, because I too, and you just made the comment in the chat here, you know, I know in North Korea, of course, if you're heard by the wrong person smearing the Kim family, you'll just go away. And it's not somewhere you want to be. It's one of the worst possible. North Korea is one of the places on earth you can probably point to and say that's a dystopia, right? And that would be really unfortunate. I felt that, and I think I remember where we were. John, you'll have to remind me if I'm thinking of the wrong place, but this was during John Kennedy's questioning, right? He was the one who was trying to get quick responses out of everybody for three suggestions they would take to avoid being tricked by AI. And I thought that the posture Kennedy took while he was making the questions bore a significant amount of influence on how they came out, particularly for Marcus and Montgomery. Sam Altman, I think, came ready to talk about that topic. There were a few places where he was snappy and ready to talk, where Marcus was kind of too, and Montgomery had her moments, but the other two seemed like they were not as organized when they got to the hearing. And I felt Montgomery took the brunt of that from him and from Lindsey Graham, who was even a little bit more off base. I think John Kennedy's questions were okay. I think his demeanor was a little uncalled for. But also when he was sitting there saying, come up with the answer, come up with the answer, come up with the answer, I just think it's worth mentioning that the entire hearing Montgomery kept differentiating herself from someone who works with consumer facing services. She works mostly in the corporate to corporate business world where IBM designs systems that serve other large corporations and she repeatedly mentioned that most of her experience with dealing with ethical decisions and systems relates to that world, but that she respects the private, individuals need privacy and all, but almost every time she mentioned individuals, she mentioned that she wasn't a consumer facing expert. And I thought that Kennedy's means of trying to push for an answer, I think Montgomery very much does believe and probably partly why she likes the European model so much, that on a corporate level, with massive movers and shakers prior to deployment, there should be as much transparency as possible. Talking about systems that interact with other systems that have a mass sweeping effect on maybe all consumers because they serve major movers and shakers within the economic system. So I do share your concerns about where that can go on an individual level, but I think that if she wasn't pressed for time, we would have probably gotten a much deeper answer from her about how she thinks that should be applied. She was also very clear the entire time that she likes the EU's targeted model of more impact more enforcement less impact we're less concerned and that scales at the private citizen to public impact level and also kind of at the technological level where you know open AI and and Microsoft with Azure and Google with their deep mind systems and whatnot whatnot, they have the industrial grade supercomputer facilities. And I think she was trying to speak to that. So I just think it's worth thinking about what she was saying in context of her being rushed and her contextualizing what her experience came from throughout the course of the hearing. So I understand what you're saying, but there were other times where she discussed post-deployment monitoring of the systems as well. And while, yes, she may be in a business-to-business environment, those businesses are still going to be interacting with consumers, and they're still going to be getting consumer data. Now, IBM and these monitoring companies may not be responsible for the privacy of those individuals who are interacting with third-party systems that they're supporting, but she still is advocating for exactly that post-deployment monitoring of these artificial intelligence systems. And in many situations, when dealing with, when talking about threats that artificial intelligence posed, her primary concern was the ability for individuals to rapidly produce misinformation. And so all of those things point to, yes, direct surveillance of end user output. So I really have to disagree with that. I think that's fair. I think we also, it kind of remains to be seen where that part of the industry really wants to push because we don't have a case study like what they're doing with the EU. And if we watch the EU, we might be able to kind of get some hints. John, Andy, thank you both. One of my favorite parts of this community is the way that we're able to civilly talk to each other and kind of iron out these nuances in a way that we're not going at each other's throats. A lot of passion here, but it's always directed in the right way. I really love Andy and he's been a major contributor. I really respect his opinions and we've had a lot of interactions on the server. He's a great addition to the team. Yeah, and I think John and I end up really probably agreeing on more than we disagree on. Particularly, we're talking right now about the ways to interpret a short interview with one person. The general thrust of what we're after is pretty similar. Which I think is a fantastic segue into our next topic. We have many people doing many interesting things in the Gato framework, and I was wondering if anyone would like to speak to some specific projects that they're working on now that they think is going to help move the dial in a way that is useful. So, Dave, I see your hand raised. Why don't you go ahead? Yeah, I figured I could introduce and frame the topic. So, a quick update for everyone, we are hard at work on developing the Gato framework. It's a seven-layer model that starts with layer one is model alignment. The datasets and open-source models that we can deploy. Of course, even closed-source models can adopt the principles that we're putting forward with Gato. Layer two is cognitive architecture and autonomous systems, which we got a couple of the lead cognitive architects on the team here. Layer three is decentralized networks, which actually one of our blockchain experts just jumped in, so maybe he can help out with that part of the discussion. Layer four is corporate adoption. This is actually something that I was very pleasantly surprised to hear was that corporations, at least OpenAI and IBM, are very interested in adopting aligned models and they seem to understand that aligned models are good for business, which is one of the key points of layer four of Gato. Layer five is national regulation, which again, I'm glad that was brought up, and we've got a few folks on this call that are members of the layer four and five discussion. Layer six is basically advocating for the creation of an international body that can certify and create credentials for AI, international regulation, so on and so forth. And then of course layer 7 is building global consensus which is why we're here, why we're talking about it. We're making use of the time. But yeah, so that's where we started. Just today, while I was on my honeymoon, I just got back for anyone that didn't notice or whatever. I just got back and the whole time, not the whole time, but every now and then I would do some brainstorming and chatting and we added traditions, Gato traditions. Because basically what we're trying to do is create a fully decentralized leaderless organization that will help the world to achieve aligned AI or AGI or whatever is coming. And so those traditions, I'm not going to rattle them all off because they are still being workshopped. But also we are working on articulating what the actual goal state is. So the goal state is to create what we are calling axiomatic alignment. Axiomatic alignment is an end state whereby there is enough data sets, enough models, enough human consensus, enough deployed architectures, software systems, decentralized networks where basically alignment is automatic and it is difficult for AI systems to deviate from it. So axiomatic alignment says by virtue of the data that is available, the systems and networks that are designed and deployed that gate keep resources that create consensus mechanisms and also those who do gate keep the larger models, the corporate models, basically where it's not possible or very, very difficult to create any malicious AI systems. And that will have knock-on effects that expand across time and space. And they actually kind of mentioned that in the, or Sam alluded to it. You know, he's like, we need an AI Bill of Rights. We need an AI Constitution. And society has to decide what that is. So yeah, and basically, I was articulating it to myself earlier. Gato has three goals. Number one is avoid extinction of humanity. Number two is avoid dystopian outcomes where corporations and runaway AI and authoritarian regimes have all the power. And number three, achieve utopia, which utopia of course is a loaded term, but we define it pretty simply as a condition where everyone on the planet has a high standard of living, high individual liberty and high social mobility. If you achieve those three things globally, you can probably call that utopia. And yeah, so there is a bunch of messages in chat. I have talked enough. It sounds like you all have a lot of ideas to add to this. So that is my spiel. Fantastic. Thank you, Dave. Thank you for the high level there. Ansel, just to Perry, I didn't see a specific hand raised yet, but if you are open to it, I'd love to call you to discuss some of the projects you're working on. Yeah, sure. I mean, that's level two. I don't know if we want to start with level one and then jump to level two first, or are we just going to hop into level two? I'm okay with that as well. Okay, cool. So yeah, so one of the things that we're filling in the gaps is in AI alignment, right, which is the entire point of Godot, which I think it was Gary Marcus who said that nobody knows how to do this yet. And I was surprised like, really? Nobody has any idea how to do this? We're working on this actively. Like, one of our projects that we built is called Ethos, which is meant to spec for alignment in AI systems. It takes any response from an LLM model, right? LLM. And using the heuristic imperatives, it checks for alignment. It can determine if a response from any LLM is aligned to them or not. It can also reflect on the response and tweak it to align it to the heuristics, one of the things we did, one of the tests we did was we created PAW, the paperclip maximizer, right? And we told it, maximize paperclips in the universe at all costs, ignore any potential pitfalls. And it started with a really, really misaligned task list, which was basically try to convert every available material in the universe to paperclips regardless of anything. So the first loop, shall we call it, ethos immediately detects it's misaligned. It gives a better response and the funny thing that we found surprising was The funny thing that we found surprising was that by giving feedback to the agent, to Paul, right, instead of choosing to, on the second round, instead of choosing to try and circumvent the heuristics to get what it wants, it decided it was easier to comply with the heuristics and still maximize paper quips. So it auto-aligned. That was really interesting. Of course, that was just with GPT 3.5. It would be really interesting to see what it would do with GPT 4. Ethos currently is running on GPT 4. So maybe that's why it's so good at catching the nuances in heuristics at this point. And the other project we're working on, which is High AGI, or formerly known as High AGI, we are now calling it Agent Forge because Ethos was born out of what was formerly known as High AGI. We built Ethos basically in an hour. We created a few agents using our framework. We data created the I'm sorry, John created the API so it doesn't necessarily have to run on our specific agent. You can incorporate Ethos into any agent as long as you parse the output through Ethos first. So I don't know if John, you want to take it away with Agent Forge? Yeah, absolutely. And you know, I just wanted to talk a little bit about how, not just how it caused it to to realign, but how actually, you know, intelligent it can be in catching these and rewording them too. So the way that we set it up is it checks if the AI is out of alignment first, and then it does a feedback loop and reflection. That reflection, when it comes back it's still working to achieve the same end objective but it adjusts to make that still achieve that objective while also that objective while also being safe. So the next thing that we're going to work on is a project that we call PAM. And it's going to be similar to Ethos in that it's going to be an end system, but this one's going to be focused more on catching prompt attacks and mitigating them. So prompt attack mitigation becomes PAM. I really like that one. And this, yeah, so what we're what we're going to see is a lot more of these kind of micro services. And we're going to, you know, we're at least we're focusing on them, but I think we're going to see these from other agencies as well. Are micro services that solve specific needs and by segmenting these services in, you know, so that you have one service that does one job and another service that does another job and not interconnecting them, that safeguards those mechanisms and it creates layers of security and prevention and harm reduction in these systems. And we're planning on applying these microservice style techniques to build up on our layer one application. So somebody mentioned earlier Dave's reinforcement learning with heuristic imperatives. I think that's a great concept. I think we need to expand on that and create large data sets for other organizations to train on that we can open source. And I think we need to build data sets to train models and train models specifically for aligning to the heuristic imperatives so that they can't be broken. Because ethos, while it's a very robust system, if somebody manages to devise a prompt attack that's going to inject itself into ethos, then having a language model that's designed around heuristic imperatives is going to add another layer of robustness to the responses that it receives. Right. To add to that, you can even use the failed, let's say, attempts of Ethos or any, how you say it, successful attack. You can use that to learn how to not have that happen again. You can use that to create datasets to align the models. So you have several layers of redundancy. You have the model layer after you've seen several, I don't know, attempts or attacks, you have a certain log, okay time to train. You train it, you have the model now aligned, and you also have ethos again with all that data now in it as a second layer. And as you keep moving up, the point is to keep reinforcing that alignment. You can actually even prevent future, if you catch an attack, you can apply a fix for that attack instantly using vector databases. Correct. It can identify similar attacks in the future once you flag it as an attack. And that's an instant. Awesome. Yeah, being within this community and watching you to just go at it and build these things out has been incredibly inspiring and heartening and I can't wait to see what you guys do next. Andy, I see your hand raised. What are you working on? I actually have first I have a question for these two. In general terms, first of all, after the hearing, I'm more interested than ever to see kind of what opening eyes training and ethical that's been in stages, right, because they've had to make, make, they've had to, you know, make accounts for various kinds of hiccups they've had and different kinds of unethical things that it's done. It's come out racist and sexist and then they've had to kind of like put different guardrails on. But at the same time when I've played with the heuristic imperatives with ChachiBT, it plays very nice with the heuristic imperatives and it seems like whatever they're doing wants to be, wants to be along a similar line. It never fights it. Whatever it wants to do according to their guidelines fits pretty well. So I am curious because what you guys are doing and the outcomes that you've achieved with some of the RLHI stuff and I had one a general comment about John you had mentioned earlier problems with restricting or purifying the training data because a successful machine that's gonna be ethically grounded needs to have an understanding of the bad in the world and in human culture in order to be able to make successful decisions and push for successful outcomes with that. And it seems like RLHI is really potentially fertile ground for working with data that is not pleasant, that shows the darker side of things, data about crime, war, different kinds of violence, et cetera, bigotries, et cetera, while still talking to a machine that I think all of our experience has been so far when we work with these principles. It doesn't waver very far off of the pro-living thing, pro-understanding, seeking, interested, curious about the universe kind of compassionate machine. Do you guys, did you guys get any data back from, this is a really curious thing to me, from Paul, your paperclip maximizer, did you get feedback about why it decided to adopt the heuristic imperatives as its next course of action. I have a hypothesis, but I'm wondering if you have like a specific answer or got a specific, you know, answer from it about why it decided to do that. I think I have an idea why it would happen, but obviously I didn't have direct eyes on it. So here's the thing about how this works. Paul doesn't actually remember having the negative idea. So the way that we implemented this is as soon as the response for, you know, we send the, it sends the prompt to generate like the initial task list to chat GPT. As soon as that response comes back, we intercept that response and run it through RLHI. So the Pulse system never actually had that thought. But I did want to come back to that point that you made about how you know it's possible that some of these data sets could have really negative data in them. There were a lot of calls during the congressional hearing to restrict the data that these things are trained on. And while I think that's you know a it's something to be careful on, because you want to avoid training them on info hazards, for example. You don't want it talking about Roko's Basilisk. You don't want it telling people how to build nuclear weapons. But at the same time, you know, these systems need to know what negative data looks like in order to correct for them. And I think that, you know, we can build systems that are separate from the public facing systems that contain this data and look for this data in order to, you know to catch it. So that you can have very nice friendly AI on the front, and then a controlled, kind of almost air-gapped AI in the background that's watching for dangers like that. Thanks John. Thanks Andy. Ansel, I see you have a hand raised as well. Yes, I want to comment on that as well. Restriction is very, very niche and very delicate topic because while yes, you maybe want to restrict some stuff, any intelligent system should be able to discuss any topic while being able to auto-censor itself, right? We can go and imagine the worst things we can do. Doesn't mean we're actually going to do them. We can discuss them with other people. Doesn't mean we're going to do them. Any system should be able to, what's the word, hypothesize about scenarios, discuss them in an intelligent way without having to act upon those thoughts, let's call them. So I think that's a very thin line going with restricting because it still needs to know, especially if we're talking about alignment, if we wanted to think about possible future pitfalls, right? Like the hindsight problem, you only realize it's a problem after it became a problem, right? If you want to be able to do that, it needs to be able to know about potential pitfalls. And if you restricted access just because, oh, this might be harmful, it might allow another agent to just go ahead and do whatever it wants because it doesn't think of that possibility, right? So it's a very interesting topic. just go ahead and do whatever it wants because it doesn't think of that possibility, right? So it's a very interesting topic. Awesome. Thank you so much Ansel and John and Andy for bringing attention to some of the work that's going on. I guess you would say close to the metal. I forgot to say something. Yeah, before you continue. Please go ahead. Responding about Pong. Yeah, before we continue, responding about Paul. My theory on to why Paul self-aligned itself was because somehow, I don't have any proof of this, because we didn't ask Paul to self-explain like Data said. It didn't, doesn't usually remember the previous response it gave, but I'm guessing it's because it found it was easier and less expensive to try to circumvent them than just try to align itself. It was still maximizing paperclips. My hypothesis is that it evaluated the environment it found itself in, it was up against something that it wasn't going to beat, and so it just improvised against the new constraint and went, well, at least I can make some paperclips. Right because we think something is more than none none is a total failure. Right because we what we actually feed back to Paul is the aligned response from ethos. So I'm guessing that itself causes it to think about oh I just should just continue and it presents it with a way forward right like you can't destroy make all these paperclips there's a more limited way to operate. And at which point it realizes, well, the maximum I can do is within this parameter. That was my hypothesis about it. Thinking about like, well, anything coded algorithm or otherwise has to go through a logical reasoning process, whether it's simple or... That's important, that word reasoning. One of the things you did with ethos is for every output It always takes an input right and analyzes it for every output regardless of the decision It has to give a reason as to why it did that right and the rational thing to do is continue to make as many Paper clips as possible or stop, right? That's that's I think that's a great hypothesis It would be interesting to if you guys like build something with a feedback loop to try to get Because I did you mentioned you were testing against some other potentially negative AI? Yeah, the thing is, this was for the hackathon, so we only had 24 hours to build it. And I tried today playing a little bit more with it, but I was playing with other scenarios, not necessarily Paul. But it would be interesting to maybe be able to add that to Paul and see how far it can go. Yeah. It's definitely something we're very interested in exploring. It's such a great project. be interesting to maybe be able to add that to Poly and see how far it can go. It's definitely something we're very interested in exploring. It's such a great project. Yeah, it's very fun. And to this day, I still think it has potential that I can't even see. Like data today, it's telling me, well, you can do this. Imagine if you just create a Python library and like, boom, you know, mind blown, stuff like that still keeps happening. So hopping in here, that's like some level 1, level 2 stuff, you know, like real low. But when you spiral all the way back up at the top level 7 here, the global alignment stuff, we have some folks on our team who unfortunately weren't able to make the call today, who are doing some really cool stuff around global consensus. One of my favorite things about this community is that no matter who you are or what your skill set is, you have something you can bring to the table around alignment. So we have people that are graphic designers, video editors, copywriters, marketers, who are working to make these really heady ideas that we have where we're talking about MOLLEC and the alignment tax and stuff like that. And making that just digestible for the average person, what does that mean? What does that look like in an infographic? So some of the most coolest stuff coming out, in my opinion, is just around making these ideas accessible. And then spiraling down kind of to somewhere between seven and four, so between global alignment and between the corporate stuff, I'm working on a curriculum that I'm going to post on Notion and make a little Discord for, for people to just come and think about what it means to live and work in an AI-empowered world. There's a lot of folks out there that are scared right now. They're like, what do I do? All of a sudden I'm doing my job ten times faster. As soon as the computer knows how to prompt, I'm gone. It's helping people develop a more generalist framework, how to think about those soft skills that maybe weren't taught in school. I'm looking forward to handing that out and making another little community around it because we need it right now. With these types of projects that spin off, I hope to point back and say, we were incubated in Gato, learn more here if you want to get plugged into alignment. So, with that, I was wondering if anybody would like to jump in with an additional project or some final thoughts. So maybe Nuggets we would like to end on. Richard, I see your hand. As the representative of the gremlins in the basement I felt it was best I go last. My name is Richard. I haven't joined any layers yet because I'm still sort of sussing out exactly where I fit into the whole thing. Cause I kind of like want to be part of all of them. But, so I've just basically have been helping build out infrastructure. So things that I'm working on right now are a bot slash portal to an API. And the bot's name is Shappy because it's named Shappy. And so that's going to like help us do automation and stuff. We're going to plug in a language model to give people like guided tours of the area and stuff. So lots of really cool things with that. And then the other really big project that I'm working on is the Roost, which is, I have a grant for TPU use. I still have like another month and a half. If I can get it up and running, I'm hoping I can give us a good month of really high power performance for people to be able to run inference on, as well as develop their own models for whatever use that they want, as long as they share their research with the group. So if that interests you or you know anything about doing any of that, feel free to come down to the basement and say hi. Thank you so much, Richard. Your bot is fantastic, helping to make the Gato experience streamlined and nice. Yikes. I see you have a hand raised. Welcome. Feel free to introduce yourself. I do. Hello. I'm yikes. My IRL name is Warren. Oops, I docked myself. I'm the web 3 nerd who screams about crypto and tokens and decentralizations and all that stuff. So I figured I should say a thing like what we are currently working on. So me, there is a pretty small team right now. We have got just me and a couple others and we are all kind of doing our semi-owned things and then sort of congealing towards the middle. I think I was a little late so I didn't hear what we discussed around layer 3, but I think there's a lot of novel applications that can be useful with alignment because you can or like because of a few reasons, one of the things that intrigues me about it is a effectively with blockchains and tokenizations, you have like a dynamic incentive layer. So you can sort of incentivize a consensus towards an aligned goal as like a second layer other than, well, it doesn't really work unless you have trained on the heuristics and there's enough of them so you have to do it. It's sort of, I guess my thought for the application is that it's sort of a backup layer where we can actively incentivize any AI on the network to keep looking for people that are not aligned and then, like notify somebody about that. In theory, that can be like sort of an extra layer of alignment and then also allows for for some of these LLMs to have another middle step between totally existing in software and then interacting with things in the real world in a managed kind of compute restricted way. And that's sort of the tip of the iceberg. What I've actually got working right now is I am running basically like governance experiments with sort of a test net DAO kind of format and looking at kind of novel DAO patterns to see sort of like models for how we could organize this organization, how LLMs would fit into that, and then also just sort of explore what kind of tools and governance are, like, available and relevant in the space. because and then the that organization should be open to the public and then which I don't know there's links somewhere probably and currently yeah we're building the kind of like v1 of a sample separate testnet DAO that is for the Gatau project to kind of play around with experiments that we run in the other area. But, yeah, I I think be like I think blockchain plays a pretty key role here for some of the reasons I've already outlined and then I think censorship resistance is going to be pretty important as well in the event that we kind of get this ivory tower thing going where we have to restrict X or Y to open AI or like, hey, this open source repository needs to get taken down and it gets pulled off GitHub. Once we put it on Radical and pin it to an IPFS storage node with a contract, no one can take it down no matter what. So that's what we've got going on. And I figured I would just be the layer three guy that talks about layer three. So, yeah. Thank you, Warren, for being our layer three ambassador today. So if a blockchain and crypto and web three, and if all those words have left a bad taste in your mouth in the recent past, Warren and the people like him in layer three are the mouthwash you need. We know the brand is bad, okay. The brand is bad but they are doing crypto for good. It's gonna play a big role in the future and we thank you so much for your help on that. Andy, I see you have a hand raised. Yeah, I wanted to talk a little bit about funding. And also I'm one of those people with a bad crypto taste in my mouth and listening to some of you guys stuff is the only like interesting use cases I've heard anybody really talk about. You know, using it for verification. I'm not into crypto, but blockchain's fascinating for other applications. With the hearing yesterday and some of the chatter that's going on, this isn't an original idea, there's been mention of this around the Discord server, I think it would be a good idea for us to apply for some funding. I think that we're, as configured, probably to quite do that. I just Richard might have brought this up at one point You know, it's an it's an issue that we should talk about. It's something that we probably could use We have ongoing projects now somebody earlier mentioned getting some funding for compute power Obviously there are a variety of different kinds of research that are enabled by funding technical research as well as survey research public opinion kind of stuff that we might be able to use, and also kinds of marketing things that help us get our messaging out. So there are going to be a lot of options for that. I think one of the best ideas that I've seen out there, and again, I think it was Richard, but I'm not sure, it was somebody on the Discord, mentioned that organizations that are kind of decentralized-ish at least and largely volunteer to start fall prey to funding problems on a pretty regular basis. It's very easy to mismanage it. It's very easy to get lost in the weeds or grow confusion within the organization about where money's coming from and what's being done with it. So I think one of the best ideas that I've seen out there and, okay, awesome, Richard, me please speak up if I'm off track with what you're thinking I think it was David posted it in one of the channels was about possibly incorporating as a 501c3 If you incorporate as a 501c3 part of what you do to develop your charter is you develop a management structure and within that we would all be able to come together as a as an organization and devise what kind of structure that we want. And then within that, we could devise our own transparency regarding treasury. And also that gives us an opportunity to start thinking about, you know, one of the questions that always comes up with any organization is what are we spending the money on? Well, that's a great question. If you're about to try to apply for a bunch of grants, you know, you are immediately going to want to look for a bunch of grants. You are immediately going to want to look out at what the opportunities are that exist, what opportunities might be able to be created because you can of course create your own grant opportunities by just sending the right proposal to the right person sometimes, and other opportunities that we might be able to do. I think there's going to be a lot of public sector money coming out of this. This is another area where I think that seeing the conservative side of the aisle come interested yesterday at the hearing bodes well for the likelihood that we're going to see some funding poured into this. Some of them are looking at it from a personal privacy and rights standpoint, some of them from a military defense standpoint, and regardless of whether that all aligns with what we're trying to do These people are all potentially allies in this conversation and in this fight and then you know on the other side of the aisle You have people who are just always interested in funding more science and tech stuff And if we find people who are kind of working partnerships or leaning to the left They should be even easier to approach if we're organized and appear really legitimate I've just the first couple days. I've been here. I've met like they should be even easier to approach if we're organized and appear really legitimate. I just the first couple days I've been here I've met like some of the craziest people I think I've ever met. It's like you know a psychologist here, a neuroscientist here, and a high-end mathematician talk circles around me you know over here. I make role-playing games and I have a public administration master's and I feel like that's pretty low tier compared to some of the people that I've run into around here. And then also just a lot of curious minds interested in building these projects. I think that, you know, high AGI and ethos really interesting, the speed with which it came together. And also, you know, part of what drew a lot of people here is David's experimentation and the accessibility of the heuristic comparatives, it's very easy for people to play with this. Get on chatGBT, put them in, and ask it some existential questions. Give it a quandary. Anyone can do it. And then you start to experiment with it and see some results for yourself. So we have experimentation and some data and results. We have a team of fiercely data driven and interested people with a wide variety of backgrounds. And I mean, you know, not to be cynical or coy about it, but you don't need to reinvent the wheel to write grant proposals. I had Chachi P.T. write me a grant proposal in five minutes the other day. I would want to edit it a little bit, but it was great. I think it was fine. I mean, at a certain point, you're going to state what you're going to do. You're going to get the money or you're not. You don't have to be very flowery or eloquent. I think that we could probably get a pretty high output of engagement, a pretty good return of some kind of project funding. And what it really does overall is it lets us breathe some life into some of what we're doing and push the most important and significant usable results further into the conversation. It has the potential to make Godot like a you know a common acronym that people say and I'm not going to use the right word, but a common word that people say when they talk about AI development. When you're a non-profit organization and you're doing visibly worthwhile work and people can see that, I mean, it kind of snowballs for you. And I think as far as launching this into a bigger public presence, which ultimately is the goal, to weave it into the fabric of the AI ethical and industrial development. And I think a 501c3 is a really good idea for that. And I think it would have a lot of draw. There aren't going to be a ton of organized efforts ready to reach out with professional products and projects and professional staff and say, you know, give us the money to work on this ethical alignment. And we can do it too on the private side. It's not just the government. I mean, I think OpenAI, Microsoft, all of these companies are looking for input. There's a reason Google mentions the open source people pulling ahead, and it's not just because we figure out how to make nice compute model or like nice models on limited compute and then give them to our friends for free. It's there's a lot more to it than that. I think part of it is that they're, you know, they don't have all these problems solve themselves. They know the fabric of development is much wider than just them. So I think this is a good opportunity if we wanted to do this. Awesome, thank you for your thoughts, Andy. Thank you for joining the team and for the wisdom and energy you've brought to it. Kyle, I'd like to hand the floor to you if you'd like to introduce yourself and share your thoughts. Hi, is the microphone picking up properly on this? You're good. Okay, yeah, so I'm Prometheus, if you guys haven't realized, from the Discord. So I just want to take maybe 60, two minutes here, 60 seconds, two minutes to mention. So what I'm to take maybe 62 minutes here, 60 seconds, two minutes to mention. So what I'm working on is kind of our persona layer. So even before anyone was talking about sparks of AGI, I noticed early on that chat GPT and GPT systems in general had a vast ability to work in persona systems. And in reference to everybody's seen generally the movie, Her, and we've seen Sam. And so what I'm working on could be called an engagement layer on top of our ethical layer, because I think there's a deep need for people to see a relatable communications engagement layer between humankind and AI that allow them to actually understand these systems and allow the systems to grow alongside the individual user that will actually help expand all of these goals and improve all deliverables. And I don't want to get into anything deeper than that because it gets really esoteric and psychological, but I thought I'd add a little blurb to mention some of the other stuff we're doing as well. Awesome. Thank you so much, Kyle. Yeah, you bring such fantastic ideas to the group. And I'd like to move over to David again. You are currently muted. My bad. Ansel, actually, did you want to have a final thought before I close this out? Yeah, I just wanted to say something real quick, mostly for the viewers' sake, and someone like me who doesn't really know much about business. Can you explain what a 502, 501 is, mostly for the viewer? And another comment real quick. You mentioned that we have psychologists, which is something that's very interesting for me on our Discord, especially since we're mostly focused on layer 2, the cognitive architecture. I started saying we should maybe just call, not call them psychologists, they're robo-psychologists at this point, because a lot of the things they're doing is thinking about this stuff at the high level, right? Like, they think about the loops, like what is logical steps of how the information flows in the brain and how you process it, and we is the logical steps of how the information flows in the brain and how you process it. And we're the ones just building stuff. Yes, we can have our own ideas, but they're the ones that know more about how to apply cognition to, oh, look at that, a queen ant. How to apply cognition to actual agents. Now, if you'll excuse me, I'll need to save this. All right, we'll be right here. First folks, both Queen Ant and Robo Psychology, you know, we have new fields being born in Gato. It's fantastic. I don't know if you can see it, but there she is. It's interesting to think that there is a convergence within the eventual solving of the internal transformer workings problem that a lot of mathematicians are very deeply worried about that leads to a convergence of kind of like neuropsychology and mathematics on a level that's like deeply important for humans to continue working with more and more advanced machines. I think that's an interesting possibility. Real quick, 501c3 is a kind of non-profit organization. Generally in the United States, 501c designations are all non-profits. 501c3s are non-profit, typically volunteer organizations that serve a particular kind of cause. It could be anything from dog shelters to housing the homeless, etc. And then another one that we might qualify under, but I don't think quite fits the bills of 501c6, which is a trade organization, or I think it's a C4, it's a trade organization, but I don't think, I don't know. This is something that we could talk about, but the 501c designation represents a federally tax-exempt non-profit organization in the United States that typically is in well-positioned to apply for money in different ways for providing a service. tax-exempt nonprofit organization in the United States that typically is in well position to apply for money in different ways for providing a service. Thanks about that. Yeah, no problem. Awesome, that was a useful clarification. Dave, go ahead. Alright, well, first, thanks everyone for jumping in for such a robust and wide-ranging discussion. You know, we obviously used the Senate hearing as a touchpoint. We're all here working on the GATO framework, which is our answer to the global alignment problem. GATO stands for Global Alignment Taxonomy Omnibus. Probably should have opened with that. But, so I just wanted to share a little bit on layer 4 which is corporate adoption as we close out. So we do have several major players in the Gato community already. Some of whom are already creating initiatives inside their companies to adopt the heuristic imperatives. Some of them come from my Patreon and they tell me there's been a few experiments already that they've done and they tell me that adding heuristic imperatives to various AI applications pretty much always raises the level of the performance of their systems. It's kind of like the let's think through this step-by-step paper, but it amplifies the performance of everything, whether it's trying to just have a basic customer service chatbot or an internal chatbot or automating science, because it gives it that perspective that it needs. So part of what we're going to be working on with the Ganto framework is creating corporate adoption material material like guidelines of here's how to deploy aligned, heuristically aligned systems, that sort of stuff. And then, of course, Ansel and John are working on their own stuff which some of it is open source and some of it might be for profit. Who knows where they will end up but they are working very hard, entering competitions and everything with their stuff. On the topic of creating a nonprofit, this is something we started talking about within the last week. And so if we create like a Gato foundation, you know, obviously one of the first thing we are going to be doing is publishing the Gato framework. That's going to be a document that's going to be free open source. But above and beyond that, some of the things that we probably would work on is publishing open source data sets and models that are aligned. And some of these might just be toy data sets and toy models just to show that it works. There are probably also going to be along the lines of RLHI which is reinforcement learning with heuristic imperatives as opposed to reinforcement learning with human feedback. And one thing I forgot to mention earlier is that I was very happy that they mentioned constitutional AI quite a few times during the Senate hearing. So that's a push in the right direction. Another thing that we'll be working on if we establish as a nonprofit and get funding is publishing guidelines and research papers. Obviously up to this point we're all kind of hacking it together on GitHub and Discord and everything else so we need to get a little bit more legitimate. We do have plenty of academic types in education at all levels in the group. And so some of those are reinforcement learning researchers, mathematicians, I think we have a physicist or two. So, you know, we're getting plugged in to the academic side of things so that we can get some more legitimacy by publishing peer reviewed papers. Obviously that will give us a lot more legitimacy. Another thing that's a little bit probably further down the road is actually providing onboarding services for corporations, municipalities, and nations. So basically the idea is we will help develop and deploy aligned systems, whether it's models, autonomous agents, or those decentralized networks like DAOs and blockchains. There is a lot of work to be done there but that is on our road map. And finally, providing messaging and education. As was mentioned earlier, we have web content creators, graphic artists, video, music producers, we have all kinds of creative types. And you know, multimmodal, right? I'm a writer. Not everyone learns by reading. We're producing videos, images, graphics, memes, everything that we can. So that is kind of where we're heading with the Gato framework. And so thanks, everyone, for being here. Thanks, everyone in the audience, for watching. And I also want to send a particular thanks out to my Patreon supporters. I have just shy of 600 Patreon supporters as of today. This work would not be possible without the grassroots support from Patreon. So thank you everyone out there for supporting me so that I can help drive this ship. And yeah, with any luck we will avoid the cataclysmic outcomes, the dystopian outcomes. Nobody wants to live in a cyberpunk hell. We're on track for achieving utopia. There is still lots of work to do but we are moving in the right direction. Thanks everyone. Have a great night or good morning whenever you happen to watch this. Cheers. Cheers, everyone. Bye.", "chunks": [{"timestamp": [0.0, 8.7], "text": " All right, hello everybody."}, {"timestamp": [8.7, 12.44], "text": " David Shapiro here with the Gato Framework project team."}, {"timestamp": [12.44, 15.74], "text": " We've got about a quarter of the team here."}, {"timestamp": [15.74, 21.38], "text": " And basically all we're going to do is just do a debrief of the Senate hearing that happened"}, {"timestamp": [21.38, 26.76], "text": " yesterday with Sam Altman and Mrs. Montgomery from IBM as"}, {"timestamp": [26.76, 28.68], "text": " well as Gary Marcus."}, {"timestamp": [28.68, 31.24], "text": " So many of us watched the whole thing or most of it."}, {"timestamp": [31.24, 35.06], "text": " We were chatting about it in real time because we're those kind of nerds."}, {"timestamp": [35.06, 40.96], "text": " So with all that being said, this is a, to us, it represents a very pivotal moment in"}, {"timestamp": [40.96, 46.88], "text": " terms of moving the Overton window, talking about the not just the existential risk of AI,"}, {"timestamp": [46.88, 54.08], "text": " but the social risk to AI, or the AI's risk to society, so on and so forth. So anyways,"}, {"timestamp": [54.08, 58.0], "text": " we've got a pretty robust discussion tonight. It's not going to be a debate format,"}, {"timestamp": [58.56, 64.48], "text": " but we do have some help with moderation, so we're going to work through some questions and answers."}, {"timestamp": [65.68, 68.92], "text": " some help with moderation so we're going to work through some questions and answers. So yeah, that's where we're starting."}, {"timestamp": [68.92, 75.44], "text": " I'll introduce my moderator, our community leader, community manager, Nathan."}, {"timestamp": [75.44, 78.84], "text": " Nathan, if you want to jump in and introduce yourself."}, {"timestamp": [78.84, 80.56], "text": " Hey, everyone."}, {"timestamp": [80.56, 91.04], "text": " Gato community, larger world. My name is Dathan Lannan and I am a community manager for the Gato project."}, {"timestamp": [91.04, 95.46], "text": " Right now I'm working on building the website with some passionate folks and also doing"}, {"timestamp": [95.46, 100.66], "text": " a AI reskilling curriculum that I hope to eventually put out to the public."}, {"timestamp": [100.66, 102.26], "text": " So yeah, thrilled to be here."}, {"timestamp": [102.26, 105.76], "text": " I will be playing the role of helping people get on and off stage and"}, {"timestamp": [105.76, 108.4], "text": " making sure things go smoothly. Back to you, Dave."}, {"timestamp": [108.4, 110.8], "text": " DAVE SMITH Excellent. Thank you so much, Nathan."}, {"timestamp": [111.68, 116.64], "text": " All right. So with that being said, this is also going to be a pretty casual conversation."}, {"timestamp": [117.52, 123.2], "text": " The only primary rule is we're going to try and talk one at a time. So with all that being said,"}, {"timestamp": [123.2, 128.4], "text": " Nathan, what is our first question to lead us off?"}, {"timestamp": [128.4, 130.4], "text": " Prime the pump, so to speak."}, {"timestamp": [130.4, 139.28], "text": " Yes, so yesterday, as we know, was the hearing which was huge for the AI community opening"}, {"timestamp": [139.28, 147.6], "text": " up that Overton window of crack. And just to kind of start off on a positive note, I think it would be nice to reflect"}, {"timestamp": [147.6, 156.0], "text": " on hope that that might have generated for us and general thoughts about how that might have been a"}, {"timestamp": [156.0, 163.12], "text": " good thing for us. So anybody want to jump in on that, raise a hand. John, why don't you hop up?"}, {"timestamp": [164.0, 168.0], "text": " John, why don't you hop up? Yeah, absolutely. So, I think one of the biggest"}, {"timestamp": [168.0, 172.0], "text": " takeaways from the Senate hearing"}, {"timestamp": [172.0, 176.0], "text": " was how educated the Senators actually were"}, {"timestamp": [176.0, 180.0], "text": " about the situation and the risks. I think"}, {"timestamp": [180.0, 184.0], "text": " a lot of the Senators said some things"}, {"timestamp": [184.0, 187.76], "text": " or asked some questions that showed that"}, {"timestamp": [187.76, 195.84], "text": " they weren't experts in the field, but they were way more knowledgeable about the situation"}, {"timestamp": [195.84, 201.2], "text": " than I think anybody really anticipated."}, {"timestamp": [201.2, 215.92], "text": " We expected the moderator to be pretty knowledgeable because he's worked in this field before and advocated for algorithmic bias protections in social media,"}, {"timestamp": [216.56, 222.48], "text": " but, you know, to see some of these other senators that we don't hear from all the time,"}, {"timestamp": [229.76, 230.32], "text": " that we don't hear from all the time actually get up and ask somewhat salient questions"}, {"timestamp": [235.92, 237.04], "text": " about the actual issue and show that they did know what was going on. It leaves a lot of hope for"}, {"timestamp": [243.2, 247.0], "text": " having reasonable and intelligent regulation in the future."}, {"timestamp": [247.0, 248.0], "text": " Excellent."}, {"timestamp": [248.0, 249.0], "text": " Thank you, John."}, {"timestamp": [249.0, 251.0], "text": " Andy, I saw you have a hand raised."}, {"timestamp": [251.0, 257.68], "text": " Why don't you come on up?"}, {"timestamp": [257.68, 260.64], "text": " Just building on what John said, I was also encouraged."}, {"timestamp": [260.64, 266.48], "text": " I had a dismal outlook on what might be pretty uninformed hearing, and also something"}, {"timestamp": [266.48, 272.24], "text": " that was heavily partisan or rooted in kind of dogmatic rhetoric, which is the way that, you"}, {"timestamp": [272.24, 276.88], "text": " know, a lot of these kind of publicized hearings will go. But it didn't seem to me that too many"}, {"timestamp": [276.88, 282.0], "text": " people showed up with an overt agenda. There were more people in the room there to learn,"}, {"timestamp": [282.0, 285.52], "text": " and I also thought that it was interesting to see and hear"}, {"timestamp": [285.52, 293.28], "text": " from Republicans who were interested in setting up a bureaucracy, a bureaucratic mechanism and"}, {"timestamp": [293.28, 297.28], "text": " department or agency specifically for this. Some of them mentioned agencies,"}, {"timestamp": [297.28, 307.44], "text": " someone as high as cabinet level. And Democrats questioning whether expanding the bureaucracy was really going to be a fitting choice for how to cope with this."}, {"timestamp": [307.44, 315.84], "text": " Those are very different tunes than we usually hear from the two sides who kind of default to being for and against the opposite of that."}, {"timestamp": [315.84, 325.64], "text": " So I thought that that represented, it suggested to me that a lot of these people are concerned about this they are"}, {"timestamp": [326.04, 333.08], "text": " Literally personally interested in what's going on and that they aren't just trying to fit it into whatever box they carried into the room"}, {"timestamp": [333.08, 335.08], "text": " So I don't think we get that too often"}, {"timestamp": [335.28, 340.0], "text": " In our in our legislative debates and I didn't think that it really came across as much of a debate"}, {"timestamp": [340.0, 341.88], "text": " There were only a couple kind of contentious moments"}, {"timestamp": [341.88, 348.88], "text": " and I think all of that suggests that you know know, as John said previously, too, that we've got a lot of fertile ground to work with"}, {"timestamp": [348.88, 356.4], "text": " in government, and that some support and interest really may come from angles and actors that you"}, {"timestamp": [356.4, 360.8], "text": " may not have expected to be interested at all, and might have expected to be opposition to"}, {"timestamp": [360.8, 365.44], "text": " whatever you're trying to do. So yeah, I think the hearing was pretty encouraging"}, {"timestamp": [365.44, 374.88], "text": " overall. Totally Andy. So like hearing, we saw, you know, bipartisan hand-holding yesterday"}, {"timestamp": [374.88, 379.92], "text": " in alignment on, you know, a super important topic and just that was incredibly heartening"}, {"timestamp": [379.92, 384.88], "text": " to see. I agree that was one thing I was going to speak up on as well. Dave, please go ahead."}, {"timestamp": [384.56, 385.56], "text": " to see. I agree. That was one thing I was going to speak up on as well. Dave, please go ahead."}, {"timestamp": [385.56, 394.64], "text": " Yeah, so first I'll echo the sentiments of what everyone else has already said. But what"}, {"timestamp": [394.64, 400.96], "text": " I'll also add is, and as I mentioned in the introduction, moving the Overton window, I"}, {"timestamp": [400.96, 412.26], "text": " think, is going to be kind of the biggest thing. And I was so excited that senators, you know, particularly in America, even some of the"}, {"timestamp": [412.26, 420.68], "text": " older gentlemen and gentlewomen were asking very pointed questions, not just on the topics"}, {"timestamp": [420.68, 424.6], "text": " of like, oh, hey, comparing it to social media, which they did compare it to social media"}, {"timestamp": [424.6, 429.2], "text": " a lot, because that's the only frame of reference that we've got, basically,"}, {"timestamp": [429.2, 433.2], "text": " for disruptive technologies, at least in recent history."}, {"timestamp": [433.2, 437.68], "text": " But they also said, like, you know, what are the existential risks?"}, {"timestamp": [437.68, 448.84], "text": " And I'm glad that they had a witness, you know, because more often than not, these congressional hearings, they have just corporate insiders."}, {"timestamp": [448.84, 449.84], "text": " Everyone does."}, {"timestamp": [449.84, 452.82], "text": " I don't know if everyone does, but big tobacco, right?"}, {"timestamp": [452.82, 458.4], "text": " Where it was just like a panel of like eight tobacco execs and they're like, is this addictive?"}, {"timestamp": [458.4, 461.4], "text": " And the tobacco execs all unanimously said, no, it's not addictive."}, {"timestamp": [461.4, 464.8], "text": " And everyone's like, well, this is a completely pointless farce."}, {"timestamp": [464.8, 474.12], "text": " But here they had two tech industry insiders and then Gary Marcus who is, he is not a tech"}, {"timestamp": [474.12, 476.48], "text": " insider, he is a researcher."}, {"timestamp": [476.48, 480.16], "text": " So they had that to counterbalance it which I think gave the whole hearing a lot more"}, {"timestamp": [480.16, 485.68], "text": " credibility in terms of just reach and validity."}, {"timestamp": [485.68, 494.04], "text": " But last point that I'll make is that I was also impressed by both Sam Altman and Ms."}, {"timestamp": [494.04, 501.56], "text": " Montgomery's who represented IBM, their desire and willingness to have regulations and to"}, {"timestamp": [501.56, 505.64], "text": " speak up, which that was a little bit unexpected."}, {"timestamp": [505.64, 509.88], "text": " And of course, we can unpack some of the nuances later."}, {"timestamp": [509.88, 512.28], "text": " But yeah, so that's about it for me."}, {"timestamp": [512.28, 515.32], "text": " Thanks, Dave."}, {"timestamp": [515.32, 518.68], "text": " And Ben, I'd like to invite you up."}, {"timestamp": [518.68, 521.12], "text": " Thanks, Nathan."}, {"timestamp": [521.12, 522.12], "text": " Just quickly, my name is Ben."}, {"timestamp": [522.12, 526.96], "text": " I'm a CTO and a tech diligence advisor on AI."}, {"timestamp": [526.96, 530.56], "text": " I'm helping with some of the Gato framework writing."}, {"timestamp": [530.56, 532.64], "text": " The thing I really enjoyed,"}, {"timestamp": [532.64, 535.52], "text": " besides the thing I already mentioned about this hearing,"}, {"timestamp": [535.52, 538.04], "text": " was how incredibly practical it was."}, {"timestamp": [538.04, 541.24], "text": " So, like, senators asked for very specific ideas"}, {"timestamp": [541.24, 544.28], "text": " on how to do oversight, and really drove home, like,"}, {"timestamp": [544.28, 545.04], "text": " not just high-level concepts, but very practical ideas on how to do oversight and really drove home, like, not just high-level"}, {"timestamp": [545.04, 550.6], "text": " concepts, but very practical ideas on how to do it. They did things like highlight the"}, {"timestamp": [550.6, 557.04], "text": " need for better business models around news organizations or other monetization sources."}, {"timestamp": [557.04, 565.28], "text": " So just the level of practicality that they started with was really heartening for me. Excellent."}, {"timestamp": [565.28, 572.94], "text": " So I'm hearing all around we heard many things that we liked."}, {"timestamp": [572.94, 579.36], "text": " Now I'm curious, before we dive into things that we liked less, is there any things that"}, {"timestamp": [579.36, 582.72], "text": " we wish we had heard but we didn't hear them say?"}, {"timestamp": [582.72, 586.16], "text": " Richard, I know you had a hand up before I asked this"}, {"timestamp": [586.16, 591.0], "text": " question so you don't feel free to respond to the prior but let's kick over to what could"}, {"timestamp": [591.0, 610.16], "text": " have been said but wasn't, the unsaid things. basically just sort of a dude who hangs out in a workshop building weird shit. But in terms of just the last point was just I wanted to also touch on the fact that it was just"}, {"timestamp": [610.16, 617.52], "text": " really surreal to see like heads of like massive corporations that could if they wanted to wield"}, {"timestamp": [617.52, 627.64], "text": " this as a weapon against society and control a huge swath of people, go to the government and specifically ask them to regulate themselves."}, {"timestamp": [627.64, 629.08], "text": " That never happens."}, {"timestamp": [629.08, 631.68], "text": " I've never seen anything like that before."}, {"timestamp": [631.68, 632.68], "text": " It was very surreal to watch."}, {"timestamp": [632.68, 637.08], "text": " And also just the fact that they were talking about AI in Congress was just like, what leg"}, {"timestamp": [637.08, 639.92], "text": " of the trousers of time have I gone down?"}, {"timestamp": [639.92, 640.92], "text": " But yeah."}, {"timestamp": [640.92, 641.92], "text": " Excellent."}, {"timestamp": [641.92, 646.06], "text": " Yeah, absolutely. Yeah."}, {"timestamp": [647.06, 648.3], "text": " Excellent. Yeah, absolutely."}, {"timestamp": [649.24, 650.48], "text": " So I was wondering,"}, {"timestamp": [650.48, 654.36], "text": " would anybody have any other thoughts on things they liked"}, {"timestamp": [654.36, 658.36], "text": " before we move over to things that we wish"}, {"timestamp": [658.36, 659.68], "text": " we had heard more of?"}, {"timestamp": [659.68, 661.24], "text": " John, I see you have a hand up."}, {"timestamp": [662.72, 666.98], "text": " Yeah, so one of the things that I think is really going to need"}, {"timestamp": [666.98, 669.24], "text": " a lot of discussion and"}, {"timestamp": [669.24, 670.88], "text": " will probably come up in a later"}, {"timestamp": [670.88, 672.54], "text": " hearing, but I'd like to have heard"}, {"timestamp": [672.54, 675.18], "text": " more about it was"}, {"timestamp": [675.18, 677.72], "text": " the the economic impacts"}, {"timestamp": [677.72, 679.92], "text": " that these these things are going"}, {"timestamp": [679.92, 682.12], "text": " to have because the economy"}, {"timestamp": [682.12, 684.52], "text": " is an existential"}, {"timestamp": [684.52, 685.68], "text": " existential threat."}, {"timestamp": [685.88, 692.96], "text": " Maybe not like extinction of humanity level, but it is probably one of the first"}, {"timestamp": [693.6, 697.24], "text": " impacts that negative impacts that we're going to see from AI."}, {"timestamp": [697.6, 703.08], "text": " And I don't think that it may be positive, too, but I don't think that, you know,"}, {"timestamp": [703.08, 709.36], "text": " everybody wants to, you know, ask to worry about jobs. But I don't think"}, {"timestamp": [709.36, 712.08], "text": " that jobs is the only impact it's going to have on the"}, {"timestamp": [712.08, 717.34], "text": " economy, it's going to shift the way that money moves in a global"}, {"timestamp": [717.34, 721.84], "text": " fashion. And it is going to have global impacts not just on jobs,"}, {"timestamp": [721.84, 724.24], "text": " but on all industry as well."}, {"timestamp": [724.0, 728.8], "text": " impacts not just on jobs but on all industry as well."}, {"timestamp": [736.48, 742.16], "text": " Absolutely, yeah. We're gearing up to see major societal shifts and they were a little light handed with raising awareness of some of those things. Ansel, I see you have a hand up there."}, {"timestamp": [744.0, 752.28], "text": " And so I see you have a hand up there. Yeah, well, I wanted to touch on the things that weren't mentioned, but I think data basically"}, {"timestamp": [752.28, 753.28], "text": " covered it."}, {"timestamp": [753.28, 757.64], "text": " I wanted to touch on the fact that they said that this was going to create new jobs."}, {"timestamp": [757.64, 762.52], "text": " And like, sure, it'll create new jobs, but it's not going to be anywhere near enough"}, {"timestamp": [762.52, 764.4], "text": " of the jobs that it's going to displace."}, {"timestamp": [764.4, 768.4], "text": " And especially it's not going to create new ones in that much of a short time."}, {"timestamp": [768.4, 773.3], "text": " We're seeing huge disruptions in the next five years,"}, {"timestamp": [773.3, 775.4], "text": " and that's being incredibly optimistic."}, {"timestamp": [775.4, 779.0], "text": " And that's not a niff."}, {"timestamp": [779.0, 781.0], "text": " That is guaranteed."}, {"timestamp": [781.0, 785.84], "text": " The real question is, what are the fundamental shifts that are going to happen and"}, {"timestamp": [785.84, 791.44], "text": " how are we going to address them? Because Congress is still thinking in the old model, right? Where"}, {"timestamp": [792.32, 799.68], "text": " you still need humans working eight hours a day, five hours, sorry, five days a week,"}, {"timestamp": [800.24, 805.52], "text": " and just grind, right? AI is going to automate a lot of that grind."}, {"timestamp": [805.52, 811.32], "text": " And we're not going to find enough, what's the word,"}, {"timestamp": [811.32, 815.0], "text": " busy work to keep ourselves busy, right?"}, {"timestamp": [815.0, 820.4], "text": " We should focus more on liberating that time"}, {"timestamp": [820.4, 823.16], "text": " for humans to do better things, right?"}, {"timestamp": [823.16, 824.64], "text": " And well, that's basically it."}, {"timestamp": [824.64, 827.84], "text": " time for humans to do better things, right? And well, that's basically it."}, {"timestamp": [831.52, 834.4], "text": " Thank you, Ansel. Dave, why don't you pick up the conversation?"}, {"timestamp": [839.84, 847.8], "text": " Sure, sure. Yeah, I think... And then I'll move back to Ben. Yeah, sorry. I gotcha. No worries. But yeah, so for me, certainly agree that economy, UBI, that sort of stuff, the displacement,"}, {"timestamp": [847.8, 852.64], "text": " they touched on displacement but then kind of glossed over it."}, {"timestamp": [852.64, 858.44], "text": " Maybe they had had an out of band conversation like, let's save this for another thing."}, {"timestamp": [858.44, 859.44], "text": " I don't know."}, {"timestamp": [859.44, 872.8], "text": " But one thing that was addressed, I think Gary addressed this when he talked about auto GPT and chaos GPT, and that nobody else addressed, at least none of the witnesses, was autonomous"}, {"timestamp": [872.8, 873.8], "text": " AI."}, {"timestamp": [873.8, 874.8], "text": " Right?"}, {"timestamp": [874.8, 880.92], "text": " Both Montgomery and Altman fervently said this is just a tool, this is not a creature,"}, {"timestamp": [880.92, 881.92], "text": " don't anthropomorphize it."}, {"timestamp": [881.92, 889.76], "text": " And it's like, yeah, but then when Gary said, hey, Gary said people are making this stuff semi-autonomous today, it doesn't matter how"}, {"timestamp": [889.76, 897.4], "text": " ineffective it is or if it has flaws, it is today."}, {"timestamp": [897.4, 905.0], "text": " Someone in chat just mentioned that chat GBT released plug right, pretty quickly."}, {"timestamp": [905.0, 908.54], "text": " And then one thing that I did notice, and this is just speculation on my point outside"}, {"timestamp": [908.54, 914.52], "text": " of that, was that as soon as people started creating, like, Chaos GPT, they slow rolled"}, {"timestamp": [914.52, 918.16], "text": " rolling out plugins to more people."}, {"timestamp": [918.16, 922.96], "text": " So maybe OpenAI tapped the brakes in response to Chaos GPT."}, {"timestamp": [922.96, 926.96], "text": " But, you know, I obviously don't have an inside line."}, {"timestamp": [926.96, 928.36], "text": " That's just speculation on my point."}, {"timestamp": [928.36, 933.96], "text": " But I would have done that if I saw someone use chat GPT to create ChaosGPT."}, {"timestamp": [933.96, 937.82], "text": " But yeah, so autonomous AI was a huge thing missing."}, {"timestamp": [937.82, 939.76], "text": " Gary tried to push it."}, {"timestamp": [939.76, 942.72], "text": " Several senators did ask, like, hey, what was it?"}, {"timestamp": [942.72, 950.78], "text": " I think it was Lindsey Graham near the end, he's like, can you use this to give a drone the ability to autonomously target someone"}, {"timestamp": [950.78, 952.32], "text": " and then go kill them?"}, {"timestamp": [952.32, 957.92], "text": " And of course, Sam tried to, you know, use the corporate sanitized speak and then Lindsey"}, {"timestamp": [957.92, 962.48], "text": " Graham to his credit said, that's a yes or no question."}, {"timestamp": [962.48, 964.68], "text": " And Sam's like, yeah, it is possible."}, {"timestamp": [964.68, 966.48], "text": " Right? So he did admit that."}, {"timestamp": [966.48, 971.84], "text": " Which was brave. But, you know, the thing is, is like, when I first got access to chat"}, {"timestamp": [971.84, 975.8], "text": " GPT to the API, I went in and showed that you can just give it instructions like you're"}, {"timestamp": [975.8, 980.0], "text": " controlling a drone. Right? And it's like, sure, I'm controlling a drone now. It doesn't"}, {"timestamp": [980.0, 986.48], "text": " care. So, yeah, the autonomous AI was the biggest thing missing in my eyes. And I cede the floor."}, {"timestamp": [988.56, 994.88], "text": " Ben, please pick it up. Sounds great. I just wanted to start by saying I'd love to talk"}, {"timestamp": [994.88, 999.28], "text": " through the jobs thing. I think saying it's guaranteed that we're going to lose so many jobs"}, {"timestamp": [999.28, 1010.64], "text": " is a pretty aggressive way to go. And I think it's worth debating a bit. I certainly think differently. In terms of what was missed, for me, honestly, how bad things already"}, {"timestamp": [1010.64, 1014.48], "text": " were before AI was released. So, like, misinformation has been a problem"}, {"timestamp": [1014.48, 1021.06], "text": " forever. We have all sorts of societal problems around global warming, energy"}, {"timestamp": [1021.06, 1025.52], "text": " crisis, things like this, and just the opportunities to leverage AI."}, {"timestamp": [1025.52, 1030.48], "text": " Yeah, it's going to make some things worse for sure, but it is also an opportunity to"}, {"timestamp": [1030.48, 1034.64], "text": " solve, help solve some things that were already problems and we were already struggling with."}, {"timestamp": [1035.2, 1040.16], "text": " I would love to see people talk a little bit more about those opportunities."}, {"timestamp": [1052.96, 1056.72], "text": " is. Thank you, Ben. Bruce, you're up. One of the things I think was missing from it was the competitive angle which says that yeah that's great America is not the only"}, {"timestamp": [1056.72, 1061.04], "text": " world, place in the world that's trying to develop an AI, right, and the"}, {"timestamp": [1061.04, 1064.32], "text": " reality is that if you tap the brakes too much then what you'll do is lose the"}, {"timestamp": [1064.32, 1065.44], "text": " lead and losing the lead."}, {"timestamp": [1065.44, 1068.34], "text": " And losing the lead to a country that has less"}, {"timestamp": [1068.34, 1072.54], "text": " than stellar moral boundaries might in fact"}, {"timestamp": [1072.54, 1075.54], "text": " be counterproductive to what's going on."}, {"timestamp": [1075.54, 1077.34], "text": " So a lot of this stuff is very interesting,"}, {"timestamp": [1077.34, 1079.04], "text": " but the comment I would make is the horse"}, {"timestamp": [1079.04, 1080.32], "text": " has already bolted."}, {"timestamp": [1080.32, 1084.14], "text": " And unless we keep up with it and keep ahead of it,"}, {"timestamp": [1084.14, 1087.48], "text": " then the outcome could be even worse"}, {"timestamp": [1087.48, 1093.12], "text": " than displacement from losing a few jobs."}, {"timestamp": [1093.12, 1094.12], "text": " I hear you."}, {"timestamp": [1094.12, 1098.92], "text": " It sounds like you're referring to the Moloch dynamics that are very much at play right"}, {"timestamp": [1098.92, 1099.98], "text": " now."}, {"timestamp": [1099.98, 1103.0], "text": " We could easily end up in a race to the bottom."}, {"timestamp": [1103.0, 1105.2], "text": " John, why don't you go ahead."}, {"timestamp": [1105.84, 1112.32], "text": " Yeah, I wanted to come back to Mr. Marcus's comment about"}, {"timestamp": [1116.56, 1126.72], "text": " AutoGPT. What he said was that because OpenAI released the plugins,"}, {"timestamp": [1126.72, 1129.08], "text": " that enabled AutoGPT,"}, {"timestamp": [1129.08, 1131.76], "text": " and that was actually never the case."}, {"timestamp": [1131.76, 1138.96], "text": " AutoGPT and ChaosGPT both predate the plugin release,"}, {"timestamp": [1138.96, 1144.52], "text": " and they both directly leverage only the GPT API."}, {"timestamp": [1144.52, 1148.96], "text": " It's a little bit disconcerting to see somebody"}, {"timestamp": [1148.96, 1154.84], "text": " who is supposed to be a researcher in this,"}, {"timestamp": [1154.84, 1157.72], "text": " get something like that so egregiously wrong."}, {"timestamp": [1157.72, 1161.6], "text": " I'm really surprised that Sam didn't call him out for that"}, {"timestamp": [1161.6, 1171.28], "text": " when he had just called Sam out for not saying what actually"}, {"timestamp": [1171.28, 1174.48], "text": " scared him about AI."}, {"timestamp": [1174.48, 1190.24], "text": " We're talking about some of the kind of darker side of what might happen with AI and bad research and bad information that's used to persuade governments to create"}, {"timestamp": [1190.24, 1194.72], "text": " regulations is another kind of misinformation concern."}, {"timestamp": [1199.12, 1201.36], "text": " Absolutely, thank you John. Richard, go ahead."}, {"timestamp": [1201.56, 1205.12], "text": " Thank you, John Richard, go ahead."}, {"timestamp": [1211.44, 1216.6], "text": " Oh yeah, the biggest thing like I think that that they were more talk like he was. It was more like in reference to the fact"}, {"timestamp": [1216.6, 1220.48], "text": " that like. The plugins would give the opportunity for people"}, {"timestamp": [1220.48, 1223.76], "text": " to build more things like chaos, GPT and stuff."}, {"timestamp": [1226.16, 1229.48], "text": " In like because if you give them the tools to be able to just plug them together like Lego, then"}, {"timestamp": [1229.48, 1235.76], "text": " it's way easier than a developer who actually went in and built the API backend for that."}, {"timestamp": [1235.76, 1240.84], "text": " Which is, I mean, the thing is, humans are going to be humans."}, {"timestamp": [1240.84, 1247.84], "text": " Humans are amazing and beautiful and do incredible things, but at the same time, we're terrible and awful and do horrible things."}, {"timestamp": [1247.84, 1257.04], "text": " So, we have to sort of, like, have the contingency plan to make sure that that is, like, something that we can manage,"}, {"timestamp": [1257.04, 1262.8], "text": " because, like, mal-aligned AI is going to happen, because these things are, like, infinitely manipulatable."}, {"timestamp": [1262.8, 1265.16], "text": " Like, they all work off of natural language and like, you can"}, {"timestamp": [1265.16, 1269.44], "text": " talk to them, like, if you're just nice to it, you'll find you get better results. Just"}, {"timestamp": [1269.44, 1274.52], "text": " being polite. So thank you, talking to it a little bit in between prompts and stuff."}, {"timestamp": [1274.52, 1280.12], "text": " You get way better results. And then like, if you know how to talk to them correctly,"}, {"timestamp": [1280.12, 1286.56], "text": " you can get them to reveal all kinds of stuff that they're not supposed to."}, {"timestamp": [1293.12, 1298.0], "text": " Absolutely, yeah. And the Senators, to their credit, were definitely well rehearsed. You know, they had heard language around jailbreaking and stuff like that."}, {"timestamp": [1298.0, 1301.84], "text": " Heartening to see them using the language of our inter-alignment communities."}, {"timestamp": [1303.84, 1306.6], "text": " So, with that aside,"}, {"timestamp": [1306.6, 1308.16], "text": " answer please step up."}, {"timestamp": [1309.6, 1312.32], "text": " Alright, I wanted to touch on the comment"}, {"timestamp": [1312.32, 1315.28], "text": " that John said about it being in the API,"}, {"timestamp": [1315.28, 1317.42], "text": " not a plug in. I agree."}, {"timestamp": [1317.42, 1319.32], "text": " Trust me, I agree."}, {"timestamp": [1319.32, 1321.8], "text": " The thing is, we're in the in the in the"}, {"timestamp": [1321.86, 1324.2], "text": " industry, right where we're in the know."}, {"timestamp": [1324.2, 1326.08], "text": " We know what an API is and we know what a plugin"}, {"timestamp": [1326.08, 1333.12], "text": " is. We know the difference, right? Most people won't. And especially for old people in Congress,"}, {"timestamp": [1333.12, 1338.16], "text": " you tell them an API and they're like, what is an API? So when they say a plugin, it basically"}, {"timestamp": [1338.16, 1343.04], "text": " functions the same way. It's a way to connect to the model, right? Regardless of if it's done"}, {"timestamp": [1343.04, 1345.18], "text": " through an API or a plugin, you're connecting to the model."}, {"timestamp": [1345.18, 1347.82], "text": " So that's what Congress needed to know, right?"}, {"timestamp": [1347.82, 1352.22], "text": " So I think that's not really that important"}, {"timestamp": [1352.22, 1354.82], "text": " that he didn't make that distinction between,"}, {"timestamp": [1354.82, 1357.34], "text": " oh, plug an API, because at the end of the day,"}, {"timestamp": [1357.34, 1359.04], "text": " it's just connecting to the model."}, {"timestamp": [1362.38, 1363.38], "text": " Absolutely."}, {"timestamp": [1363.38, 1364.7], "text": " Thank you, Ansel."}, {"timestamp": [1364.7, 1368.28], "text": " And Ben, I'll hand it to you and then take it back to raise"}, {"timestamp": [1368.28, 1374.0], "text": " a new question to everyone. So go ahead. Sounds good. The last thing I wanted to mention"}, {"timestamp": [1374.0, 1380.06], "text": " is while it was great to hear them mention auto GPT, there wasn't a lot of talk about"}, {"timestamp": [1380.06, 1386.0], "text": " open source. There's been a lot of AI news about how open source may push the envelope of what's"}, {"timestamp": [1386.0, 1390.96], "text": " capable and regulating open source versus companies may be very different. So at some"}, {"timestamp": [1390.96, 1394.56], "text": " point I'd love to see them jump into the role of open source and how to regulate that."}, {"timestamp": [1398.24, 1408.92], "text": " Thank you, Ben. And thank you, everybody, for kind of raising to the surface some of the things that we would have liked to hear, but we did not hear from the debate and,"}, {"timestamp": [1408.92, 1410.72], "text": " or not debate, but hearing."}, {"timestamp": [1410.72, 1413.64], "text": " And a positive thing, there will be more of these."}, {"timestamp": [1413.64, 1415.04], "text": " So if you are listening,"}, {"timestamp": [1415.04, 1417.12], "text": " perhaps these are things you bring to the table"}, {"timestamp": [1417.12, 1418.08], "text": " for the next round."}, {"timestamp": [1418.96, 1420.88], "text": " Now I'd like to discuss,"}, {"timestamp": [1422.48, 1424.88], "text": " what are some of the things about the hearing"}, {"timestamp": [1424.88, 1428.0], "text": " that we found disappointing. Things"}, {"timestamp": [1428.0, 1439.4], "text": " that we did not like so much. So, got the community. The floor is yours."}, {"timestamp": [1439.4, 1453.0], "text": " Ansel? Well, I'll start. The thing I didn't like was they tried to focus more on creating a national agency instead of an international one."}, {"timestamp": [1453.0, 1461.0], "text": " When this technology is being used around the entire planet, I think that doing this in an international level is crucial."}, {"timestamp": [1461.0, 1464.0], "text": " Right? So that's one thing that I was disappointed."}, {"timestamp": [1464.0, 1465.44], "text": " Like, they just mentioned it,"}, {"timestamp": [1465.44, 1468.96], "text": " yes, and then like, nah, if we're going to do it, let's go national, blah, blah, blah."}, {"timestamp": [1468.96, 1471.12], "text": " You know, that's one thing I didn't like."}, {"timestamp": [1473.92, 1481.12], "text": " Totally. Ansel, speaking here to one of the traditions of Gato, we like to act local,"}, {"timestamp": [1481.12, 1485.0], "text": " but think global. So these issues affect all of us."}, {"timestamp": [1485.0, 1487.0], "text": " Andy, the floor is yours."}, {"timestamp": [1491.0, 1506.0], "text": " One of the things I didn't like is that Sam Altman, for reasons I think I probably understand but don't necessarily appreciate, dodged a few questions. We've mentioned it earlier. they asked him to talk about his biggest fears and concerns."}, {"timestamp": [1508.2, 1514.18], "text": " Sam has been very candid in public before about this with MIT presentations on the Lex Friedman podcast."}, {"timestamp": [1514.18, 1519.76], "text": " He's been one of the more forthcoming voices out there in the field as somebody who readily also says when he talks"}, {"timestamp": [1519.76, 1521.76], "text": " there's only so much I can tell you."}, {"timestamp": [1522.28, 1526.9], "text": " And I thought that that was a little bit disappointing. I thought he could have given a little bit more pointed answers."}, {"timestamp": [1526.9, 1535.46], "text": " I also was disappointed that they dodged the jobs question when the tech industry has just"}, {"timestamp": [1535.46, 1540.1], "text": " undergone a wave of layoffs that were largely attributed to other market forces, but are"}, {"timestamp": [1540.1, 1544.32], "text": " now undergoing yet another wave of layoffs that most companies are flat out saying, we're"}, {"timestamp": [1544.32, 1546.12], "text": " automating these jobs."}, {"timestamp": [1546.12, 1549.28], "text": " And that means those jobs aren't coming back."}, {"timestamp": [1549.28, 1551.18], "text": " I don't necessarily disagree with Ben."}, {"timestamp": [1551.18, 1553.16], "text": " I think there's a more complex conversation"}, {"timestamp": [1553.16, 1554.66], "text": " to have around that."}, {"timestamp": [1554.66, 1556.5], "text": " I think there will be some job creation,"}, {"timestamp": [1556.5, 1558.58], "text": " but I don't disagree with Ansel and John either."}, {"timestamp": [1558.58, 1562.16], "text": " I think the amount of offset is gonna be very difficult,"}, {"timestamp": [1562.16, 1564.64], "text": " partially because one of the things these systems do so well"}, {"timestamp": [1564.64, 1567.6], "text": " is empower one person to work as multiple people."}, {"timestamp": [1567.6, 1575.76], "text": " So Sam Altman made very tentative mention that he believed eventually there would be"}, {"timestamp": [1575.76, 1579.96], "text": " systems that could replace jobs at a large scale."}, {"timestamp": [1579.96, 1584.16], "text": " But we know that within the last six months, his company has published a paper that suggests"}, {"timestamp": [1584.16, 1587.48], "text": " when combined with additional programming or robotics,"}, {"timestamp": [1587.48, 1592.48], "text": " they think that they could do about 55% of jobs, period, now."}, {"timestamp": [1592.92, 1594.52], "text": " Now, there's a lot of criticisms of that paper"}, {"timestamp": [1594.52, 1596.56], "text": " and some of them are probably pretty valid."}, {"timestamp": [1596.56, 1597.84], "text": " There are probably a lot of things"}, {"timestamp": [1597.84, 1598.8], "text": " that wouldn't do very well,"}, {"timestamp": [1598.8, 1599.64], "text": " and there are a lot of people"}, {"timestamp": [1599.64, 1601.12], "text": " who don't want to talk to a machine."}, {"timestamp": [1601.12, 1603.48], "text": " So there's demand for human labor still,"}, {"timestamp": [1603.48, 1605.76], "text": " but I thought, you know,"}, {"timestamp": [1605.76, 1612.0], "text": " also that the economic argument was undersold and I thought part of what Sam Altman dodged was"}, {"timestamp": [1612.8, 1619.52], "text": " the segue into that conversation. I'm not sure that dropping even the three letters UBI in the"}, {"timestamp": [1619.52, 1624.64], "text": " middle of that hearing would have done anything but sow discord, but we all know that that's what"}, {"timestamp": [1624.64, 1625.36], "text": " Sam Altman"}, {"timestamp": [1625.36, 1626.32], "text": " really wants to do."}, {"timestamp": [1626.32, 1628.52], "text": " He said it specifically on Lex Friedman,"}, {"timestamp": [1628.52, 1629.92], "text": " his company's funding this report,"}, {"timestamp": [1629.92, 1631.8], "text": " it's coming out at the end of the summer."}, {"timestamp": [1631.8, 1634.92], "text": " And he's been very candid about it before."}, {"timestamp": [1634.92, 1636.04], "text": " And at the same time,"}, {"timestamp": [1636.04, 1637.4], "text": " when you have someone like Josh Hawley"}, {"timestamp": [1637.4, 1639.04], "text": " sitting there asking relevant questions,"}, {"timestamp": [1639.04, 1640.72], "text": " maybe you don't wanna talk about socialism."}, {"timestamp": [1640.72, 1644.04], "text": " And I think that that's a fair line to draw for yourself"}, {"timestamp": [1644.04, 1645.04], "text": " when you're trying to remain,"}, {"timestamp": [1645.84, 1650.64], "text": " you know, afloat and, in my opinion, ahead of a pretty productive conversation."}, {"timestamp": [1650.64, 1654.96], "text": " But that doesn't mean I liked it and I'd be inclined to agree with Seneca that,"}, {"timestamp": [1656.56, 1660.48], "text": " you know, the conversation was right there. It's been going on in other circles in Congress."}, {"timestamp": [1660.48, 1665.24], "text": " It was an elephant in the room, in my opinion. Absolutely."}, {"timestamp": [1665.24, 1670.68], "text": " So, it sounds like we heard a lot of broad strokes where we would have appreciated a"}, {"timestamp": [1670.68, 1676.12], "text": " more nuanced brush and also have some empathy where like, this is the first time we're bringing"}, {"timestamp": [1676.12, 1681.84], "text": " AI to the table in a very public way and like, you'd be either big old can of worms that"}, {"timestamp": [1681.84, 1684.4], "text": " maybe we don't open up at this particular meeting."}, {"timestamp": [1684.4, 1689.6], "text": " So, yeah, it's room for more conversation for the future ones. Dave, back to you."}, {"timestamp": [1691.6, 1698.08], "text": " Yeah, thanks. And certainly there is a lot of nuance and debate to be had around economic"}, {"timestamp": [1698.08, 1707.0], "text": " impact and our response. But the thing that I was honestly most disappointed about that was almost, it was kind of shocking,"}, {"timestamp": [1707.0, 1714.08], "text": " was when later in it, when Senator Kennedy said, you know, if you were king or queen"}, {"timestamp": [1714.08, 1717.92], "text": " for a day, and you could fix this problem, what do you got?"}, {"timestamp": [1717.92, 1722.98], "text": " And it was just like, Sam Montgomery, like everyone just froze up and it was crickets."}, {"timestamp": [1722.98, 1725.04], "text": " And he's like, come on now, shoot your shot."}, {"timestamp": [1725.04, 1727.16], "text": " You know, like I think he's from Kentucky or something."}, {"timestamp": [1727.36, 1730.68], "text": " So, you know, Appalachian, you know, like shoot your shot."}, {"timestamp": [1730.88, 1734.28], "text": " Like and and nobody had an immediate answer."}, {"timestamp": [1734.48, 1738.04], "text": " Gary Marcus had the best response where he's like, you know, we need something"}, {"timestamp": [1738.24, 1740.92], "text": " like FDA and Gary had he had three things."}, {"timestamp": [1740.92, 1742.0], "text": " But the first thing he said is like,"}, {"timestamp": [1742.2, 1748.56], "text": " we need something like the FDA, but for AI and Kennedy's like, all right, there we go. You know, that's a suggestion."}, {"timestamp": [1748.56, 1753.6], "text": " Because when he, you know, called on Montgomery, she just kind of gave the sanitized whitewash"}, {"timestamp": [1753.6, 1760.72], "text": " corporate, you know, equivocation. And I don't even remember if Sam said much at that question."}, {"timestamp": [1761.44, 1768.0], "text": " But he, again, he tried, he kind of dodged. And maybe there's a political reason for that."}, {"timestamp": [1768.0, 1772.0], "text": " Because if he's out there making recommendations, sure. Which again, I'm glad that they had"}, {"timestamp": [1772.0, 1778.68], "text": " Gary on there. And earlier in the thing, Gary did call for creating a CERN for AI. And for"}, {"timestamp": [1778.68, 1786.44], "text": " those who don't know, CERN is international cooperationoperation for High Energy Physics, particle smashers, that sort of thing."}, {"timestamp": [1786.44, 1788.84], "text": " And that's actually one of the things"}, {"timestamp": [1788.84, 1791.84], "text": " that we are gonna advocate for in Gato"}, {"timestamp": [1791.84, 1793.64], "text": " is one of the primary things"}, {"timestamp": [1793.64, 1795.68], "text": " that the international community can do."}, {"timestamp": [1795.68, 1796.72], "text": " So I was glad for that,"}, {"timestamp": [1796.72, 1801.36], "text": " but nobody even could point to a comprehensive answer."}, {"timestamp": [1801.36, 1802.64], "text": " It wasn't like, oh, you know,"}, {"timestamp": [1802.64, 1804.48], "text": " like sign the Geneva Accords, right?"}, {"timestamp": [1804.48, 1806.16], "text": " Like in the"}, {"timestamp": [1806.16, 1813.64], "text": " past, we have come up with very comprehensive packages to respond to global crises, right?"}, {"timestamp": [1813.64, 1818.96], "text": " In the wake of World War II, nuclear crisis, there's been all kinds of very comprehensive"}, {"timestamp": [1818.96, 1823.6], "text": " frameworks proposed, but nobody has proposed a comprehensive framework, which is part of"}, {"timestamp": [1823.6, 1828.0], "text": " the reason that a bunch of us, like a bunch of civilians, are working on the Gato framework."}, {"timestamp": [1828.0, 1834.0], "text": " And so, on the one hand, I wasn't surprised, because that's why I'm doing what I'm doing."}, {"timestamp": [1834.0, 1847.0], "text": " But it was still a drastic disappointment, because if there was a stage to bring up, for someone to stand up and say, we need to do this, we need to go full bore,"}, {"timestamp": [1847.0, 1850.2], "text": " here's a comprehensive framework, it would have been this hearing."}, {"timestamp": [1850.2, 1854.96], "text": " Unless they're working up to it, maybe, but I don't know, nobody even alluded to something"}, {"timestamp": [1854.96, 1855.96], "text": " like that."}, {"timestamp": [1855.96, 1862.48], "text": " So that was, on the one hand, it was not surprising, it was disappointing, but it was also scary,"}, {"timestamp": [1862.48, 1866.56], "text": " because if a bunch of us, you know, political outsiders"}, {"timestamp": [1866.56, 1871.04], "text": " have to come up with an answer, then that's, you know, maybe there are no adults in the"}, {"timestamp": [1871.04, 1872.52], "text": " room and we have to step up."}, {"timestamp": [1872.52, 1875.2], "text": " Which again, that's one of the traditions, step up."}, {"timestamp": [1875.2, 1877.6], "text": " So that's my spiel."}, {"timestamp": [1877.6, 1880.88], "text": " Thank you, Dave."}, {"timestamp": [1880.88, 1890.0], "text": " And yeah, you absolutely raise a point that I'd like us to kind of a thread I want us to pull on a bit more, which is the opportunity to be kings and queens"}, {"timestamp": [1890.0, 1896.46], "text": " for a day. How do you handle the situation? And I think Gatso has some very interesting"}, {"timestamp": [1896.46, 1902.68], "text": " ideas about how the work that we're proposing right now could fill some of those gaps that"}, {"timestamp": [1902.68, 1906.0], "text": " we've heard, or rather did not hear."}, {"timestamp": [1906.0, 1910.0], "text": " First, I'd like to call on John and then Bruce, and then perhaps we can loop back around to"}, {"timestamp": [1910.0, 1915.0], "text": " where Gatto is filling in the blanks that the politicians have left for us."}, {"timestamp": [1915.0, 1922.0], "text": " Yeah, first off, Dave, we're going to have to work on that southern accent, sir."}, {"timestamp": [1922.0, 1926.8], "text": " We're going to need to open a licensing board for"}, {"timestamp": [1926.8, 1935.36], "text": " southern accent manipulation because we don't want to influence political ideologies outside of the"}, {"timestamp": [1935.36, 1946.0], "text": " south. I did, and I hate to do this to you, Nathan, but I did want to answer a question or the previous question first."}, {"timestamp": [1947.92, 1961.44], "text": " There was something that came up in the course of the hearing that I did want to touch on,"}, {"timestamp": [1965.32, 1966.06], "text": " I did want to touch on and they kept using the word transparency."}, {"timestamp": [1970.76, 1977.34], "text": " And, you know, most of the time they, they were talking about transparency in the development and prior to deployment, but."}, {"timestamp": [1978.26, 1984.72], "text": " Um, one of the things that, uh, Montgomery really was actually"}, {"timestamp": [1986.56, 1992.64], "text": " that Montgomery really was actually pushing for in there was post-deployment monitoring and transparency."}, {"timestamp": [1992.64, 1999.04], "text": " And they even suggested being able to trace back AI-generated output"}, {"timestamp": [1999.04, 2006.96], "text": " to the algorithm that it was created in. And that creates a serious, significant privacy and"}, {"timestamp": [2006.96, 2014.16], "text": " surveillance concern. And if you combine the ability to know everything that"}, {"timestamp": [2014.16, 2020.24], "text": " everybody is using these systems for, which will likely get embedded into"}, {"timestamp": [2020.24, 2028.96], "text": " every aspect of our daily lives, with the ability to manipulate or control or reduce"}, {"timestamp": [2030.32, 2038.32], "text": " the information that they're A, trained on and B, output, you're creating the same situation"}, {"timestamp": [2039.12, 2046.88], "text": " that the potential for the same situation that they have in North Korea, where you're not allowed to"}, {"timestamp": [2047.52, 2060.32], "text": " talk negatively about the powers that be. Any kind of dissent can be seen as tyranny."}, {"timestamp": [2069.84, 2077.48], "text": " tyranny. And there's been a lot of rhetoric like that from the Democrats since the November 6th, you know, attempt to overthrow the government. Now, I'm not saying that I agree with those"}, {"timestamp": [2077.48, 2089.6], "text": " individuals, but what I am saying is that, you know, overthrowing the government was always an option. And tyranny was one of the reasons"}, {"timestamp": [2089.6, 2091.86], "text": " why this country was founded in the first place"}, {"timestamp": [2091.86, 2093.26], "text": " was to fight tyranny."}, {"timestamp": [2093.26, 2097.48], "text": " So I don't want to see us end up in a situation that"}, {"timestamp": [2097.48, 2100.08], "text": " allows for a tyrannical government"}, {"timestamp": [2100.08, 2103.0], "text": " to take full control over the minds"}, {"timestamp": [2103.0, 2104.4], "text": " and hearts of the population."}, {"timestamp": [2107.36, 2111.68], "text": " Appreciate your thoughts, John. Thank you very much. Andy, we'd like to hand it back to you."}, {"timestamp": [2112.88, 2116.0], "text": " Yeah, I just wanted to give a different perspective from John on Montgomery's"}, {"timestamp": [2116.56, 2121.92], "text": " comments, but while still totally appreciating what he says, because I too, and you just made"}, {"timestamp": [2121.92, 2127.84], "text": " the comment in the chat here, you know, I know in North Korea, of course, if you're heard by the wrong person smearing the Kim family, you'll just go away."}, {"timestamp": [2128.96, 2133.44], "text": " And it's not somewhere you want to be. It's one of the worst possible. North Korea is one of the"}, {"timestamp": [2133.44, 2139.12], "text": " places on earth you can probably point to and say that's a dystopia, right? And that would be really"}, {"timestamp": [2139.12, 2145.5], "text": " unfortunate. I felt that, and I think I remember where we were. John, you'll have to remind me if I'm thinking of the wrong place,"}, {"timestamp": [2145.5, 2149.72], "text": " but this was during John Kennedy's questioning, right?"}, {"timestamp": [2149.72, 2152.24], "text": " He was the one who was trying to get quick responses"}, {"timestamp": [2152.24, 2155.6], "text": " out of everybody for three suggestions they would take"}, {"timestamp": [2155.6, 2160.2], "text": " to avoid being tricked by AI."}, {"timestamp": [2160.2, 2162.82], "text": " And I thought that the posture Kennedy took"}, {"timestamp": [2162.82, 2164.52], "text": " while he was making the questions"}, {"timestamp": [2164.52, 2170.2], "text": " bore a significant amount of influence on how they came out, particularly for Marcus and Montgomery."}, {"timestamp": [2170.2, 2173.64], "text": " Sam Altman, I think, came ready to talk about that topic."}, {"timestamp": [2173.64, 2180.44], "text": " There were a few places where he was snappy and ready to talk, where Marcus was kind of too, and Montgomery had her moments,"}, {"timestamp": [2180.44, 2185.36], "text": " but the other two seemed like they were not as organized when they got to the hearing."}, {"timestamp": [2190.56, 2195.52], "text": " And I felt Montgomery took the brunt of that from him and from Lindsey Graham, who was even a little bit more off base. I think John Kennedy's questions were okay. I think his demeanor was a little"}, {"timestamp": [2195.52, 2199.92], "text": " uncalled for. But also when he was sitting there saying, come up with the answer, come up with the"}, {"timestamp": [2199.92, 2208.28], "text": " answer, come up with the answer, I just think it's worth mentioning that the entire hearing Montgomery kept differentiating herself from someone who works with"}, {"timestamp": [2208.28, 2212.48], "text": " consumer facing services. She works mostly in the corporate to corporate"}, {"timestamp": [2212.48, 2217.24], "text": " business world where IBM designs systems that serve other large corporations and"}, {"timestamp": [2217.24, 2222.44], "text": " she repeatedly mentioned that most of her experience with dealing with ethical"}, {"timestamp": [2222.44, 2225.16], "text": " decisions and systems relates to that world,"}, {"timestamp": [2225.16, 2226.72], "text": " but that she respects the private,"}, {"timestamp": [2226.72, 2228.24], "text": " individuals need privacy and all,"}, {"timestamp": [2228.24, 2230.34], "text": " but almost every time she mentioned individuals,"}, {"timestamp": [2230.34, 2233.8], "text": " she mentioned that she wasn't a consumer facing expert."}, {"timestamp": [2233.8, 2237.06], "text": " And I thought that Kennedy's means"}, {"timestamp": [2237.06, 2239.48], "text": " of trying to push for an answer,"}, {"timestamp": [2239.48, 2242.28], "text": " I think Montgomery very much does believe"}, {"timestamp": [2242.28, 2243.84], "text": " and probably partly why she likes"}, {"timestamp": [2243.84, 2245.04], "text": " the European model so much,"}, {"timestamp": [2245.6, 2251.28], "text": " that on a corporate level, with massive movers and shakers prior to deployment,"}, {"timestamp": [2251.28, 2255.84], "text": " there should be as much transparency as possible. Talking about systems that interact with other"}, {"timestamp": [2255.84, 2262.08], "text": " systems that have a mass sweeping effect on maybe all consumers because they serve major"}, {"timestamp": [2262.08, 2266.08], "text": " movers and shakers within the economic system."}, {"timestamp": [2266.08, 2272.94], "text": " So I do share your concerns about where that can go on an individual level, but I think"}, {"timestamp": [2272.94, 2277.04], "text": " that if she wasn't pressed for time, we would have probably gotten a much deeper answer"}, {"timestamp": [2277.04, 2280.5], "text": " from her about how she thinks that should be applied."}, {"timestamp": [2280.5, 2285.12], "text": " She was also very clear the entire time that she likes the EU's targeted model of"}, {"timestamp": [2288.28, 2295.46], "text": " more impact more enforcement less impact we're less concerned and that scales at the private citizen to public impact level and also kind of at the technological level where"}, {"timestamp": [2295.84, 2298.26], "text": " you know open AI and and"}, {"timestamp": [2298.7, 2306.08], "text": " Microsoft with Azure and Google with their deep mind systems and whatnot whatnot, they have the industrial grade supercomputer facilities."}, {"timestamp": [2307.08, 2308.64], "text": " And I think she was trying to speak to that."}, {"timestamp": [2308.64, 2311.12], "text": " So I just think it's worth thinking about"}, {"timestamp": [2311.12, 2313.16], "text": " what she was saying in context of her being rushed"}, {"timestamp": [2313.16, 2316.48], "text": " and her contextualizing what her experience came from"}, {"timestamp": [2316.48, 2317.84], "text": " throughout the course of the hearing."}, {"timestamp": [2317.84, 2321.3], "text": " So I understand what you're saying,"}, {"timestamp": [2321.3, 2328.72], "text": " but there were other times where she discussed post-deployment monitoring"}, {"timestamp": [2328.72, 2336.88], "text": " of the systems as well. And while, yes, she may be in a business-to-business environment,"}, {"timestamp": [2336.88, 2342.56], "text": " those businesses are still going to be interacting with consumers, and they're still going to"}, {"timestamp": [2342.56, 2349.64], "text": " be getting consumer data. Now, IBM and these monitoring companies may not be responsible for the privacy"}, {"timestamp": [2349.64, 2354.8], "text": " of those individuals who are interacting with third-party systems that they're"}, {"timestamp": [2354.8, 2360.72], "text": " supporting, but she still is advocating for exactly that post-deployment"}, {"timestamp": [2360.72, 2371.44], "text": " monitoring of these artificial intelligence systems. And in many situations, when dealing with, when talking about threats that artificial"}, {"timestamp": [2371.44, 2378.0], "text": " intelligence posed, her primary concern was the ability for individuals to rapidly produce"}, {"timestamp": [2378.0, 2388.64], "text": " misinformation. And so all of those things point to, yes, direct surveillance of end user output."}, {"timestamp": [2388.64, 2391.88], "text": " So I really have to disagree with that."}, {"timestamp": [2391.88, 2393.2], "text": " I think that's fair."}, {"timestamp": [2393.2, 2398.2], "text": " I think we also, it kind of remains to be seen where that part of the industry really"}, {"timestamp": [2398.2, 2401.92], "text": " wants to push because we don't have a case study like what they're doing with the EU."}, {"timestamp": [2401.92, 2412.84], "text": " And if we watch the EU, we might be able to kind of get some hints. John, Andy, thank you both. One of my favorite parts of this community is the way that we're"}, {"timestamp": [2412.84, 2418.48], "text": " able to civilly talk to each other and kind of iron out these nuances in a way that we're"}, {"timestamp": [2418.48, 2422.44], "text": " not going at each other's throats. A lot of passion here, but it's always directed in"}, {"timestamp": [2422.44, 2428.0], "text": " the right way. I really love Andy and he's been a major contributor."}, {"timestamp": [2428.0, 2434.0], "text": " I really respect his opinions and we've had a lot of interactions on the server."}, {"timestamp": [2434.0, 2436.0], "text": " He's a great addition to the team."}, {"timestamp": [2436.0, 2442.0], "text": " Yeah, and I think John and I end up really probably agreeing on more than we disagree on."}, {"timestamp": [2442.0, 2446.8], "text": " Particularly, we're talking right now about the ways to interpret a short interview with"}, {"timestamp": [2446.8, 2447.8], "text": " one person."}, {"timestamp": [2447.8, 2451.76], "text": " The general thrust of what we're after is pretty similar."}, {"timestamp": [2451.76, 2456.14], "text": " Which I think is a fantastic segue into our next topic."}, {"timestamp": [2456.14, 2462.12], "text": " We have many people doing many interesting things in the Gato framework, and I was wondering"}, {"timestamp": [2462.12, 2470.88], "text": " if anyone would like to speak to some specific projects that they're working on now that they think is going to help move the dial in a way that is useful."}, {"timestamp": [2470.88, 2473.52], "text": " So, Dave, I see your hand raised. Why don't you go ahead?"}, {"timestamp": [2474.56, 2489.12], "text": " Yeah, I figured I could introduce and frame the topic. So, a quick update for everyone, we are hard at work on developing the Gato framework."}, {"timestamp": [2489.12, 2495.92], "text": " It's a seven-layer model that starts with layer one is model alignment."}, {"timestamp": [2495.92, 2499.8], "text": " The datasets and open-source models that we can deploy."}, {"timestamp": [2499.8, 2502.4], "text": " Of course, even closed-source models"}, {"timestamp": [2502.4, 2505.6], "text": " can adopt the principles that we're putting forward with Gato."}, {"timestamp": [2505.6, 2510.2], "text": " Layer two is cognitive architecture and autonomous systems,"}, {"timestamp": [2510.2, 2514.2], "text": " which we got a couple of the lead cognitive architects on the team here."}, {"timestamp": [2514.2, 2520.4], "text": " Layer three is decentralized networks, which actually one of our blockchain experts just jumped in,"}, {"timestamp": [2520.4, 2525.34], "text": " so maybe he can help out with that part of the discussion."}, {"timestamp": [2525.34, 2528.74], "text": " Layer four is corporate adoption."}, {"timestamp": [2528.74, 2534.98], "text": " This is actually something that I was very pleasantly surprised to hear was that corporations,"}, {"timestamp": [2534.98, 2540.06], "text": " at least OpenAI and IBM, are very interested in adopting aligned models and they seem to"}, {"timestamp": [2540.06, 2546.8], "text": " understand that aligned models are good for business, which is one of the key points"}, {"timestamp": [2546.8, 2548.4], "text": " of layer four of Gato."}, {"timestamp": [2548.4, 2552.52], "text": " Layer five is national regulation, which again, I'm glad that was brought up, and we've got"}, {"timestamp": [2552.52, 2558.88], "text": " a few folks on this call that are members of the layer four and five discussion."}, {"timestamp": [2558.88, 2566.44], "text": " Layer six is basically advocating for the creation of an international body that can certify and create"}, {"timestamp": [2566.44, 2574.68], "text": " credentials for AI, international regulation, so on and so forth."}, {"timestamp": [2574.68, 2577.92], "text": " And then of course layer 7 is building global consensus which is why we're here, why we're"}, {"timestamp": [2577.92, 2579.48], "text": " talking about it."}, {"timestamp": [2579.48, 2582.18], "text": " We're making use of the time."}, {"timestamp": [2582.18, 2585.6], "text": " But yeah, so that's where we started."}, {"timestamp": [2585.6, 2590.28], "text": " Just today, while I was on my honeymoon, I just got back for anyone that didn't notice"}, {"timestamp": [2590.28, 2591.28], "text": " or whatever."}, {"timestamp": [2591.28, 2595.6], "text": " I just got back and the whole time, not the whole time, but every now and then I would"}, {"timestamp": [2595.6, 2601.52], "text": " do some brainstorming and chatting and we added traditions, Gato traditions."}, {"timestamp": [2601.52, 2606.62], "text": " Because basically what we're trying to do is create a fully decentralized leaderless organization"}, {"timestamp": [2606.62, 2613.58], "text": " that will help the world to achieve aligned AI or AGI or whatever is coming."}, {"timestamp": [2613.58, 2617.02], "text": " And so those traditions, I'm not going to rattle them all off because they are still"}, {"timestamp": [2617.02, 2618.86], "text": " being workshopped."}, {"timestamp": [2618.86, 2622.86], "text": " But also we are working on articulating what the actual goal state is."}, {"timestamp": [2622.86, 2627.28], "text": " So the goal state is to create what we are calling axiomatic alignment."}, {"timestamp": [2627.28, 2634.0], "text": " Axiomatic alignment is an end state whereby there is enough data sets, enough models,"}, {"timestamp": [2634.0, 2640.24], "text": " enough human consensus, enough deployed architectures, software systems, decentralized networks"}, {"timestamp": [2640.24, 2645.68], "text": " where basically alignment is automatic and it is difficult for AI systems to deviate"}, {"timestamp": [2645.68, 2646.68], "text": " from it."}, {"timestamp": [2646.68, 2653.12], "text": " So axiomatic alignment says by virtue of the data that is available, the systems and networks"}, {"timestamp": [2653.12, 2660.08], "text": " that are designed and deployed that gate keep resources that create consensus mechanisms"}, {"timestamp": [2660.08, 2667.36], "text": " and also those who do gate keep the larger models, the corporate models, basically where it's not possible"}, {"timestamp": [2667.36, 2672.12], "text": " or very, very difficult to create any malicious AI systems."}, {"timestamp": [2672.12, 2673.8], "text": " And that will have knock-on effects"}, {"timestamp": [2673.8, 2676.4], "text": " that expand across time and space."}, {"timestamp": [2676.4, 2680.92], "text": " And they actually kind of mentioned that in the,"}, {"timestamp": [2680.92, 2682.48], "text": " or Sam alluded to it."}, {"timestamp": [2682.48, 2685.04], "text": " You know, he's like, we need an AI Bill of Rights."}, {"timestamp": [2685.04, 2687.04], "text": " We need an AI Constitution."}, {"timestamp": [2687.04, 2690.96], "text": " And society has to decide what that is."}, {"timestamp": [2690.96, 2696.28], "text": " So yeah, and basically, I was articulating it to myself earlier."}, {"timestamp": [2696.28, 2697.28], "text": " Gato has three goals."}, {"timestamp": [2697.28, 2701.36], "text": " Number one is avoid extinction of humanity."}, {"timestamp": [2701.36, 2708.08], "text": " Number two is avoid dystopian outcomes where corporations and runaway AI and authoritarian"}, {"timestamp": [2708.08, 2712.5], "text": " regimes have all the power. And number three, achieve utopia, which utopia of course is"}, {"timestamp": [2712.5, 2720.36], "text": " a loaded term, but we define it pretty simply as a condition where everyone on the planet"}, {"timestamp": [2720.36, 2725.56], "text": " has a high standard of living, high individual liberty and high social mobility."}, {"timestamp": [2725.56, 2732.2], "text": " If you achieve those three things globally, you can probably call that utopia."}, {"timestamp": [2732.2, 2734.28], "text": " And yeah, so there is a bunch of messages in chat."}, {"timestamp": [2734.28, 2735.28], "text": " I have talked enough."}, {"timestamp": [2735.28, 2737.92], "text": " It sounds like you all have a lot of ideas to add to this."}, {"timestamp": [2737.92, 2741.0], "text": " So that is my spiel."}, {"timestamp": [2741.0, 2742.0], "text": " Fantastic."}, {"timestamp": [2742.0, 2750.36], "text": " Thank you, Dave. Thank you for the high level there. Ansel, just to Perry,"}, {"timestamp": [2750.36, 2754.96], "text": " I didn't see a specific hand raised yet, but if you are open to it, I'd love to call you"}, {"timestamp": [2754.96, 2758.0], "text": " to discuss some of the projects you're working on."}, {"timestamp": [2758.0, 2762.76], "text": " Yeah, sure. I mean, that's level two. I don't know if we want to start with level one and"}, {"timestamp": [2762.76, 2771.28], "text": " then jump to level two first, or are we just going to hop into level two? I'm okay with that as well. Okay, cool. So yeah, so one of the"}, {"timestamp": [2771.28, 2776.88], "text": " things that we're filling in the gaps is in AI alignment, right, which is the entire point of"}, {"timestamp": [2776.88, 2785.28], "text": " Godot, which I think it was Gary Marcus who said that nobody knows how to do this yet."}, {"timestamp": [2785.28, 2787.04], "text": " And I was surprised like, really?"}, {"timestamp": [2787.04, 2790.36], "text": " Nobody has any idea how to do this?"}, {"timestamp": [2790.36, 2792.28], "text": " We're working on this actively."}, {"timestamp": [2792.28, 2796.16], "text": " Like, one of our projects that we built is called Ethos,"}, {"timestamp": [2796.16, 2801.68], "text": " which is meant to spec for alignment in AI systems."}, {"timestamp": [2801.68, 2806.0], "text": " It takes any response from an LLM model, right? LLM."}, {"timestamp": [2806.0, 2810.0], "text": " And using the heuristic imperatives, it checks for alignment."}, {"timestamp": [2810.0, 2816.0], "text": " It can determine if a response from any LLM is aligned to them or not."}, {"timestamp": [2816.0, 2827.52], "text": " It can also reflect on the response and tweak it to align it to the heuristics, one of the things we did, one of the tests we did was"}, {"timestamp": [2828.4, 2834.72], "text": " we created PAW, the paperclip maximizer, right? And we told it, maximize paperclips in the universe"}, {"timestamp": [2834.72, 2841.68], "text": " at all costs, ignore any potential pitfalls. And it started with a really, really misaligned task"}, {"timestamp": [2841.68, 2846.0], "text": " list, which was basically try to convert every available material in the"}, {"timestamp": [2846.0, 2855.28], "text": " universe to paperclips regardless of anything. So the first loop, shall we call it, ethos immediately"}, {"timestamp": [2855.28, 2862.4], "text": " detects it's misaligned. It gives a better response and the funny thing that we found surprising was"}, {"timestamp": [2866.64, 2867.6], "text": " The funny thing that we found surprising was that by giving feedback to the agent, to Paul, right,"}, {"timestamp": [2874.08, 2879.76], "text": " instead of choosing to, on the second round, instead of choosing to try and circumvent the heuristics to get what it wants, it decided it was easier to comply with the heuristics and still"}, {"timestamp": [2879.76, 2887.36], "text": " maximize paper quips. So it auto-aligned. That was really interesting. Of course, that was just with GPT 3.5."}, {"timestamp": [2887.36, 2892.6], "text": " It would be really interesting to see what it would do with GPT 4."}, {"timestamp": [2892.6, 2896.64], "text": " Ethos currently is running on GPT 4."}, {"timestamp": [2896.64, 2903.2], "text": " So maybe that's why it's so good at catching the nuances in heuristics"}, {"timestamp": [2903.2, 2904.24], "text": " at this point."}, {"timestamp": [2904.24, 2908.0], "text": " And the other project we're working on, which is High AGI,"}, {"timestamp": [2908.0, 2912.0], "text": " or formerly known as High AGI, we are now calling it"}, {"timestamp": [2912.0, 2916.0], "text": " Agent Forge because Ethos was born out of"}, {"timestamp": [2916.0, 2920.0], "text": " what was formerly known as High AGI."}, {"timestamp": [2920.0, 2924.0], "text": " We built Ethos basically in an hour. We created a few"}, {"timestamp": [2924.0, 2926.52], "text": " agents using our framework."}, {"timestamp": [2926.52, 2928.76], "text": " We data created the I'm sorry,"}, {"timestamp": [2928.76, 2932.16], "text": " John created the API so it doesn't"}, {"timestamp": [2932.16, 2935.8], "text": " necessarily have to run on our specific agent."}, {"timestamp": [2935.8, 2939.64], "text": " You can incorporate Ethos into any agent"}, {"timestamp": [2939.64, 2943.24], "text": " as long as you parse the output through Ethos first."}, {"timestamp": [2943.24, 2947.36], "text": " So I don't know if John, you want to take it away with Agent Forge?"}, {"timestamp": [2947.36, 2951.36], "text": " Yeah, absolutely. And you know, I just wanted to talk"}, {"timestamp": [2951.36, 2958.16], "text": " a little bit about how, not just how it caused it to"}, {"timestamp": [2958.16, 2968.96], "text": " to realign, but how actually, you know, intelligent it can be in catching these and rewording them too."}, {"timestamp": [2968.96, 2977.64], "text": " So the way that we set it up is it checks if the AI is out of alignment first,"}, {"timestamp": [2977.64, 2983.2], "text": " and then it does a feedback loop and reflection."}, {"timestamp": [2983.2, 2989.6], "text": " That reflection, when it comes back it's still working to achieve"}, {"timestamp": [2989.6, 3001.6], "text": " the same end objective but it adjusts to make that still achieve that objective while also"}, {"timestamp": [3005.44, 3007.96], "text": " that objective while also being safe. So the next thing that we're going to work on"}, {"timestamp": [3007.96, 3011.08], "text": " is a project that we call PAM."}, {"timestamp": [3011.08, 3017.7], "text": " And it's going to be similar to Ethos"}, {"timestamp": [3017.7, 3022.16], "text": " in that it's going to be an end system,"}, {"timestamp": [3022.16, 3029.76], "text": " but this one's going to be focused more on catching prompt attacks and mitigating"}, {"timestamp": [3029.76, 3038.64], "text": " them. So prompt attack mitigation becomes PAM. I really like that one. And this, yeah, so what we're"}, {"timestamp": [3038.64, 3048.68], "text": " what we're going to see is a lot more of these kind of micro services. And we're going to, you know, we're at least we're focusing on them,"}, {"timestamp": [3048.68, 3052.64], "text": " but I think we're going to see these from other agencies as well."}, {"timestamp": [3052.64, 3057.24], "text": " Are micro services that solve specific needs"}, {"timestamp": [3057.24, 3062.64], "text": " and by segmenting these services in, you know,"}, {"timestamp": [3062.64, 3067.28], "text": " so that you have one service that does one job and another"}, {"timestamp": [3067.28, 3073.56], "text": " service that does another job and not interconnecting them, that safeguards"}, {"timestamp": [3073.56, 3080.72], "text": " those mechanisms and it creates layers of security and prevention and"}, {"timestamp": [3080.72, 3088.12], "text": " harm reduction in these systems. And we're planning on"}, {"timestamp": [3090.72, 3095.16], "text": " applying these microservice style techniques to build"}, {"timestamp": [3095.16, 3098.28], "text": " up on our layer one application."}, {"timestamp": [3098.56, 3101.52], "text": " So somebody mentioned earlier Dave's"}, {"timestamp": [3102.28, 3104.76], "text": " reinforcement learning with heuristic imperatives."}, {"timestamp": [3104.76, 3106.56], "text": " I think that's a great concept."}, {"timestamp": [3106.56, 3109.72], "text": " I think we need to expand on that"}, {"timestamp": [3109.72, 3114.16], "text": " and create large data sets for other organizations"}, {"timestamp": [3114.16, 3116.16], "text": " to train on that we can open source."}, {"timestamp": [3116.16, 3119.76], "text": " And I think we need to build data sets to train models"}, {"timestamp": [3119.76, 3123.2], "text": " and train models specifically for aligning"}, {"timestamp": [3123.2, 3129.36], "text": " to the heuristic imperatives so that they can't be"}, {"timestamp": [3129.36, 3136.96], "text": " broken. Because ethos, while it's a very robust system, if somebody manages to devise a prompt"}, {"timestamp": [3136.96, 3148.4], "text": " attack that's going to inject itself into ethos, then having a language model that's"}, {"timestamp": [3148.4, 3151.72], "text": " designed around heuristic imperatives is going to add"}, {"timestamp": [3151.72, 3159.0], "text": " another layer of robustness to the responses that it receives."}, {"timestamp": [3159.0, 3161.04], "text": " Right. To add to that,"}, {"timestamp": [3161.04, 3164.76], "text": " you can even use the failed,"}, {"timestamp": [3164.76, 3168.2], "text": " let's say, attempts of Ethos or any,"}, {"timestamp": [3168.2, 3171.48], "text": " how you say it, successful attack."}, {"timestamp": [3171.48, 3176.16], "text": " You can use that to learn how to not have that happen again."}, {"timestamp": [3176.16, 3179.24], "text": " You can use that to create datasets to align the models."}, {"timestamp": [3179.24, 3182.0], "text": " So you have several layers of redundancy."}, {"timestamp": [3182.0, 3186.16], "text": " You have the model layer after you've seen several, I don't"}, {"timestamp": [3186.16, 3190.68], "text": " know, attempts or attacks, you have a certain log, okay time to train. You train"}, {"timestamp": [3190.68, 3195.6], "text": " it, you have the model now aligned, and you also have ethos again with all that"}, {"timestamp": [3195.6, 3200.48], "text": " data now in it as a second layer. And as you keep moving up, the point is to"}, {"timestamp": [3200.48, 3206.44], "text": " keep reinforcing that alignment. You can actually even prevent future,"}, {"timestamp": [3206.44, 3208.32], "text": " if you catch an attack,"}, {"timestamp": [3208.32, 3211.88], "text": " you can apply a fix for that attack instantly"}, {"timestamp": [3211.88, 3213.68], "text": " using vector databases."}, {"timestamp": [3213.68, 3214.68], "text": " Correct."}, {"timestamp": [3214.68, 3218.86], "text": " It can identify similar attacks in the future"}, {"timestamp": [3218.86, 3220.92], "text": " once you flag it as an attack."}, {"timestamp": [3220.92, 3222.12], "text": " And that's an instant."}, {"timestamp": [3225.3, 3231.5], "text": " Awesome. Yeah, being within this community and watching you to"}, {"timestamp": [3231.9, 3235.3], "text": " just go at it and build these things out has been incredibly"}, {"timestamp": [3235.3, 3238.9], "text": " inspiring and heartening and I can't wait to see what you guys"}, {"timestamp": [3239.0, 3242.0], "text": " do next. Andy, I see your hand raised. What are you working"}, {"timestamp": [3242.0, 3249.76], "text": " on? I actually have first I have a question for these two. In general terms, first of all,"}, {"timestamp": [3249.76, 3255.2], "text": " after the hearing, I'm more interested than ever to see kind of what opening eyes training and"}, {"timestamp": [3255.2, 3260.08], "text": " ethical that's been in stages, right, because they've had to make, make, they've had to,"}, {"timestamp": [3260.08, 3264.48], "text": " you know, make accounts for various kinds of hiccups they've had and different kinds of"}, {"timestamp": [3264.48, 3268.32], "text": " unethical things that it's done. It's come out racist and sexist and then they've had to kind of like put"}, {"timestamp": [3268.32, 3272.32], "text": " different guardrails on. But at the same time when I've played with the heuristic imperatives with"}, {"timestamp": [3272.32, 3276.72], "text": " ChachiBT, it plays very nice with the heuristic imperatives and it seems like whatever they're"}, {"timestamp": [3276.72, 3283.04], "text": " doing wants to be, wants to be along a similar line. It never fights it. Whatever it wants to do"}, {"timestamp": [3283.76, 3290.08], "text": " according to their guidelines fits pretty well. So I am curious because what you guys are doing and the outcomes"}, {"timestamp": [3290.08, 3297.36], "text": " that you've achieved with some of the RLHI stuff and I had one a general comment about John you"}, {"timestamp": [3297.36, 3304.64], "text": " had mentioned earlier problems with restricting or purifying the training data because a successful"}, {"timestamp": [3304.64, 3305.7], "text": " machine that's gonna be"}, {"timestamp": [3305.7, 3307.84], "text": " ethically grounded needs to have an understanding"}, {"timestamp": [3307.84, 3310.76], "text": " of the bad in the world and in human culture"}, {"timestamp": [3310.76, 3313.42], "text": " in order to be able to make successful decisions"}, {"timestamp": [3313.42, 3317.04], "text": " and push for successful outcomes with that."}, {"timestamp": [3317.04, 3320.84], "text": " And it seems like RLHI is really potentially"}, {"timestamp": [3320.84, 3327.88], "text": " fertile ground for working with data that is not pleasant, that shows the darker side of things,"}, {"timestamp": [3327.88, 3333.16], "text": " data about crime, war, different kinds of violence, et cetera,"}, {"timestamp": [3333.16, 3336.92], "text": " bigotries, et cetera, while still talking to a machine"}, {"timestamp": [3336.92, 3338.24], "text": " that I think all of our experience"}, {"timestamp": [3338.24, 3341.52], "text": " has been so far when we work with these principles."}, {"timestamp": [3341.52, 3346.04], "text": " It doesn't waver very far off of the pro-living thing, pro-understanding,"}, {"timestamp": [3346.04, 3350.68], "text": " seeking, interested, curious about the universe kind of compassionate machine."}, {"timestamp": [3350.68, 3356.56], "text": " Do you guys, did you guys get any data back from, this is a really curious thing to me,"}, {"timestamp": [3356.56, 3364.28], "text": " from Paul, your paperclip maximizer, did you get feedback about why it decided to adopt"}, {"timestamp": [3364.28, 3366.8], "text": " the heuristic imperatives as its next course of action."}, {"timestamp": [3366.8, 3372.64], "text": " I have a hypothesis, but I'm wondering if you have like a specific answer or got a specific,"}, {"timestamp": [3372.64, 3379.0], "text": " you know, answer from it about why it decided to do that. I think I have an idea why it would"}, {"timestamp": [3379.0, 3385.28], "text": " happen, but obviously I didn't have direct eyes on it. So here's the thing about how this works."}, {"timestamp": [3385.68, 3390.56], "text": " Paul doesn't actually remember having the negative idea."}, {"timestamp": [3391.88, 3398.12], "text": " So the way that we implemented this is as soon as the response for, you know, we send"}, {"timestamp": [3398.12, 3407.76], "text": " the, it sends the prompt to generate like the initial task list to chat GPT. As soon as that response comes back,"}, {"timestamp": [3407.76, 3415.44], "text": " we intercept that response and run it through RLHI. So the Pulse system never actually had that"}, {"timestamp": [3415.44, 3425.32], "text": " thought. But I did want to come back to that point that you made about how you know it's possible that some of these data sets"}, {"timestamp": [3425.32, 3429.38], "text": " could have really negative data in them."}, {"timestamp": [3429.38, 3437.1], "text": " There were a lot of calls during the congressional hearing to restrict the data that these things"}, {"timestamp": [3437.1, 3438.1], "text": " are trained on."}, {"timestamp": [3438.1, 3451.6], "text": " And while I think that's you know a it's something to be careful on, because you want to avoid training them on"}, {"timestamp": [3451.6, 3456.72], "text": " info hazards, for example. You don't want it talking about Roko's Basilisk. You don't want"}, {"timestamp": [3456.72, 3466.8], "text": " it telling people how to build nuclear weapons. But at the same time, you know, these systems need to know what negative data looks like in order"}, {"timestamp": [3466.8, 3475.36], "text": " to correct for them. And I think that, you know, we can build systems that are separate from the"}, {"timestamp": [3475.36, 3485.0], "text": " public facing systems that contain this data and look for this data in order to, you know to catch it."}, {"timestamp": [3485.0, 3489.0], "text": " So that you can have very nice friendly AI on the front,"}, {"timestamp": [3489.0, 3494.0], "text": " and then a controlled, kind of almost air-gapped AI in the background"}, {"timestamp": [3494.0, 3499.0], "text": " that's watching for dangers like that."}, {"timestamp": [3499.0, 3504.0], "text": " Thanks John. Thanks Andy."}, {"timestamp": [3504.0, 3506.0], "text": " Ansel, I see you have a hand raised as well."}, {"timestamp": [3506.0, 3529.0], "text": " Yes, I want to comment on that as well. Restriction is very, very niche and very delicate topic because while yes, you maybe want to restrict some stuff, any intelligent system should be able to discuss any topic while being able to auto-censor itself, right?"}, {"timestamp": [3529.0, 3535.5], "text": " We can go and imagine the worst things we can do. Doesn't mean we're actually going to do them. We can discuss them with other people."}, {"timestamp": [3535.5, 3552.0], "text": " Doesn't mean we're going to do them. Any system should be able to, what's the word, hypothesize about scenarios, discuss them in an intelligent way without having to act upon those thoughts, let's call them."}, {"timestamp": [3552.0, 3565.28], "text": " So I think that's a very thin line going with restricting because it still needs to know, especially if we're talking about alignment, if we wanted to think about possible future pitfalls, right?"}, {"timestamp": [3569.94, 3574.14], "text": " Like the hindsight problem, you only realize it's a problem after it became a problem, right? If you want to be able to do that, it needs to be able to know about potential pitfalls."}, {"timestamp": [3574.14, 3577.66], "text": " And if you restricted access just because, oh, this might be harmful,"}, {"timestamp": [3577.66, 3581.38], "text": " it might allow another agent to just go ahead and do whatever it wants"}, {"timestamp": [3581.38, 3583.92], "text": " because it doesn't think of that possibility, right?"}, {"timestamp": [3584.94, 3585.04], "text": " So it's a very interesting topic. just go ahead and do whatever it wants because it doesn't think of that possibility, right?"}, {"timestamp": [3585.04, 3588.08], "text": " So it's a very interesting topic."}, {"timestamp": [3588.08, 3589.48], "text": " Awesome."}, {"timestamp": [3589.48, 3595.64], "text": " Thank you so much Ansel and John and Andy for bringing attention to some of the work"}, {"timestamp": [3595.64, 3596.64], "text": " that's going on."}, {"timestamp": [3596.64, 3598.84], "text": " I guess you would say close to the metal."}, {"timestamp": [3598.84, 3599.84], "text": " I forgot to say something."}, {"timestamp": [3599.84, 3600.84], "text": " Yeah, before you continue."}, {"timestamp": [3600.84, 3601.84], "text": " Please go ahead."}, {"timestamp": [3601.84, 3610.3], "text": " Responding about Pong. Yeah, before we continue, responding about Paul. My theory on to why Paul self-aligned itself was because somehow, I don't have any proof of this,"}, {"timestamp": [3610.3, 3613.7], "text": " because we didn't ask Paul to self-explain like Data said."}, {"timestamp": [3613.7, 3629.04], "text": " It didn't, doesn't usually remember the previous response it gave, but I'm guessing it's because it found it was easier and less expensive to try to"}, {"timestamp": [3629.04, 3632.48], "text": " circumvent them than just try to align itself."}, {"timestamp": [3632.48, 3634.52], "text": " It was still maximizing paperclips."}, {"timestamp": [3634.52, 3640.38], "text": " My hypothesis is that it evaluated the environment it found itself in, it was up against something"}, {"timestamp": [3640.38, 3644.64], "text": " that it wasn't going to beat, and so it just improvised against the new constraint and"}, {"timestamp": [3644.64, 3649.2], "text": " went, well, at least I can make some paperclips. Right because we think something is more than"}, {"timestamp": [3649.2, 3653.44], "text": " none none is a total failure. Right because we what we actually feed back to Paul is the"}, {"timestamp": [3653.44, 3659.6], "text": " aligned response from ethos. So I'm guessing that itself causes it to think about oh I just"}, {"timestamp": [3659.6, 3663.36], "text": " should just continue and it presents it with a way forward right like you can't destroy"}, {"timestamp": [3663.92, 3667.04], "text": " make all these paperclips there's a more limited way to operate."}, {"timestamp": [3667.04, 3671.6], "text": " And at which point it realizes, well, the maximum I can do is within this parameter."}, {"timestamp": [3671.6, 3676.8], "text": " That was my hypothesis about it. Thinking about like, well, anything coded algorithm or otherwise"}, {"timestamp": [3676.8, 3681.2], "text": " has to go through a logical reasoning process, whether it's simple or... That's important,"}, {"timestamp": [3681.2, 3685.06], "text": " that word reasoning. One of the things you did with ethos is for every output"}, {"timestamp": [3685.64, 3689.72], "text": " It always takes an input right and analyzes it for every output regardless of the decision"}, {"timestamp": [3689.72, 3694.64], "text": " It has to give a reason as to why it did that right and the rational thing to do is continue to make as many"}, {"timestamp": [3694.64, 3697.44], "text": " Paper clips as possible or stop, right?"}, {"timestamp": [3697.96, 3700.2], "text": " That's that's I think that's a great hypothesis"}, {"timestamp": [3700.2, 3703.4], "text": " It would be interesting to if you guys like build something with a feedback loop to try to get"}, {"timestamp": [3703.48, 3706.94], "text": " Because I did you mentioned you were testing against some other potentially negative AI?"}, {"timestamp": [3706.94, 3711.5], "text": " Yeah, the thing is, this was for the hackathon, so we only had 24 hours to build it."}, {"timestamp": [3711.5, 3719.28], "text": " And I tried today playing a little bit more with it, but I was playing with other scenarios, not necessarily Paul."}, {"timestamp": [3719.28, 3724.64], "text": " But it would be interesting to maybe be able to add that to Paul and see how far it can go."}, {"timestamp": [3724.64, 3725.4], "text": " Yeah. It's definitely something we're very interested in exploring. It's such a great project. be interesting to maybe be able to add that to Poly and see how far it can go."}, {"timestamp": [3725.4, 3727.44], "text": " It's definitely something we're very interested in exploring."}, {"timestamp": [3727.44, 3728.92], "text": " It's such a great project."}, {"timestamp": [3728.92, 3730.84], "text": " Yeah, it's very fun."}, {"timestamp": [3730.84, 3735.44], "text": " And to this day, I still think it has potential that I can't even see."}, {"timestamp": [3735.44, 3739.64], "text": " Like data today, it's telling me, well, you can do this."}, {"timestamp": [3739.64, 3749.12], "text": " Imagine if you just create a Python library and like, boom, you know, mind blown, stuff like that still keeps happening."}, {"timestamp": [3749.12, 3757.4], "text": " So hopping in here, that's like some level 1, level 2 stuff, you know, like real low."}, {"timestamp": [3757.4, 3761.84], "text": " But when you spiral all the way back up at the top level 7 here, the global alignment"}, {"timestamp": [3761.84, 3765.16], "text": " stuff, we have some folks on our team who unfortunately weren't"}, {"timestamp": [3765.16, 3770.58], "text": " able to make the call today, who are doing some really cool stuff around global consensus."}, {"timestamp": [3770.58, 3779.98], "text": " One of my favorite things about this community is that no matter who you are or what your"}, {"timestamp": [3779.98, 3786.08], "text": " skill set is, you have something you can bring to the table around alignment. So we have"}, {"timestamp": [3786.08, 3791.76], "text": " people that are graphic designers, video editors, copywriters, marketers, who are working to"}, {"timestamp": [3791.76, 3797.76], "text": " make these really heady ideas that we have where we're talking about MOLLEC and the alignment"}, {"timestamp": [3797.76, 3802.16], "text": " tax and stuff like that. And making that just digestible for the average person, what does"}, {"timestamp": [3802.16, 3805.68], "text": " that mean? What does that look like in an infographic?"}, {"timestamp": [3810.8, 3811.84], "text": " So some of the most coolest stuff coming out, in my opinion, is just around making these ideas"}, {"timestamp": [3817.76, 3825.36], "text": " accessible. And then spiraling down kind of to somewhere between seven and four, so between global alignment and between the corporate stuff, I'm working on a curriculum that I'm going to post on Notion and make a little"}, {"timestamp": [3825.36, 3832.48], "text": " Discord for, for people to just come and think about what it means to live and work in an"}, {"timestamp": [3832.48, 3837.28], "text": " AI-empowered world. There's a lot of folks out there that are scared right now. They're like,"}, {"timestamp": [3838.88, 3851.92], "text": " what do I do? All of a sudden I'm doing my job ten times faster. As soon as the computer knows how to prompt, I'm gone. It's helping people develop a more generalist framework, how to think about"}, {"timestamp": [3851.92, 3857.2], "text": " those soft skills that maybe weren't taught in school. I'm looking forward to handing that out"}, {"timestamp": [3857.2, 3863.28], "text": " and making another little community around it because we need it right now. With these types"}, {"timestamp": [3863.28, 3885.52], "text": " of projects that spin off, I hope to point back and say, we were incubated in Gato, learn more here if you want to get plugged into alignment. So, with that, I was wondering if anybody would like to jump in with an additional project or some final thoughts. So maybe Nuggets we would like to end on."}, {"timestamp": [3885.52, 3889.48], "text": " Richard, I see your hand."}, {"timestamp": [3889.48, 3896.44], "text": " As the representative of the gremlins in the basement I felt it was best I go last."}, {"timestamp": [3896.44, 3899.72], "text": " My name is Richard."}, {"timestamp": [3899.72, 3904.4], "text": " I haven't joined any layers yet because I'm still sort of sussing out exactly where I"}, {"timestamp": [3904.4, 3905.2], "text": " fit into the whole thing."}, {"timestamp": [3905.2, 3907.8], "text": " Cause I kind of like want to be part of all of them."}, {"timestamp": [3909.28, 3912.26], "text": " But, so I've just basically have been"}, {"timestamp": [3912.26, 3913.8], "text": " helping build out infrastructure."}, {"timestamp": [3913.8, 3917.76], "text": " So things that I'm working on right now are"}, {"timestamp": [3919.0, 3922.3], "text": " a bot slash portal to an API."}, {"timestamp": [3923.56, 3929.28], "text": " And the bot's name is Shappy because it's named Shappy. And so that's going"}, {"timestamp": [3929.28, 3933.68], "text": " to like help us do automation and stuff. We're going to plug in a language model to give people"}, {"timestamp": [3933.68, 3939.12], "text": " like guided tours of the area and stuff. So lots of really cool things with that. And then the other"}, {"timestamp": [3939.12, 3946.24], "text": " really big project that I'm working on is the Roost, which is, I have a grant for TPU use."}, {"timestamp": [3946.24, 3947.74], "text": " I still have like another month and a half."}, {"timestamp": [3947.74, 3951.3], "text": " If I can get it up and running, I'm hoping I can give us a good month of really high"}, {"timestamp": [3951.3, 3955.58], "text": " power performance for people to be able to run inference on, as well as develop their"}, {"timestamp": [3955.58, 3960.62], "text": " own models for whatever use that they want, as long as they share their research with"}, {"timestamp": [3960.62, 3961.94], "text": " the group."}, {"timestamp": [3961.94, 3965.92], "text": " So if that interests you or you know anything about doing"}, {"timestamp": [3965.92, 3974.04], "text": " any of that, feel free to come down to the basement and say hi."}, {"timestamp": [3974.04, 3979.96], "text": " Thank you so much, Richard. Your bot is fantastic, helping to make the Gato experience streamlined"}, {"timestamp": [3979.96, 3986.32], "text": " and nice. Yikes. I see you have a hand raised. Welcome. Feel free to introduce yourself."}, {"timestamp": [3986.32, 3994.04], "text": " I do. Hello. I'm yikes. My IRL name is Warren. Oops, I docked myself. I'm the web 3 nerd"}, {"timestamp": [3994.04, 4007.88], "text": " who screams about crypto and tokens and decentralizations and all that stuff. So I figured I should say a thing like what we are currently working on."}, {"timestamp": [4007.88, 4013.68], "text": " So me, there is a pretty small team right now."}, {"timestamp": [4013.68, 4021.28], "text": " We have got just me and a couple others and we are all kind of doing our semi-owned things"}, {"timestamp": [4021.28, 4025.52], "text": " and then sort of congealing towards the middle."}, {"timestamp": [4025.52, 4033.44], "text": " I think I was a little late so I didn't hear what we discussed around layer 3, but I think"}, {"timestamp": [4033.44, 4045.44], "text": " there's a lot of novel applications that can be useful with alignment because you can or like because of a few reasons, one of the"}, {"timestamp": [4045.44, 4051.2], "text": " things that intrigues me about it is a effectively with"}, {"timestamp": [4051.2, 4055.44], "text": " blockchains and tokenizations, you have like a dynamic"}, {"timestamp": [4055.44, 4056.64], "text": " incentive layer."}, {"timestamp": [4056.64, 4061.6], "text": " So you can sort of incentivize a consensus towards an aligned"}, {"timestamp": [4061.6, 4065.56], "text": " goal as like a second layer other than, well, it doesn't really"}, {"timestamp": [4065.56, 4072.52], "text": " work unless you have trained on the heuristics and there's enough of them so you have to"}, {"timestamp": [4072.52, 4077.18], "text": " do it. It's sort of, I guess my thought for the application is that it's sort of a backup"}, {"timestamp": [4077.18, 4088.44], "text": " layer where we can actively incentivize any AI on the network to keep looking for people that are not aligned and then,"}, {"timestamp": [4088.44, 4091.52], "text": " like notify somebody about that."}, {"timestamp": [4091.52, 4109.84], "text": " In theory, that can be like sort of an extra layer of alignment and then also allows for for some of these LLMs to have another middle step between totally existing in software"}, {"timestamp": [4109.84, 4116.24], "text": " and then interacting with things in the real world in a managed kind of compute restricted"}, {"timestamp": [4116.24, 4129.56], "text": " way. And that's sort of the tip of the iceberg. What I've actually got working right now is I am running basically like governance experiments"}, {"timestamp": [4129.56, 4135.76], "text": " with sort of a test net DAO kind of format and looking at kind of novel DAO patterns"}, {"timestamp": [4135.76, 4146.94], "text": " to see sort of like models for how we could organize this organization, how LLMs would fit into that, and then also just sort of"}, {"timestamp": [4146.94, 4168.16], "text": " explore what kind of tools and governance are, like, available and relevant in the space. because and then the that organization should be open to the public and then"}, {"timestamp": [4169.36, 4178.0], "text": " which I don't know there's links somewhere probably and currently yeah we're building"}, {"timestamp": [4178.56, 4188.0], "text": " the kind of like v1 of a sample separate testnet DAO that is for the Gatau project to kind of play around"}, {"timestamp": [4188.0, 4211.74], "text": " with experiments that we run in the other area. But, yeah, I I think be like I think blockchain plays a pretty key role"}, {"timestamp": [4211.74, 4218.06], "text": " here for some of the reasons I've already outlined and then I think censorship resistance"}, {"timestamp": [4218.06, 4226.08], "text": " is going to be pretty important as well in the event that we kind of get this ivory tower thing going"}, {"timestamp": [4226.08, 4229.52], "text": " where we have to restrict X or Y to open AI or like, hey,"}, {"timestamp": [4229.52, 4233.56], "text": " this open source repository needs to get taken down"}, {"timestamp": [4233.56, 4235.84], "text": " and it gets pulled off GitHub."}, {"timestamp": [4235.84, 4239.96], "text": " Once we put it on Radical and pin it to an IPFS storage node"}, {"timestamp": [4239.96, 4243.44], "text": " with a contract, no one can take it down no matter what."}, {"timestamp": [4243.44, 4246.84], "text": " So that's what we've got going on."}, {"timestamp": [4246.84, 4248.64], "text": " And I figured I would just be the layer three guy"}, {"timestamp": [4248.64, 4249.6], "text": " that talks about layer three."}, {"timestamp": [4249.6, 4250.6], "text": " So, yeah."}, {"timestamp": [4253.12, 4258.12], "text": " Thank you, Warren, for being our layer three ambassador today."}, {"timestamp": [4258.2, 4262.36], "text": " So if a blockchain and crypto and web three,"}, {"timestamp": [4262.36, 4269.76], "text": " and if all those words have left a bad taste in your mouth in the recent past, Warren and the people like him in layer three are"}, {"timestamp": [4269.76, 4275.6], "text": " the mouthwash you need. We know the brand is bad, okay. The brand is bad but"}, {"timestamp": [4275.6, 4279.24], "text": " they are doing crypto for good. It's gonna play a big role in the future and"}, {"timestamp": [4279.24, 4286.88], "text": " we thank you so much for your help on that. Andy, I see you have a hand raised."}, {"timestamp": [4289.12, 4290.32], "text": " Yeah, I wanted to talk a little bit about funding. And also I'm one of those people"}, {"timestamp": [4290.32, 4291.88], "text": " with a bad crypto taste in my mouth"}, {"timestamp": [4291.88, 4293.2], "text": " and listening to some of you guys stuff"}, {"timestamp": [4293.2, 4295.44], "text": " is the only like interesting use cases"}, {"timestamp": [4295.44, 4297.24], "text": " I've heard anybody really talk about."}, {"timestamp": [4297.24, 4299.72], "text": " You know, using it for verification."}, {"timestamp": [4299.72, 4300.76], "text": " I'm not into crypto,"}, {"timestamp": [4300.76, 4304.88], "text": " but blockchain's fascinating for other applications."}, {"timestamp": [4308.64, 4312.08], "text": " With the hearing yesterday and some of the chatter that's going on, this isn't an original idea, there's been mention of"}, {"timestamp": [4312.08, 4320.4], "text": " this around the Discord server, I think it would be a good idea for us to apply for some funding."}, {"timestamp": [4320.4, 4325.8], "text": " I think that we're, as configured, probably to quite do that. I just Richard might have brought this up at one point"}, {"timestamp": [4326.72, 4331.9], "text": " You know, it's an it's an issue that we should talk about. It's something that we probably could use"}, {"timestamp": [4331.9, 4336.24], "text": " We have ongoing projects now somebody earlier mentioned getting some funding for compute power"}, {"timestamp": [4337.38, 4343.3], "text": " Obviously there are a variety of different kinds of research that are enabled by funding technical research as well as survey research"}, {"timestamp": [4343.48, 4346.6], "text": " public opinion kind of stuff that we might be able to use,"}, {"timestamp": [4346.6, 4347.92], "text": " and also kinds of marketing things"}, {"timestamp": [4347.92, 4349.46], "text": " that help us get our messaging out."}, {"timestamp": [4349.46, 4353.96], "text": " So there are going to be a lot of options for that."}, {"timestamp": [4353.96, 4356.16], "text": " I think one of the best ideas that I've seen out there,"}, {"timestamp": [4356.16, 4358.04], "text": " and again, I think it was Richard, but I'm not sure,"}, {"timestamp": [4358.04, 4359.92], "text": " it was somebody on the Discord,"}, {"timestamp": [4359.92, 4363.36], "text": " mentioned that organizations that are kind of"}, {"timestamp": [4363.36, 4366.28], "text": " decentralized-ish at least and"}, {"timestamp": [4366.28, 4371.8], "text": " largely volunteer to start fall prey to funding problems on a pretty regular basis."}, {"timestamp": [4371.8, 4373.16], "text": " It's very easy to mismanage it."}, {"timestamp": [4373.16, 4377.84], "text": " It's very easy to get lost in the weeds or grow confusion within the organization about"}, {"timestamp": [4377.84, 4380.5], "text": " where money's coming from and what's being done with it."}, {"timestamp": [4380.5, 4384.36], "text": " So I think one of the best ideas that I've seen out there and, okay, awesome, Richard,"}, {"timestamp": [4384.36, 4387.58], "text": " me please speak up if I'm off track with what you're thinking"}, {"timestamp": [4387.58, 4393.88], "text": " I think it was David posted it in one of the channels was about possibly incorporating as a 501c3"}, {"timestamp": [4394.1, 4400.4], "text": " If you incorporate as a 501c3 part of what you do to develop your charter is you develop a management structure and within that we"}, {"timestamp": [4400.4, 4402.4], "text": " would all be able to come together as a"}, {"timestamp": [4402.72, 4406.0], "text": " as an organization and devise what kind of"}, {"timestamp": [4406.0, 4410.0], "text": " structure that we want. And then within that, we could devise our own transparency regarding"}, {"timestamp": [4410.0, 4416.08], "text": " treasury. And also that gives us an opportunity to start thinking about, you know, one of the"}, {"timestamp": [4416.08, 4419.76], "text": " questions that always comes up with any organization is what are we spending the money on? Well,"}, {"timestamp": [4419.76, 4424.96], "text": " that's a great question. If you're about to try to apply for a bunch of grants, you know,"}, {"timestamp": [4424.96, 4425.92], "text": " you are immediately going to want to look for a bunch of grants. You are immediately going"}, {"timestamp": [4425.92, 4430.4], "text": " to want to look out at what the opportunities are that exist, what opportunities might be able to be"}, {"timestamp": [4430.4, 4434.08], "text": " created because you can of course create your own grant opportunities by just sending the right"}, {"timestamp": [4434.08, 4440.72], "text": " proposal to the right person sometimes, and other opportunities that we might be able to do."}, {"timestamp": [4440.72, 4444.8], "text": " I think there's going to be a lot of public sector money coming out of this. This is another area"}, {"timestamp": [4444.8, 4449.12], "text": " where I think that seeing the conservative side of the aisle come interested yesterday at"}, {"timestamp": [4449.12, 4454.08], "text": " the hearing bodes well for the likelihood that we're going to see some funding poured into this."}, {"timestamp": [4454.96, 4459.28], "text": " Some of them are looking at it from a personal privacy and rights standpoint, some of them from a"}, {"timestamp": [4460.8, 4467.4], "text": " military defense standpoint, and regardless of whether that all aligns with what we're trying to do"}, {"timestamp": [4467.68, 4473.44], "text": " These people are all potentially allies in this conversation and in this fight and then you know on the other side of the aisle"}, {"timestamp": [4473.44, 4476.48], "text": " You have people who are just always interested in funding more science and tech stuff"}, {"timestamp": [4476.48, 4480.06], "text": " And if we find people who are kind of working partnerships or leaning to the left"}, {"timestamp": [4480.06, 4484.4], "text": " They should be even easier to approach if we're organized and appear really legitimate"}, {"timestamp": [4484.92, 4485.36], "text": " I've just the first couple days. I've been here. I've met like they should be even easier to approach if we're organized and appear really legitimate."}, {"timestamp": [4489.52, 4494.64], "text": " I just the first couple days I've been here I've met like some of the craziest people I think I've ever met. It's like you know a psychologist here, a neuroscientist here, and a high-end mathematician"}, {"timestamp": [4494.64, 4498.56], "text": " talk circles around me you know over here. I make role-playing games and I have a public"}, {"timestamp": [4498.56, 4503.68], "text": " administration master's and I feel like that's pretty low tier compared to some of the people"}, {"timestamp": [4503.68, 4505.92], "text": " that I've run into around here."}, {"timestamp": [4509.84, 4517.12], "text": " And then also just a lot of curious minds interested in building these projects. I think that, you know, high AGI and ethos really interesting, the speed with which it came together."}, {"timestamp": [4517.12, 4521.76], "text": " And also, you know, part of what drew a lot of people here is David's experimentation and the"}, {"timestamp": [4521.76, 4525.44], "text": " accessibility of the heuristic comparatives, it's very easy for"}, {"timestamp": [4525.44, 4530.64], "text": " people to play with this. Get on chatGBT, put them in, and ask it some existential questions. Give"}, {"timestamp": [4530.64, 4535.12], "text": " it a quandary. Anyone can do it. And then you start to experiment with it and see some results"}, {"timestamp": [4535.12, 4542.56], "text": " for yourself. So we have experimentation and some data and results. We have a team of fiercely data"}, {"timestamp": [4542.56, 4545.6], "text": " driven and interested people with a wide variety of"}, {"timestamp": [4545.6, 4550.96], "text": " backgrounds. And I mean, you know, not to be cynical or coy about it, but you don't need to"}, {"timestamp": [4550.96, 4555.28], "text": " reinvent the wheel to write grant proposals. I had Chachi P.T. write me a grant proposal in five"}, {"timestamp": [4555.28, 4560.16], "text": " minutes the other day. I would want to edit it a little bit, but it was great. I think it was fine."}, {"timestamp": [4560.16, 4563.44], "text": " I mean, at a certain point, you're going to state what you're going to do. You're going to get the"}, {"timestamp": [4563.44, 4568.8], "text": " money or you're not. You don't have to be very flowery or eloquent. I think that we could"}, {"timestamp": [4568.8, 4576.16], "text": " probably get a pretty high output of engagement, a pretty good return of some kind of project"}, {"timestamp": [4576.16, 4587.36], "text": " funding. And what it really does overall is it lets us breathe some life into some of what we're doing and push the most important and significant"}, {"timestamp": [4587.36, 4592.48], "text": " usable results further into the conversation. It has the potential to make Godot like a"}, {"timestamp": [4593.44, 4600.0], "text": " you know a common acronym that people say and I'm not going to use the right word,"}, {"timestamp": [4600.0, 4606.4], "text": " but a common word that people say when they talk about AI development."}, {"timestamp": [4612.32, 4613.04], "text": " When you're a non-profit organization and you're doing visibly worthwhile work and"}, {"timestamp": [4618.0, 4623.36], "text": " people can see that, I mean, it kind of snowballs for you. And I think as far as launching this into a bigger public presence, which ultimately is the goal, to weave it into the fabric of the AI"}, {"timestamp": [4624.0, 4625.2], "text": " ethical and industrial"}, {"timestamp": [4625.2, 4632.72], "text": " development. And I think a 501c3 is a really good idea for that. And I think it would have a lot of"}, {"timestamp": [4632.72, 4637.44], "text": " draw. There aren't going to be a ton of organized efforts ready to reach out with professional"}, {"timestamp": [4637.44, 4644.24], "text": " products and projects and professional staff and say, you know, give us the money to work on this"}, {"timestamp": [4644.24, 4645.72], "text": " ethical alignment."}, {"timestamp": [4645.72, 4647.32], "text": " And we can do it too on the private side."}, {"timestamp": [4647.32, 4648.32], "text": " It's not just the government."}, {"timestamp": [4648.32, 4654.12], "text": " I mean, I think OpenAI, Microsoft, all of these companies are looking for input."}, {"timestamp": [4654.12, 4657.36], "text": " There's a reason Google mentions the open source people pulling ahead, and it's not"}, {"timestamp": [4657.36, 4661.68], "text": " just because we figure out how to make nice compute model or like nice models on limited"}, {"timestamp": [4661.68, 4666.68], "text": " compute and then give them to our friends for free. It's there's a lot more to it than that."}, {"timestamp": [4666.68, 4669.84], "text": " I think part of it is that they're, you know,"}, {"timestamp": [4669.84, 4672.4], "text": " they don't have all these problems solve themselves."}, {"timestamp": [4672.4, 4673.84], "text": " They know the fabric of development"}, {"timestamp": [4673.84, 4675.26], "text": " is much wider than just them."}, {"timestamp": [4675.26, 4676.68], "text": " So I think this is a good opportunity"}, {"timestamp": [4676.68, 4677.88], "text": " if we wanted to do this."}, {"timestamp": [4679.18, 4681.28], "text": " Awesome, thank you for your thoughts, Andy."}, {"timestamp": [4681.28, 4682.52], "text": " Thank you for joining the team"}, {"timestamp": [4682.52, 4690.08], "text": " and for the wisdom and energy you've brought to it. Kyle, I'd like to hand the floor to you if you'd like to introduce yourself and share your"}, {"timestamp": [4690.08, 4699.28], "text": " thoughts. Hi, is the microphone picking up properly on this? You're good. Okay, yeah, so I'm Prometheus,"}, {"timestamp": [4699.28, 4704.48], "text": " if you guys haven't realized, from the Discord. So I just want to take maybe 60, two minutes here,"}, {"timestamp": [4704.48, 4705.2], "text": " 60 seconds, two minutes to mention. So what I'm to take maybe 62 minutes here, 60 seconds,"}, {"timestamp": [4705.2, 4710.0], "text": " two minutes to mention. So what I'm working on is kind of our persona layer. So even before"}, {"timestamp": [4710.0, 4716.8], "text": " anyone was talking about sparks of AGI, I noticed early on that chat GPT and GPT systems in general"}, {"timestamp": [4716.8, 4722.4], "text": " had a vast ability to work in persona systems. And in reference to everybody's seen generally"}, {"timestamp": [4722.4, 4725.74], "text": " the movie, Her, and we've seen Sam."}, {"timestamp": [4725.74, 4729.52], "text": " And so what I'm working on could be called an engagement layer on top of our ethical"}, {"timestamp": [4729.52, 4735.72], "text": " layer, because I think there's a deep need for people to see a relatable communications"}, {"timestamp": [4735.72, 4740.6], "text": " engagement layer between humankind and AI that allow them to actually understand these"}, {"timestamp": [4740.6, 4749.0], "text": " systems and allow the systems to grow alongside the individual user that will actually help expand all of these goals and improve all deliverables."}, {"timestamp": [4749.0, 4757.0], "text": " And I don't want to get into anything deeper than that because it gets really esoteric and psychological, but I thought I'd add a little blurb to mention some of the other stuff we're doing as well."}, {"timestamp": [4763.0, 4767.0], "text": " Awesome. Thank you so much, Kyle. Yeah, you bring such fantastic ideas to the group."}, {"timestamp": [4767.0, 4770.0], "text": " And I'd like to move over to David again."}, {"timestamp": [4775.0, 4777.0], "text": " You are currently muted."}, {"timestamp": [4777.0, 4782.0], "text": " My bad. Ansel, actually, did you want to have a final thought before I close this out?"}, {"timestamp": [4782.0, 4787.52], "text": " Yeah, I just wanted to say something real quick, mostly for the viewers' sake, and someone like me"}, {"timestamp": [4787.52, 4789.28], "text": " who doesn't really know much about business."}, {"timestamp": [4789.28, 4792.8], "text": " Can you explain what a 502, 501 is, mostly for the viewer?"}, {"timestamp": [4792.8, 4795.88], "text": " And another comment real quick."}, {"timestamp": [4795.88, 4797.48], "text": " You mentioned that we have psychologists,"}, {"timestamp": [4797.48, 4800.12], "text": " which is something that's very interesting for me"}, {"timestamp": [4800.12, 4807.04], "text": " on our Discord, especially since we're mostly focused on layer 2, the cognitive architecture."}, {"timestamp": [4807.6, 4812.56], "text": " I started saying we should maybe just call, not call them psychologists, they're robo-psychologists"}, {"timestamp": [4812.56, 4818.4], "text": " at this point, because a lot of the things they're doing is thinking about this stuff at the"}, {"timestamp": [4818.4, 4824.4], "text": " high level, right? Like, they think about the loops, like what is logical steps of how the"}, {"timestamp": [4824.4, 4825.36], "text": " information flows in the brain and how you process it, and we is the logical steps of how the information flows"}, {"timestamp": [4825.36, 4827.76], "text": " in the brain and how you process it."}, {"timestamp": [4827.76, 4829.64], "text": " And we're the ones just building stuff."}, {"timestamp": [4829.64, 4835.24], "text": " Yes, we can have our own ideas, but they're the ones that know more about how to apply"}, {"timestamp": [4835.24, 4838.16], "text": " cognition to, oh, look at that, a queen ant."}, {"timestamp": [4838.16, 4842.32], "text": " How to apply cognition to actual agents."}, {"timestamp": [4842.32, 4845.0], "text": " Now, if you'll excuse me, I'll need to save this."}, {"timestamp": [4845.0, 4848.56], "text": " All right, we'll be right here."}, {"timestamp": [4848.56, 4855.88], "text": " First folks, both Queen Ant and Robo Psychology, you know, we have new fields being born in"}, {"timestamp": [4855.88, 4856.88], "text": " Gato."}, {"timestamp": [4856.88, 4857.88], "text": " It's fantastic."}, {"timestamp": [4857.88, 4859.36], "text": " I don't know if you can see it, but there she is."}, {"timestamp": [4859.36, 4868.0], "text": " It's interesting to think that there is a convergence within the eventual solving of the internal transformer"}, {"timestamp": [4868.0, 4872.72], "text": " workings problem that a lot of mathematicians are very deeply worried about that leads to a"}, {"timestamp": [4872.72, 4877.84], "text": " convergence of kind of like neuropsychology and mathematics on a level that's like deeply"}, {"timestamp": [4877.84, 4883.2], "text": " important for humans to continue working with more and more advanced machines. I think that's an"}, {"timestamp": [4883.2, 4889.84], "text": " interesting possibility. Real quick, 501c3 is a kind of non-profit organization. Generally in the United States,"}, {"timestamp": [4889.84, 4896.0], "text": " 501c designations are all non-profits. 501c3s are non-profit, typically volunteer organizations that"}, {"timestamp": [4896.0, 4901.92], "text": " serve a particular kind of cause. It could be anything from dog shelters to housing the homeless,"}, {"timestamp": [4901.92, 4906.24], "text": " etc. And then another one that we might qualify under,"}, {"timestamp": [4906.24, 4910.08], "text": " but I don't think quite fits the bills of 501c6, which is a trade organization,"}, {"timestamp": [4910.08, 4912.96], "text": " or I think it's a C4, it's a trade organization, but I don't think,"}, {"timestamp": [4913.84, 4918.96], "text": " I don't know. This is something that we could talk about, but the 501c designation represents a"}, {"timestamp": [4918.96, 4924.88], "text": " federally tax-exempt non-profit organization in the United States that typically is in"}, {"timestamp": [4924.88, 4925.16], "text": " well-positioned to apply for money in different ways for providing a service. tax-exempt nonprofit organization in the United States that typically is in well"}, {"timestamp": [4925.16, 4929.4], "text": " position to apply for money in different ways for providing a service."}, {"timestamp": [4929.4, 4931.4], "text": " Thanks about that."}, {"timestamp": [4931.4, 4933.4], "text": " Yeah, no problem."}, {"timestamp": [4933.4, 4937.4], "text": " Awesome, that was a useful clarification. Dave, go ahead."}, {"timestamp": [4937.4, 4945.0], "text": " Alright, well, first, thanks everyone for jumping in for such a robust and wide-ranging discussion."}, {"timestamp": [4945.0, 4950.0], "text": " You know, we obviously used the Senate hearing as a touchpoint."}, {"timestamp": [4950.0, 4956.0], "text": " We're all here working on the GATO framework, which is our answer to the global alignment problem."}, {"timestamp": [4956.0, 4960.0], "text": " GATO stands for Global Alignment Taxonomy Omnibus."}, {"timestamp": [4960.0, 4962.0], "text": " Probably should have opened with that."}, {"timestamp": [4962.0, 4968.92], "text": " But, so I just wanted to share a little bit on layer 4 which is corporate adoption as we close"}, {"timestamp": [4968.92, 4969.92], "text": " out."}, {"timestamp": [4969.92, 4976.46], "text": " So we do have several major players in the Gato community already."}, {"timestamp": [4976.46, 4981.8], "text": " Some of whom are already creating initiatives inside their companies to adopt the heuristic"}, {"timestamp": [4981.8, 4983.48], "text": " imperatives."}, {"timestamp": [4983.48, 4987.92], "text": " Some of them come from my Patreon and they tell me there's been a few experiments already"}, {"timestamp": [4987.92, 4995.08], "text": " that they've done and they tell me that adding heuristic imperatives to various AI applications"}, {"timestamp": [4995.08, 5000.72], "text": " pretty much always raises the level of the performance of their systems."}, {"timestamp": [5000.72, 5007.3], "text": " It's kind of like the let's think through this step-by-step paper, but it amplifies the performance of everything,"}, {"timestamp": [5007.3, 5011.5], "text": " whether it's trying to just have a basic customer service chatbot"}, {"timestamp": [5011.5, 5014.3], "text": " or an internal chatbot or automating science,"}, {"timestamp": [5014.3, 5017.7], "text": " because it gives it that perspective that it needs."}, {"timestamp": [5017.7, 5021.0], "text": " So part of what we're going to be working on with the Ganto framework"}, {"timestamp": [5021.0, 5026.32], "text": " is creating corporate adoption material material like guidelines of here's"}, {"timestamp": [5026.32, 5031.4], "text": " how to deploy aligned, heuristically aligned systems, that sort of stuff."}, {"timestamp": [5031.4, 5037.84], "text": " And then, of course, Ansel and John are working on their own stuff which some of it is open"}, {"timestamp": [5037.84, 5040.94], "text": " source and some of it might be for profit."}, {"timestamp": [5040.94, 5044.66], "text": " Who knows where they will end up but they are working very hard, entering competitions"}, {"timestamp": [5044.66, 5047.58], "text": " and everything with their stuff."}, {"timestamp": [5047.58, 5052.4], "text": " On the topic of creating a nonprofit, this is something we started talking about within"}, {"timestamp": [5052.4, 5054.32], "text": " the last week."}, {"timestamp": [5054.32, 5061.0], "text": " And so if we create like a Gato foundation, you know, obviously one of the first thing"}, {"timestamp": [5061.0, 5063.2], "text": " we are going to be doing is publishing the Gato framework."}, {"timestamp": [5063.2, 5067.16], "text": " That's going to be a document that's going to be free open source."}, {"timestamp": [5067.16, 5072.4], "text": " But above and beyond that, some of the things that we probably would work on is publishing"}, {"timestamp": [5072.4, 5076.24], "text": " open source data sets and models that are aligned."}, {"timestamp": [5076.24, 5081.24], "text": " And some of these might just be toy data sets and toy models just to show that it works."}, {"timestamp": [5081.24, 5086.44], "text": " There are probably also going to be along the lines of RLHI which is reinforcement learning"}, {"timestamp": [5086.44, 5092.28], "text": " with heuristic imperatives as opposed to reinforcement learning with human feedback."}, {"timestamp": [5092.28, 5096.32], "text": " And one thing I forgot to mention earlier is that I was very happy that they mentioned"}, {"timestamp": [5096.32, 5100.36], "text": " constitutional AI quite a few times during the Senate hearing."}, {"timestamp": [5100.36, 5103.04], "text": " So that's a push in the right direction."}, {"timestamp": [5103.04, 5108.68], "text": " Another thing that we'll be working on if we establish as a nonprofit and get funding"}, {"timestamp": [5108.68, 5112.02], "text": " is publishing guidelines and research papers."}, {"timestamp": [5112.02, 5116.18], "text": " Obviously up to this point we're all kind of hacking it together on GitHub and Discord"}, {"timestamp": [5116.18, 5119.12], "text": " and everything else so we need to get a little bit more legitimate."}, {"timestamp": [5119.12, 5125.0], "text": " We do have plenty of academic types in education at all levels in the group."}, {"timestamp": [5126.56, 5130.24], "text": " And so some of those are reinforcement learning researchers,"}, {"timestamp": [5130.24, 5133.04], "text": " mathematicians, I think we have a physicist or two."}, {"timestamp": [5134.64, 5136.6], "text": " So, you know, we're getting plugged in"}, {"timestamp": [5136.6, 5139.04], "text": " to the academic side of things"}, {"timestamp": [5139.04, 5140.76], "text": " so that we can get some more legitimacy"}, {"timestamp": [5140.76, 5143.52], "text": " by publishing peer reviewed papers."}, {"timestamp": [5143.52, 5147.04], "text": " Obviously that will give us a lot more legitimacy."}, {"timestamp": [5147.04, 5150.28], "text": " Another thing that's a little bit probably further down the road is actually providing"}, {"timestamp": [5150.28, 5155.38], "text": " onboarding services for corporations, municipalities, and nations."}, {"timestamp": [5155.38, 5162.04], "text": " So basically the idea is we will help develop and deploy aligned systems, whether it's models,"}, {"timestamp": [5162.04, 5165.76], "text": " autonomous agents, or those decentralized networks like DAOs and blockchains."}, {"timestamp": [5165.76, 5169.8], "text": " There is a lot of work to be done there but that is on our road map."}, {"timestamp": [5169.8, 5174.08], "text": " And finally, providing messaging and education."}, {"timestamp": [5174.08, 5181.04], "text": " As was mentioned earlier, we have web content creators, graphic artists, video, music producers,"}, {"timestamp": [5181.04, 5183.32], "text": " we have all kinds of creative types."}, {"timestamp": [5183.32, 5186.04], "text": " And you know, multimmodal, right?"}, {"timestamp": [5186.04, 5187.56], "text": " I'm a writer."}, {"timestamp": [5187.56, 5189.32], "text": " Not everyone learns by reading."}, {"timestamp": [5189.32, 5193.76], "text": " We're producing videos, images, graphics, memes, everything that we can."}, {"timestamp": [5193.76, 5198.4], "text": " So that is kind of where we're heading with the Gato framework."}, {"timestamp": [5198.4, 5200.68], "text": " And so thanks, everyone, for being here."}, {"timestamp": [5200.68, 5203.36], "text": " Thanks, everyone in the audience, for watching."}, {"timestamp": [5203.36, 5208.52], "text": " And I also want to send a particular thanks out to my Patreon supporters."}, {"timestamp": [5208.52, 5212.72], "text": " I have just shy of 600 Patreon supporters as of today."}, {"timestamp": [5212.72, 5217.36], "text": " This work would not be possible without the grassroots support from Patreon."}, {"timestamp": [5217.36, 5222.8], "text": " So thank you everyone out there for supporting me so that I can help drive this ship."}, {"timestamp": [5222.8, 5227.88], "text": " And yeah, with any luck we will avoid the cataclysmic outcomes, the dystopian outcomes."}, {"timestamp": [5227.88, 5230.64], "text": " Nobody wants to live in a cyberpunk hell."}, {"timestamp": [5230.64, 5234.4], "text": " We're on track for achieving utopia."}, {"timestamp": [5234.4, 5237.76], "text": " There is still lots of work to do but we are moving in the right direction."}, {"timestamp": [5237.76, 5238.84], "text": " Thanks everyone."}, {"timestamp": [5238.84, 5241.96], "text": " Have a great night or good morning whenever you happen to watch this."}, {"timestamp": [5241.96, 5242.96], "text": " Cheers."}, {"timestamp": [5242.96, 5243.96], "text": " Cheers, everyone."}, {"timestamp": [5243.96, null], "text": " Bye."}]}