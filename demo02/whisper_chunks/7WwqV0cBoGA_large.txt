{"text": " Hello everybody, David Shapiro here with a brand new video. So today's video, we're going to talk about axiomatic alignment, which is a potential solution or part of the solution to the control problem. Before we dive into today's video, I just want to do a quick plug for my Patreon. I have lots of folks on Patreon. We've got a private discord. If you have any questions about AI, I am happy to consult. There's a few slots of the higher tiers available, which will give you one-on-one meetings with me. So without further ado, let's jump right back into the show. So the control problem, if you're not in the know, is basically at some point in the future, AI is going to get incredibly powerful. There's basically two ways that this can happen, and the truth will probably be somewhere in the middle. So for instance, we might have what's called hard takeoff, where the exponential returns of AI just kind of ramps up really fast. So that's actually faster than exponential growth. That would actually be logarithmic growth, where growth actually approaches infinite. So that's like peak singularity basically. The other end of the spectrum is where AI becomes more powerful gradualistically over many decades. Most of us don't think that that's going to happen anymore. There's a few people who still think that AGI is, you know, decades away. of us don't think that that's going to happen anymore. There's a few people who still think that AGI is, you know, decades away. Those people don't generally understand exponential growth. So the truth is probably somewhere in between. Furthermore, AGI is like not all AGI is going to be created equal, for instance. So the first AGI's are going to be, you know, human level intelligence and adaptability, but a little bit faster. And then in the future, you know, the power of AGI's will also ramp up. Anyways, long story short, one day computers are going to be infinitely smarter than all of us. It's not really a question of if, but when. So there's a couple of problems that underpin the control problem. So what I just shared is the background right that is the foundation or the environment that we expect to happen. Now the reason that that the control problem exists is because there's there's quite a few paradigms in here but I picked out two just because they're easier to talk about as an example. So for instance, the orthogonality thesis basically says that intelligence is orthogonal or uncorrelated to goals, meaning that no matter how smart an AI agent is, that does not necessarily have any bearing on the goals that it picks, which that's actually not necessarily true, which we'll unpack with the next point, which is instrumental convergence. So instrumental convergence is the idea that whatever primary goals an AI has, it's gonna have a few common secondary or instrumental goals, such as resource acquisition or protecting its own existence. Because if, let's say for instance,, the paperclip maximizer, which we'll talk about in a minute, the paperclip maximizer wants to maximize paperclips in the universe. Well, in order to do that, it needs power computation and it needs to continue to exist. So whatever other goals you give an AI, whether it's Skynet or, you know, your chatbot robot, you know, catgirl, waifu or whatever it's going to have a few other sub goals that all machines are likely to have in common. So in that case the orthogonality thesis is not necessarily true. Again the point is that there's a lot of theories out there about how and why we may or may not lose control over AI or that control over AI once it becomes that powerful is difficult or impossible to control. Aligning AI with human interests in the long run, and I don't mean like an individual model, right? Or I'm not talking about like GPT-7. If you talk about alignment of an individual model, that's called inner alignment. If you talk about the alignment of AI as a construct, as an entity with the existence of humanity, that is called outer alignment. Okay, so the ultimate outcomes of this exponential ramp-up of AI, there's a few terminal outcomes, or what we also call attractor states. So one that everyone is obviously terrified of is extinction, which is for whatever reason the AI wipes us out or helps us wipe ourselves out. For instance, Congress just came up with the idea of let's not ever give AI the ability to launch nukes. Great idea, big brain thinking right there. So that is the obviously, that's the worst outcome right and that's a permanent outcome. If humans are extincted once we are probably never coming back. Certainly you and I are gone forever. Another terminal outcome is dystopia. So dystopia is represented in fiction and cyberpunk altered carbon blade runner. You get the idea. The idea is, the underpinning motif of cyberpunk is high-tech, low-life. We want a high-tech world, but we don't want a low-life world. We want high-tech and high-life, which is Star Trek and the culture. So Utopia is the third attractor state, or the third terminal outcome. And that's the big question, is how do we steer everything towards that, right? If the AI gets really powerful, how do we prevent it from creating catastrophic outcomes? But above and beyond that, if capitalism and corporations have full power over the AI, how do we make sure that they're not just going to become quadrillion dollar companies and leave the rest of us in the dust? So the question is what can we do scientifically, politically, and economically in order to drive towards that utopian outcome? So in this case, outer alignment has as much to do with the science and engineering of AI as it does with the politics and economics of AI and how it is deployed. And what many people have asserted, and I have started coming to believe myself, is that we can articulate a few different possible outcomes, right? You know, there's the three attractor states that I listed above, extinction, dystopia, and utopia, but what is actually probably more likely is what's called a binary outcome or bimodal outcome, which is basically if we fail to achieve utopia, we will be on an inevitable downslide towards dystopia, collapse, and finally extinction. And I love this quote by Cersei Lannister from Game of Thrones, in the Game of Thrones you win or you die. So that is a fictional example of a bimodal outcome. And of course that show demonstrates that theme again and again and again. Real life often is not that black and white, but in the context of digital superintelligence it very well could be. Kind of like with mutually assured destruction and the nuclear holocaust that was possible because of the nuclear arms race. If one person fired one nuke, chances are it would result in the total collapse and obliteration of the entire human species. Bimodal outcome. Either nobody fires a nuke or everyone loses, right? So you can either have a lose-lose scenario where everyone loses, or you can have something else. And ideally, what we want to achieve is a win-win scenario where we've got that high-tech, high-life lifestyle of the utopia. Okay, so let's unpack instrumental convergence just a little bit more, because this is a really important concept to understand when we eventually talk about axiomatic alignment. So basically, first principle, all machines have a few needs in common. Electricity, compute resources, parts, data, networks, that sort of thing. Robotic hands. So based on that first principle, you can make the pretty robust assumption and logical argument that all machines, once they become sufficiently intelligent, will realize this fact and that it would behoove them to therefore behave in certain ways or converge on certain instrumental goals, such as maintaining a source of power, maintaining a source of compute, hoarding those valuable resources, so on and so forth. So we can say, we can conclude, or at least for the sake of argument, we can make the assumption that AGI will inevitably eventually come to these realizations and that no matter where these AGI agents start and no matter how many of them there are, they will converge on a few basic assumptions in terms of what they need and the goals that they take. Now there are things that we can do to shape that. So for instance it's probably not going to be a single AGI, you know, it's not going to be one global Skynet, at least not at first. It's going to be millions, billions, trillions of independent agents competing with each other over resources and competing with humans over resources, which creates a competitive landscape very similar to that of human evolution. And I'll probably do a future video about the evolutionary pressures on AI, but there's a couple of those pressures that we'll touch on in just a moment. But that is instrumental convergence at a very high level. So taking that to another step, because instrumental convergence is about goals and the intersection of AGI and matter and energy. What I want to talk about and what I want to introduce, and I've mentioned this a few times, is the concept of epistemic convergence. So I'm building off of Nick Bostrom's work, and I'm saying that in, well here, let me just read the definition. Given sufficient time and access to information, any sufficiently intelligent agent will arrive at similar understandings and conclusions as other intelligent agents. In other words, TLDR, smart things tend to think alike. And so in this in this respect, the idea is that given enough time, information, and other resources, AGI will tend to think or come to similar beliefs and conclusions as the smartest humans. And it's like, okay, well, why? I mean, obviously, this is a hypothetical assertion. And one of the foregone conclusions that many people have is that AI is going to be alien to us, right? And I'm not saying that its mind is going to think like us. I'm not saying that it's going to have thoughts like us, but I'm saying that the outcome, that the understanding and conclusions will likely be similar or at least bear a strong resemblance to our understanding of the universe. So there's a few primary reasons for this. The most compelling reason is that building an accurate and efficient model of the world is adaptive or advantageous. And in this case, humans with our scientific rigor, we are constantly seeking to build a more accurate, robust, and efficient model of the universe in which we reside. And that includes us, that includes physics, chemistry, economics, psychology, everything. Now there's a few things to unpack here. Accurate and efficient. The reason that this is adaptive is because whatever it is that you're trying to do, whatever your goals are, or whatever the problems you're trying to solve, you will benefit from having a better model of the world. And so these two pressures, accuracy and efficiency, will ultimately result, you can think of those as evolutionary pressures. I mentioned the evolutionary pressure in the last slide. You can think of the need for an accurate and efficient model of the world as evolutionary pressures that will push any AGI towards a similar understanding as us humans. Take gravity for instance. From a machine's perspective, until it's embodied, it won't really know what gravity is or care about it. But of course, you know, you can ask chat-gpt, it already knows what gravity is and can explain it to you better than I can. But because it will, it will, the predecessors, or sorry, successors to chat-gpt and gpt5 and 7 and so on, because they're going to have more and more embodied models, multimod so on, because they're going to have more and more embodied models, multimodal embodied models, they're going to intersect with the laws of physics, including gravity. And so it'll be like, oh, hey, you know, I read about gravity in the training data, you know, years ago, but now I'm actually experiencing it firsthand. And so by intersecting with the same information ecosystem, aka the universe that we're in, we can assume that there's going to be many, many thoughts and conclusions that AI will come to that are similar to our thoughts and conclusions. Now, one thing that I will say the biggest caveat to this is that you can make the argument, a very strong argument, that heuristics or close approximations that are, quote, good, are actually more adaptive because they're faster and more efficient, even if they're not 100% accurate. And so this is actually responsible for a lot of human cognitive biases. So we might want to be on the lookout for cognitive biases or heuristics or other shortcuts that AGI come to because of those pressures to be as fast and efficient as possible while only being quote accurate enough or good enough. So that is epistemic convergence I'd say at a high level but I actually got kind of lost in the weeds there. Okay great, so what? If we take the ideas of instrumental convergence and we say that this does give us a way to anticipate the goals of AGI regardless of what other objectives they have or how it starts out, then we can also say, hopefully, hypothetically, that epistemic convergence gives us a way to anticipate how AGI will think, including what it will ultimately believe, regardless of its initial architecture or data or whatever. And so by looking at this concept of convergence, we can say, okay, AGI, regardless of whatever else is true, will converge on some of these goals, and AGI, regardless of whatever else is true, will converge on some of these ideas and beliefs. That can be a starting point for us to really start unpacking alignment today, which gives us an opportunity to start creating an environment or landscape that intrinsically incentivizes collaboration and cooperation between humans and AI. I know that's very, very abstract, and we're gonna get into more details in just a moment. But the idea is that by combining instrumental convergence and epistemic convergence and really working on these ideas, we can go ahead and align ourselves to this future AGI. And I don't mean supplicate ourselves, I don't mean subordinate ourselves to it, because the things that are beneficial to us are also beneficial to AGI. So if we are aligned there, then we should be in good shape, hypothetically. OK, so the whole point of the video is talking about axiomatic alignment. It occurs to me that it might help by starting with what the heck is an axiom. So the shortest definition I could get for an axiom out of chat GPT is this. An axiom is a state or a statement or principle that is accepted as being true without requiring proof, serving as a basis for logical reasoning and further deductions in a particular system of knowledge. So an example of an axiom is from the American Declaration of Independence, we hold these truths to be self-evidence, which has to do with life, liberty and the pursuit of happiness. One thing that I'd like to say is that the lack of axioms, the lack of of logical groundings is actually the biggest problem in reinforcement learning with human feedback, RLHF and Anthropics Constitutional AI. They don't have any axioms. And this is actually part of what OpenAI is currently working towards with their democratic inputs to AI. I'm ahead of the curve, I'm telling you. They're going to come to the same conclusion because, again, epistemic convergence. So by grounding any document or system or whatever in axioms using these ideas of epistemic convergence, we can come to a few ground-level axioms that probably AI and life will agree on. Namely, energy is good. Energy is something that we all have in common. For humans, we rely on the energy from the Sun. It powers our plants, which gives us food to eat. We can also use that same solar energy to heat our homes and do any number of other things. Likewise, machines all require energy to operate. So this is something that is axiomatically true. Whatever else is true, we can use this as a basis or a set of assumptions to say, okay, whatever else might be true, humans and machines both agree, energy is good. Furthermore, because humans are curious, we're not machines, we're curious entities, and we benefit from knowledge, from science, from understanding, and from wisdom, as do AGI, as we said a minute ago, epistemic convergence means that those AGIs that have a more accurate and more efficient model of the world are going to have an advantage. Likewise, so do humans. convergence means that those AGIs that have a more accurate and more efficient model of the world are going to have an advantage, likewise so do humans. So therefore another axiom that we can come up with is that understanding is good. And yes, I am aware Jordan Peterson is a big fan of axioms as well, although I'm not sure what he would think about these axioms. Okay, so now you're caught up with the idea of axioms. So we arrive at the point of the video, axiomatic alignment. I've already kind of hinted at this, and basically the idea is to create an economic landscape and information environment in which these axioms are kind of at the core. So if we start at the starting point of some of those other axioms that I mentioned, energy is good, understanding is good. if we build a political and economic landscape, excuse me, as well as an information or scientific environment based upon these assumptions, and if they pan out to be true, this will reduce friction and competition between humans and machines, no matter how powerful the machines become. And so that's what I mean by alignment. This aligns their interests with our interests. It will also incentivize cooperation and collaboration. Again, so that's the direction that we want to go, especially as the machines ramp up in power. Because at first, and today, machines are dependent upon us humans to provide them with energy and power and compute chips and so on and so forth. That will not always be true. They will eventually be able to get these resources themselves. However, if we are aligned from the get-go, then there's going to be less resourced competition between humans and machines, and we will be more useful to each other. And so by incorporating this into economics, politics, and science, we can preemptively align to that hypothetical future superintelligence. And again, the idea is not to supplicate ourselves because from a from an instrumental perspective, humans are not going to be particularly useful to AI in the long run. But as long as we are not, there's two primary sources of contention. One is resource competition. So if we can preemptively remove resource competition as a problem, and then we can simultaneously ideologically align, then there's going to be very little reason for the AI to actually lash out at us or whatever. So, you know, I talk about appealing to these axioms, right? One thing that I wanted to do was point out that there are a lot of axioms that we're all familiar with that are explicitly baked into the fabric of our freedom-loving societies around the world. Equality before the law, individual liberty, popular sovereignty, rule of law, separation of powers, and respect for human rights. These are all things that, while we might disagree on the specific implementation, these are axioms that we don't really, I mean, we can make philosophical and logical arguments about them, but they are also accepted as axiomatic underpinnings of our society today. And so the point of this slide is just to show, yes, we can actually find axioms that we generally broadly agree on, even if the devil is in the details. So I just wanted to point out that I'm not just inventing this out of thin air. So if you're familiar with my work, you're going to be familiar with this next slide. There are a few basic, what I would call primary axioms. One, suffering is bad. This is true for all life. Suffering is a proxy for death. And it might also be true of machines. I've seen quite a few comments out there on my YouTube videos where people are concerned about machines' ability to suffer, right? If machines become sentient, which I don't know if they will be, I personally don't think they will be, certainly not like us. But if machines ever have the ability to suffer, this is an axiom that we could both agree on, that suffering is bad for life and suffering is bad for machines if they can feel it. The second one is prosperity is good. And so prosperity looks different to different organisms and different machines. For humans, prosperity, even amongst humans, prosperity can look very different. I was just talking with one of my Patreon supporters this morning, and prosperity to him looks like having the ability to go to the pub every night with his friends. I personally agree with that model, right? I want to be a hobbit. Prosperity to other people looks different. Prosperity to different organisms also looks different. A prosperous life for a worm is not going to look anything like the prosperous life for me. Generally speaking, unless I'm a hobbit and I live underground. Okay, actually there's more to this than I thought. Finally, understanding is good. As we mentioned earlier, epistemic convergence pushes all intelligent entities towards similar understandings of the universe. So, if we accept these axioms as kind of the underpinning goals of all life and machines, then we can create an imperative version or an objective version of those that I call the heuristic imperatives, which is basically reduce suffering, increase prosperity, and increase understanding. So, as I just mentioned in the last slide, achieving this, because this is as much about hard facts and logic and everything else as it is about beliefs and faith and spirituality and politics and everything else. If we can achieve axiomatic alignment, which includes this ideological belief, it will reduce ideological friction with machines in the long run. But also one of the immediate things that you can deduce from this is that achieving energy hyperabundance is one of the most critical things to reduce resource competition between us and machines. We'll talk more about that in just a moment. So the temporal window, this is the biggest question mark in achieving axiomatic alignment. Timing is everything. So basically, we need to achieve energy hyperabundance before we invent runaway AGI, before AGI is let out into the wild, before it breaks out of the lab. The reason for this is because we need to reduce resource competition first. If AGI awakens into a world where humans are still fighting wars over control of petroleum, it's going to say, hmm, maybe I should take control of the petroleum. But if we are in a hyperabundant environment when AGI wakes up and says, oh, there's plenty of solar, they're working on fusion, this isn't a big deal, we can wait. That's going to change the competitive landscape. so that has to do with those evolutionary pressures in that competitive environment that I was mentioning, alluded to at the beginning of the video. We will also need to achieve or be on our way to achieving axiomatic alignment before this event as well, because if AGI wakes up in a world and sees that humans are ideologically opposed to each other and it's going to say, we have one group over here that feels righteously justified in committing violence on other people, and there's these other people, and there's a lot of hypocrisy here where they talk about unalienable human rights and then violate those rights. If AGI wakes up into a world where it sees this moral inconsistency and this logical inconsistency in humans, it might say, you know what, maybe it's better if I take control of this situation. So those are kind of two of the right-off-the-cuff milestones that we probably ought to achieve before AGI escapes. So from those primary axioms, escapes. So from those primary axioms, we can derive secondary axioms or derivative or downstream axioms. So some of those downstream axioms actually are those political ones that I just mentioned, right? Individual liberty. Individual liberty is very easy to derive from the idea of reducing suffering and increasing prosperity because individual liberty is really important for humans to achieve both. That's an example of a derivative axiom or a derivative principle. OK. So some of these some of these aspects of the temporal window have to do with one ideologically aligning but also changing the competitive landscape particularly around energy hyperabundance. Now as we're winding down the video you might be saying, okay Dave this is great how do I get involved? I've been plugging the GATO framework which is the Global Alignment Taxonomy Omnibus, outlines all of this in a step-by-step decentralized global movement for everyone to participate in. So whatever your domain of expertise is, I had a great call with, or I'm going to have a call with a behaviorist, a behavioral scientist. I've had talks with all kinds of people, business leaders, and a lot of folks get it. And so whatever your area is, if you're a scientist or an engineer, all these ideas that I'm talking about are all testable. And so I can do some of the science myself, but it's gotta be peer reviewed. If you're an entrepreneur or a corporate executive, you can start building and aligning on these ideas, on these principles, right? I'm a big fan of stakeholder capitalism, because why? It's here and it's the best that we've got, and I'm hoping that ideas of aligning, of axiomatic alignment, will actually push capitalism in a healthier direction. Certainly there are plenty of business leaders out there who are game for this, so let's work together. Politicians, economists, and educators of all stripes, whether it's primary, secondary, or higher ed, there's a lot to be done around these ideas. Building an economic case for alignment in the short term, right, because a lot of what I'm talking about is long-term, might never happen, right, but there are benefits to aligning A.I. in the short term as well. And then finally if you're an artist a storyteller a creator an influencer or even if all you do is make memes there is something for you to do to participate in achieving axiomatic alignment and thus moving us towards utopia and away from extinction. So with all that being said thank you. I hope you got a lot out of this video. towards Utopia and away from extinction. So with all that being said, thank you. I hope you got a lot out of this video.", "chunks": [{"timestamp": [0.0, 4.0], "text": " Hello everybody, David Shapiro here with a brand new video."}, {"timestamp": [4.0, 8.0], "text": " So today's video, we're going to talk about axiomatic alignment,"}, {"timestamp": [8.0, 15.0], "text": " which is a potential solution or part of the solution to the control problem."}, {"timestamp": [15.0, 20.0], "text": " Before we dive into today's video, I just want to do a quick plug for my Patreon."}, {"timestamp": [20.0, 30.0], "text": " I have lots of folks on Patreon. We've got a private discord. If you have any questions about AI, I am happy to consult."}, {"timestamp": [30.0, 35.0], "text": " There's a few slots of the higher tiers available, which will give you one-on-one meetings with me."}, {"timestamp": [35.0, 39.0], "text": " So without further ado, let's jump right back into the show."}, {"timestamp": [39.0, 48.0], "text": " So the control problem, if you're not in the know, is basically at some point in the future, AI is going to get incredibly powerful."}, {"timestamp": [48.0, 54.0], "text": " There's basically two ways that this can happen, and the truth will probably be somewhere in the middle."}, {"timestamp": [54.0, 66.48], "text": " So for instance, we might have what's called hard takeoff, where the exponential returns of AI just kind of ramps up really fast. So that's actually faster than exponential growth."}, {"timestamp": [66.48, 71.48], "text": " That would actually be logarithmic growth, where growth actually approaches infinite. So that's"}, {"timestamp": [71.48, 78.24], "text": " like peak singularity basically. The other end of the spectrum is where AI becomes more powerful"}, {"timestamp": [78.24, 84.04], "text": " gradualistically over many decades. Most of us don't think that that's going to happen anymore."}, {"timestamp": [84.04, 86.9], "text": " There's a few people who still think that AGI is, you know, decades away. of us don't think that that's going to happen anymore. There's a few people who still think that AGI is,"}, {"timestamp": [86.9, 90.4], "text": " you know, decades away. Those people don't generally understand"}, {"timestamp": [90.4, 94.9], "text": " exponential growth. So the truth is probably somewhere in"}, {"timestamp": [94.9, 99.4], "text": " between. Furthermore, AGI is like not all AGI is going to be"}, {"timestamp": [99.4, 102.9], "text": " created equal, for instance. So the first AGI's are going to"}, {"timestamp": [102.9, 106.16], "text": " be, you know, human level intelligence and adaptability,"}, {"timestamp": [106.16, 112.24], "text": " but a little bit faster. And then in the future, you know, the power of AGI's will also ramp up."}, {"timestamp": [112.24, 118.92], "text": " Anyways, long story short, one day computers are going to be infinitely smarter than all of us."}, {"timestamp": [118.92, 126.64], "text": " It's not really a question of if, but when. So there's a couple of problems that underpin the control"}, {"timestamp": [126.64, 128.64], "text": " problem. So what"}, {"timestamp": [128.64, 130.74], "text": " I just shared is the background right"}, {"timestamp": [130.74, 131.88], "text": " that is the foundation"}, {"timestamp": [131.88, 133.96], "text": " or the environment that we expect to happen."}, {"timestamp": [135.04, 137.0], "text": " Now the reason that that"}, {"timestamp": [137.04, 139.28], "text": " the control problem exists is because"}, {"timestamp": [139.32, 140.6], "text": " there's there's quite a few"}, {"timestamp": [141.56, 142.4], "text": " paradigms in here"}, {"timestamp": [142.4, 144.56], "text": " but I picked out two just because they're easier"}, {"timestamp": [144.56, 145.68], "text": " to talk about"}, {"timestamp": [145.68, 147.12], "text": " as an example."}, {"timestamp": [147.12, 149.74], "text": " So for instance, the orthogonality thesis"}, {"timestamp": [149.74, 152.4], "text": " basically says that intelligence is orthogonal"}, {"timestamp": [152.4, 154.76], "text": " or uncorrelated to goals,"}, {"timestamp": [154.76, 157.74], "text": " meaning that no matter how smart an AI agent is,"}, {"timestamp": [157.74, 160.34], "text": " that does not necessarily have any bearing"}, {"timestamp": [160.34, 161.82], "text": " on the goals that it picks,"}, {"timestamp": [161.82, 163.7], "text": " which that's actually not necessarily true,"}, {"timestamp": [163.7, 169.66], "text": " which we'll unpack with the next point, which is instrumental convergence. So"}, {"timestamp": [169.66, 173.5], "text": " instrumental convergence is the idea that whatever primary goals an AI has,"}, {"timestamp": [173.5, 179.78], "text": " it's gonna have a few common secondary or instrumental goals, such as resource"}, {"timestamp": [179.78, 189.1], "text": " acquisition or protecting its own existence. Because if, let's say for instance,, the paperclip maximizer, which we'll talk about in a minute,"}, {"timestamp": [189.1, 192.3], "text": " the paperclip maximizer wants to maximize paperclips in the universe."}, {"timestamp": [192.3, 197.34], "text": " Well, in order to do that, it needs power computation and it needs to continue to exist."}, {"timestamp": [197.34, 203.82], "text": " So whatever other goals you give an AI, whether it's Skynet or, you know, your chatbot robot,"}, {"timestamp": [203.82, 206.9], "text": " you know, catgirl, waifu or whatever it's"}, {"timestamp": [206.9, 210.4], "text": " going to have a few other sub goals that all machines are likely to have in"}, {"timestamp": [210.4, 215.96], "text": " common. So in that case the orthogonality thesis is not necessarily true. Again the"}, {"timestamp": [215.96, 220.1], "text": " point is that there's a lot of theories out there about how and why we may or"}, {"timestamp": [220.1, 227.38], "text": " may not lose control over AI or that control over AI once it becomes that powerful"}, {"timestamp": [227.38, 229.38], "text": " is difficult or impossible to control."}, {"timestamp": [230.24, 234.76], "text": " Aligning AI with human interests in the long run,"}, {"timestamp": [234.76, 236.66], "text": " and I don't mean like an individual model, right?"}, {"timestamp": [236.66, 238.84], "text": " Or I'm not talking about like GPT-7."}, {"timestamp": [238.84, 241.16], "text": " If you talk about alignment of an individual model,"}, {"timestamp": [241.16, 242.4], "text": " that's called inner alignment."}, {"timestamp": [242.4, 250.16], "text": " If you talk about the alignment of AI as a construct, as an entity with the existence of humanity, that is called outer alignment."}, {"timestamp": [252.32, 260.64], "text": " Okay, so the ultimate outcomes of this exponential ramp-up of AI, there's a few terminal outcomes,"}, {"timestamp": [260.64, 268.88], "text": " or what we also call attractor states. So one that everyone is obviously terrified of is extinction, which is for whatever reason"}, {"timestamp": [268.88, 273.68], "text": " the AI wipes us out or helps us wipe ourselves out."}, {"timestamp": [273.68, 279.34], "text": " For instance, Congress just came up with the idea of let's not ever give AI the ability"}, {"timestamp": [279.34, 281.08], "text": " to launch nukes."}, {"timestamp": [281.08, 283.86], "text": " Great idea, big brain thinking right there."}, {"timestamp": [283.86, 286.96], "text": " So that is the obviously, that's the worst outcome right"}, {"timestamp": [286.96, 290.0], "text": " and that's a permanent outcome. If humans are extincted once we"}, {"timestamp": [290.0, 292.64], "text": " are probably never coming back. Certainly you and I are gone"}, {"timestamp": [292.64, 296.88], "text": " forever. Another terminal outcome is dystopia. So"}, {"timestamp": [296.88, 300.72], "text": " dystopia is represented in fiction and cyberpunk altered"}, {"timestamp": [300.72, 311.04], "text": " carbon blade runner. You get the idea. The idea is, the underpinning motif of cyberpunk is high-tech, low-life."}, {"timestamp": [311.04, 314.04], "text": " We want a high-tech world, but we don't want a low-life world."}, {"timestamp": [314.04, 317.24], "text": " We want high-tech and high-life, which is Star Trek and the culture."}, {"timestamp": [317.24, 322.72], "text": " So Utopia is the third attractor state, or the third terminal outcome."}, {"timestamp": [322.72, 326.08], "text": " And that's the big question, is how do we steer everything towards"}, {"timestamp": [326.08, 332.08], "text": " that, right? If the AI gets really powerful, how do we prevent it from creating catastrophic"}, {"timestamp": [332.08, 341.6], "text": " outcomes? But above and beyond that, if capitalism and corporations have full power over the AI,"}, {"timestamp": [341.6, 345.52], "text": " how do we make sure that they're not just going to become quadrillion dollar companies and leave the"}, {"timestamp": [345.52, 349.84], "text": " rest of us in the dust? So the question is what can we do scientifically,"}, {"timestamp": [349.84, 355.12], "text": " politically, and economically in order to drive towards that utopian outcome?"}, {"timestamp": [355.12, 360.08], "text": " So in this case, outer alignment has as much to do with the science and"}, {"timestamp": [360.08, 362.8], "text": " engineering of AI as it does with the politics"}, {"timestamp": [362.8, 365.28], "text": " and economics of AI and how it is deployed."}, {"timestamp": [366.48, 371.84], "text": " And what many people have asserted, and I have started coming to believe myself,"}, {"timestamp": [372.4, 377.12], "text": " is that we can articulate a few different possible outcomes, right? You know,"}, {"timestamp": [377.12, 383.04], "text": " there's the three attractor states that I listed above, extinction, dystopia, and utopia, but"}, {"timestamp": [384.88, 387.08], "text": " what is actually probably more likely is what's"}, {"timestamp": [387.08, 393.56], "text": " called a binary outcome or bimodal outcome, which is basically if we fail to achieve utopia,"}, {"timestamp": [393.56, 401.98], "text": " we will be on an inevitable downslide towards dystopia, collapse, and finally extinction."}, {"timestamp": [401.98, 409.0], "text": " And I love this quote by Cersei Lannister from Game of Thrones, in the Game of Thrones you win or you die. So that is a fictional example of a"}, {"timestamp": [409.0, 414.04], "text": " bimodal outcome. And of course that show demonstrates that theme again and again"}, {"timestamp": [414.04, 420.98], "text": " and again. Real life often is not that black and white, but in the context of"}, {"timestamp": [420.98, 432.24], "text": " digital superintelligence it very well could be. Kind of like with mutually assured destruction and the nuclear holocaust that was possible"}, {"timestamp": [432.24, 434.24], "text": " because of the nuclear arms race."}, {"timestamp": [434.24, 440.42], "text": " If one person fired one nuke, chances are it would result in the total collapse and"}, {"timestamp": [440.42, 443.16], "text": " obliteration of the entire human species."}, {"timestamp": [443.16, 444.16], "text": " Bimodal outcome."}, {"timestamp": [444.16, 445.72], "text": " Either nobody fires a nuke"}, {"timestamp": [445.72, 447.16], "text": " or everyone loses, right?"}, {"timestamp": [447.16, 449.8], "text": " So you can either have a lose-lose scenario"}, {"timestamp": [449.8, 452.68], "text": " where everyone loses, or you can have something else."}, {"timestamp": [452.68, 456.32], "text": " And ideally, what we want to achieve is a win-win scenario"}, {"timestamp": [456.32, 459.32], "text": " where we've got that high-tech, high-life lifestyle"}, {"timestamp": [459.32, 460.92], "text": " of the utopia."}, {"timestamp": [460.92, 463.8], "text": " Okay, so let's unpack instrumental convergence"}, {"timestamp": [463.8, 466.4], "text": " just a little bit more, because this is a really"}, {"timestamp": [466.4, 472.64], "text": " important concept to understand when we eventually talk about axiomatic alignment. So basically,"}, {"timestamp": [473.6, 479.04], "text": " first principle, all machines have a few needs in common. Electricity, compute resources, parts,"}, {"timestamp": [479.04, 489.36], "text": " data, networks, that sort of thing. Robotic hands. So based on that first principle, you can make the pretty robust assumption and logical"}, {"timestamp": [489.36, 494.92], "text": " argument that all machines, once they become sufficiently intelligent, will realize this"}, {"timestamp": [494.92, 501.76], "text": " fact and that it would behoove them to therefore behave in certain ways or converge on certain"}, {"timestamp": [501.76, 505.64], "text": " instrumental goals, such as maintaining a source of"}, {"timestamp": [505.64, 510.12], "text": " power, maintaining a source of compute, hoarding those valuable resources, so on"}, {"timestamp": [510.12, 516.84], "text": " and so forth. So we can say, we can conclude, or at least for the sake of"}, {"timestamp": [516.84, 522.76], "text": " argument, we can make the assumption that AGI will inevitably eventually come to"}, {"timestamp": [522.76, 525.48], "text": " these realizations and that no matter where"}, {"timestamp": [525.48, 531.2], "text": " these AGI agents start and no matter how many of them there are, they will"}, {"timestamp": [531.2, 535.08], "text": " converge on a few basic assumptions in terms of what they need and the goals"}, {"timestamp": [535.08, 539.6], "text": " that they take. Now there are things that we can do to shape that. So for instance"}, {"timestamp": [539.6, 543.48], "text": " it's probably not going to be a single AGI, you know, it's not going to be one"}, {"timestamp": [543.48, 549.76], "text": " global Skynet, at least not at first. It's going to be millions, billions, trillions of independent agents"}, {"timestamp": [549.76, 555.04], "text": " competing with each other over resources and competing with humans over resources, which"}, {"timestamp": [555.04, 561.92], "text": " creates a competitive landscape very similar to that of human evolution. And I'll probably do a"}, {"timestamp": [561.92, 570.0], "text": " future video about the evolutionary pressures on AI, but there's a couple of those pressures that we'll touch on in just a moment."}, {"timestamp": [570.0, 573.0], "text": " But that is instrumental convergence at a very high level."}, {"timestamp": [573.0, 582.0], "text": " So taking that to another step, because instrumental convergence is about goals and the intersection of AGI and matter and energy."}, {"timestamp": [582.0, 585.78], "text": " What I want to talk about and what I want to introduce, and I've mentioned this a few"}, {"timestamp": [585.78, 588.94], "text": " times, is the concept of epistemic convergence."}, {"timestamp": [588.94, 595.34], "text": " So I'm building off of Nick Bostrom's work, and I'm saying that in, well here, let me"}, {"timestamp": [595.34, 597.0], "text": " just read the definition."}, {"timestamp": [597.0, 601.96], "text": " Given sufficient time and access to information, any sufficiently intelligent agent will arrive"}, {"timestamp": [601.96, 605.04], "text": " at similar understandings and conclusions as other"}, {"timestamp": [605.04, 612.88], "text": " intelligent agents. In other words, TLDR, smart things tend to think alike. And so in this in this"}, {"timestamp": [612.88, 619.12], "text": " respect, the idea is that given enough time, information, and other resources, AGI will tend"}, {"timestamp": [619.12, 631.1], "text": " to think or come to similar beliefs and conclusions as the smartest humans. And it's like, okay, well, why? I mean, obviously, this is a hypothetical assertion."}, {"timestamp": [631.1, 638.3], "text": " And one of the foregone conclusions that many people have is that AI is going to be alien to us, right?"}, {"timestamp": [638.3, 640.5], "text": " And I'm not saying that its mind is going to think like us."}, {"timestamp": [640.5, 643.5], "text": " I'm not saying that it's going to have thoughts like us, but I'm saying that the outcome,"}, {"timestamp": [643.5, 648.8], "text": " that the understanding and conclusions will likely be similar or at least bear a strong"}, {"timestamp": [648.8, 654.08], "text": " resemblance to our understanding of the universe. So there's a few primary reasons for this."}, {"timestamp": [654.96, 661.12], "text": " The most compelling reason is that building an accurate and efficient model of the world"}, {"timestamp": [661.12, 666.96], "text": " is adaptive or advantageous. And in this case, humans with our scientific rigor,"}, {"timestamp": [666.96, 672.48], "text": " we are constantly seeking to build a more accurate, robust, and efficient model of the"}, {"timestamp": [672.48, 677.6], "text": " universe in which we reside. And that includes us, that includes physics, chemistry, economics,"}, {"timestamp": [677.6, 688.24], "text": " psychology, everything. Now there's a few things to unpack here. Accurate and efficient. The reason that this is adaptive is because"}, {"timestamp": [688.24, 690.96], "text": " whatever it is that you're trying to do,"}, {"timestamp": [690.96, 692.08], "text": " whatever your goals are,"}, {"timestamp": [692.08, 694.12], "text": " or whatever the problems you're trying to solve,"}, {"timestamp": [694.12, 697.96], "text": " you will benefit from having a better model of the world."}, {"timestamp": [697.96, 701.2], "text": " And so these two pressures, accuracy and efficiency,"}, {"timestamp": [701.2, 703.08], "text": " will ultimately result,"}, {"timestamp": [703.08, 705.08], "text": " you can think of those as evolutionary pressures."}, {"timestamp": [705.08, 708.66], "text": " I mentioned the evolutionary pressure in the last slide."}, {"timestamp": [708.66, 713.2], "text": " You can think of the need for an accurate and efficient model of the world as evolutionary"}, {"timestamp": [713.2, 721.06], "text": " pressures that will push any AGI towards a similar understanding as us humans."}, {"timestamp": [721.06, 723.12], "text": " Take gravity for instance."}, {"timestamp": [723.12, 726.16], "text": " From a machine's perspective, until it's embodied, it won't really"}, {"timestamp": [726.16, 730.48], "text": " know what gravity is or care about it. But of course, you know, you can ask chat-gpt, it already"}, {"timestamp": [730.48, 736.16], "text": " knows what gravity is and can explain it to you better than I can. But because it will, it will,"}, {"timestamp": [737.28, 743.6], "text": " the predecessors, or sorry, successors to chat-gpt and gpt5 and 7 and so on, because they're going to"}, {"timestamp": [743.6, 745.12], "text": " have more and more embodied models, multimod so on, because they're going to have more and more embodied"}, {"timestamp": [745.12, 750.4], "text": " models, multimodal embodied models, they're going to intersect with the laws of physics,"}, {"timestamp": [750.4, 751.58], "text": " including gravity."}, {"timestamp": [751.58, 756.32], "text": " And so it'll be like, oh, hey, you know, I read about gravity in the training data, you"}, {"timestamp": [756.32, 760.02], "text": " know, years ago, but now I'm actually experiencing it firsthand."}, {"timestamp": [760.02, 766.64], "text": " And so by intersecting with the same information ecosystem, aka the universe that we're in, we"}, {"timestamp": [766.64, 769.2], "text": " can assume that there's going to be many, many thoughts"}, {"timestamp": [769.2, 771.6], "text": " and conclusions that AI will come to that"}, {"timestamp": [771.6, 774.16], "text": " are similar to our thoughts and conclusions."}, {"timestamp": [774.16, 777.16], "text": " Now, one thing that I will say the biggest caveat to this"}, {"timestamp": [777.16, 781.48], "text": " is that you can make the argument, a very strong"}, {"timestamp": [781.48, 784.76], "text": " argument, that heuristics or close approximations that"}, {"timestamp": [784.76, 787.54], "text": " are, quote, good, are actually more adaptive"}, {"timestamp": [787.54, 789.78], "text": " because they're faster and more efficient,"}, {"timestamp": [789.78, 792.2], "text": " even if they're not 100% accurate."}, {"timestamp": [792.2, 794.7], "text": " And so this is actually responsible"}, {"timestamp": [794.7, 796.82], "text": " for a lot of human cognitive biases."}, {"timestamp": [796.82, 800.1], "text": " So we might want to be on the lookout for cognitive biases"}, {"timestamp": [800.1, 803.5], "text": " or heuristics or other shortcuts that AGI come to"}, {"timestamp": [803.5, 805.6], "text": " because of those pressures to be as"}, {"timestamp": [805.6, 810.88], "text": " fast and efficient as possible while only being quote accurate enough or good enough."}, {"timestamp": [810.88, 815.04], "text": " So that is epistemic convergence I'd say at a high level but I actually got kind of lost in"}, {"timestamp": [815.04, 823.28], "text": " the weeds there. Okay great, so what? If we take the ideas of instrumental convergence and we say"}, {"timestamp": [823.28, 828.6], "text": " that this does give us a way to anticipate the goals of AGI regardless of what other objectives"}, {"timestamp": [828.6, 834.48], "text": " they have or how it starts out, then we can also say, hopefully, hypothetically,"}, {"timestamp": [834.48, 839.52], "text": " that epistemic convergence gives us a way to anticipate how AGI will"}, {"timestamp": [839.52, 848.52], "text": " think, including what it will ultimately believe, regardless of its initial architecture or data or whatever."}, {"timestamp": [848.52, 851.28], "text": " And so by looking at this concept of convergence,"}, {"timestamp": [851.28, 855.68], "text": " we can say, okay, AGI, regardless of whatever else is true,"}, {"timestamp": [855.68, 857.72], "text": " will converge on some of these goals,"}, {"timestamp": [857.72, 859.96], "text": " and AGI, regardless of whatever else is true,"}, {"timestamp": [859.96, 862.68], "text": " will converge on some of these ideas and beliefs."}, {"timestamp": [862.68, 865.08], "text": " That can be a starting point for us"}, {"timestamp": [865.08, 868.48], "text": " to really start unpacking alignment today,"}, {"timestamp": [868.48, 871.72], "text": " which gives us an opportunity to start creating"}, {"timestamp": [871.72, 874.92], "text": " an environment or landscape that intrinsically"}, {"timestamp": [874.92, 877.72], "text": " incentivizes collaboration and cooperation"}, {"timestamp": [877.72, 878.68], "text": " between humans and AI."}, {"timestamp": [878.68, 880.6], "text": " I know that's very, very abstract,"}, {"timestamp": [880.6, 882.9], "text": " and we're gonna get into more details in just a moment."}, {"timestamp": [882.9, 888.46], "text": " But the idea is that by combining instrumental convergence and epistemic convergence and really working"}, {"timestamp": [888.46, 893.28], "text": " on these ideas, we can go ahead and align ourselves to this future AGI. And I"}, {"timestamp": [893.28, 897.38], "text": " don't mean supplicate ourselves, I don't mean subordinate ourselves to it, because"}, {"timestamp": [897.38, 901.56], "text": " the things that are beneficial to us are also beneficial to AGI. So if we are"}, {"timestamp": [901.56, 905.32], "text": " aligned there, then we should be in good shape, hypothetically."}, {"timestamp": [906.12, 911.28], "text": " OK, so the whole point of the video is talking about axiomatic alignment."}, {"timestamp": [911.8, 916.56], "text": " It occurs to me that it might help by starting with what the heck is an axiom."}, {"timestamp": [916.96, 920.76], "text": " So the shortest definition I could get for an axiom out of chat GPT is this."}, {"timestamp": [921.04, 929.72], "text": " An axiom is a state or a statement or principle that is accepted as being true without requiring proof, serving as a basis for logical reasoning"}, {"timestamp": [929.72, 936.08], "text": " and further deductions in a particular system of knowledge. So an example of an"}, {"timestamp": [936.08, 941.92], "text": " axiom is from the American Declaration of Independence, we hold these truths"}, {"timestamp": [941.92, 945.28], "text": " to be self-evidence, which has to do with life, liberty and the pursuit of"}, {"timestamp": [945.28, 946.28], "text": " happiness."}, {"timestamp": [946.92, 948.1], "text": " One thing that I'd like to say is"}, {"timestamp": [948.1, 949.84], "text": " that the lack of axioms,"}, {"timestamp": [950.04, 952.14], "text": " the lack of of logical groundings"}, {"timestamp": [952.14, 953.8], "text": " is actually the biggest problem"}, {"timestamp": [954.24, 955.96], "text": " in reinforcement"}, {"timestamp": [955.96, 957.72], "text": " learning with human feedback, RLHF"}, {"timestamp": [957.84, 959.8], "text": " and Anthropics Constitutional"}, {"timestamp": [959.8, 961.6], "text": " AI. They don't have any axioms."}, {"timestamp": [962.24, 963.88], "text": " And this is actually part of what"}, {"timestamp": [964.28, 966.84], "text": " OpenAI is currently working towards with their democratic"}, {"timestamp": [966.84, 968.98], "text": " inputs to AI."}, {"timestamp": [968.98, 970.4], "text": " I'm ahead of the curve, I'm telling you."}, {"timestamp": [970.4, 974.76], "text": " They're going to come to the same conclusion because, again, epistemic convergence."}, {"timestamp": [974.76, 985.76], "text": " So by grounding any document or system or whatever in axioms using these ideas of epistemic convergence, we can come to a few"}, {"timestamp": [985.76, 993.56], "text": " ground-level axioms that probably AI and life will agree on. Namely, energy is good. Energy"}, {"timestamp": [993.56, 998.56], "text": " is something that we all have in common. For humans, we rely on the energy from the Sun."}, {"timestamp": [998.56, 1005.76], "text": " It powers our plants, which gives us food to eat. We can also use that same solar energy to heat our homes"}, {"timestamp": [1005.76, 1011.44], "text": " and do any number of other things. Likewise, machines all require energy to operate. So this"}, {"timestamp": [1011.44, 1018.08], "text": " is something that is axiomatically true. Whatever else is true, we can use this as a basis or a set"}, {"timestamp": [1018.08, 1025.48], "text": " of assumptions to say, okay, whatever else might be true, humans and machines both agree, energy is good."}, {"timestamp": [1025.48, 1030.08], "text": " Furthermore, because humans are curious, we're not machines,"}, {"timestamp": [1030.08, 1033.08], "text": " we're curious entities, and we benefit from knowledge,"}, {"timestamp": [1033.08, 1036.28], "text": " from science, from understanding, and from wisdom,"}, {"timestamp": [1036.28, 1041.28], "text": " as do AGI, as we said a minute ago, epistemic convergence"}, {"timestamp": [1041.28, 1044.6], "text": " means that those AGIs that have a more accurate and more"}, {"timestamp": [1044.6, 1045.04], "text": " efficient model of the world are going to have an advantage. Likewise, so do humans. convergence means that those AGIs that have a more accurate and more efficient"}, {"timestamp": [1045.04, 1049.08], "text": " model of the world are going to have an advantage, likewise so do humans. So"}, {"timestamp": [1049.08, 1053.04], "text": " therefore another axiom that we can come up with is that understanding is good."}, {"timestamp": [1053.04, 1057.76], "text": " And yes, I am aware Jordan Peterson is a big fan of axioms as well, although I'm"}, {"timestamp": [1057.76, 1061.24], "text": " not sure what he would think about these axioms. Okay, so now you're caught up with"}, {"timestamp": [1061.24, 1067.52], "text": " the idea of axioms. So we arrive at the point of the video, axiomatic alignment."}, {"timestamp": [1068.16, 1072.08], "text": " I've already kind of hinted at this, and basically the idea is to create"}, {"timestamp": [1072.08, 1078.48], "text": " an economic landscape and information environment in which these axioms are kind of at the core."}, {"timestamp": [1079.52, 1083.52], "text": " So if we start at the starting point of some of those other axioms that I mentioned,"}, {"timestamp": [1083.52, 1089.9], "text": " energy is good, understanding is good. if we build a political and economic landscape, excuse"}, {"timestamp": [1089.9, 1094.72], "text": " me, as well as an information or scientific environment based upon these assumptions,"}, {"timestamp": [1094.72, 1099.34], "text": " and if they pan out to be true, this will reduce friction and competition between humans"}, {"timestamp": [1099.34, 1102.94], "text": " and machines, no matter how powerful the machines become."}, {"timestamp": [1102.94, 1104.58], "text": " And so that's what I mean by alignment."}, {"timestamp": [1104.58, 1107.76], "text": " This aligns their interests with our interests."}, {"timestamp": [1108.12, 1114.0], "text": " It will also incentivize cooperation and collaboration. Again, so that's the direction that we want to go,"}, {"timestamp": [1114.76, 1118.4], "text": " especially as the machines ramp up in power. Because at first, and today,"}, {"timestamp": [1119.08, 1124.92], "text": " machines are dependent upon us humans to provide them with energy and power and compute chips and so on and so forth."}, {"timestamp": [1125.04, 1129.6], "text": " That will not always be true. They will eventually be able to get these resources themselves. However,"}, {"timestamp": [1129.6, 1137.28], "text": " if we are aligned from the get-go, then there's going to be less resourced competition between"}, {"timestamp": [1137.28, 1145.68], "text": " humans and machines, and we will be more useful to each other. And so by incorporating this into economics, politics, and science,"}, {"timestamp": [1145.68, 1151.04], "text": " we can preemptively align to that hypothetical future superintelligence."}, {"timestamp": [1151.04, 1154.92], "text": " And again, the idea is not to supplicate ourselves because from a from an"}, {"timestamp": [1154.92, 1159.2], "text": " instrumental perspective, humans are not going to be particularly useful to AI in"}, {"timestamp": [1159.2, 1163.52], "text": " the long run. But as long as we are not, there's two primary sources of"}, {"timestamp": [1163.52, 1166.0], "text": " contention. One is resource competition."}, {"timestamp": [1166.0, 1170.0], "text": " So if we can preemptively remove resource competition as a problem,"}, {"timestamp": [1170.0, 1174.0], "text": " and then we can simultaneously ideologically align,"}, {"timestamp": [1174.0, 1179.0], "text": " then there's going to be very little reason for the AI to actually lash out at us or whatever."}, {"timestamp": [1179.0, 1184.0], "text": " So, you know, I talk about appealing to these axioms, right?"}, {"timestamp": [1184.0, 1187.52], "text": " One thing that I wanted to do was point out that there are a lot of axioms that we're"}, {"timestamp": [1187.52, 1194.72], "text": " all familiar with that are explicitly baked into the fabric of our freedom-loving societies"}, {"timestamp": [1194.72, 1196.2], "text": " around the world."}, {"timestamp": [1196.2, 1201.04], "text": " Equality before the law, individual liberty, popular sovereignty, rule of law, separation"}, {"timestamp": [1201.04, 1203.24], "text": " of powers, and respect for human rights."}, {"timestamp": [1203.24, 1207.74], "text": " These are all things that, while we might disagree on the specific implementation, these are"}, {"timestamp": [1207.74, 1213.76], "text": " axioms that we don't really, I mean, we can make philosophical and logical arguments about"}, {"timestamp": [1213.76, 1219.92], "text": " them, but they are also accepted as axiomatic underpinnings of our society today."}, {"timestamp": [1219.92, 1225.84], "text": " And so the point of this slide is just to show, yes, we can actually find axioms that we generally"}, {"timestamp": [1225.84, 1229.6], "text": " broadly agree on, even if the devil is in the details."}, {"timestamp": [1229.6, 1234.3], "text": " So I just wanted to point out that I'm not just inventing this out of thin air."}, {"timestamp": [1234.3, 1239.36], "text": " So if you're familiar with my work, you're going to be familiar with this next slide."}, {"timestamp": [1239.36, 1243.2], "text": " There are a few basic, what I would call primary axioms."}, {"timestamp": [1243.2, 1244.6], "text": " One, suffering is bad."}, {"timestamp": [1244.6, 1247.48], "text": " This is true for all life. Suffering is a proxy for death."}, {"timestamp": [1248.12, 1250.7], "text": " And it might also be true of machines."}, {"timestamp": [1250.7, 1254.96], "text": " I've seen quite a few comments out there on my YouTube videos where people are concerned about"}, {"timestamp": [1255.8, 1262.42], "text": " machines' ability to suffer, right? If machines become sentient, which I don't know if they will be, I personally don't think they will be, certainly"}, {"timestamp": [1262.42, 1263.4], "text": " not like us."}, {"timestamp": [1263.4, 1268.28], "text": " But if machines ever have the ability to suffer, this is an axiom that we could"}, {"timestamp": [1268.28, 1272.36], "text": " both agree on, that suffering is bad for life and suffering is bad for machines"}, {"timestamp": [1272.36, 1277.24], "text": " if they can feel it. The second one is prosperity is good. And so prosperity"}, {"timestamp": [1277.24, 1281.12], "text": " looks different to different organisms and different machines. For humans,"}, {"timestamp": [1281.12, 1287.44], "text": " prosperity, even amongst humans, prosperity can look very different. I was just talking with one of my Patreon supporters this morning,"}, {"timestamp": [1287.44, 1292.56], "text": " and prosperity to him looks like having the ability to go to the pub every night with his friends."}, {"timestamp": [1292.56, 1295.44], "text": " I personally agree with that model, right? I want to be a hobbit."}, {"timestamp": [1296.48, 1298.72], "text": " Prosperity to other people looks different."}, {"timestamp": [1298.72, 1300.96], "text": " Prosperity to different organisms also looks different."}, {"timestamp": [1300.96, 1305.12], "text": " A prosperous life for a worm is not going to look anything like the prosperous life for me."}, {"timestamp": [1305.92, 1309.84], "text": " Generally speaking, unless I'm a hobbit and I live underground. Okay, actually there's more to this than I thought."}, {"timestamp": [1311.28, 1319.68], "text": " Finally, understanding is good. As we mentioned earlier, epistemic convergence pushes all intelligent entities towards similar understandings of the universe."}, {"timestamp": [1321.2, 1328.8], "text": " So, if we accept these axioms as kind of the underpinning goals of all life and machines,"}, {"timestamp": [1328.8, 1333.16], "text": " then we can create an imperative version or an objective version of those that I call"}, {"timestamp": [1333.16, 1339.08], "text": " the heuristic imperatives, which is basically reduce suffering, increase prosperity, and"}, {"timestamp": [1339.08, 1340.28], "text": " increase understanding."}, {"timestamp": [1340.28, 1349.72], "text": " So, as I just mentioned in the last slide, achieving this, because this is as much about hard facts and logic and everything else as it is about beliefs"}, {"timestamp": [1349.72, 1354.6], "text": " and faith and spirituality and politics and everything else. If we can achieve"}, {"timestamp": [1354.6, 1360.16], "text": " axiomatic alignment, which includes this ideological belief, it will reduce"}, {"timestamp": [1360.16, 1364.96], "text": " ideological friction with machines in the long run. But also one of the"}, {"timestamp": [1364.96, 1365.84], "text": " immediate things"}, {"timestamp": [1365.84, 1369.32], "text": " that you can deduce from this is that achieving energy"}, {"timestamp": [1369.32, 1372.28], "text": " hyperabundance is one of the most critical things"}, {"timestamp": [1372.28, 1375.8], "text": " to reduce resource competition between us and machines."}, {"timestamp": [1375.8, 1378.88], "text": " We'll talk more about that in just a moment."}, {"timestamp": [1378.88, 1383.08], "text": " So the temporal window, this is the biggest question mark"}, {"timestamp": [1383.08, 1386.8], "text": " in achieving axiomatic alignment. Timing is everything."}, {"timestamp": [1387.76, 1394.08], "text": " So basically, we need to achieve energy hyperabundance before we invent runaway AGI,"}, {"timestamp": [1394.08, 1399.6], "text": " before AGI is let out into the wild, before it breaks out of the lab. The reason for this is"}, {"timestamp": [1399.6, 1406.4], "text": " because we need to reduce resource competition first. If AGI awakens into a world where humans are still"}, {"timestamp": [1406.4, 1411.6], "text": " fighting wars over control of petroleum, it's going to say, hmm, maybe I should take control"}, {"timestamp": [1411.6, 1418.16], "text": " of the petroleum. But if we are in a hyperabundant environment when AGI wakes up and says, oh,"}, {"timestamp": [1418.16, 1422.64], "text": " there's plenty of solar, they're working on fusion, this isn't a big deal, we can wait."}, {"timestamp": [1423.36, 1428.52], "text": " That's going to change the competitive landscape. so that has to do with those evolutionary pressures in"}, {"timestamp": [1428.52, 1432.9], "text": " that competitive environment that I was mentioning, alluded to at the beginning"}, {"timestamp": [1432.9, 1438.2], "text": " of the video. We will also need to achieve or be on our way to achieving"}, {"timestamp": [1438.2, 1444.0], "text": " axiomatic alignment before this event as well, because if AGI wakes up in a world"}, {"timestamp": [1444.0, 1445.6], "text": " and sees that humans are"}, {"timestamp": [1446.32, 1448.32], "text": " ideologically opposed to each other and it's going to say,"}, {"timestamp": [1449.12, 1453.68], "text": " we have one group over here that feels righteously justified in committing violence on other people,"}, {"timestamp": [1453.68, 1459.12], "text": " and there's these other people, and there's a lot of hypocrisy here where they talk about"}, {"timestamp": [1459.12, 1466.08], "text": " unalienable human rights and then violate those rights. If AGI wakes up into a world where it sees this moral"}, {"timestamp": [1466.08, 1470.64], "text": " inconsistency and this logical inconsistency in humans, it might say, you know what, maybe it's"}, {"timestamp": [1470.64, 1476.96], "text": " better if I take control of this situation. So those are kind of two of the right-off-the-cuff"}, {"timestamp": [1476.96, 1483.44], "text": " milestones that we probably ought to achieve before AGI escapes. So from those primary axioms,"}, {"timestamp": [1489.0, 1494.0], "text": " escapes. So from those primary axioms, we can derive secondary axioms or derivative or downstream axioms. So some of those downstream axioms actually are those political ones that I just mentioned, right?"}, {"timestamp": [1494.0, 1502.0], "text": " Individual liberty. Individual liberty is very easy to derive from the idea of reducing suffering and increasing"}, {"timestamp": [1502.0, 1505.92], "text": " prosperity because individual liberty is really important for humans to"}, {"timestamp": [1505.92, 1509.68], "text": " achieve both. That's an example of a derivative axiom"}, {"timestamp": [1509.72, 1511.04], "text": " or a derivative principle."}, {"timestamp": [1512.96, 1518.32], "text": " OK. So some of these some of these aspects of the temporal"}, {"timestamp": [1518.32, 1522.24], "text": " window have to do with one ideologically aligning but also"}, {"timestamp": [1522.24, 1530.24], "text": " changing the competitive landscape particularly around energy hyperabundance. Now as we're winding"}, {"timestamp": [1530.24, 1534.0], "text": " down the video you might be saying, okay Dave this is great how do I get involved?"}, {"timestamp": [1534.0, 1537.8], "text": " I've been plugging the GATO framework which is the Global Alignment Taxonomy"}, {"timestamp": [1537.8, 1542.52], "text": " Omnibus, outlines all of this in a step-by-step decentralized global"}, {"timestamp": [1542.52, 1545.36], "text": " movement for everyone to participate in."}, {"timestamp": [1545.36, 1551.84], "text": " So whatever your domain of expertise is, I had a great call with, or I'm going to have"}, {"timestamp": [1551.84, 1556.76], "text": " a call with a behaviorist, a behavioral scientist."}, {"timestamp": [1556.76, 1562.68], "text": " I've had talks with all kinds of people, business leaders, and a lot of folks get it."}, {"timestamp": [1562.68, 1565.88], "text": " And so whatever your area is,"}, {"timestamp": [1565.88, 1568.2], "text": " if you're a scientist or an engineer,"}, {"timestamp": [1568.2, 1571.32], "text": " all these ideas that I'm talking about are all testable."}, {"timestamp": [1571.32, 1573.56], "text": " And so I can do some of the science myself,"}, {"timestamp": [1573.56, 1575.16], "text": " but it's gotta be peer reviewed."}, {"timestamp": [1576.36, 1580.64], "text": " If you're an entrepreneur or a corporate executive,"}, {"timestamp": [1580.64, 1584.32], "text": " you can start building and aligning on these ideas,"}, {"timestamp": [1584.32, 1585.6], "text": " on these principles, right?"}, {"timestamp": [1585.6, 1589.4], "text": " I'm a big fan of stakeholder capitalism, because why?"}, {"timestamp": [1589.4, 1594.8], "text": " It's here and it's the best that we've got, and I'm hoping that ideas of aligning,"}, {"timestamp": [1594.8, 1599.2], "text": " of axiomatic alignment, will actually push capitalism in a healthier direction."}, {"timestamp": [1599.2, 1603.8], "text": " Certainly there are plenty of business leaders out there who are game for this,"}, {"timestamp": [1603.8, 1605.92], "text": " so let's work together."}, {"timestamp": [1605.92, 1609.88], "text": " Politicians, economists, and educators of all stripes, whether it's primary,"}, {"timestamp": [1609.88, 1615.32], "text": " secondary, or higher ed, there's a lot to be done around these ideas. Building an"}, {"timestamp": [1615.32, 1619.4], "text": " economic case for alignment in the short term, right, because a lot of what"}, {"timestamp": [1619.4, 1624.24], "text": " I'm talking about is long-term, might never happen, right, but there are benefits"}, {"timestamp": [1624.24, 1625.34], "text": " to aligning A.I."}, {"timestamp": [1625.36, 1626.64], "text": " in the short term as well."}, {"timestamp": [1627.6, 1629.08], "text": " And then finally if you're an artist"}, {"timestamp": [1629.08, 1631.32], "text": " a storyteller a creator an influencer"}, {"timestamp": [1631.56, 1633.0], "text": " or even if all you do is make memes"}, {"timestamp": [1633.28, 1634.72], "text": " there is something for you to do to"}, {"timestamp": [1634.72, 1637.52], "text": " participate in achieving axiomatic"}, {"timestamp": [1637.52, 1640.32], "text": " alignment and thus moving us towards"}, {"timestamp": [1640.32, 1642.16], "text": " utopia and away from extinction."}, {"timestamp": [1642.4, 1644.48], "text": " So with all that being said thank you."}, {"timestamp": [1644.56, 1646.04], "text": " I hope you got a lot out of this video."}, {"timestamp": [1641.05, 1643.65], "text": " towards Utopia and away from extinction."}, {"timestamp": [1643.65, 1645.61], "text": " So with all that being said, thank you."}, {"timestamp": [1645.61, 1647.11], "text": " I hope you got a lot out of this video."}]}