{"text": " Hey everyone, David Shapiro here. Before we get started, I just wanted to say that I have updated my Patreon page with new tiers and clarified what each tier gets you. A lot of folks asked for some time and help, and so what I did was I created the two higher tiers will entitle you to a little bit of a dedicated one-on-one time with me each month so go check out my patreon page and consider supporting me on patreon and if you want any one-on-one time I'm happy to talk to with you and your team and you know just provide a little bit of insight and guidance so anyways thanks for watching and I hope you enjoy this video about automated literature review for scientific papers. Thanks. Good morning everybody, David Shapiro here with a video. I have created an automated literature review engine. It is broken up into two parts. There is the search engine which searches all of archive with semantic embedding. And then the second part is a GPT-3 script that will take whatever papers you have selected and create literature reviews for them. Let me show you how it's done. Hey everybody, David Shapiro here with a video. Okay, so ignore this, I haven't updated it yet. This repo has gone through a few iterations. I was talking to my fiancee who's a librarian and I was like, okay, I've got this idea. I posted on Twitter. All kinds of people gave me all kinds of ideas like research assistant, knowledge graph, interactive data, fine-tuned generation, teaching aid. Someone was saying like, you know, it'd be great if you could help write grant proposals. There's all kinds of scientific papers, there's academic texts, and I'm like, okay, what does all this have in common? And she asked me, she said, what do you want to achieve with this? What's the output? And I was like, well, I want it to be something useful, right? One, it's got to be something useful, but it's also got to like be attention grabbing. Um, you know, it's got to make a good YouTube video. And, um, she's like, well, it sounds like what you're trying to do is a, is a literature review, like automatic literature review. And I was like, that's it. So that's what we're doing. Okay. So, uh, obviously there's a lot of data here. I'm not going to get to all of it, but let's start with something simple. So there's a dataset called the archive dataset up on Kaggle and it is a metadata of everything on archive. So 1.7 million articles. Here's an example of an archive paper. So this was published 2020, battery drainingining Attacks Against Edge Computing Nodes and IoT Networks. Okay, super techie, right? And obviously not everything in archive is like this, but we've got all the metadata. So like the metadata is basically what you see on this page. Title, date, authors, that kind of stuff. So let me show you what it looks like. So I went ahead and downloaded it and you get 2.1 million lines of JSON-L. And here's what one entry looks like. So you get the ID. So the ID is what you can use to go to archive and just plug it in right here. So like you just grab any archive or any, like use this as the base URL, and then you can also get the PDF, which is just archive.org slash PDF slash the ID dot PDF. Download it directly, you can get the full text. We're not gonna go quite that far yet. So we'll see how far we get with this. But what we're gonna do is we're going to index with a semantic vector, we're gonna index all of the abstracts and titles together. So I was gonna borrow code from my quadrant stress test, which also means I need to go ahead and fire up Docker with the quadrantrant container. So, oh, it's starting. Okay, so anyways, as a quick review, in case you don't know what Quadrant is, it is a vector-based search engine, a semantic search engine. It runs in a container, it's very small. So we'll go ahead and fire this up. It is connected to this data store. Actually, hang on, I might need to start it a different method. Where is the, here we go. Let's see. Docker run, here we go. So I need to start it with this. So we'll do that. Ta-da. Okay, because this command actually tells it what storage to map. And it says it's listening, so that's good. Okay, I think we should be good there. All right, so we've got this data, which I've got. So under archive, so here's that file I was showing you. So it's 3.4 gigabytes of metadata. That's a lot of science. It's all in one file. That may or may not be problematic. We will see. So let's go ahead and just get started. I've got, so that's the quadrant demo, and then I've got my literature review bot. We got nothing going on in here. So we're going to start a new file, and let's just play around with this data. Let's see what we can get out of it. So let's go ahead and just copy my quadrant stress test, and we'll save this under literature review bot. Excuse me. I just ate lunch. I'm a little gonna wash it down. All right. So we will do, um, uh, what are we trying to do? We're trying to load. Um, we're trying to load it. So let's do index archive metadata.py. Okay, so we got JSON NumPy stress test. Right. So, oh, that's what I did. So what I might actually do is break it up because in my stress test experiment, I broke it up into two steps where I prepared the data and then I uploaded it. So that's probably, let's go that way. Let me open up that file real quick. Quadrant demo. Okay, prepare data. There we go. So in this one, we did, yeah, we'll use TensorFlow Hub, and I've actually got an Anaconda environment. Was it in base? Anaconda navigator. So for, I know I'm going so fast. Actually, you'll like it, because y'all, y'all all watch my video on 1.5 X or 2X anyways. So this is probably perfect speed. Okay. So quick recap, if you're not familiar with Anaconda, it is a Python virtual environment that allows you to have, yeah, here we go. So I've got TensorFlow GPU environment ready to go. And so what this does is it allows you to use GPU acceleration to run your TensorFlow instance. In this case, I'm going to be using Universal Sentence Encoder version five in order to do this. So, all right, so we are going to do, this is gonna be generate embeddings.py. Okay, so we'll come, so the file is gonna be here. So let's just open a new Explorer. That's not it. Auto Muse Blogger, that's not it either. We need Literature Review Bot. All right, so we will have a folder and we'll call this Embeddings, I guess. And then let me take a quick look at how I did the quadrant demo because how did I format the data? Yeah, it's just in a bunch of JSON files. Okay, yep, Embedding and String. Perfect. Okay, that's probably in a bunch of JSON files. Okay, yep. Embedding and string. Perfect. Okay, that's probably how I was going to do it anyways. So what I'm going to do is I'm going to take each entry of this guy, because each entry is one JSON, it's JSON-L, right? So basically what I'm going to do is I'm going to take the abstract and the title, I'm going to smush them together so that they're going gonna be one long string, and generate a 512 dimension embedding like this guy. So that's what I'll do, and we'll see how fast it goes. All right, so let's do some coding. We've got prepared data here. This one is in the quadrant demo, and I've got a copy of it here. So let's see, what do we need to do different? We've got universal sentence encoder five. Here, let me close this one, because that's superfluous. We're not doing that one yet. We've got Docker running here. I'll just copy this over into the new readme so that I can keep track of that. Literature review bot, readme. And we're just a little reminder. Okay, so that's a sample, generate embeddings, curl command, close that, stress test, close that. We'll keep this open just for reference. Okay, yes, generate embeddings, okay. We can delete this comment. Files equals os-lister. Actually, no. So we need to go ahead and change that because we're gonna be opening this and we're gonna do it as split lines. That's gonna be, ooh, that's gonna be messy. Okay, so archive equals open file. So, archive equals open file, that was C, archive, and it's called, well here, I'll just copy, copy this guy. All right, so first, let's make sure that, that we can just open the file as is and do split lines. Alright, so that means archive will be a list of JSON strings for everything that we want. So then we'll do print length of archive and then we'll do exit. So let's just see if this works. Let's see if it works, if it blows up, because I mean, I've got 32 gigs of RAM on my, on this PC, so I should be okay. And then let's see, literature review bot. All right. So we'll just do Python generate embeddings. Oh, right. I don't. Yeah. Okay. So what it's doing right now, oh, here, let me make this bigger. Yeah. I'm not surprised that that blew up. So let's do properties so that you can see what I'm doing. All right, so what it does is saved model does not exist. freaking happens where, um, for whatever reason, it can't find the model. Um, but anyways, what I need to do is come in here and start it with, um, GPU. Come on, wake up. What's the matter with you? There we go. All right. So we'll start this conda environment. All right. so it's cd, cd, literature, viewbot, Python, generate embeddings. So now what it's going to do, it's going to have to download Universal Sentence Encoder version 5. And so, oh, so something to tell you, Universal Sentence Encoder is Google's deep neural network that does, all it does is it generates embeddings and they're 512 dimension embeddings, but they're ideally for something that is sentenced to paragraph length and that's about the length of an abstract. It might be a little bit long, but this is enough to get a good solid embedding and one advantage over something like GPT 3 Embeddings is this is free and it's also Stupid fast. So there we go Yep, tensorflow libraries deep network with the following instructions AVX and AVX to Build it but okay, so it's spotted. Here's my NVIDIA GeForce RTX 270. There we go. So it's still loading the model, what I'm going to use for the embedding in the background. So then this is the length. So actually, I probably should have had some more output. So 2.131 million things. So it loaded it all just fine. Actually, I probably should have had my performance monitor up so I could see how much CPU and memory it took. Let's see, what's using all my memory right now? Notepad. Notepad and the quadrant thing of my job. All right, so first, let's go ahead. So I'm glad that Python was able to run that. I think I have, here, let's close this. Okay, so that was a big chunk of, no, I still have something running Python. What else is running Python? I got that guy and that guy. Okay, whatever. All right. All right. So we want something that's going to end up looking basically like this. I'm not going to worry about the start time. Let's see. Articles loaded. We'll add a little bit of debug output. Oh, I need to set an alarm because I have a meeting here soon. I've got an alarm set. Alright, there we go. Fix it in post. Alright. Don't care about times, print files, overall start time, whatever. For file, in files. No. So we'll go say for article in archive. We don't need the chunks, we just need the title. So we'll say, let's see, info equals json.loads, load s, because that tells it to load string. So we'll say article. So that means info should then contain something like id, title, and abstract. So let's just start there. Okay, so print info dot, no not dot, it'll be title, and then info abstract. And then we'll do exit here. Alright, let's make sure that works. And also for now, I will comment out TensorFlow Hub. So now it's loading. Let's see how much juice we use. Oh yeah, look at that. We're chewing up memory. Name archive, oop, I misspelled it. Don't do that, kids. But yeah, we surged up to, yeah, we used most of my memory. So because I'm running an index engine and loading a three and a half gigabyte file. And I'm running a, well, I guess this model is gonna be in my GPU. So yeah, I'm doing a lot on this workhorse. All right, let's try this again. Very high power usage. Yeah, boy. There we go. All right, it's going up, going up, going up, going back down. Okay, cool. So articles loaded, 2. All right, it's going up, going up, going up, going back down. Okay, cool. So articles loaded, 2.1 million calculation of prompt di-photon production across sections at Tevatron and LHC energy. So fully differential calculated. Okay, blah, blah, blah. It works, good. Yes, we are in good shape. So, so far so good. So now what we need to do is chunks equals wrap text wrap. Okay. Probably what would be faster is if we pass, pass it a list. Eh, no, whatever. Cause I don't know how many, how many, how much you can pass at once. But we'll just comment that out. So we've got all the information we need. So then we'll just do embedding equals um, uh, da da da da da, we'll do a list that contains, well here, the string equals these two, info title plus a nice little space, plus that guy, and then we'll say string equals string.replace actually I probably need import re. So what I'm doing here is, because there's a lot of superfluous new lines, so I'll say string equals resub and we'll replace backslash s plus with just a space and we'll do that on string. Alright, so this will clean it up, make it a nice clean string, and so then we'll do embeddings equals embedString. And so then the vector will be embeddings numpy toList. So then we'll get that as just 1. And then all we do is save data. So the saveData file, we pass it a payload and it will save it into, we'll call that the embeddings folder, I think that's what I called it. This is going faster than I thought, something is going to go wrong. Wow, I've got a lot of tabs open. Yes, so embeddings, embed, so the vector equals that. So the string equals, so we'll just call this the article. Actually, I guess it's more like the abstract. No, we should, yeah, so we'll have the title, we'll keep the title separate. So we'll do title equals info title, because why not? No reason to smush it together, but we're just going to index them together. So that way when you search you're searching the title as well as the abstract. And then I really need to plan on this bombing. Because if it blows up or if it fails I'm not going to do any timekeeping. But what I will do is we'll do one exit here real quick. Go ahead and load this. And let's do a quick run. And this should spit out a folder or a file here. Then we watch the GPU catch on fire. There it goes. GPU memory is loaded up, so it has loaded the model. Article's loaded. There it goes. And now we'll watch. Vectors is not defined. Oh my God. Embedding vector. Okay. It's fine. It's fine. That should be good. Let's try again. And then we will fix this in post as well. Okay. So, I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. And then we will fix this in post as well. Here it comes. Yep, yep, yep. And then it exits, and we should, in theory, come here. Where is my folder? Embeddings. Why is it not showing up? There we go. Okay. Hey, look at this. So we've got abstract. It did not replace. Oh, I didn't, okay. Yeah, I need to replace both, but we did get it. Okay, so we need to do a little bit more cleanup. See, you got the backslash N next. So I want to clean that up and just make that a space. And I also want to trim these. Okay, so what I'll do is let's fix it. We can fix it. No way. Silence. I get you. That's how old I am. I remember when Jeff Dunham was live and popular. I watched him while hanging out in the dorm room of one of my friends many years ago. Okay. All right. So first what we're going to do is we'll just do title equals info dot, no, info.title.strip We'll also wrap that in resub. And we'll do the same treatment for abstract. So then we're only touching it once because here's the thing, when you're doing something a million times, literally millions of times, you kind of want to, if you have to do an operation on it, you do it once. If you're only doing it a few times, you can afford to be less efficient, but for this, in this case, you want to be as efficient as possible. And so what we're doing here is we're going to do title plus space plus abstract. And so we're doing the regex on the title and abstract once. And then we'll also just, because we called it out into a variable so rather than modify it a couple times, we modify it once and away we go. So this should be a little bit better. All right. All right. Let's try it again. And then, okay, so while this is loading, let me tell you what we're going to do. So by having a semantic search engine that allows you to to measure the semantic similarity between basically titles and abstracts across a million articles, it will allow you to find things that are similar that you might not find just through other conventional means. So basically what you'll put in is you'll what I'll ultimately do is say there's an article and you find one it's like yes give me every other article like this one and it will find you every single article like that one. And from there there's all kinds of things that you can do. You can create a semantic meaning graph, you can do blah blah blah. Okay so we got some more embeddings. So 238, so this should be the next one. There we go, that's a little bit prettier. See how there's no markup or anything? So abstract title. And oh, so another thing that I did, let me show you this. With the file save, I say out file, ensure ASCII equals false. So that allows it to use UTF-8, which is nice. Sort keys equals true. So what sort keys equals true does, is it always puts them in the same order, which that's not necessary for the machine, but it's nice for the human so that you know that the abstract will always be first. I don't know what the logic is, but sometimes when you save a dictionary out to JSON, it'll be all garbled up. So it'll always be abstract, embedding, and then title, which is nice for us. So then you see title here at the end, calculation of prompt, diphoton, production, cross sections, at Tivitron, and LHC energies. Wow. Okay. So then you put this abstract into the search engine that we're going to eventually produce and we'll see where it goes. Anyways, so we've got this, so pretty much all I have to do is remove this exit here and just let it go and it will produce millions of these. So what I'm going to do is I'm going to add one last thing. Try, accept, and then what I'll do is errors. And so errors equals list, and then if it blows up I will just say let's see, errors.append article so that if there's any problems I can at least have an archive of the ones that mess up. And so then what I'll do is at the very end I'll do save data. And so I'm gonna save errors. Wait, no, it's the payload. I need that to go to a different place though. So I'll just do, whoops. So I'll do with open and this will be errors.json as out file. We'll do errors. So this is just, I'm going to save however many blow up, but we'll just let it run. Alright, so CLS. I actually probably ought to have some kind of output here. We'll just print the title. There we go. So I'll know it's working because there's nothing worse than just a command prompt there blinking and not telling you anything. Granted, I guess I could check in the embeddings folder, but it's nice to see it actually working. Okay, so let's get this running, make sure it works, and then I'll pause the video and we'll come back once it's done. And actually, probably I'll need to come back to this later, and we will continue this work later tonight or tomorrow, but for you watching it'll just all be one continuous video. But at the end we will have something that you put in an example article and it will spit out a literature review. That's the goal at least. Alright, let's see how hot this gets. Alright, we got the GPU spun up, we got our memory climbing. Oh, this is going to take a while. Because even at this speed, it's going to, oh boy, it's going to take a while to do a million, what, 2.1 million of these? Yeah, so we're only at 200, 300. Okay, this is going to be running for a few days. I wonder if there's a way I can speed this up. Anyways, you don't need to watch all that. If I find a way to speed it up, I will come back and show you. Okay, so yeah, it wasn't going fast enough for me, so this should work. But yeah, so what I was working on was I made the pro, well, all right, so a few things. First, loading 2.1 million or 2.3, however many millions of chunks or things and processing them one at a time is slow, especially when your model can do batching. So I'm creating batches of a thousand and we're feeding it in in chunks. And then we're processing each chunk as a batch and you can look at the code if you want. But it's pretty straightforward. But the secret sauce is here is where you do the hundred or a thousand in batches and it should go a little bit faster. So instead of, let's see what just happened. Oh, I ran out of memory. Okay, so I can't do batches of a thousand. Yeah, that's not working. Okay, so that's too big. So let's go back down to a hundred, cause that did work. So we'll do that of a thousand. So there's no embeddings and this is filling up the error directory. Yes. Okay. So delete that. Okay. So we'll do a batch of a hundred. So rather than doing one embedding at a time, we'll do a hundred at a time. Yeah. And we'll go from there. Okay. So clear that out. So generate embeddings. So the reason that I'm doing this is because I was doing it one at a time and I was like, okay, you know, I've got a small GPU, but it was like 36 hours that it was going to take. And I'm like, I'm not going to let this thing run for 36 hours straight. I can do better than that. I'm going to challenge myself to do better. So I'm going to see, make sure that this works and make sure that it's filling up my, not errors directory up my embeddings directory. Oh, and another thing that I forgot was you need to have the article ID in there if you want to be able to find the full article later. So all right, so there we go. That looks a little bit better. So we have, yeah, look at that. So doing it in chunks rather than doing it one at a time, we're down to 3.3 hours. So it looks like it'll take about three hours to do it in chunks this big. Do I want to, yeah, I'm gonna be bold. Let's see, let's see. Okay, so here it's already processed 4,600 are these articles by doing it in batches of 100 rather than one at a time. So yeah, this is how you do it. And there's the ID, so you got the article ID, so you can go get it from the original location. But let's try, let's just be a little bit bolder, just a little bit. Delete these, because if we can do them once, we can do them again. So let's do a chunk size of 300. And if this works, so we're down to 2.8 hours remaining. CLS. All right. So if we do chunks of 300, I don't think it'll be three times faster and it also might run out of memory, but we'll see. So the batch size has to do with how much memory your GPU has. And so, you know, you go to performance, click on the GPU tab, it'll preload all the memory as it's spooling up the model, but then there's a certain amount of, I'm not sure how the inner workings are, but so you see it shoots up to use like seven and a half gigabytes. All right, let's see what happens. If we see it, we see it blow up with that, you know, OOM. So now we're down to 7000 chunks total rather than 2.1 million individually. Okay, so we're doing larger chunks, the time is coming down. And so what I'm doing in the background is I'm averaging out like how long is it taking overall. So we're down to 3.5, 3.4. And so it's because there's some inefficiencies or whatever, you're, you know, like the time bar, however long it's gonna take. So it looks like you don't get much faster by doing larger chunks because it's down to about, you know, just about three hours. So it looks like that's kind of the optimal because, you know, you do bigger chunks, marginal, whatever. Okay, so two hours, much better, or three hours, much better than 36 hours. So it's 12 times faster, if not a little bit more, because you see this number still coming down pretty quick. All right, so we will go ahead and let this run and then pretty soon we will have 2.1 million embedded archive things. This is going on Data Hoarder. I think the folks on Data Hoarder will love this. All right. And we're back. Okay, so quickly to bring you up to speed as to what I have achieved and what I'm working on next. Let's see. So we will here let me show you the code. Okay, generate embeddings. So I do things in stages, modularize it. So this is the final form of the generate embeddings. And so we load our embedding engine, which is TensorFlow Hub's Universal Sentence Encoder 5. You've been watching, so you know that. I'm just bringing myself back up to speed because this is the following day. Okay, so we break it into chunks of 300. That took the processing time from 36 hours down to less than 3, so we got a factor of 12, more than 12 improvement. And then I also just broke it out into a chunk. Now I had a try except so that it would handle errors, we actually ended up we did get a few errors. So we had four chunks. So that's 1200 articles that didn't get indexed. But when you compare that to you know, the 20, 20, 2.1 million articles that did get indexed or embedded rather. We haven't done the indexing yet. We'll say that that's a pretty good average. So I'm not going to worry about those. Yeah, 1,200 articles is nothing to shake a stick at, and that could be 1,200 of the most important articles. But chances are, whatever. If you want to go back through this and fix it yourself, feel free. Okay. So looking at the directory, let's come back up here. So I took all the embeddings, which is about 29 gigabytes, and I 7-zipped it so that it's now 9 gigabytes. And so that you don't have to redo this yourself, I put it up on Kaggle. So the URL is kaggle.com slash data set slash lieutenant commander data because I'm that kind of nerd and slash archive embeddings. So you can download this yourself and so there's two things that I'm going to do from here, well three actually. So first we have to, I've got to update this and I'm not going to show you, I'll just do this and show you once it's done. You don't need to watch me code this stuff, this is boring. So we're going to index all of this into Quadrant. And so Quadrant is a semantic search engine. It's very lightweight It's very lightweight and we'll see how well it handles 2.1 million articles. Yeah, this will definitely be a stress test for that because I only tested it, I think, when I did my stress test, I did like, well, I did less than that. Let's see, quadrant demo data. I did 88,000. So we're doing a few orders of magnitude more data. Now granted, it handled that much data very well. It only took about, I think, less than three minutes to index all of that. So yeah, that's going to be next. And then once I've got it all hosted in a search server, we will then, I'm going to be next. And then once I've got it all hosted in a search server, we will, then I'm going to build a, a Flask web client to do the search for us. So that way I can, you'll just, there'll be a payload window where you put in like, you know, find me abstracts like this. And then it'll give you like the top 50 results or whatever of similar articles. I mean, heck, we could even do the top 500 and just, you know, scroll down through it. So that way you can just see that the search works and we can look at how good this method methodology is. And you can also see how fast it is. And then the last thing that I'm going to do is actually do like a, so we'll do like to do actually generate a literature review. So like given, like basically the, excuse me, the input will be, you give it like an article or an abstract and it says, okay, find me every abstract like this one, you know, within a certain boundary, and then generate a literature review based on those. Yeah, so that's where we're going from here. Go ahead and save this just so you can see what I'm doing. And then we'll say generate literature review.py. Okay, so I've got to do this, this, and this. We'll see how far I get. Okay, so I got it running. We can see I've got it going up in batches. And it's just kind of running in the background. I'm doing batches of 256 at a time and we're at 22,000 out of 2.1 million. The time that it's going to take keeps slowly ticking up. We're at two and a half hours now. Part of that is the delay of uploading the record, so it's like there's a little bit of marginal cost baked in. But then there's also, I'm not sure, but I think Quadrant slows down a little bit the bigger that it gets. But we'll see. If this takes too long, I might have to try a different thing. But yeah, so far so good. Actually, what I might do is update it so that it's not giving me that much output. Because there's a little bit of marginal overhead for each output, and I also might do bigger batches. Yeah, let's do that. So let me show you what I mean. So we'll cancel this. So this script, what it does is it starts the client, you recreate collection. So this, I looked up what the client does. So recreate collection, it dumps whatever's there and recreates it. So if you're doing testing, it's great. You just start from scratch. And then I updated this function load data. So it just starts here. It gets all the files, you just start from scratch. And then I updated this function load data, so it just starts here. It gets all the files, you instantiate vectors and payloads, and I do a little bit of numbers math. And so then what I wanted to do was let's just have all this come up here. So if the length of vectors is greater than or equal to, let's do chunks of a thousand at a time. So then we're gonna be sending, when we do send a batch, we're sending a batch of a thousand vectors and a thousand payloads up to the client. And all that this does is uploads records, and this will send it in chunks of 256 at a time. And it'll allow the machine to figure out the IDs in the background, uploads it to the archive. So yeah, so this will hopefully keep it a little bit faster because there's gonna be a little bit of overhead cost, one of even just doing a print screen. And so we're only gonna be calculating times and stuff every 1000 rather than every single loop. But then, so all we're doing here is for every file, we will increment our counter, and then we will open the JSON, and we'll have, well, you know, info equals JSON.load.in file. And then we're gonna spool up the vectors and payloads because for whatever reason with Quadrant, you have those as separate lists and you send them independently. Excuse me, I gotta sneeze. I'll fix that in post. Anyways, sorry. Okay. But yeah, so anyways, let's restart this and see if we're going to go any faster. Uploading records. So you can see here, it looks like it might actually be a little bit faster because instead of creeping up to like two and a half hours it's sitting at 1.17 and it looks like it's holding steady. 1.18 yeah it looks like it looks like this method's a bit faster. Cool. And I can probably even remove the uploading records, because that's just extra noise. Like, of course we know you're uploading records. That's what we programmed you to do. Okay, so it looks like this will take about an hour to run. And then in the meantime, we can see, you know, VMM is using that. I wonder if there's any volumes. Yeah, so this is the \u2013 we'll see the volume size go up. So this is the actual storage that Quadrant is using. For 98. Quadrant is really efficient. I'm not sure how it, what kind of compression it does. So we'll see what that gets to at the end. Anyways, probably what I'm going to do is while this is running is I'll work on the Flask app and I'll give you an update here in just a minute. Okay, so I've made some progress. I've got the search server up and running. It's pretty straightforward. Close out that comment. So basically it's just a super simple Flask server. I've got some basic HTML. Actually, I realized that I need to close out the HTML. So then we'll do, let's see, HTML equals HTML plus, and then we'll do, do HTML close equals, and we'll say slash body and then slash HTML. It's not strictly required because most browsers will still render it properly. But if you want to be nice, you will do this. HTML plus, HTML close. nice, you will do this. HTML plus, HTML close. Okay, anyways. Yeah, so here it is. It's up and running. It uses the same embedding engine that is running, that generated the 512 dimension, which I'm going to call it, embeddings, vectors. It's still uploading, but we're at about two and a half hours left. And you see it's pretty stable. It was creeping up, but now it's creeping back down. So I've uploaded half a million. And yeah, it's a really simple thing. So you just put in a search term. So like you might say like pancreatic cancer in mice, and then you do a search. That was faster than before. Yep. And so semantic similarity 0.17. So that's really low. So that must mean that we don't have anything in here or that maybe, um, but yeah, so still you're getting like proton cancer therapy. Let's see if it mice. So there's nothing pancreatic. Yep. So there's nothing, uh, here, let's just do cancer. Why is this search so much faster? Um, okay. Automatic tumor segmentation, whole images of the pancreas. So it's interesting that that didn't come up. So I'm wondering, so basically, I don't know. I'm using a relatively low dimension thing, but you see how fast the search is. I have no idea why it's so much faster right now. Is this still running? Okay, that is still running. But yeah, so... And also I have no idea how this is only still at half a gigabyte. The container that's running it, it says it's using, like if I click on this volume, it's using it, it's the one that's running. See, it's Qtrant storage or Quadrant storage. I'm afraid that it might be running somewhere else. Anyways, so let's say, let's see. If you copy, so like I've got one that I like, so okay, let's just copy this, and we'll put that in there there and we'll do a search. And you see how the more information you give it, the better it gets. And that's because the embeddings that I use include the title and the abstract. So the more information you give it, the closer the embedding will be to what you're looking for. And you see how these scores go way up. And so the whole idea is, if you find one abstract that you like, or if you can think of kind of the title of what you're looking for, you just kind of keep typing and you give it as much as you can, and it will generate a more accurate embedding. And so then I've got it where it just, you know, here's the title, here's the score, and then here's a link directly to the archive. So it'll just take you straight to it. I suppose I could have had a little bit more metadata in here like the date and authors and stuff, but I didn't include that like because I was like, why duplicate effort? Like I just got the bare minimum that was required, which is the title, abstract, and ID, which allows you to come straight here and then you can say, okay, this is when that was. But yeah, so then I did a, what was it, like a large Hadron Collider. Is that one word? L or two? Higgs vacuum decay from particle collisions. And so it's just like, okay, cool. Everything related to colliders and et cetera, et cetera. Bigger, better, faster, more at the LHC. And you see the semantic similarity score of 0.54. And yeah, so I think probably the next thing that I'll do is I might add a tab or another function where it's like, okay, you know, if you have the results that you want, like generate a literature review. But you see how fast this is and the quality of the results. And this is using a very low dimensional vector, relatively speaking, because 512 is pretty small. You can go up to 12,000 with DaVinci. That would be really expensive to embed 2.1 million things with DaVinci. But anyway, so let's just grab this. And so we'll do here, actually let's copy the, both the title and the embedding. And then we'll just put this in our search. Okay, so this gives us even higher results. So hadron spectroscopy structure. So if you put the title and the abstract for a paper, you're going to get all the most similar papers. And so then it's just, you know, you saw how fast that was and you don't have to put in keywords. You put in this big, huge thing and it will say, okay, let's find the payloads that are the most semantically similar, which could lead to like, this is just the first step in automated research, right? Because if you can find all the right research, then you can go from there. There's obviously a few keywords like Hadron, Higgs, LHC, that keep popping up, but you see that one advantage that you get with semantic similarity is that you're not looking for keywords. It will actually kind of get an understanding. And with a low dimensional embedding like 512, you lose a lot, right? Because there's so much specific domain information, like the Randall-Sundrum model. I guarantee you that a low dimensional model or a low dimensional embedding doesn't fully understand that but we keep talking about equations and motion and black holes and standard model. And so all of these things go into creating that 512 dimension, excuse me, embedding. I'm out of coffee. That's unfortunate. Um, anyways, so we're off to a good start and you see, uh, the top, you know, 0.7, sorry, the top 10, they start at 0.75 and go down to 0.62. And so what you can do is easily just come in here to the search server and change the result query from limit to 10 to let's say 100. And so let's do that and then restart our server here. And so then, because obviously, if you're doing a literature review and you might be looking for things that are a little more distal from your core search, because if you're trying to branch out, you don't want to focus on just the things, just the closest cluster. You want to like, okay, let's find everything that's vaguely related. And so then let's do, let's see if OpenAI GPT-3. And one thing that I found that's interesting is that, oh, the fewer search terms you use, the slower it is. I think it's because it has to search further. Whereas like if you have, if you've got a good vector, it's just really fast. So let's do deep neural network computer vision. So that's much faster. Good Lord, that was faster. Okay. Okay, so then let's just grab a whole abstract from one of these guys. And so then you scroll down and it's like, okay, cool. We've got all kinds of stuff. And you see that there's kind of, there's gonna be a whole lot that are at like 0.5, 0.4. So you can just keep going. Yeah. And you'll find everything that is vaguely related and it's all kind of in the same space. So there's a few things that we can do to improve this, which I'm not gonna do right now, I'm just doing this as a proof of concept, is you can use a higher dimensional vector, such as Ada from OpenAI, which is a 1024, or you can go up from there, right? But you're gonna spend a lot of money, whereas Universal Sentence Encoder version 5 is free and it's fast. It only took like three hours to embed all of these. And so then once you're on the page, you could search for, you know, OpenAI. Okay, there's no papers from OpenAI here. Yeah, but, and you see how fast it is and it's just, okay, cool. And I've got it nice and simple so it's easily readable. You've got the abstract here. You've got a link that's very obvious. I'll probably shorten this because that's a little bit of visual noise. So in terms of, we'll just do score. Yeah, because you reduce visual noise so that it's easier to parse. Because this, if it's the same thing, it doesn't matter. What you really care about is this. But you've got different sizes, colors, and shapes to really draw your eyes. So you've got a really reliable pattern. So this blue helps break up. So, you know, like, okay, there's the link. If I want that one, I just click there. Honestly, actually, probably what I ought to do is have, ooh, I can simplify it. We can do the title is the link and then just get rid of the link there because why duplicate it? Yeah, let's do that real quick. Okay, so we will move this up And so instead, I guess here, do ahref. Okay, and then we'll grab payload ID and title. There we go. And so now we'll have a little bit less noise. So now it'll be, this will be the link. So you just click on the title instead of having a separate link, which is smaller. So that's a smaller target. So I have title score and then the abstract and that's that's it. So it's can't get much simpler than that. And so this is the most like cut down basic search tool. You can possibly have Okay. This is the most cut down basic search tool you can possibly have. Okay, so then let's go back in. Let's wait for this to finish loading. Come on, there we go. Okay, so do a quick reload. All right, cool. So now you see the format's a little bit different where you've got this. It's a little bit, it's not quite as pretty. I might change the style so that it's, well, I don't know, I guess that's okay. And then you got the score and then the abstract. So there's a little bit less, I don't know if I like this as much. I don't know, what do you think? It is nice to have the score just here so you can get a quick view of like, okay, how close is this? Image quality assessment. I guess when you pay attention, I wonder if it's the underline that's making it a little harder to read. Anyways, I'll fiddle around with it. We've still got a couple hours left. So two and a quarter hours. that's making it a little harder to read. Anyways, I'll fiddle around with it. We've still got a couple hours left, so two and a quarter hours, so two hours, 15 minutes of these uploading. Probably one reason that we're not getting good results is because it's not done uploading. So we're getting the best that we can with what's here. But yeah, so, okay, I guess I'll stop the video here. We've got this. What have I been working on next? Oh, generate literature review. You know what? I'll probably just integrate the literature review into this server. I think that's what I'll do. Okay. Okay, we're still making progress. However, I was looking at archives bulk download stuff and open access, yes, but they also ask users to play nice and be good with harvesting, so I'm not going to add a bulk download function to my web interface. This will run strictly locally because I mean, you know, they're providing a huge service for free. So let's not abuse the, you know, the free service. But what you can do is once you use this tool to find the papers that you want, you can go manually download them. And so what I've done is I have a folder, so you just accumulate whatever papers you want reviewed in this folder. And so what I'm working on now is a script to do the literature review part separately. So I'll take that out of the web interface. It's pretty straightforward. So here's the that out of the web interface. It's pretty straightforward. So here's the prompt. It just says summarize the following paper for literature review paper, literature review summary, and this is what it looks like. So you take a big chunk and then you have here, and it does a really good job of just kind of distilling it down. So what I'm going to do is I'm going to break it into chunks. And well, here I'll just show you. So I've got it here. Let's see. So we take out the print. And so then we do chunks equals text wrap dot wrap. And then we'll do paper and we'll say 6,000 because that's usually about the size that you can get. And we'll say, okay, let's see, result equals that. And so then we'll say for chunk in chunks, prompt equals open file, and we'll say prompt summary.txt, replace paper with chunk, and we'll do summary equals GPT-3 completion prompt, and so result equals result plus, and we'll say add a nice little space and we'll add summary. Okay, so then for each of these there will be a literature review for each paper and then we will just save it out to file. save it out to file. OK, so probably the way that we'll make this look is we'll say output equals list. And so then we'll have, so then we'll say info equals file. And we'll do file and then summary equals result. Okay, so that will be that and then output.append info. And so then when we're finally done at the very end of all things, we'll save it out as a JSON. Actually, no, let's not save it like that. We will just do, we'll do, text equals, we'll do new line, new line, dot join, output. No, because that'll still be formatted like this. Why don't we do it this way instead then. So final output, we'll do new line, new line, and then we'll do, that'll be the file name, and then we'll also just do the summary. Yeah, that'll work. Okay, so then we'll do file and then resulting summary. Yeah, because then you can do whatever you want, but the point is that it will give you kind of a compact summary with the file name. And again, there's lots of little things we can do to clean this up. Okay, so then save file, and then I have file path and then content. So the file path will be literature review dot text and the content will be the output. All right and then let's run this real quick. So I've got five files that are related. Oh I guess it's not um yep I forgot to put the logs. I forgot to put the logs. No! Okay, so first we will print file and then we will print result and we'll let it go. Okay, come back here. GPT-3 logs do this every darn time. All right, so I've got a handful of files that I want to generate a literature review for. And so what it's going to do is it's going to go through and extract the text from those PDFs and then break that, the whole chunk, or the whole PDF into chunks, and then it's going to go through and extract the text from those PDFs and then break that the whole chunk or the whole PDF into chunks and then it's going to do a quick summary for each of those. Should be running in the background. Yeah, here we go. So summarize the paper. There we go. So it is running. Oh, here we go. Yeah. And so what it'll do is it'll give me the file name and then the the full summary, which you can then use to just plug and play into your paper. And obviously there's there's all kinds of formatting and official stuff, and I'm not a research scientist, but once you get this far, it's pretty easy to clean up. But yeah, I think we're almost done. So this is exciting. Now obviously, this is only halfway done, it's got an hour and a half left, but you can run this on your own to index all of archive yourself and so actually I was thinking through this this is a pretty useful service so tell me what you guys think should I should I set this up as a like an actual website, like, should I try and market this and monetize it? And would that be valuable? And maybe I could do a Kickstarter or something, just set this up. And so then like, you just plug and play and you've got your own literature review engine. So let me know what you think in the comments. But yeah, I'm going to call this done. We'll wait for this to finish spitting out its review.", "chunks": [{"timestamp": [0.0, 13.0], "text": " Hey everyone, David Shapiro here. Before we get started, I just wanted to say that I have updated my Patreon page with new tiers and clarified what each tier gets you."}, {"timestamp": [13.0, 25.28], "text": " A lot of folks asked for some time and help, and so what I did was I created the two higher tiers will entitle you to a little bit of a dedicated one-on-one time with me each"}, {"timestamp": [25.28, 30.56], "text": " month so go check out my patreon page and consider supporting me on patreon"}, {"timestamp": [30.56, 35.32], "text": " and if you want any one-on-one time I'm happy to talk to with you and your team"}, {"timestamp": [35.32, 39.96], "text": " and you know just provide a little bit of insight and guidance so anyways"}, {"timestamp": [39.96, 44.48], "text": " thanks for watching and I hope you enjoy this video about automated literature"}, {"timestamp": [44.48, 45.0], "text": " review for"}, {"timestamp": [45.0, 47.0], "text": " scientific papers."}, {"timestamp": [47.0, 48.6], "text": " Thanks."}, {"timestamp": [48.6, 54.0], "text": " Good morning everybody, David Shapiro here with a video."}, {"timestamp": [54.0, 58.76], "text": " I have created an automated literature review engine."}, {"timestamp": [58.76, 60.84], "text": " It is broken up into two parts."}, {"timestamp": [60.84, 65.0], "text": " There is the search engine which searches all of archive with semantic embedding."}, {"timestamp": [65.0, 74.0], "text": " And then the second part is a GPT-3 script that will take whatever papers you have selected and create literature reviews for them."}, {"timestamp": [74.0, 76.0], "text": " Let me show you how it's done."}, {"timestamp": [81.0, 88.52], "text": " Hey everybody, David Shapiro here with a video. Okay, so ignore this, I haven't updated it yet."}, {"timestamp": [88.52, 92.48], "text": " This repo has gone through a few iterations."}, {"timestamp": [92.48, 97.68], "text": " I was talking to my fiancee who's a librarian and I was like, okay, I've got this idea."}, {"timestamp": [97.68, 99.26], "text": " I posted on Twitter."}, {"timestamp": [99.26, 104.0], "text": " All kinds of people gave me all kinds of ideas like research assistant, knowledge graph,"}, {"timestamp": [104.0, 105.12], "text": " interactive data, fine-tuned"}, {"timestamp": [105.12, 108.88], "text": " generation, teaching aid. Someone was saying like, you know, it'd be great if you could"}, {"timestamp": [108.88, 113.84], "text": " help write grant proposals. There's all kinds of scientific papers, there's academic texts,"}, {"timestamp": [113.84, 118.96], "text": " and I'm like, okay, what does all this have in common? And she asked me, she said, what do you"}, {"timestamp": [118.96, 121.68], "text": " want to achieve with this? What's the output? And I was like, well, I want it to be something"}, {"timestamp": [121.68, 126.34], "text": " useful, right? One, it's got to be something useful, but it's also got to like be"}, {"timestamp": [126.34, 127.2], "text": " attention grabbing."}, {"timestamp": [127.72, 130.12], "text": " Um, you know, it's got to make a good YouTube video."}, {"timestamp": [130.12, 133.28], "text": " And, um, she's like, well, it sounds like what you're trying to do is a, is a"}, {"timestamp": [133.28, 135.48], "text": " literature review, like automatic literature review."}, {"timestamp": [135.48, 137.16], "text": " And I was like, that's it."}, {"timestamp": [137.52, 138.32], "text": " So that's what we're doing."}, {"timestamp": [139.52, 139.84], "text": " Okay."}, {"timestamp": [139.84, 142.76], "text": " So, uh, obviously there's a lot of data here."}, {"timestamp": [142.76, 146.2], "text": " I'm not going to get to all of it, but let's start with something simple."}, {"timestamp": [146.2, 150.6], "text": " So there's a dataset called the archive dataset up on Kaggle"}, {"timestamp": [150.6, 155.12], "text": " and it is a metadata of everything on archive."}, {"timestamp": [155.12, 157.34], "text": " So 1.7 million articles."}, {"timestamp": [158.56, 162.64], "text": " Here's an example of an archive paper."}, {"timestamp": [162.64, 167.82], "text": " So this was published 2020, battery drainingining Attacks Against Edge Computing Nodes"}, {"timestamp": [167.82, 172.5], "text": " and IoT Networks. Okay, super techie, right? And obviously not everything in archive is"}, {"timestamp": [172.5, 176.6], "text": " like this, but we've got all the metadata. So like the metadata is basically what you"}, {"timestamp": [176.6, 189.96], "text": " see on this page. Title, date, authors, that kind of stuff. So let me show you what it looks like. So I went ahead and downloaded it and you get 2.1 million lines of JSON-L."}, {"timestamp": [192.52, 194.88], "text": " And here's what one entry looks like."}, {"timestamp": [194.88, 196.4], "text": " So you get the ID."}, {"timestamp": [196.4, 201.04], "text": " So the ID is what you can use to go to archive"}, {"timestamp": [201.04, 202.84], "text": " and just plug it in right here."}, {"timestamp": [202.84, 205.92], "text": " So like you just grab any archive or any,"}, {"timestamp": [205.92, 208.1], "text": " like use this as the base URL,"}, {"timestamp": [208.1, 209.92], "text": " and then you can also get the PDF,"}, {"timestamp": [209.92, 214.92], "text": " which is just archive.org slash PDF slash the ID dot PDF."}, {"timestamp": [215.14, 218.12], "text": " Download it directly, you can get the full text."}, {"timestamp": [218.12, 220.36], "text": " We're not gonna go quite that far yet."}, {"timestamp": [220.36, 222.96], "text": " So we'll see how far we get with this."}, {"timestamp": [222.96, 226.6], "text": " But what we're gonna do is we're going to index"}, {"timestamp": [227.92, 229.72], "text": " with a semantic vector, we're gonna index"}, {"timestamp": [229.72, 233.98], "text": " all of the abstracts and titles together."}, {"timestamp": [233.98, 237.52], "text": " So I was gonna borrow code from my quadrant stress test,"}, {"timestamp": [237.52, 240.56], "text": " which also means I need to go ahead and fire up Docker"}, {"timestamp": [240.56, 245.0], "text": " with the quadrantrant container."}, {"timestamp": [245.32, 246.44], "text": " So, oh, it's starting."}, {"timestamp": [246.44, 249.2], "text": " Okay, so anyways, as a quick review,"}, {"timestamp": [249.2, 250.6], "text": " in case you don't know what Quadrant is,"}, {"timestamp": [250.6, 253.16], "text": " it is a vector-based search engine,"}, {"timestamp": [253.16, 254.4], "text": " a semantic search engine."}, {"timestamp": [255.52, 258.4], "text": " It runs in a container, it's very small."}, {"timestamp": [258.4, 261.12], "text": " So we'll go ahead and fire this up."}, {"timestamp": [261.12, 265.0], "text": " It is connected to this data store."}, {"timestamp": [266.0, 268.88], "text": " Actually, hang on, I might need to start it"}, {"timestamp": [270.16, 271.22], "text": " a different method."}, {"timestamp": [273.36, 275.26], "text": " Where is the, here we go."}, {"timestamp": [276.78, 277.7], "text": " Let's see."}, {"timestamp": [278.96, 280.54], "text": " Docker run, here we go."}, {"timestamp": [280.54, 282.44], "text": " So I need to start it with this."}, {"timestamp": [282.44, 285.14], "text": " So we'll do that."}, {"timestamp": [285.14, 285.98], "text": " Ta-da."}, {"timestamp": [285.98, 287.68], "text": " Okay, because this command actually tells it"}, {"timestamp": [287.68, 289.32], "text": " what storage to map."}, {"timestamp": [290.56, 292.76], "text": " And it says it's listening, so that's good."}, {"timestamp": [293.98, 297.26], "text": " Okay, I think we should be good there."}, {"timestamp": [297.26, 301.12], "text": " All right, so we've got this data, which I've got."}, {"timestamp": [302.36, 308.28], "text": " So under archive, so here's that file I was showing you."}, {"timestamp": [308.28, 312.2], "text": " So it's 3.4 gigabytes of metadata."}, {"timestamp": [312.2, 313.4], "text": " That's a lot of science."}, {"timestamp": [314.52, 315.88], "text": " It's all in one file."}, {"timestamp": [315.88, 317.2], "text": " That may or may not be problematic."}, {"timestamp": [317.2, 318.04], "text": " We will see."}, {"timestamp": [319.28, 322.0], "text": " So let's go ahead and just get started."}, {"timestamp": [323.04, 326.4], "text": " I've got, so that's the quadrant demo,"}, {"timestamp": [326.4, 328.18], "text": " and then I've got my literature review bot."}, {"timestamp": [328.18, 329.6], "text": " We got nothing going on in here."}, {"timestamp": [329.6, 333.22], "text": " So we're going to start a new file,"}, {"timestamp": [333.22, 336.22], "text": " and let's just play around with this data."}, {"timestamp": [336.22, 338.02], "text": " Let's see what we can get out of it."}, {"timestamp": [339.4, 344.22], "text": " So let's go ahead and just copy my quadrant stress test,"}, {"timestamp": [344.22, 346.84], "text": " and we'll save this under literature"}, {"timestamp": [346.84, 347.84], "text": " review bot."}, {"timestamp": [347.84, 348.84], "text": " Excuse me."}, {"timestamp": [348.84, 349.84], "text": " I just ate lunch."}, {"timestamp": [349.84, 354.44], "text": " I'm a little gonna wash it down."}, {"timestamp": [354.44, 356.7], "text": " All right."}, {"timestamp": [356.7, 361.1], "text": " So we will do, um, uh, what are we trying to do?"}, {"timestamp": [361.1, 362.8], "text": " We're trying to load."}, {"timestamp": [362.8, 376.64], "text": " Um, we're trying to load it. So let's do index archive metadata.py."}, {"timestamp": [377.44, 387.34], "text": " Okay, so we got JSON NumPy stress test. Right. So, oh, that's what I did."}, {"timestamp": [387.34, 393.06], "text": " So what I might actually do is break it up because in my stress test experiment, I broke"}, {"timestamp": [393.06, 395.98], "text": " it up into two steps where I prepared the data and then I uploaded it."}, {"timestamp": [395.98, 398.06], "text": " So that's probably, let's go that way."}, {"timestamp": [398.06, 400.74], "text": " Let me open up that file real quick."}, {"timestamp": [400.74, 403.5], "text": " Quadrant demo."}, {"timestamp": [403.5, 407.0], "text": " Okay, prepare data. There we go."}, {"timestamp": [407.0, 411.0], "text": " So in this one, we did, yeah,"}, {"timestamp": [411.0, 414.0], "text": " we'll use TensorFlow Hub, and I've actually got"}, {"timestamp": [414.0, 417.0], "text": " an Anaconda environment."}, {"timestamp": [417.0, 420.0], "text": " Was it in base? Anaconda"}, {"timestamp": [420.0, 424.0], "text": " navigator. So for, I know I'm going so fast."}, {"timestamp": [424.0, 426.4], "text": " Actually, you'll like it, because y'all, y'all"}, {"timestamp": [426.4, 433.36], "text": " all watch my video on 1.5 X or 2X anyways. So this is probably perfect speed. Okay. So quick recap,"}, {"timestamp": [433.36, 438.4], "text": " if you're not familiar with Anaconda, it is a Python virtual environment that allows you to"}, {"timestamp": [438.4, 446.58], "text": " have, yeah, here we go. So I've got TensorFlow GPU environment ready to go. And so what this does is it allows you to use"}, {"timestamp": [446.58, 451.58], "text": " GPU acceleration to run your TensorFlow instance."}, {"timestamp": [453.52, 456.88], "text": " In this case, I'm going to be using"}, {"timestamp": [456.88, 459.08], "text": " Universal Sentence Encoder version five"}, {"timestamp": [459.92, 463.96], "text": " in order to do this."}, {"timestamp": [463.96, 466.24], "text": " So, all right, so we are going to do,"}, {"timestamp": [466.24, 471.24], "text": " this is gonna be generate embeddings.py."}, {"timestamp": [471.88, 476.4], "text": " Okay, so we'll come, so the file is gonna be here."}, {"timestamp": [476.4, 479.44], "text": " So let's just open a new Explorer."}, {"timestamp": [479.44, 480.64], "text": " That's not it."}, {"timestamp": [480.64, 482.64], "text": " Auto Muse Blogger, that's not it either."}, {"timestamp": [482.64, 486.8], "text": " We need Literature Review Bot. All right, so we will"}, {"timestamp": [486.8, 493.76], "text": " have a folder and we'll call this Embeddings, I guess. And then let me take a quick look at how I"}, {"timestamp": [493.76, 502.72], "text": " did the quadrant demo because how did I format the data? Yeah, it's just in a bunch of JSON files."}, {"timestamp": [503.6, 506.64], "text": " Okay, yep, Embedding and String. Perfect. Okay, that's probably in a bunch of JSON files. Okay, yep. Embedding and string. Perfect."}, {"timestamp": [506.64, 511.28], "text": " Okay, that's probably how I was going to do it anyways. So what I'm going to do is I'm"}, {"timestamp": [511.28, 519.36], "text": " going to take each entry of this guy, because each entry is one JSON, it's JSON-L, right?"}, {"timestamp": [519.36, 524.64], "text": " So basically what I'm going to do is I'm going to take the abstract and the title, I'm going"}, {"timestamp": [524.64, 530.24], "text": " to smush them together so that they're going gonna be one long string, and generate a 512 dimension"}, {"timestamp": [530.24, 533.24], "text": " embedding like this guy."}, {"timestamp": [533.24, 536.76], "text": " So that's what I'll do, and we'll see how fast it goes."}, {"timestamp": [536.76, 538.52], "text": " All right, so let's do some coding."}, {"timestamp": [538.52, 539.76], "text": " We've got prepared data here."}, {"timestamp": [539.76, 544.56], "text": " This one is in the quadrant demo, and I've got a copy of it here."}, {"timestamp": [544.56, 546.12], "text": " So let's see, what do we need to do different?"}, {"timestamp": [546.12, 548.64], "text": " We've got universal sentence encoder five."}, {"timestamp": [548.64, 551.16], "text": " Here, let me close this one, because that's superfluous."}, {"timestamp": [551.16, 552.92], "text": " We're not doing that one yet."}, {"timestamp": [552.92, 555.18], "text": " We've got Docker running here."}, {"timestamp": [555.18, 558.92], "text": " I'll just copy this over into the new readme"}, {"timestamp": [558.92, 561.24], "text": " so that I can keep track of that."}, {"timestamp": [561.24, 563.4], "text": " Literature review bot, readme."}, {"timestamp": [567.84, 569.12], "text": " And we're just a little reminder."}, {"timestamp": [569.12, 571.84], "text": " Okay, so that's a sample, generate embeddings,"}, {"timestamp": [571.84, 574.96], "text": " curl command, close that, stress test, close that."}, {"timestamp": [574.96, 577.06], "text": " We'll keep this open just for reference."}, {"timestamp": [577.06, 581.2], "text": " Okay, yes, generate embeddings, okay."}, {"timestamp": [582.76, 585.56], "text": " We can delete this comment."}, {"timestamp": [585.56, 588.34], "text": " Files equals os-lister."}, {"timestamp": [588.34, 589.18], "text": " Actually, no."}, {"timestamp": [589.18, 591.82], "text": " So we need to go ahead and change that"}, {"timestamp": [591.82, 593.86], "text": " because we're gonna be opening this"}, {"timestamp": [593.86, 595.5], "text": " and we're gonna do it as split lines."}, {"timestamp": [595.5, 598.1], "text": " That's gonna be, ooh, that's gonna be messy."}, {"timestamp": [599.36, 604.36], "text": " Okay, so archive equals open file."}, {"timestamp": [605.0, 613.6], "text": " So, archive equals open file, that was C, archive,"}, {"timestamp": [616.06, 618.84], "text": " and it's called, well here, I'll just copy,"}, {"timestamp": [620.2, 622.3], "text": " copy this guy. All right, so first, let's make sure that,"}, {"timestamp": [624.44, 628.88], "text": " that we can just open the file as is and do split lines."}, {"timestamp": [629.52, 635.84], "text": " Alright, so that means archive will be a list of JSON strings for everything that we want."}, {"timestamp": [636.56, 643.36], "text": " So then we'll do print length of archive and then we'll do exit. So let's just see if this works."}, {"timestamp": [644.0, 645.14], "text": " Let's see if it works,"}, {"timestamp": [645.14, 649.56], "text": " if it blows up, because I mean, I've got 32 gigs of RAM on my, on this PC, so I should"}, {"timestamp": [649.56, 657.8], "text": " be okay. And then let's see, literature review bot. All right. So we'll just do Python generate"}, {"timestamp": [657.8, 667.68], "text": " embeddings. Oh, right. I don't. Yeah. Okay. So what it's doing right now, oh, here, let me make this bigger."}, {"timestamp": [667.68, 669.58], "text": " Yeah."}, {"timestamp": [669.58, 672.22], "text": " I'm not surprised that that blew up."}, {"timestamp": [672.22, 675.86], "text": " So let's do properties so that you can see what I'm doing."}, {"timestamp": [675.86, 686.52], "text": " All right, so what it does is saved model does not exist. freaking happens where, um,"}, {"timestamp": [687.56, 689.84], "text": " for whatever reason,"}, {"timestamp": [692.16, 696.16], "text": " it can't find the model. Um, but anyways, what I need to do is come in here and start it with, um, GPU."}, {"timestamp": [698.76, 702.52], "text": " Come on, wake up. What's the matter with you? There we go. All right."}, {"timestamp": [702.52, 703.36], "text": " So we'll start"}, {"timestamp": [706.6, 713.2], "text": " this conda environment. All right. so it's cd, cd, literature, viewbot, Python, generate embeddings."}, {"timestamp": [713.2, 717.72], "text": " So now what it's going to do, it's going to have to download Universal Sentence Encoder"}, {"timestamp": [717.72, 726.12], "text": " version 5. And so, oh, so something to tell you, Universal Sentence Encoder is Google's deep neural network that does, all"}, {"timestamp": [726.12, 733.72], "text": " it does is it generates embeddings and they're 512 dimension embeddings, but they're ideally"}, {"timestamp": [733.72, 741.76], "text": " for something that is sentenced to paragraph length and that's about the length of an abstract."}, {"timestamp": [741.76, 748.18], "text": " It might be a little bit long, but this is enough to get a good solid embedding and one advantage over something like GPT 3"}, {"timestamp": [748.58, 750.58], "text": " Embeddings is this is free and it's also"}, {"timestamp": [751.46, 753.56], "text": " Stupid fast. So there we go"}, {"timestamp": [754.76, 760.74], "text": " Yep, tensorflow libraries deep network with the following instructions AVX and AVX to"}, {"timestamp": [761.54, 766.96], "text": " Build it but okay, so it's spotted. Here's my NVIDIA GeForce RTX 270. There we go."}, {"timestamp": [767.84, 773.2], "text": " So it's still loading the model, what I'm going to use for the embedding in the background."}, {"timestamp": [774.48, 780.56], "text": " So then this is the length. So actually, I probably should have had some more output. So"}, {"timestamp": [780.56, 786.0], "text": " 2.131 million things. So it loaded it all just fine."}, {"timestamp": [786.0, 789.6], "text": " Actually, I probably should have had my performance monitor up"}, {"timestamp": [789.6, 792.56], "text": " so I could see how much CPU and memory it took."}, {"timestamp": [792.56, 794.8], "text": " Let's see, what's using all my memory right now?"}, {"timestamp": [794.8, 795.3], "text": " Notepad."}, {"timestamp": [796.8, 802.48], "text": " Notepad and the quadrant thing of my job."}, {"timestamp": [802.48, 805.54], "text": " All right, so first, let's go ahead."}, {"timestamp": [805.54, 809.88], "text": " So I'm glad that Python was able to run that."}, {"timestamp": [810.98, 813.72], "text": " I think I have, here, let's close this."}, {"timestamp": [814.8, 816.42], "text": " Okay, so that was a big chunk of,"}, {"timestamp": [816.42, 819.04], "text": " no, I still have something running Python."}, {"timestamp": [819.04, 820.52], "text": " What else is running Python?"}, {"timestamp": [823.3, 825.0], "text": " I got that guy and that guy."}, {"timestamp": [825.0, 827.0], "text": " Okay, whatever."}, {"timestamp": [827.0, 830.0], "text": " All right."}, {"timestamp": [830.0, 831.0], "text": " All right."}, {"timestamp": [831.0, 834.0], "text": " So we want something that's going to end up looking basically like this."}, {"timestamp": [834.0, 839.0], "text": " I'm not going to worry about the start time."}, {"timestamp": [839.0, 842.0], "text": " Let's see."}, {"timestamp": [842.0, 844.0], "text": " Articles loaded."}, {"timestamp": [844.0, 846.5], "text": " We'll add a little bit of debug output."}, {"timestamp": [846.5, 849.5], "text": " Oh, I need to set an alarm because I have a meeting here soon."}, {"timestamp": [849.5, 851.5], "text": " I've got an alarm set."}, {"timestamp": [852.5, 854.5], "text": " Alright, there we go."}, {"timestamp": [854.5, 856.5], "text": " Fix it in post."}, {"timestamp": [856.5, 858.5], "text": " Alright."}, {"timestamp": [858.5, 861.5], "text": " Don't care about times, print files, overall start time, whatever."}, {"timestamp": [861.5, 863.5], "text": " For file, in files. No."}, {"timestamp": [863.5, 868.12], "text": " So we'll go say for article"}, {"timestamp": [868.12, 894.0], "text": " in archive. We don't need the chunks, we just need the title. So we'll say, let's see, info equals json.loads, load s, because that tells it to load string."}, {"timestamp": [894.0, 896.0], "text": " So we'll say article."}, {"timestamp": [896.0, 903.0], "text": " So that means info should then contain something like id, title, and abstract."}, {"timestamp": [903.0, 911.36], "text": " So let's just start there. Okay, so print info dot, no not dot, it'll"}, {"timestamp": [911.36, 929.44], "text": " be title, and then info abstract. And then we'll do exit here. Alright, let's make sure that works."}, {"timestamp": [929.44, 933.24], "text": " And also for now, I will comment out TensorFlow Hub."}, {"timestamp": [933.24, 935.68], "text": " So now it's loading."}, {"timestamp": [935.68, 938.12], "text": " Let's see how much juice we use."}, {"timestamp": [938.12, 939.72], "text": " Oh yeah, look at that."}, {"timestamp": [939.72, 943.06], "text": " We're chewing up memory."}, {"timestamp": [943.06, 951.6], "text": " Name archive, oop, I misspelled it. Don't do that, kids. But yeah,"}, {"timestamp": [951.6, 959.36], "text": " we surged up to, yeah, we used most of my memory. So because I'm running an index engine"}, {"timestamp": [959.36, 966.44], "text": " and loading a three and a half gigabyte file. And I'm running a,"}, {"timestamp": [970.24, 972.76], "text": " well, I guess this model is gonna be in my GPU. So yeah, I'm doing a lot on this workhorse."}, {"timestamp": [972.76, 974.2], "text": " All right, let's try this again."}, {"timestamp": [976.44, 977.8], "text": " Very high power usage."}, {"timestamp": [977.8, 979.1], "text": " Yeah, boy."}, {"timestamp": [979.94, 980.78], "text": " There we go."}, {"timestamp": [980.78, 982.2], "text": " All right, it's going up, going up, going up,"}, {"timestamp": [982.2, 983.36], "text": " going back down."}, {"timestamp": [984.2, 985.36], "text": " Okay, cool. So articles loaded, 2. All right, it's going up, going up, going up, going back down. Okay, cool."}, {"timestamp": [985.36, 988.36], "text": " So articles loaded, 2.1 million calculation"}, {"timestamp": [988.36, 992.4], "text": " of prompt di-photon production across sections"}, {"timestamp": [992.4, 995.0], "text": " at Tevatron and LHC energy."}, {"timestamp": [995.0, 996.52], "text": " So fully differential calculated."}, {"timestamp": [996.52, 997.76], "text": " Okay, blah, blah, blah."}, {"timestamp": [997.76, 999.0], "text": " It works, good."}, {"timestamp": [1000.0, 1002.48], "text": " Yes, we are in good shape."}, {"timestamp": [1002.48, 1004.38], "text": " So, so far so good."}, {"timestamp": [1004.38, 1009.38], "text": " So now what we need to do is chunks equals wrap text wrap."}, {"timestamp": [1009.66, 1010.5], "text": " Okay."}, {"timestamp": [1011.56, 1014.74], "text": " Probably what would be faster is if we pass,"}, {"timestamp": [1018.32, 1020.32], "text": " pass it a list."}, {"timestamp": [1020.32, 1021.52], "text": " Eh, no, whatever."}, {"timestamp": [1024.28, 1028.0], "text": " Cause I don't know how many, how many, how much you can pass at once."}, {"timestamp": [1030.0, 1032.0], "text": " But we'll just comment that out."}, {"timestamp": [1034.0, 1036.0], "text": " So we've got all the information we need."}, {"timestamp": [1036.0, 1038.0], "text": " So then we'll just do"}, {"timestamp": [1038.0, 1040.0], "text": " embedding equals"}, {"timestamp": [1040.0, 1042.0], "text": " um,"}, {"timestamp": [1042.0, 1044.0], "text": " uh, da da da da da,"}, {"timestamp": [1044.0, 1049.56], "text": " we'll do a list that contains, well here,"}, {"timestamp": [1049.56, 1055.8], "text": " the string equals these two,"}, {"timestamp": [1055.8, 1064.18], "text": " info title plus a nice little space,"}, {"timestamp": [1064.18, 1072.0], "text": " plus that guy, and then we'll say string equals string.replace"}, {"timestamp": [1072.0, 1087.0], "text": " actually I probably need import re. So what I'm doing here is, because there's a lot of superfluous new lines, so I'll say string equals resub"}, {"timestamp": [1087.0, 1095.0], "text": " and we'll replace backslash s plus with just a space and we'll do that on string."}, {"timestamp": [1095.0, 1108.0], "text": " Alright, so this will clean it up, make it a nice clean string, and so then we'll do embeddings equals embedString. And so then the vector"}, {"timestamp": [1108.0, 1112.0], "text": " will be embeddings numpy toList. So then we'll"}, {"timestamp": [1112.0, 1116.0], "text": " get that as just 1. And then all"}, {"timestamp": [1116.0, 1120.0], "text": " we do is save data."}, {"timestamp": [1120.0, 1124.0], "text": " So the saveData file, we pass it a payload"}, {"timestamp": [1124.0, 1132.0], "text": " and it will save it into, we'll call that the embeddings folder, I think that's what I called it."}, {"timestamp": [1132.0, 1150.32], "text": " This is going faster than I thought, something is going to go wrong. Wow, I've got a lot of tabs open. Yes, so embeddings, embed, so the vector equals that. So the string equals, so we'll just call"}, {"timestamp": [1150.32, 1158.32], "text": " this the article. Actually, I guess it's more like the abstract. No, we should, yeah, so we'll"}, {"timestamp": [1158.32, 1167.84], "text": " have the title, we'll keep the title separate. So we'll do title equals info title, because why not? No reason to smush"}, {"timestamp": [1167.84, 1173.56], "text": " it together, but we're just going to index them together. So that way when you search"}, {"timestamp": [1173.56, 1185.6], "text": " you're searching the title as well as the abstract."}, {"timestamp": [1185.6, 1192.4], "text": " And then I really need to plan on this bombing."}, {"timestamp": [1192.4, 1207.6], "text": " Because if it blows up or if it fails I'm not going to do any timekeeping. But what I will do is we'll do one exit here real quick."}, {"timestamp": [1207.6, 1210.4], "text": " Go ahead and load this."}, {"timestamp": [1210.4, 1216.2], "text": " And let's do a quick run."}, {"timestamp": [1216.2, 1238.0], "text": " And this should spit out a folder or a file here."}, {"timestamp": [1241.0, 1243.0], "text": " Then we watch the GPU catch on fire."}, {"timestamp": [1253.36, 1254.36], "text": " There it goes. GPU memory is loaded up, so it has loaded the model. Article's loaded."}, {"timestamp": [1254.36, 1255.36], "text": " There it goes."}, {"timestamp": [1255.36, 1258.52], "text": " And now we'll watch."}, {"timestamp": [1258.52, 1259.52], "text": " Vectors is not defined."}, {"timestamp": [1259.52, 1265.0], "text": " Oh my God. Embedding vector. Okay. It's fine. It's fine."}, {"timestamp": [1265.0, 1266.0], "text": " That should be good."}, {"timestamp": [1266.0, 1267.0], "text": " Let's try again."}, {"timestamp": [1267.0, 1268.0], "text": " And then we will fix this in post as well."}, {"timestamp": [1268.0, 1269.0], "text": " Okay."}, {"timestamp": [1269.0, 1270.0], "text": " So, I'm going to go ahead and do that."}, {"timestamp": [1270.0, 1271.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1271.0, 1272.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1272.0, 1273.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1273.0, 1274.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1274.0, 1275.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1275.0, 1276.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1276.0, 1277.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1277.0, 1278.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1278.0, 1279.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1279.0, 1280.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1280.0, 1281.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1281.0, 1282.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1282.0, 1283.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1283.0, 1284.0], "text": " I'm going to go ahead and do that."}, {"timestamp": [1284.0, 1314.96], "text": " I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. I'm going to go ahead and do that. And then we will fix this in post as well. Here it comes."}, {"timestamp": [1318.16, 1319.2], "text": " Yep, yep, yep."}, {"timestamp": [1321.04, 1327.68], "text": " And then it exits, and we should, in theory, come here."}, {"timestamp": [1327.68, 1329.84], "text": " Where is my folder?"}, {"timestamp": [1329.84, 1330.96], "text": " Embeddings."}, {"timestamp": [1330.96, 1333.2], "text": " Why is it not showing up?"}, {"timestamp": [1333.2, 1336.64], "text": " There we go."}, {"timestamp": [1336.64, 1337.64], "text": " Okay."}, {"timestamp": [1337.64, 1340.6], "text": " Hey, look at this."}, {"timestamp": [1340.6, 1343.16], "text": " So we've got abstract."}, {"timestamp": [1343.16, 1344.16], "text": " It did not replace."}, {"timestamp": [1344.16, 1346.56], "text": " Oh, I didn't, okay."}, {"timestamp": [1346.56, 1350.06], "text": " Yeah, I need to replace both, but we did get it."}, {"timestamp": [1350.06, 1351.92], "text": " Okay, so we need to do a little bit more cleanup."}, {"timestamp": [1351.92, 1354.4], "text": " See, you got the backslash N next."}, {"timestamp": [1354.4, 1356.7], "text": " So I want to clean that up and just make that a space."}, {"timestamp": [1356.7, 1359.12], "text": " And I also want to trim these."}, {"timestamp": [1359.12, 1362.04], "text": " Okay, so what I'll do is let's fix it."}, {"timestamp": [1362.04, 1362.88], "text": " We can fix it."}, {"timestamp": [1362.88, 1364.06], "text": " No way."}, {"timestamp": [1369.76, 1377.04], "text": " Silence. I get you. That's how old I am. I remember when Jeff Dunham was live and popular. I watched him while hanging out in the dorm room of"}, {"timestamp": [1377.04, 1411.32], "text": " one of my friends many years ago. Okay. All right. So first what we're going to do is we'll just do title equals info dot, no, info.title.strip We'll also wrap that in resub."}, {"timestamp": [1411.32, 1417.04], "text": " And we'll do the same treatment for abstract."}, {"timestamp": [1417.04, 1419.44], "text": " So then we're only touching it once because here's the thing, when you're doing something"}, {"timestamp": [1419.44, 1424.92], "text": " a million times, literally millions of times, you kind of want to, if you have to do an"}, {"timestamp": [1424.92, 1427.0], "text": " operation on it, you do it once."}, {"timestamp": [1427.0, 1434.0], "text": " If you're only doing it a few times, you can afford to be less efficient, but for this, in this case,"}, {"timestamp": [1434.0, 1449.84], "text": " you want to be as efficient as possible. And so what we're doing here is we're going to do title plus space plus abstract."}, {"timestamp": [1449.84, 1456.04], "text": " And so we're doing the regex on the title and abstract once."}, {"timestamp": [1456.04, 1461.34], "text": " And then we'll also just, because we called it out into a variable so rather than modify"}, {"timestamp": [1461.34, 1468.72], "text": " it a couple times, we modify it once and away we go."}, {"timestamp": [1468.72, 1472.56], "text": " So this should be a little bit better."}, {"timestamp": [1472.56, 1474.24], "text": " All right."}, {"timestamp": [1474.24, 1475.64], "text": " All right."}, {"timestamp": [1475.64, 1478.68], "text": " Let's try it again."}, {"timestamp": [1478.68, 1482.2], "text": " And then, okay, so while this is loading, let me tell you what we're going to do."}, {"timestamp": [1482.2, 1485.36], "text": " So by having a semantic search engine"}, {"timestamp": [1485.36, 1492.0], "text": " that allows you to to measure the semantic similarity between basically titles and abstracts"}, {"timestamp": [1492.0, 1498.56], "text": " across a million articles, it will allow you to find things that are similar that you might not"}, {"timestamp": [1498.56, 1503.04], "text": " find just through other conventional means. So basically what you'll put in is you'll"}, {"timestamp": [1504.0, 1505.68], "text": " what I'll ultimately do is say"}, {"timestamp": [1505.68, 1511.52], "text": " there's an article and you find one it's like yes give me every other article like this one and it"}, {"timestamp": [1511.52, 1517.84], "text": " will find you every single article like that one. And from there there's all kinds of things that"}, {"timestamp": [1517.84, 1523.12], "text": " you can do. You can create a semantic meaning graph, you can do blah blah blah. Okay so we got"}, {"timestamp": [1523.12, 1526.84], "text": " some more embeddings. So 238, so this should be the next one."}, {"timestamp": [1526.84, 1528.36], "text": " There we go, that's a little bit prettier."}, {"timestamp": [1528.36, 1533.24], "text": " See how there's no markup or anything?"}, {"timestamp": [1533.24, 1536.2], "text": " So abstract title."}, {"timestamp": [1536.2, 1539.0], "text": " And oh, so another thing that I did, let me show you this."}, {"timestamp": [1540.2, 1542.4], "text": " With the file save, I say out file,"}, {"timestamp": [1542.4, 1544.56], "text": " ensure ASCII equals false."}, {"timestamp": [1544.56, 1548.5], "text": " So that allows it to use UTF-8, which is nice."}, {"timestamp": [1548.5, 1551.1], "text": " Sort keys equals true."}, {"timestamp": [1551.1, 1553.68], "text": " So what sort keys equals true does,"}, {"timestamp": [1553.68, 1555.78], "text": " is it always puts them in the same order,"}, {"timestamp": [1555.78, 1558.0], "text": " which that's not necessary for the machine,"}, {"timestamp": [1558.0, 1559.64], "text": " but it's nice for the human so that you"}, {"timestamp": [1559.64, 1561.96], "text": " know that the abstract will always be first."}, {"timestamp": [1561.96, 1563.5], "text": " I don't know what the logic is,"}, {"timestamp": [1563.5, 1565.12], "text": " but sometimes when you save"}, {"timestamp": [1570.16, 1575.52], "text": " a dictionary out to JSON, it'll be all garbled up. So it'll always be abstract, embedding, and then title, which is nice for us. So then you see title here at the end,"}, {"timestamp": [1575.52, 1580.64], "text": " calculation of prompt, diphoton, production, cross sections, at Tivitron, and LHC energies."}, {"timestamp": [1580.64, 1589.0], "text": " Wow. Okay. So then you put this abstract into the search engine that we're going to eventually produce and we'll see where it goes."}, {"timestamp": [1589.0, 1606.0], "text": " Anyways, so we've got this, so pretty much all I have to do is remove this exit here and just let it go and it will produce millions of these. So what I'm going to do is I'm going to add one last thing."}, {"timestamp": [1606.0, 1614.0], "text": " Try, accept,"}, {"timestamp": [1614.0, 1619.0], "text": " and then what I'll do is errors."}, {"timestamp": [1619.0, 1625.0], "text": " And so errors equals list, and then if it blows up I will just say"}, {"timestamp": [1625.0, 1630.0], "text": " let's see, errors.append"}, {"timestamp": [1630.0, 1635.0], "text": " article so that if there's any problems I can at least have an archive"}, {"timestamp": [1635.0, 1640.0], "text": " of the ones that mess up. And so then what I'll do"}, {"timestamp": [1640.0, 1645.12], "text": " is at the very end I'll do save data. And so I'm gonna save errors."}, {"timestamp": [1645.12, 1648.28], "text": " Wait, no, it's the payload."}, {"timestamp": [1649.24, 1651.78], "text": " I need that to go to a different place though."}, {"timestamp": [1653.26, 1657.16], "text": " So I'll just do, whoops."}, {"timestamp": [1661.8, 1674.48], "text": " So I'll do with open and this will be errors.json as out file."}, {"timestamp": [1674.48, 1677.72], "text": " We'll do errors."}, {"timestamp": [1677.72, 1691.12], "text": " So this is just, I'm going to save however many blow up, but we'll just let it run. Alright, so CLS. I actually probably ought to have some kind of output here. We'll just"}, {"timestamp": [1691.12, 1695.6], "text": " print the title. There we go. So I'll know it's working because there's nothing worse"}, {"timestamp": [1695.6, 1698.96], "text": " than just a command prompt there blinking and not telling you anything. Granted, I guess"}, {"timestamp": [1698.96, 1704.1], "text": " I could check in the embeddings folder, but it's nice to see it actually working. Okay,"}, {"timestamp": [1704.1, 1706.16], "text": " so let's get this running, make sure it"}, {"timestamp": [1706.16, 1710.72], "text": " works, and then I'll pause the video and we'll come back once it's done. And actually, probably"}, {"timestamp": [1710.72, 1718.88], "text": " I'll need to come back to this later, and we will continue this work later tonight or tomorrow,"}, {"timestamp": [1718.88, 1725.88], "text": " but for you watching it'll just all be one continuous video. But at the end we will have something that you put in"}, {"timestamp": [1725.88, 1733.16], "text": " an example article and it will spit out a literature review. That's the goal at least."}, {"timestamp": [1733.16, 1737.56], "text": " Alright, let's see how hot this gets. Alright, we got the GPU spun up, we got our memory"}, {"timestamp": [1737.56, 1745.68], "text": " climbing. Oh, this is going to take a while."}, {"timestamp": [1745.68, 1751.04], "text": " Because even at this speed, it's going to, oh boy, it's going to take a while to do a"}, {"timestamp": [1751.04, 1754.88], "text": " million, what, 2.1 million of these?"}, {"timestamp": [1754.88, 1757.6], "text": " Yeah, so we're only at 200, 300."}, {"timestamp": [1757.6, 1762.32], "text": " Okay, this is going to be running for a few days."}, {"timestamp": [1762.32, 1764.4], "text": " I wonder if there's a way I can speed this up."}, {"timestamp": [1764.4, 1767.88], "text": " Anyways, you don't need to watch all that."}, {"timestamp": [1767.88, 1772.8], "text": " If I find a way to speed it up, I will come back and show you."}, {"timestamp": [1772.8, 1782.16], "text": " Okay, so yeah, it wasn't going fast enough for me, so this should work."}, {"timestamp": [1782.16, 1785.0], "text": " But yeah, so what I was working on was I made the pro,"}, {"timestamp": [1785.0, 1786.48], "text": " well, all right, so a few things."}, {"timestamp": [1786.48, 1790.72], "text": " First, loading 2.1 million or 2.3,"}, {"timestamp": [1790.72, 1793.88], "text": " however many millions of chunks or things"}, {"timestamp": [1793.88, 1796.16], "text": " and processing them one at a time is slow,"}, {"timestamp": [1796.16, 1799.24], "text": " especially when your model can do batching."}, {"timestamp": [1799.24, 1801.92], "text": " So I'm creating batches of a thousand"}, {"timestamp": [1801.92, 1804.14], "text": " and we're feeding it in in chunks."}, {"timestamp": [1804.14, 1808.0], "text": " And then we're processing each chunk as a batch and you can look at the code if you want."}, {"timestamp": [1808.0, 1825.98], "text": " But it's pretty straightforward. But the secret sauce is here is where you do the hundred or a thousand in batches and it should go a little bit faster. So instead of, let's see what just happened."}, {"timestamp": [1828.52, 1829.44], "text": " Oh, I ran out of memory."}, {"timestamp": [1829.44, 1831.6], "text": " Okay, so I can't do batches of a thousand."}, {"timestamp": [1834.74, 1836.0], "text": " Yeah, that's not working."}, {"timestamp": [1837.3, 1838.68], "text": " Okay, so that's too big."}, {"timestamp": [1838.68, 1840.6], "text": " So let's go back down to a hundred,"}, {"timestamp": [1840.6, 1841.68], "text": " cause that did work."}, {"timestamp": [1842.56, 1844.84], "text": " So we'll do that of a thousand."}, {"timestamp": [1844.84, 1847.88], "text": " So there's no embeddings and this is filling up the error directory."}, {"timestamp": [1847.88, 1848.6], "text": " Yes."}, {"timestamp": [1848.6, 1848.84], "text": " Okay."}, {"timestamp": [1848.84, 1850.0], "text": " So delete that."}, {"timestamp": [1850.0, 1850.32], "text": " Okay."}, {"timestamp": [1850.32, 1851.64], "text": " So we'll do a batch of a hundred."}, {"timestamp": [1851.64, 1857.4], "text": " So rather than doing one embedding at a time, we'll do a hundred at a time."}, {"timestamp": [1857.4, 1857.72], "text": " Yeah."}, {"timestamp": [1857.72, 1859.44], "text": " And we'll go from there."}, {"timestamp": [1859.44, 1860.56], "text": " Okay."}, {"timestamp": [1860.56, 1863.04], "text": " So clear that out."}, {"timestamp": [1863.04, 1863.96], "text": " So generate embeddings."}, {"timestamp": [1863.96, 1868.0], "text": " So the reason that I'm doing this is because I was doing it one at a time and I was like,"}, {"timestamp": [1868.0, 1873.0], "text": " okay, you know, I've got a small GPU, but it was like 36 hours that it was going to take."}, {"timestamp": [1873.0, 1876.0], "text": " And I'm like, I'm not going to let this thing run for 36 hours straight."}, {"timestamp": [1876.0, 1877.0], "text": " I can do better than that."}, {"timestamp": [1877.0, 1879.0], "text": " I'm going to challenge myself to do better."}, {"timestamp": [1879.0, 1884.0], "text": " So I'm going to see, make sure that this works and make sure that it's filling up my,"}, {"timestamp": [1884.0, 1886.3], "text": " not errors directory up my embeddings"}, {"timestamp": [1886.3, 1887.3], "text": " directory."}, {"timestamp": [1887.3, 1894.8], "text": " Oh, and another thing that I forgot was you need to have the article ID in there if you"}, {"timestamp": [1894.8, 1897.28], "text": " want to be able to find the full article later."}, {"timestamp": [1897.28, 1898.96], "text": " So all right, so there we go."}, {"timestamp": [1898.96, 1899.96], "text": " That looks a little bit better."}, {"timestamp": [1899.96, 1908.36], "text": " So we have, yeah, look at that. So doing it in chunks rather than doing it one at a time,"}, {"timestamp": [1908.36, 1910.88], "text": " we're down to 3.3 hours."}, {"timestamp": [1910.88, 1912.3], "text": " So it looks like it'll take about three hours"}, {"timestamp": [1912.3, 1914.18], "text": " to do it in chunks this big."}, {"timestamp": [1915.2, 1918.84], "text": " Do I want to, yeah, I'm gonna be bold."}, {"timestamp": [1918.84, 1920.76], "text": " Let's see, let's see."}, {"timestamp": [1920.76, 1924.4], "text": " Okay, so here it's already processed 4,600"}, {"timestamp": [1925.52, 1928.6], "text": " are these articles by doing it in batches of 100"}, {"timestamp": [1928.6, 1930.52], "text": " rather than one at a time."}, {"timestamp": [1930.52, 1932.8], "text": " So yeah, this is how you do it."}, {"timestamp": [1932.8, 1934.84], "text": " And there's the ID, so you got the article ID,"}, {"timestamp": [1934.84, 1938.76], "text": " so you can go get it from the original location."}, {"timestamp": [1938.76, 1941.44], "text": " But let's try, let's just be a little bit bolder,"}, {"timestamp": [1941.44, 1942.78], "text": " just a little bit."}, {"timestamp": [1944.04, 1946.32], "text": " Delete these, because if we can do them once,"}, {"timestamp": [1946.32, 1956.32], "text": " we can do them again. So let's do a chunk size of 300. And if this works, so we're down to 2.8 hours"}, {"timestamp": [1957.44, 1967.08], "text": " remaining. CLS. All right. So if we do chunks of 300, I don't think it'll be three times faster"}, {"timestamp": [1967.08, 1972.0], "text": " and it also might run out of memory, but we'll see. So the batch size has to do with how"}, {"timestamp": [1972.0, 1978.72], "text": " much memory your GPU has. And so, you know, you go to performance, click on the GPU tab,"}, {"timestamp": [1978.72, 1984.6], "text": " it'll preload all the memory as it's spooling up the model, but then there's a certain amount"}, {"timestamp": [1984.6, 1991.0], "text": " of, I'm not sure how the inner workings are, but so you see it shoots up to use like seven and a half gigabytes."}, {"timestamp": [1991.0, 2005.28], "text": " All right, let's see what happens. If we see it, we see it blow up with that, you know, OOM. So now we're down to 7000 chunks total rather than 2.1 million individually. Okay, so we're doing larger chunks,"}, {"timestamp": [2005.28, 2007.24], "text": " the time is coming down."}, {"timestamp": [2007.24, 2008.64], "text": " And so what I'm doing in the background"}, {"timestamp": [2008.64, 2011.82], "text": " is I'm averaging out like how long is it taking overall."}, {"timestamp": [2012.68, 2015.12], "text": " So we're down to 3.5, 3.4."}, {"timestamp": [2015.12, 2018.56], "text": " And so it's because there's some inefficiencies"}, {"timestamp": [2018.56, 2021.56], "text": " or whatever, you're, you know, like the time bar,"}, {"timestamp": [2021.56, 2022.96], "text": " however long it's gonna take."}, {"timestamp": [2022.96, 2025.68], "text": " So it looks like you don't get much faster"}, {"timestamp": [2030.8, 2036.32], "text": " by doing larger chunks because it's down to about, you know, just about three hours. So it looks like that's kind of the optimal because, you know, you do bigger chunks, marginal, whatever. Okay,"}, {"timestamp": [2036.32, 2042.16], "text": " so two hours, much better, or three hours, much better than 36 hours. So it's 12 times faster,"}, {"timestamp": [2042.16, 2048.8], "text": " if not a little bit more, because you see this number still coming down pretty quick. All right, so we will go ahead and let this run and then"}, {"timestamp": [2048.8, 2057.4], "text": " pretty soon we will have 2.1 million embedded archive things. This is going on Data Hoarder."}, {"timestamp": [2057.4, 2074.0], "text": " I think the folks on Data Hoarder will love this. All right. And we're back. Okay, so quickly to bring you up to speed as to what I have achieved and what I'm working on next."}, {"timestamp": [2074.0, 2085.0], "text": " Let's see. So we will here let me show you the code. Okay, generate embeddings. So I do things in stages, modularize it."}, {"timestamp": [2085.0, 2090.0], "text": " So this is the final form of the generate embeddings."}, {"timestamp": [2090.0, 2095.0], "text": " And so we load our embedding engine, which is TensorFlow Hub's"}, {"timestamp": [2095.0, 2099.0], "text": " Universal Sentence Encoder 5. You've been watching, so you know that."}, {"timestamp": [2099.0, 2103.0], "text": " I'm just bringing myself back up to speed because this is the following day."}, {"timestamp": [2103.0, 2106.88], "text": " Okay, so we break it into chunks of 300."}, {"timestamp": [2106.88, 2111.68], "text": " That took the processing time from 36 hours down to less than 3, so we got a factor of"}, {"timestamp": [2111.68, 2115.98], "text": " 12, more than 12 improvement."}, {"timestamp": [2115.98, 2119.4], "text": " And then I also just broke it out into a chunk."}, {"timestamp": [2119.4, 2126.0], "text": " Now I had a try except so that it would handle errors, we actually ended up we did get a few errors."}, {"timestamp": [2126.0, 2133.0], "text": " So we had four chunks. So that's 1200 articles that didn't get indexed. But when you compare that to"}, {"timestamp": [2133.0, 2150.52], "text": " you know, the 20, 20, 2.1 million articles that did get indexed or embedded rather. We haven't done the indexing yet. We'll say that that's a pretty good average."}, {"timestamp": [2150.52, 2153.2], "text": " So I'm not going to worry about those."}, {"timestamp": [2153.2, 2155.92], "text": " Yeah, 1,200 articles is nothing to shake a stick at,"}, {"timestamp": [2155.92, 2158.2], "text": " and that could be 1,200 of the most important articles."}, {"timestamp": [2158.2, 2160.84], "text": " But chances are, whatever."}, {"timestamp": [2160.84, 2164.52], "text": " If you want to go back through this and fix it yourself, feel free."}, {"timestamp": [2164.52, 2172.76], "text": " Okay. So looking at the directory, let's come back up here. So I took all the embeddings,"}, {"timestamp": [2172.76, 2179.72], "text": " which is about 29 gigabytes, and I 7-zipped it so that it's now 9 gigabytes. And so that"}, {"timestamp": [2179.72, 2188.28], "text": " you don't have to redo this yourself, I put it up on Kaggle. So the URL is kaggle.com slash data set slash lieutenant commander data because I'm that kind of"}, {"timestamp": [2188.28, 2195.12], "text": " nerd and slash archive embeddings. So you can download this yourself and so"}, {"timestamp": [2195.12, 2198.64], "text": " there's two things that I'm going to do from here, well three actually. So first"}, {"timestamp": [2198.64, 2206.32], "text": " we have to, I've got to update this and I'm not going to show you, I'll just do this and show you once"}, {"timestamp": [2206.32, 2213.04], "text": " it's done. You don't need to watch me code this stuff, this is boring. So we're going to index"}, {"timestamp": [2213.04, 2221.12], "text": " all of this into Quadrant. And so Quadrant is a semantic search engine. It's very lightweight"}, {"timestamp": [2228.64, 2234.48], "text": " It's very lightweight and we'll see how well it handles 2.1 million articles. Yeah, this will definitely be a stress test for that because I only tested it, I think,"}, {"timestamp": [2234.48, 2241.12], "text": " when I did my stress test, I did like, well, I did less than that."}, {"timestamp": [2241.12, 2248.0], "text": " Let's see, quadrant demo data. I did 88,000. So we're doing a"}, {"timestamp": [2248.0, 2252.0], "text": " few orders of magnitude more data. Now granted, it handled that much data"}, {"timestamp": [2252.0, 2256.0], "text": " very well. It only took about, I think, less than three minutes"}, {"timestamp": [2256.0, 2260.0], "text": " to index all of that. So yeah,"}, {"timestamp": [2260.0, 2264.0], "text": " that's going to be next. And then once I've got it all hosted in"}, {"timestamp": [2264.0, 2267.72], "text": " a search server, we will then, I'm going to be next. And then once I've got it all hosted in a search server, we will, then I'm going to"}, {"timestamp": [2267.72, 2272.16], "text": " build a, a Flask web client to do the search for us."}, {"timestamp": [2272.48, 2278.2], "text": " So that way I can, you'll just, there'll be a payload window where you put in like,"}, {"timestamp": [2278.2, 2280.64], "text": " you know, find me abstracts like this."}, {"timestamp": [2280.88, 2287.76], "text": " And then it'll give you like the top 50 results or whatever of similar articles. I mean,"}, {"timestamp": [2287.76, 2294.48], "text": " heck, we could even do the top 500 and just, you know, scroll down through it. So that way you can"}, {"timestamp": [2294.48, 2301.84], "text": " just see that the search works and we can look at how good this method methodology is. And you can"}, {"timestamp": [2301.84, 2306.0], "text": " also see how fast it is. And then the last thing that I'm going to do is actually"}, {"timestamp": [2306.0, 2315.76], "text": " do like a, so we'll do like to do actually generate a literature review. So like given,"}, {"timestamp": [2316.32, 2325.94], "text": " like basically the, excuse me, the input will be, you give it like an article or an abstract and it says, okay, find me every"}, {"timestamp": [2325.94, 2331.9], "text": " abstract like this one, you know, within a certain boundary, and then generate a literature"}, {"timestamp": [2331.9, 2334.7], "text": " review based on those."}, {"timestamp": [2334.7, 2338.58], "text": " Yeah, so that's where we're going from here."}, {"timestamp": [2338.58, 2342.76], "text": " Go ahead and save this just so you can see what I'm doing."}, {"timestamp": [2342.76, 2346.5], "text": " And then we'll say generate literature review.py."}, {"timestamp": [2346.5, 2349.5], "text": " Okay, so I've got to do this, this, and this."}, {"timestamp": [2349.5, 2350.5], "text": " We'll see how far I get."}, {"timestamp": [2354.0, 2356.0], "text": " Okay, so I got it running."}, {"timestamp": [2357.5, 2360.5], "text": " We can see I've got it going up in batches."}, {"timestamp": [2360.5, 2364.5], "text": " And it's just kind of running in the background."}, {"timestamp": [2364.5, 2370.32], "text": " I'm doing batches of 256 at a time and we're at"}, {"timestamp": [2370.32, 2378.8], "text": " 22,000 out of 2.1 million. The time that it's going to take keeps slowly ticking up. We're at"}, {"timestamp": [2378.8, 2387.76], "text": " two and a half hours now. Part of that is the delay of uploading the record, so it's like there's a little"}, {"timestamp": [2387.76, 2394.72], "text": " bit of marginal cost baked in. But then there's also, I'm not sure, but I think Quadrant"}, {"timestamp": [2394.72, 2405.0], "text": " slows down a little bit the bigger that it gets. But we'll see. If this takes too long, I might have to try a different thing."}, {"timestamp": [2405.0, 2410.0], "text": " But yeah, so far so good."}, {"timestamp": [2410.0, 2415.0], "text": " Actually, what I might do is update it so that it's not giving me that much output."}, {"timestamp": [2415.0, 2420.0], "text": " Because there's a little bit of marginal overhead for each output, and I also might do bigger batches."}, {"timestamp": [2420.0, 2427.86], "text": " Yeah, let's do that. So let me show you what I mean. So we'll cancel this. So this script, what it does is it starts the client,"}, {"timestamp": [2427.86, 2429.24], "text": " you recreate collection."}, {"timestamp": [2429.24, 2433.02], "text": " So this, I looked up what the client does."}, {"timestamp": [2433.02, 2436.18], "text": " So recreate collection, it dumps whatever's there"}, {"timestamp": [2436.18, 2437.94], "text": " and recreates it."}, {"timestamp": [2437.94, 2440.3], "text": " So if you're doing testing, it's great."}, {"timestamp": [2440.3, 2441.82], "text": " You just start from scratch."}, {"timestamp": [2441.82, 2443.82], "text": " And then I updated this function load data."}, {"timestamp": [2443.82, 2445.44], "text": " So it just starts here. It gets all the files, you just start from scratch. And then I updated this function load data, so it just starts here."}, {"timestamp": [2445.44, 2449.32], "text": " It gets all the files, you instantiate vectors and payloads,"}, {"timestamp": [2449.32, 2451.4], "text": " and I do a little bit of numbers math."}, {"timestamp": [2452.26, 2455.0], "text": " And so then what I wanted to do"}, {"timestamp": [2455.0, 2458.84], "text": " was let's just have all this come up here."}, {"timestamp": [2458.84, 2462.76], "text": " So if the length of vectors is greater than or equal to,"}, {"timestamp": [2462.76, 2466.56], "text": " let's do chunks of a thousand at a time."}, {"timestamp": [2467.4, 2469.36], "text": " So then we're gonna be sending,"}, {"timestamp": [2470.76, 2472.76], "text": " when we do send a batch,"}, {"timestamp": [2473.64, 2476.72], "text": " we're sending a batch of a thousand vectors"}, {"timestamp": [2476.72, 2478.6], "text": " and a thousand payloads up to the client."}, {"timestamp": [2478.6, 2481.36], "text": " And all that this does is uploads records,"}, {"timestamp": [2481.36, 2486.2], "text": " and this will send it in chunks of 256 at a time."}, {"timestamp": [2486.2, 2489.08], "text": " And it'll allow the machine to figure out the IDs"}, {"timestamp": [2489.08, 2491.88], "text": " in the background, uploads it to the archive."}, {"timestamp": [2492.72, 2497.3], "text": " So yeah, so this will hopefully keep it a little bit faster"}, {"timestamp": [2497.3, 2500.24], "text": " because there's gonna be a little bit of overhead cost,"}, {"timestamp": [2500.24, 2503.58], "text": " one of even just doing a print screen."}, {"timestamp": [2503.58, 2507.92], "text": " And so we're only gonna be calculating times and stuff"}, {"timestamp": [2507.92, 2512.36], "text": " every 1000 rather than every single loop."}, {"timestamp": [2512.36, 2516.96], "text": " But then, so all we're doing here is for every file,"}, {"timestamp": [2516.96, 2520.16], "text": " we will increment our counter,"}, {"timestamp": [2520.16, 2523.44], "text": " and then we will open the JSON,"}, {"timestamp": [2523.44, 2525.0], "text": " and we'll have,"}, {"timestamp": [2525.08, 2528.02], "text": " well, you know, info equals JSON.load.in file."}, {"timestamp": [2528.02, 2530.68], "text": " And then we're gonna spool up the vectors and payloads"}, {"timestamp": [2530.68, 2533.04], "text": " because for whatever reason with Quadrant,"}, {"timestamp": [2533.04, 2536.76], "text": " you have those as separate lists"}, {"timestamp": [2536.76, 2538.36], "text": " and you send them independently."}, {"timestamp": [2538.36, 2539.72], "text": " Excuse me, I gotta sneeze."}, {"timestamp": [2542.28, 2544.26], "text": " I'll fix that in post."}, {"timestamp": [2544.26, 2546.0], "text": " Anyways, sorry."}, {"timestamp": [2546.0, 2548.0], "text": " Okay."}, {"timestamp": [2548.0, 2552.0], "text": " But yeah, so anyways, let's restart this and see if we're going to go any faster."}, {"timestamp": [2556.0, 2558.0], "text": " Uploading records."}, {"timestamp": [2562.0, 2565.76], "text": " So you can see here, it looks like it might actually be a little bit faster because"}, {"timestamp": [2565.76, 2571.92], "text": " instead of creeping up to like two and a half hours it's sitting at 1.17 and it looks like"}, {"timestamp": [2571.92, 2578.4], "text": " it's holding steady. 1.18 yeah it looks like it looks like this method's a bit faster."}, {"timestamp": [2580.56, 2587.0], "text": " Cool. And I can probably even remove the uploading records, because that's just extra noise."}, {"timestamp": [2587.0, 2589.0], "text": " Like, of course we know you're uploading records."}, {"timestamp": [2589.0, 2591.0], "text": " That's what we programmed you to do."}, {"timestamp": [2591.0, 2594.0], "text": " Okay, so it looks like this will take about an hour to run."}, {"timestamp": [2594.0, 2599.0], "text": " And then in the meantime, we can see, you know, VMM is using that."}, {"timestamp": [2599.0, 2603.0], "text": " I wonder if there's any volumes."}, {"timestamp": [2603.0, 2609.04], "text": " Yeah, so this is the \u2013 we'll see the volume size go up. So this is the actual storage"}, {"timestamp": [2609.04, 2618.6], "text": " that Quadrant is using. For 98. Quadrant is really efficient. I'm not sure how it, what kind of"}, {"timestamp": [2618.6, 2623.0], "text": " compression it does. So we'll see what that gets to at the end. Anyways, probably what I'm going to"}, {"timestamp": [2623.0, 2626.64], "text": " do is while this is running is I'll work on the Flask app"}, {"timestamp": [2626.64, 2629.36], "text": " and I'll give you an update here in just a minute."}, {"timestamp": [2633.6, 2636.28], "text": " Okay, so I've made some progress."}, {"timestamp": [2636.28, 2637.96], "text": " I've got the search server up and running."}, {"timestamp": [2637.96, 2639.48], "text": " It's pretty straightforward."}, {"timestamp": [2641.24, 2642.92], "text": " Close out that comment."}, {"timestamp": [2642.92, 2647.16], "text": " So basically it's just a super simple Flask server."}, {"timestamp": [2647.16, 2650.04], "text": " I've got some basic HTML."}, {"timestamp": [2650.92, 2654.24], "text": " Actually, I realized that I need to close out the HTML."}, {"timestamp": [2654.24, 2658.88], "text": " So then we'll do, let's see, HTML equals HTML plus,"}, {"timestamp": [2658.88, 2659.88], "text": " and then we'll do,"}, {"timestamp": [2661.6, 2667.0], "text": " do HTML close equals,"}, {"timestamp": [2667.0, 2673.44], "text": " and we'll say slash body and then slash HTML."}, {"timestamp": [2673.44, 2675.16], "text": " It's not strictly required because"}, {"timestamp": [2675.16, 2677.36], "text": " most browsers will still render it properly."}, {"timestamp": [2677.36, 2681.48], "text": " But if you want to be nice,"}, {"timestamp": [2681.48, 2684.8], "text": " you will do this."}, {"timestamp": [2684.8, 2685.0], "text": " HTML plus, HTML close. nice, you will do this."}, {"timestamp": [2685.0, 2688.0], "text": " HTML plus, HTML close."}, {"timestamp": [2688.0, 2690.0], "text": " Okay, anyways."}, {"timestamp": [2690.0, 2691.0], "text": " Yeah, so here it is."}, {"timestamp": [2691.0, 2693.0], "text": " It's up and running."}, {"timestamp": [2693.0, 2703.0], "text": " It uses the same embedding engine that is running, that generated the 512 dimension,"}, {"timestamp": [2703.0, 2707.12], "text": " which I'm going to call it, embeddings, vectors. It's still uploading,"}, {"timestamp": [2708.08, 2713.84], "text": " but we're at about two and a half hours left. And you see it's pretty stable. It was creeping up,"}, {"timestamp": [2713.84, 2720.08], "text": " but now it's creeping back down. So I've uploaded half a million. And yeah, it's a really simple"}, {"timestamp": [2720.08, 2727.2], "text": " thing. So you just put in a search term. So like you might say like pancreatic cancer in mice,"}, {"timestamp": [2727.84, 2731.36], "text": " and then you do a search. That was faster than before."}, {"timestamp": [2734.24, 2741.04], "text": " Yep. And so semantic similarity 0.17. So that's really low. So that must mean that we don't have"}, {"timestamp": [2741.04, 2745.36], "text": " anything in here or that maybe, um,"}, {"timestamp": [2748.76, 2753.2], "text": " but yeah, so still you're getting like proton cancer therapy. Let's see if it mice. So there's nothing pancreatic."}, {"timestamp": [2753.88, 2757.68], "text": " Yep. So there's nothing, uh, here, let's just do cancer."}, {"timestamp": [2759.84, 2763.44], "text": " Why is this search so much faster? Um, okay."}, {"timestamp": [2763.46, 2767.4], "text": " Automatic tumor segmentation, whole images of the pancreas."}, {"timestamp": [2767.4, 2769.2], "text": " So it's interesting that that didn't come up."}, {"timestamp": [2769.2, 2775.28], "text": " So I'm wondering, so basically, I don't know."}, {"timestamp": [2775.28, 2779.96], "text": " I'm using a relatively low dimension thing, but you see how fast the search is."}, {"timestamp": [2779.96, 2785.24], "text": " I have no idea why it's so much faster right now."}, {"timestamp": [2786.08, 2787.86], "text": " Is this still running? Okay, that is still running."}, {"timestamp": [2787.86, 2789.46], "text": " But yeah, so..."}, {"timestamp": [2792.66, 2794.82], "text": " And also I have no idea how this is only still"}, {"timestamp": [2794.82, 2795.92], "text": " at half a gigabyte."}, {"timestamp": [2797.52, 2800.94], "text": " The container that's running it, it says it's using,"}, {"timestamp": [2800.94, 2803.86], "text": " like if I click on this volume, it's using it,"}, {"timestamp": [2803.86, 2804.7], "text": " it's the one that's running."}, {"timestamp": [2804.7, 2807.58], "text": " See, it's Qtrant storage or Quadrant storage."}, {"timestamp": [2809.2, 2813.04], "text": " I'm afraid that it might be running somewhere else."}, {"timestamp": [2813.04, 2817.0], "text": " Anyways, so let's say, let's see."}, {"timestamp": [2817.0, 2820.06], "text": " If you copy, so like I've got one that I like,"}, {"timestamp": [2820.06, 2822.72], "text": " so okay, let's just copy this,"}, {"timestamp": [2822.72, 2825.52], "text": " and we'll put that in there there and we'll do a search."}, {"timestamp": [2825.52, 2827.72], "text": " And you see how the more information you give it,"}, {"timestamp": [2827.72, 2829.28], "text": " the better it gets."}, {"timestamp": [2829.28, 2834.28], "text": " And that's because the embeddings that I use"}, {"timestamp": [2834.6, 2839.4], "text": " include the title and the abstract."}, {"timestamp": [2839.4, 2840.8], "text": " So the more information you give it,"}, {"timestamp": [2840.8, 2843.68], "text": " the closer the embedding will be to what you're looking for."}, {"timestamp": [2843.68, 2845.0], "text": " And you see how these scores go way up."}, {"timestamp": [2847.0, 2849.0], "text": " And so the whole idea is,"}, {"timestamp": [2849.0, 2851.0], "text": " if you find one abstract that you like,"}, {"timestamp": [2851.0, 2853.0], "text": " or if you can think of kind of the title"}, {"timestamp": [2853.0, 2854.0], "text": " of what you're looking for,"}, {"timestamp": [2854.0, 2856.0], "text": " you just kind of keep typing and you give it"}, {"timestamp": [2856.0, 2857.0], "text": " as much as you can,"}, {"timestamp": [2857.0, 2860.0], "text": " and it will generate a more accurate embedding."}, {"timestamp": [2860.0, 2862.0], "text": " And so then I've got it where it just, you know,"}, {"timestamp": [2862.0, 2864.0], "text": " here's the title, here's the score,"}, {"timestamp": [2864.0, 2866.8], "text": " and then here's a link directly to the archive."}, {"timestamp": [2867.24, 2869.24], "text": " So it'll just take you straight to it."}, {"timestamp": [2869.6, 2874.16], "text": " I suppose I could have had a little bit more metadata in here like the date and authors and stuff,"}, {"timestamp": [2874.2, 2878.36], "text": " but I didn't include that like because I was like, why duplicate effort?"}, {"timestamp": [2878.36, 2882.92], "text": " Like I just got the bare minimum that was required, which is the title, abstract, and ID,"}, {"timestamp": [2883.16, 2889.2], "text": " which allows you to come straight here and then you can say, okay, this is when that was."}, {"timestamp": [2889.2, 2896.2], "text": " But yeah, so then I did a, what was it, like a large Hadron Collider."}, {"timestamp": [2896.2, 2897.2], "text": " Is that one word?"}, {"timestamp": [2897.2, 2898.88], "text": " L or two?"}, {"timestamp": [2898.88, 2902.26], "text": " Higgs vacuum decay from particle collisions."}, {"timestamp": [2902.26, 2907.72], "text": " And so it's just like, okay, cool. Everything related to colliders and et cetera, et cetera."}, {"timestamp": [2908.72, 2911.44], "text": " Bigger, better, faster, more at the LHC."}, {"timestamp": [2911.44, 2916.08], "text": " And you see the semantic similarity score of 0.54."}, {"timestamp": [2917.12, 2920.72], "text": " And yeah, so I think probably the next thing that I'll do"}, {"timestamp": [2920.72, 2923.56], "text": " is I might add a tab or another function"}, {"timestamp": [2923.56, 2925.68], "text": " where it's like, okay, you know, if you"}, {"timestamp": [2925.68, 2932.08], "text": " have the results that you want, like generate a literature review. But you see how fast this is"}, {"timestamp": [2932.08, 2938.64], "text": " and the quality of the results. And this is using a very low dimensional vector, relatively speaking,"}, {"timestamp": [2938.64, 2947.28], "text": " because 512 is pretty small. You can go up to 12,000 with DaVinci. That would be really expensive to embed 2.1 million things"}, {"timestamp": [2947.28, 2948.6], "text": " with DaVinci."}, {"timestamp": [2948.6, 2950.8], "text": " But anyway, so let's just grab this."}, {"timestamp": [2952.08, 2956.08], "text": " And so we'll do here, actually let's copy the,"}, {"timestamp": [2959.0, 2963.58], "text": " both the title and the embedding."}, {"timestamp": [2963.58, 2965.12], "text": " And then we'll just put this in our search."}, {"timestamp": [2967.04, 2975.6], "text": " Okay, so this gives us even higher results. So hadron spectroscopy structure. So if you put the"}, {"timestamp": [2975.6, 2981.84], "text": " title and the abstract for a paper, you're going to get all the most similar papers."}, {"timestamp": [2982.96, 2986.08], "text": " And so then it's just, you know, you saw how fast that was"}, {"timestamp": [2986.08, 2987.78], "text": " and you don't have to put in keywords."}, {"timestamp": [2987.78, 2989.76], "text": " You put in this big, huge thing and it will say,"}, {"timestamp": [2989.76, 2992.12], "text": " okay, let's find the payloads"}, {"timestamp": [2992.12, 2995.58], "text": " that are the most semantically similar,"}, {"timestamp": [2997.02, 2999.1], "text": " which could lead to like,"}, {"timestamp": [2999.1, 3002.48], "text": " this is just the first step in automated research, right?"}, {"timestamp": [3002.48, 3005.6], "text": " Because if you can find all the right research,"}, {"timestamp": [3005.6, 3008.3], "text": " then you can go from there."}, {"timestamp": [3008.3, 3012.72], "text": " There's obviously a few keywords like Hadron, Higgs, LHC,"}, {"timestamp": [3012.72, 3014.1], "text": " that keep popping up,"}, {"timestamp": [3014.1, 3016.72], "text": " but you see that one advantage that you get"}, {"timestamp": [3016.72, 3017.9], "text": " with semantic similarity"}, {"timestamp": [3017.9, 3019.9], "text": " is that you're not looking for keywords."}, {"timestamp": [3021.06, 3023.6], "text": " It will actually kind of get an understanding."}, {"timestamp": [3023.6, 3026.84], "text": " And with a low dimensional embedding like 512,"}, {"timestamp": [3026.84, 3027.96], "text": " you lose a lot, right?"}, {"timestamp": [3027.96, 3030.9], "text": " Because there's so much specific domain information,"}, {"timestamp": [3030.9, 3033.2], "text": " like the Randall-Sundrum model."}, {"timestamp": [3033.2, 3035.64], "text": " I guarantee you that a low dimensional model"}, {"timestamp": [3035.64, 3040.64], "text": " or a low dimensional embedding doesn't fully understand that"}, {"timestamp": [3040.84, 3043.94], "text": " but we keep talking about equations and motion"}, {"timestamp": [3043.94, 3046.32], "text": " and black holes and standard model."}, {"timestamp": [3046.32, 3053.76], "text": " And so all of these things go into creating that 512 dimension, excuse me, embedding."}, {"timestamp": [3053.76, 3054.76], "text": " I'm out of coffee."}, {"timestamp": [3054.76, 3055.76], "text": " That's unfortunate."}, {"timestamp": [3055.76, 3062.44], "text": " Um, anyways, so we're off to a good start and you see, uh, the top, you know, 0.7, sorry,"}, {"timestamp": [3062.44, 3067.68], "text": " the top 10, they start at 0.75 and go down to 0.62."}, {"timestamp": [3067.68, 3071.12], "text": " And so what you can do is easily just come in here"}, {"timestamp": [3071.12, 3076.12], "text": " to the search server and change the result query"}, {"timestamp": [3077.04, 3080.76], "text": " from limit to 10 to let's say 100."}, {"timestamp": [3080.76, 3085.0], "text": " And so let's do that and then restart our server here."}, {"timestamp": [3088.12, 3089.6], "text": " And so then, because obviously,"}, {"timestamp": [3089.6, 3091.32], "text": " if you're doing a literature review"}, {"timestamp": [3091.32, 3092.68], "text": " and you might be looking for things"}, {"timestamp": [3092.68, 3095.88], "text": " that are a little more distal from your core search,"}, {"timestamp": [3095.88, 3097.46], "text": " because if you're trying to branch out,"}, {"timestamp": [3097.46, 3100.0], "text": " you don't want to focus on just the things,"}, {"timestamp": [3100.0, 3101.1], "text": " just the closest cluster."}, {"timestamp": [3101.1, 3102.26], "text": " You want to like, okay,"}, {"timestamp": [3102.26, 3105.16], "text": " let's find everything that's vaguely related."}, {"timestamp": [3105.16, 3106.6], "text": " And so then let's do,"}, {"timestamp": [3108.0, 3111.68], "text": " let's see if OpenAI GPT-3."}, {"timestamp": [3114.28, 3116.98], "text": " And one thing that I found that's interesting is that,"}, {"timestamp": [3117.84, 3121.72], "text": " oh, the fewer search terms you use,"}, {"timestamp": [3121.72, 3123.76], "text": " the slower it is."}, {"timestamp": [3123.76, 3125.74], "text": " I think it's because it has to search further."}, {"timestamp": [3125.74, 3128.46], "text": " Whereas like if you have, if you've got a good vector,"}, {"timestamp": [3128.46, 3130.06], "text": " it's just really fast."}, {"timestamp": [3130.06, 3135.06], "text": " So let's do deep neural network computer vision."}, {"timestamp": [3135.26, 3136.18], "text": " So that's much faster."}, {"timestamp": [3136.18, 3137.88], "text": " Good Lord, that was faster."}, {"timestamp": [3137.88, 3138.72], "text": " Okay."}, {"timestamp": [3140.12, 3143.82], "text": " Okay, so then let's just grab a whole abstract"}, {"timestamp": [3143.82, 3145.74], "text": " from one of these guys."}, {"timestamp": [3145.74, 3148.2], "text": " And so then you scroll down and it's like, okay, cool."}, {"timestamp": [3148.2, 3149.98], "text": " We've got all kinds of stuff."}, {"timestamp": [3149.98, 3151.88], "text": " And you see that there's kind of,"}, {"timestamp": [3151.88, 3155.6], "text": " there's gonna be a whole lot that are at like 0.5, 0.4."}, {"timestamp": [3155.6, 3157.34], "text": " So you can just keep going."}, {"timestamp": [3158.48, 3159.4], "text": " Yeah."}, {"timestamp": [3159.4, 3162.84], "text": " And you'll find everything that is vaguely related"}, {"timestamp": [3162.84, 3165.88], "text": " and it's all kind of in the same space."}, {"timestamp": [3165.88, 3168.54], "text": " So there's a few things that we can do to improve this,"}, {"timestamp": [3168.54, 3169.62], "text": " which I'm not gonna do right now,"}, {"timestamp": [3169.62, 3171.72], "text": " I'm just doing this as a proof of concept,"}, {"timestamp": [3172.6, 3176.48], "text": " is you can use a higher dimensional vector,"}, {"timestamp": [3176.48, 3180.76], "text": " such as Ada from OpenAI, which is a 1024,"}, {"timestamp": [3181.64, 3183.8], "text": " or you can go up from there, right?"}, {"timestamp": [3183.8, 3185.24], "text": " But you're gonna spend a lot of money,"}, {"timestamp": [3185.24, 3188.6], "text": " whereas Universal Sentence Encoder version 5 is free"}, {"timestamp": [3188.6, 3189.44], "text": " and it's fast."}, {"timestamp": [3189.44, 3193.44], "text": " It only took like three hours to embed all of these."}, {"timestamp": [3195.0, 3196.76], "text": " And so then once you're on the page,"}, {"timestamp": [3196.76, 3199.2], "text": " you could search for, you know, OpenAI."}, {"timestamp": [3199.2, 3201.44], "text": " Okay, there's no papers from OpenAI here."}, {"timestamp": [3202.92, 3207.44], "text": " Yeah, but, and you see how fast it is and it's just, okay, cool. And I've got it"}, {"timestamp": [3207.44, 3212.4], "text": " nice and simple so it's easily readable. You've got the abstract here. You've got a link that's"}, {"timestamp": [3212.4, 3225.0], "text": " very obvious. I'll probably shorten this because that's a little bit of visual noise. So in terms of, we'll just do score."}, {"timestamp": [3228.0, 3230.0], "text": " Yeah, because you reduce visual noise"}, {"timestamp": [3230.0, 3231.6], "text": " so that it's easier to parse."}, {"timestamp": [3231.6, 3234.36], "text": " Because this, if it's the same thing, it doesn't matter."}, {"timestamp": [3234.36, 3236.54], "text": " What you really care about is this."}, {"timestamp": [3238.48, 3241.76], "text": " But you've got different sizes, colors, and shapes"}, {"timestamp": [3241.76, 3243.28], "text": " to really draw your eyes."}, {"timestamp": [3243.28, 3245.42], "text": " So you've got a really reliable pattern."}, {"timestamp": [3245.42, 3247.98], "text": " So this blue helps break up."}, {"timestamp": [3247.98, 3249.32], "text": " So, you know, like, okay, there's the link."}, {"timestamp": [3249.32, 3252.02], "text": " If I want that one, I just click there."}, {"timestamp": [3252.02, 3255.18], "text": " Honestly, actually, probably what I ought to do is have,"}, {"timestamp": [3255.18, 3256.3], "text": " ooh, I can simplify it."}, {"timestamp": [3256.3, 3258.74], "text": " We can do the title is the link"}, {"timestamp": [3258.74, 3260.1], "text": " and then just get rid of the link there"}, {"timestamp": [3260.1, 3261.58], "text": " because why duplicate it?"}, {"timestamp": [3261.58, 3263.22], "text": " Yeah, let's do that real quick."}, {"timestamp": [3263.22, 3286.0], "text": " Okay, so we will move this up And so instead, I guess here, do ahref."}, {"timestamp": [3288.0, 3293.0], "text": " Okay, and then we'll grab payload ID and title."}, {"timestamp": [3295.0, 3296.0], "text": " There we go."}, {"timestamp": [3297.0, 3301.0], "text": " And so now we'll have a little bit less noise."}, {"timestamp": [3302.0, 3306.1], "text": " So now it'll be, this will be the link."}, {"timestamp": [3306.5, 3308.68], "text": " So you just click on the title instead of having a separate"}, {"timestamp": [3308.7, 3309.6], "text": " link, which is smaller."}, {"timestamp": [3309.6, 3311.3], "text": " So that's a smaller target."}, {"timestamp": [3311.5, 3315.68], "text": " So I have title score and then the abstract and that's that's"}, {"timestamp": [3315.7, 3316.1], "text": " it."}, {"timestamp": [3316.1, 3316.6], "text": " So it's"}, {"timestamp": [3317.3, 3318.9], "text": " can't get much simpler than that."}, {"timestamp": [3319.7, 3323.1], "text": " And so this is the most like cut down basic search tool."}, {"timestamp": [3323.1, 3324.6], "text": " You can possibly have"}, {"timestamp": [3327.64, 3330.64], "text": " Okay. This is the most cut down basic search tool you can possibly have. Okay, so then let's go back in."}, {"timestamp": [3330.64, 3332.54], "text": " Let's wait for this to finish loading."}, {"timestamp": [3336.28, 3337.68], "text": " Come on, there we go."}, {"timestamp": [3337.68, 3340.04], "text": " Okay, so do a quick reload."}, {"timestamp": [3344.24, 3345.64], "text": " All right, cool."}, {"timestamp": [3345.64, 3349.48], "text": " So now you see the format's a little bit different where you've got this."}, {"timestamp": [3349.48, 3352.64], "text": " It's a little bit, it's not quite as pretty."}, {"timestamp": [3352.64, 3357.52], "text": " I might change the style so that it's, well, I don't know, I guess that's okay."}, {"timestamp": [3357.52, 3360.32], "text": " And then you got the score and then the abstract."}, {"timestamp": [3360.32, 3363.2], "text": " So there's a little bit less, I don't know if I like this as much."}, {"timestamp": [3363.2, 3365.5], "text": " I don't know, what do you think?"}, {"timestamp": [3367.54, 3370.0], "text": " It is nice to have the score just here"}, {"timestamp": [3370.0, 3371.64], "text": " so you can get a quick view of like,"}, {"timestamp": [3371.64, 3373.38], "text": " okay, how close is this?"}, {"timestamp": [3373.38, 3375.2], "text": " Image quality assessment."}, {"timestamp": [3375.2, 3377.14], "text": " I guess when you pay attention,"}, {"timestamp": [3377.14, 3379.08], "text": " I wonder if it's the underline"}, {"timestamp": [3380.2, 3382.94], "text": " that's making it a little harder to read."}, {"timestamp": [3382.94, 3384.84], "text": " Anyways, I'll fiddle around with it."}, {"timestamp": [3384.84, 3385.08], "text": " We've still got a couple hours left. So two and a quarter hours. that's making it a little harder to read. Anyways, I'll fiddle around with it."}, {"timestamp": [3385.08, 3387.4], "text": " We've still got a couple hours left,"}, {"timestamp": [3387.4, 3389.12], "text": " so two and a quarter hours,"}, {"timestamp": [3389.12, 3391.88], "text": " so two hours, 15 minutes of these uploading."}, {"timestamp": [3392.76, 3395.9], "text": " Probably one reason that we're not getting good results"}, {"timestamp": [3395.9, 3398.1], "text": " is because it's not done uploading."}, {"timestamp": [3398.1, 3400.8], "text": " So we're getting the best that we can with what's here."}, {"timestamp": [3402.72, 3407.84], "text": " But yeah, so, okay, I guess I'll stop the video here. We've got this. What have"}, {"timestamp": [3407.84, 3413.52], "text": " I been working on next? Oh, generate literature review. You know what? I'll probably just"}, {"timestamp": [3413.52, 3426.04], "text": " integrate the literature review into this server. I think that's what I'll do. Okay. Okay, we're still making progress."}, {"timestamp": [3426.04, 3433.72], "text": " However, I was looking at archives bulk download stuff and open access, yes, but they also"}, {"timestamp": [3433.72, 3441.44], "text": " ask users to play nice and be good with harvesting, so I'm not going to add a bulk download function"}, {"timestamp": [3441.44, 3446.4], "text": " to my web interface. This will run strictly locally because I mean,"}, {"timestamp": [3446.4, 3452.56], "text": " you know, they're providing a huge service for free. So let's not abuse the, you know,"}, {"timestamp": [3452.56, 3460.96], "text": " the free service. But what you can do is once you use this tool to find the papers that you want,"}, {"timestamp": [3460.96, 3465.56], "text": " you can go manually download them. And so what I've done is I have a"}, {"timestamp": [3465.56, 3470.64], "text": " folder, so you just accumulate whatever papers you want reviewed in this folder."}, {"timestamp": [3470.64, 3479.36], "text": " And so what I'm working on now is a script to do the literature"}, {"timestamp": [3479.36, 3484.24], "text": " review part separately. So I'll take that out of the web interface. It's pretty"}, {"timestamp": [3484.24, 3485.84], "text": " straightforward. So here's the that out of the web interface. It's pretty straightforward. So"}, {"timestamp": [3485.84, 3490.32], "text": " here's the prompt. It just says summarize the following paper for literature review paper,"}, {"timestamp": [3490.32, 3496.0], "text": " literature review summary, and this is what it looks like. So you take a big chunk and then you"}, {"timestamp": [3496.0, 3500.64], "text": " have here, and it does a really good job of just kind of distilling it down. So what I'm going to"}, {"timestamp": [3500.64, 3505.0], "text": " do is I'm going to break it into chunks. And well, here I'll just show you."}, {"timestamp": [3505.0, 3507.0], "text": " So I've got it here."}, {"timestamp": [3507.0, 3508.0], "text": " Let's see."}, {"timestamp": [3508.0, 3511.0], "text": " So we take out the print."}, {"timestamp": [3511.0, 3517.0], "text": " And so then we do chunks equals text wrap dot wrap."}, {"timestamp": [3517.0, 3522.0], "text": " And then we'll do paper and we'll say 6,000 because that's usually about the size that you can get."}, {"timestamp": [3522.0, 3527.8], "text": " And we'll say, okay, let's see, result equals that."}, {"timestamp": [3528.28, 3531.64], "text": " And so then we'll say for chunk in chunks,"}, {"timestamp": [3534.0, 3536.6], "text": " prompt equals open file,"}, {"timestamp": [3536.6, 3541.0], "text": " and we'll say prompt summary.txt,"}, {"timestamp": [3541.0, 3555.8], "text": " replace paper with chunk, and we'll do summary equals GPT-3 completion prompt,"}, {"timestamp": [3555.8, 3568.72], "text": " and so result equals result plus, and we'll say add a nice little space and we'll add summary. Okay, so then for each of these"}, {"timestamp": [3569.6, 3581.04], "text": " there will be a literature review for each paper and then we will just save it out to file."}, {"timestamp": [3582.56, 3589.28], "text": " save it out to file. OK, so probably the way that we'll make this look is we'll"}, {"timestamp": [3589.28, 3593.44], "text": " say output equals list."}, {"timestamp": [3593.44, 3602.24], "text": " And so then we'll have, so then we'll say info equals file."}, {"timestamp": [3602.24, 3610.64], "text": " And we'll do file and then summary equals result."}, {"timestamp": [3610.64, 3617.36], "text": " Okay, so that will be that and then output.append info."}, {"timestamp": [3617.36, 3622.96], "text": " And so then when we're finally done at the very end of all things,"}, {"timestamp": [3622.96, 3625.64], "text": " we'll save it out as a JSON."}, {"timestamp": [3627.52, 3629.52], "text": " Actually, no, let's not save it like that."}, {"timestamp": [3629.96, 3631.96], "text": " We will just do,"}, {"timestamp": [3633.68, 3635.68], "text": " we'll do,"}, {"timestamp": [3643.76, 3653.0], "text": " text equals, we'll do new line, new line, dot join, output."}, {"timestamp": [3653.0, 3663.72], "text": " No, because that'll still be formatted like this."}, {"timestamp": [3663.72, 3691.06], "text": " Why don't we do it this way instead then. So final output, we'll do new line, new line, and then we'll do, that'll be the file name,"}, {"timestamp": [3691.06, 3693.32], "text": " and then we'll also just do the summary."}, {"timestamp": [3693.32, 3694.88], "text": " Yeah, that'll work."}, {"timestamp": [3694.88, 3699.92], "text": " Okay, so then we'll do file and then resulting summary."}, {"timestamp": [3699.92, 3706.12], "text": " Yeah, because then you can do whatever you want, but the point is that it will give you"}, {"timestamp": [3706.12, 3710.12], "text": " kind of a compact summary with the file name."}, {"timestamp": [3710.12, 3713.72], "text": " And again, there's lots of little things we can do to clean this up."}, {"timestamp": [3713.72, 3720.24], "text": " Okay, so then save file, and then I have file path and then content."}, {"timestamp": [3720.24, 3728.08], "text": " So the file path will be literature review dot text and the content will be the output."}, {"timestamp": [3729.2, 3731.28], "text": " All right and then let's run this real quick."}, {"timestamp": [3734.4, 3741.12], "text": " So I've got five files that are related. Oh I guess it's not um yep I forgot to put the logs."}, {"timestamp": [3753.04, 3754.8], "text": " I forgot to put the logs. No! Okay, so first we will print file and then we will print"}, {"timestamp": [3760.4, 3762.8], "text": " result and we'll let it go. Okay, come back here."}, {"timestamp": [3767.0, 3772.5], "text": " GPT-3 logs do this every darn time."}, {"timestamp": [3777.5, 3782.5], "text": " All right, so I've got a handful of files that I want to generate a literature review for. And so what it's going to do is it's going to go through and extract the text from those"}, {"timestamp": [3782.5, 3785.28], "text": " PDFs and then break that, the whole chunk, or the whole PDF into chunks, and then it's going to go through and extract the text from those PDFs and then break that the whole"}, {"timestamp": [3785.28, 3790.32], "text": " chunk or the whole PDF into chunks and then it's going to do a quick summary for each of those."}, {"timestamp": [3791.2, 3798.8], "text": " Should be running in the background. Yeah, here we go. So summarize the paper. There we go."}, {"timestamp": [3801.6, 3805.56], "text": " So it is running. Oh, here we go."}, {"timestamp": [3806.56, 3807.56], "text": " Yeah."}, {"timestamp": [3809.52, 3814.2], "text": " And so what it'll do is it'll give me the file name and then the"}, {"timestamp": [3814.2, 3819.4], "text": " the full summary, which you can then use to just plug and play"}, {"timestamp": [3819.4, 3821.2], "text": " into your paper."}, {"timestamp": [3821.2, 3826.2], "text": " And obviously there's there's all kinds of formatting and official stuff, and I'm not"}, {"timestamp": [3826.2, 3833.44], "text": " a research scientist, but once you get this far, it's pretty easy to clean up. But yeah,"}, {"timestamp": [3833.44, 3842.64], "text": " I think we're almost done. So this is exciting. Now obviously, this is only halfway done,"}, {"timestamp": [3842.64, 3850.68], "text": " it's got an hour and a half left, but you can run this on your own to index all of archive yourself and so actually I was"}, {"timestamp": [3850.68, 3856.12], "text": " thinking through this this is a pretty useful service so tell me what you guys"}, {"timestamp": [3856.12, 3865.0], "text": " think should I should I set this up as a like an actual website, like,"}, {"timestamp": [3865.04, 3867.48], "text": " should I try and market this and monetize it?"}, {"timestamp": [3867.48, 3869.76], "text": " And would that be valuable?"}, {"timestamp": [3869.76, 3873.6], "text": " And maybe I could do a Kickstarter or something,"}, {"timestamp": [3873.6, 3874.84], "text": " just set this up."}, {"timestamp": [3874.84, 3877.44], "text": " And so then like, you just plug and play"}, {"timestamp": [3877.44, 3881.34], "text": " and you've got your own literature review engine."}, {"timestamp": [3881.34, 3883.64], "text": " So let me know what you think in the comments."}, {"timestamp": [3883.64, 3886.2], "text": " But yeah, I'm going to call this done."}, {"timestamp": [3886.2, 3892.24], "text": " We'll wait for this to finish spitting out its review."}]}