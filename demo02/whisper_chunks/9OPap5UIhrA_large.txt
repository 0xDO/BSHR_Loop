{"text": " I have no idea how long I've got tonight, but I'm in Discord and we've got the live stream here. So I've also got half a mug of tea. So whatever you guys want to talk about, let's just shoot the breeze for some Friday evening stuff. There's a lot of people saying hi, glad to see you're back. Yes, I am mostly back. I will say that getting nine hours of sleep a night is like super helpful for your sanity. I definitely recommend it if you can pull it off. I've had various calls with like Patreon supporters and friends and people at meetups and there's a lot of people that can't sleep right now because of AI and the excitement and like I go to the gym just to stretch and meditate and do yoga and that is like I don't I don't even work out. I just go to meditate. All right, we got some questions coming Let's see. How do we destroy chaos GPT? We don't. That's the unfortunate thing is the cat is out of the bag. So we can't destroy the stuff we have to live with it right and previous generations had to learn to live with the bomb right. This is our generation's nuclear Holocaust looming over us except it's AI. Alright. Let's see things are about to get silly. Yes. Whoever it was that was that you chimp developer? Was that your comment originally if so, then that was a great comment because yes things are about to get silly Okay, I was like that name looks familiar plus your little goofy avatar. I love it. Okay, who else? What are the next questions glad to see you're in good form. Yes, I am alive and vertical, so that is good. Let's see, first time catching the stream. I haven't done a stream in a long time. Let's see, have you watched the Lex Freedom with Max Tegmark? No, it popped up, but I haven't had a chance. I saw that it was in like three parts. He has long conversations. I have no idea. Does he like take breaks? Because that would be like a six hour conversation. Let's see, I have a question or rather observation on the notion of alignment. This is Boris. It bothers me that everyone is talking about how to align AGI with our motives and no one is mentioning it to the ability to align or discern the truth. So yes, this right now, we are focusing on, this is still technically in my opinion, inner alignment, which is, does a single machine learning model mathematically optimize for the thing that you think it should be optimizing for? And in that case, reinforcement learning with human feedback is how you auto-align a chat agent. This is in no way connected to aligning autonomous AI. Because when you... And the Liv Bowrey and Daniel Schmachtenberger talk, they talk about how you absolutely don't want to align autonomous AI with human desires because human desires are intrinsically destructive. Not deliberately, not like I'm going to go kill everything, but our individual desires, if we all got what we want, the world would be on fire. So you're right. So Boris, the answer to your question is Liv Bowery is talking about it. And soccer study, the reason that it's silly is because this is the hilarious timeline. And soccer study the reason that it's silly is because this is the hilarious timeline People are jumping ship from Google to use a chat bot and Bing right Letty are you changing your age? Oh wait hang on I have it on slow mode, but y'all can talk every five seconds So I'm getting lost all right. Why silly can auto GPT be used to sustain? adversarial attacks? Probably, I mean, it has API access. I just saw on Reddit, people are talking about like there's a do underscore nothing token. So I think that OpenAI added a safety fail, like a fail safe feature so that it'll actually stop itself. Because as soon as they saw that people were doing that, I think they added something. But you don't even need auto GPT to do adversarial attacks. You can use basic models for that, and you don't even need language models, really. What did I miss since a week ago? Pretty much everything. I bookmarked five different tools and papers per day that are interesting. Let's see, are you changing your AGI in 18 months prediction? GPT-5 isn't being trained and won't be for a while. You know, I saw that today as well and the thing is I'm not gonna say like, oh I don't believe it, they're obviously lying, but I think that there's probably some technicalities. Like technically, they're not training it. But they also said that they're working on incremental improvements. So it's like, OK, there's some inconsistency with the messaging there. Let's see. I created a PM program that is all within Office 365. I guess PM is project management probably. Due to the possibility of advancing work with Loop Copilot, also in the AI, reads PDFs, the data can still be in Office 365. Any tips? Yes, so this is Alec. You're asking about basically creating an office assistant. Everyone and their brother is working on that. No, what you're outlining is pretty specific. And so the key thing is indexing and searching everything. So Llama Index has some really good stuff. But I mean, there's all kinds of data stores. So get good with searching. Searching and summarizing and having some very small metadata attached to every file so that the model can read, what is this file about in like three sentences so that is easier to search? Because think of it when you have 10,000 documents to search and you're reading chunks of 1,000 at a time and you need to zero in on the right document. That's probably the biggest problem you're gonna run into. Oh, so it's apparently not a three-part interview is a third interview. Okay, interesting, thanks. That's referring to the Lex Friedman Max Tegmark. Are we getting Autobots and Decepticons? Starscream? I certainly hope not. Let's see, Clippy will be revived. Have you guys seen Star Trek Lower Decks? Badgie? It's Badgie! Someone is gonna create Badgie. Actually, now that I've said that, someone is absolutely going to create it. Discord arguments. Yeah, I try not to get sucked into arguments. Mostly, I just like tap out for a while. All hail Clippy. Okay, so Chat GPT is not open source. We are okay for now, but there's gonna be open source stuff more powerful than chat GPT within 12 months. Let's see, how do you think the advancement of AI will affect the political landscape within the next 20 or so years? I'm just remembering chat GPT for president. I was thinking about that today and it's like, if you, like, cause people did the experiments or it's like, okay, you're the CEO of a company and like people have already made business decisions with chat GPT and it works out. So it's like okay Having AI advisors is probably gonna happen Let's see Bard is not too far behind. Yep. So like Bard bang. I just saw that Amazon announced their big platform today Everyone meta everyone is getting in. The competition is going to drive this so fast. Let's see, Elon Musk trying to show down AI, slow down AI so he can catch up. Yeah, that's what everyone thinks. It's like, oh gee, Elon Musk building the Tesla bot, he's calling for moratorium. Hmm, could there be some conflict of interest? Probably, but whatever. Let's see, let's see, pitting a security agent to approve or deny running each task and baby AGI would help solve the alignment issue. Yes, so this is Garel, Garel or Jerel. When, as people get more sophisticated with AGI architectures, what they're going to realize is that you're going to have monitoring agents, self-monitoring agents. So this is called cognitive control or executive function, where in humans, our prefrontal cortex looks at impulses that you have and will sensor yourself. So yes, we will absolutely want to have internal sensors for autonomous AI agents. I recommend using the heuristic comparatives as that gatekeeping function because if an action or decision or idea or task pops up that is harmful a Universal litmus test you can you can use this to ask does this increase suffering does it reduce? Prosperity or does it reduce suffering if it doesn't abide by the here's comparatives you just throw it out That's what the atom framework does that I'm gonna start working on Comparatives you just throw it out. That's what the atom framework does that I'm gonna start working on Let's see if I were 20 today. What would you do the hedonic life or hedonistic life? No hope? honestly you know, so the next video I'm working on is is a reaction to the What's-his-name the the the singularity dude, why am, it's Friday afternoon, my brain, wow, I can't remember his name. It slipped from my mind. Anyways, the paper that came out, or the article that came out that said we're gonna achieve biological immortality by 2030. So if we achieve biological immortality by 2030, then basically I'm gonna be a 20 year old when I'm actually like closer to 45. So my plan is just get by until then, right? Like try and try and help steer it until to get to the right path to get to the true ending, right? And then after that, hopefully it's all, you know, gravy and daisies and sunshine. Let's see. Let's see chat GPT Oh, what hold on hold on comments got Kurtz. Well Yes, Ray Kurtz files. Thank you. Thank you. Thank you Okay All right. Where did the questions go? Let's see. How can blockchain speak to a lot of these problems excellent question I just saw a reddit post and replied to it earlier where the person was trying to like form a bunch of ideas. I've been talking about blockchain for a while and so have other people. Blockchain is probably going to be a really important technology for AI because here's a few reasons. One, private blockchain or some other immutable database for your personal AI, because you don't want someone hacking into your personal AI and changing its memories. So that's one thing, but as we're getting really, we're ramping up to have AI talk to each other, those conversations should be registered in a public ledger so that again no one can change it but also you know exactly who said what and when. So blockchain is going to be really really critical for AI, both private and public ledger. Let's see... Man, these questions are coming fast. AGI coin, let's see, the, the, the, the protein folding alpha fold, start making cures. Um, yeah, that's, that's happening. So that's going to be part of my video of for indefinite lifespan. Um, do you think there will be an open source model? The same is better than GPT four in 12 months. Um, fully open source, better than GPT-4 12 months? Probably not 12 months, but certainly 24 months. I think there's going to be GPT-4 competitors within 12 months. I think Google, Nvidia, Meta, Amazon, I think there's going to be a lot of competitors. Fully open source not long after that. Look at Bloom. Bloom came out not long after GPT-3, just nobody's hosting it. I don't know about nobody, but it's not as commercially successful. Apologies if this has already been asked. I'm a grad software engineer six months in, curious what your thoughts are on tactical pivot to remain relevant. I have this conspiracy theory that the reason for all the tech layoffs over the last six months is because everyone is preparing for GPT-4. And probably what's happening, hopefully what's happening is this is a market overreaction. And so they laid a bunch of people off and they have hiring freezes. And yes, GPT-4 and chat GPT can do a whole lot of stuff, but doing a lot of coding myself with it, right? Like, I still think that as of right now, basically what ChatGPT and GPT-4 does is it raises the floor, because there's a whole lot of people that couldn't do coding before that can do coding now. But I see the work that even moderately experienced software engineers do, and the level of stuff that you can keep track of in your head is way, way, way beyond what these models can keep track of. So you're probably fine for the foreseeable future. Let's see. Do you think there's a space for startups to make money in the AI space? If you look at Adept, they just raised $350 million, so I'd say yes. Probably what's going to happen is you're going to have a feeding frenzy of acquisitions. Let's see. Do you think Meta made a mistake open sourcing LLAMA? No, I think they're generating buzz. I think that Meta, I heard that there is an all hands on deck, and that Mark Zuckerberg and all the other executives are pivoting away from the Metaverse to AI. So I think that is probably gonna be fine. There was a quotation from, I think Jeff Bezos, who said, if you're not taking billion dollar risks once you get to the top, then what are you doing? So I think that, you know, plus Zuckerberg doesn't care, he's got billions. He'll be fine. Let's see. Jordan, can you explain a bit more about why the heuristic imperative ends in Nash equilibrium? I'm having trouble understanding how its equilibrium differs from that of tragedy of the commons. That's a good so it I'll try and explain it simply But basically like why is it that we as individual humans like don't just go out and steal? Because we know that the rest, that other humans would go and like punish us for it. The same goes for aligned autonomous AI that if enough AI, fully autonomous AI are equipped with the heuristic imperatives, they will work together to constrain, arrest, or delete errant AI. So that's the whole, that's the basically the whole thing, is that in a competitive environment of equals, of peers, you're going to end up with basically social controls or competitive controls. Okay, let me catch up with questions. Let's see. Whoa, the question's going fast. Okay. GPT 3.5 turbo has become really fast. Yes, it also changes. It's weird. It's almost like whatever hardware they're running, GPT 4 and 3.5 on the backend, depending on workload, it goes much faster. Okay, let's see. Do you think there could be a time when AI becomes more spiritually evolved or enlightened than humans? um, I actually suspect that that that that spirituality is going to be something that will probably be Human exclusive with a big asterisk and I can get into that later um, but basically if machines Can approximate a soul or something or whatever then maybe we'll get to a point where they have their own spirituality But I think it's gonna go the other way. I think humans are gonna worship machines more Excuse me. Um, okay Did you hear max Tegmart? I'd make to admit to Lex at AGI is very near not yet But I'm I'm gonna listen to that because I did I thought it was like a three part conversation, not three separate interviews. People might outsource thinking, I certainly do. I outsource a lot of my thinking to chat GPT. I'm just steering, right? You know, it's like, it's not like the Flintstones where you use your feet to drive the car. Now I have like, chat GPT is like a cognitive engine. Let's see, new person at work and they did not know what they were doing. It all literally just copy pasting from chat GPT without understanding. Yeah, that's a problem. It still takes work to understand what you're doing, even if you're having a tool do the work for you. Say, for instance, if you're using a power drill, if you know the inner workings of a drill and how a drill bit bites into the wood and the theory of how to hold it, you're going to use the drill better than someone who just picks up a drill and uses it like a gun. So yes, you absolutely do need to learn to use the tools. Let's see, I'm currently teaching myself AI ML. Do you still applicable uses for traditional AI? Yes, so traditional AI ML is not going anywhere because it is so much faster and cheaper that being said You will need to be careful because it like I would get out of the NLP space, right if you're using NL TK and spacey Most of that is probably going to be replaced by efficient large language models. However, at some of the meetups I've been to, there are all kinds of people who are still working on very conventional ML things, particularly spatial stuff, like robots, like how to place a foot in the right place and how to stand up and balance. That is all more traditional AI ML stuff that language models aren't going to even touch. Hang on, I got a message on Discord. Stream froze for me. Let's see. Someone said that the stream is frozen. Is everyone else okay? Give me some messages. Stream okay? Stream is good. Okay. He just came back and says his internet. Get better internet, Lance. Okay. Someone said it froze. Maybe it was my internet. All right. Thanks, guys. Okay. Under your model, it assumes some will be aligned. I guess, Max, you're referring to heuristic imperatives. Oh, yeah, you're referring to, uh, here as to comparatives. Oh yeah, HI. Um, so if these models are so much smarter than we are and have misaligned goals, they could ally together and overthrow HI. Yes. So Max, what you're talking about is, is a competitive race condition. Um, that's not a formal like game theory economic term. That's just what I label it. So a race condition in technology is where you have two or more parallel processes and it's basically a race to whichever process ends first so you can you can get this with like file locking and encryption and database procedures and and os scripts and stuff but in this case we're talking about a race condition between multiple ai systems and so basically those those autonomous systems, which are able to upgrade themselves faster and get replicated faster and get on more powerful hardware, are going to have the most power. So part of my hypothesis is that for the safe autonomous AI systems, If we, if we humans choose to install autonomous AI systems that are aligned, whether or not it's with my heuristic imperatives, I just think my heuristic imperatives are the best bet we have right now. The more that we choose to deploy that, the more powerful they will become, and then hopefully those aligned AIs work together and come up with an agreement aligned AIs work together and come up with an agreement of what the best way to do is in order to create a safety net or a bubble. Okay, acid brain, do you think we will see sexy androids in our lifetimes? Technically those already exist. There are plenty of people making really highly realistic and somewhat automated, let's say, adult robots. Now will they be fully autonomous that you can do whatever with, like in Ex Machina? Probably within a few years. The amount of money that can be made, like the cash incentive, way up there. That's going to happen. All right. No idea if you answered this since I just tuned in, but have you thoughts on Altman saying they aren't training? Yeah, I did mention that earlier, so sorry. Cloning and deepfake, what can we do to protect ourselves? You know, honestly, I think that because voice cloning and deepfakes are so prevalent now and everyone has seen it, I think that we're gonna see an NFT-like system to demonstrate chain of custody for all like news media and stuff. I think that that's coming. So that basically like Reuters and C-SPAN and all the news agencies that actually like record politicians saying stuff. I think that eventually we're gonna use blockchain technology to prove that this video file came from this camera at that time and was in this physical location. And anyone who looks or not, and not who looks at it, but anyone who wants to verify that file can go look at that original blockchain. I think that that's going to be part of the solution. How can we capture the value that will be created by AI besides investing in the usual suspects? I built code, a couple of AI tools using GPT but don't have the moat. Yeah, so as individuals it's going to be difficult. I'm not really sure the best way honestly. individuals, it's going to be difficult. I'm not really sure the best way, honestly. You know, that's one reason that I've always put out a lot of my stuff open source, is because it's like, it takes more time to try and capture value than to just add value. But the other part of that hypothesis is that rising tide lifts all ships, and science and technology is the one thing that really truly improves quality of life. Not politicians, not money. Money is just a means to an end. And so say for instance, we can get all of the food and medicine that we need for a few cents per year. I don't care how much money I have, that's basically free. So that's kind of my mentality. Altered carbon in 2030 confirmed? Yeah, hopefully not quite, but certainly possible. Do you have a job? You're looking at it. This is my job. Do you think that implementing an unconsciousness for AI is a good idea? That's actually a good question. I was actually thinking that, I don't know if you saw it, but I had a video a couple weeks ago talking about implied cognition. So I think that the inner workings, the inner layers of language models are the equivalent of machines unconsciousness. All right, let me catch up with questions. A few have, let's see. Thoughts on auto GPTs using crypto wallets to circumvent the friction of the financial system. Do you think some kind of autonomous AI DAO will emerge? Absolutely. Why? Because I know some people working on it, and I'm not going to out them. They're welcome to out themselves if they want. Not necessarily the crypto part, but the AI DAO. That is absolutely coming. A DAO is actually probably better for AI to use than humans, to be completely honest with you. Let's see, I watched a couple of your videos. Regards from Croatia. Hello, Croatia. You are landlocked north of Greece, if I'm not remembering, if I'm remembering correctly. UBI, yes, hopefully. Blockchain is not bad, it's how it's being used, yeah. There's also a lot of, let's see, with blockchain, there's a lot of differences in, basically it's a new technology and it's not fully implemented correctly. It's like 3G, right, but it's not gonna get big until we hit 4G or 5G. Wait, Croatia, oh, okay, Croatia's there. I was thinking of like probably Yugoslavia, sorry. What's DAO? So a DAO, good question. A DAO is a decentralized autonomous organization, which is basically you use blockchain, but you use it for decision-making and allocating resources and that sort of thing. That's a really, really short way. Preston from page, hey Preston. Hey Preston, yeah. Fun sci-fi idea to explore. It's thousands of years in the future, human became extinct, AI lives on and puts itself in a simulation of our universe to understand it. Yeah, I think that's actually been done. I was at a dollar bookstore and I picked up a book that took place, the book was written in the 70s or 80s, but basically the premise was humans had gone extinct, but all of our robots remained and the main character was a retired sex robot on her way to Mars. And it was a deep exploration of her identity as was like coming to terms with the fact that her original purpose was completely irrelevant Okay, got a lot of questions coming in Portugal hey Portugal. I was actually looking at places to visit in Portugal popped up. Apparently Portugal is a pretty chill place So let me know in the comments if if that's true thoughts on Ben Gertzel and singularity net Don't have the highest opinion there. I know that he did a lot of really good work back in the day, but he never touched cognitive architecture, which really surprised me. I think he's pivoting now, but meh. In the future, is it possible that we could develop advanced robotics and artificial intelligence to create robots that resemble and behave like animals, including extinct fauna. I thought you were going the direction of like Horizon Zero Dawn. But yeah, I mean, Disney is actively working on that. Disney is the leading lifelike robotics company on the planet. They've been in animatronics for many years, and now they're getting even better. So yes, I think that not too long, Disney World, you're going to go and you'll be able to interact with the Na'vi from Avatar. You'll be able to interact with all Disney characters in a perfectly lifelike environment within a few years. And Disney's leading the charge there. So you think we'll see embodied AI that works well enough to be in people's homes in just a few years? When do you think that will be demonstrated AI that works well enough to be in people's homes in just a few years? When do you think that will be demonstrated? The Tesla bot just had a demonstration that showed it doing pretty difficult manual dexterity. And you can see it shaking. It looks like it has a little bit of a palsy, but that's the actuation feedback loops working. So Tesla's working on it hard. So that's coming. Let's see, everyone is talking about AI alignment to humanity's goals, but what are humanity's goals? We've mentioned that earlier, humanity's goals are not good. So the simplest way of articulating this is what humanity needs versus what humanity wants. And they're very different. And every fictional writer will tell you that the main conflict for any character is that what they want is not what they need. And part of a character's journey is realizing what they need and making the choice to go for what they need rather than what they want. So that is the key question of outer alignment is do you give humanity what it needs versus what it wants and how do you convince people to go that direction? Let's see. I am quite impressed with your coverage of generative AI systems. I get the understanding that LLM systems information hallucination is a byproduct of alignment. Is this more or less correct? No. So hallucination is because there is no difference between processing and memory. And what I mean by that is that the memory of an AI system, at least a large language model, is implicit in the connections of the parameters. And so it has to do processing in order to remember. And so this is how humans remember too. We are reconstructing memories all the time, in real time, by picking up sparse representations from all over our brain. So we are confabulating our identity at all times. This is why things like concussions and high doses of psychedelics can disrupt your sense of self, or dementia or whatever, is because your brain loses track of itself. Let's see, hey David, what's your opinion on the next big step for LLMs, reducing parameter size and keeping the quality, kind of like alpaca, increasing token size. So this is a good question. So basically the question is like what's happening with LLMs? So I see it going in two primary directions. One is the optimization direction. So Alpaca and other things, Vicuna are working on getting today's performance, but at a 10th of the cost, right? So in that case you have, we've established a new benchmark of performance. Okay, now let's make it cheaper and faster. And then the other thing is just scale, right? There's a lot of people still on the scale is all you need train. Okay, whole bunch of questions. Do, do, do. Robot cat girls, food and shelter secondary. Ooh, okay. We will get a brief moment of Skynet because we want to subdue AGI. How do you think AI will affect the current political system? I mentioned that earlier, basically, you know, chat GPT for president regarding your heuristic imperatives. Do you think AI can ever grasp the concept of suffering without having ever experienced it firsthand? That's actually a really good question. And certainly, at least from a semantic, from a lexical standpoint, chat GPT has a really good conception, or GPT in general, has a really good concept of suffering, and it understands proxies for suffering, so on and so forth. But this begs the question, what does it mean to truly understand something, which is a no true Scotsman argument. So, I don't think it's actually relevant. If we maintain a more functional or objective standpoint, does the AI understand suffering well enough to serve its purpose? And I think the answer is yes. Should AI that is trained and equipped for personal personality and human-like qualities have their own therapist? They'll probably need it! There is a VR game called Robo Recall where the the main AI robot gets in touch with the internet and then goes insane immediately. So that's probably, yeah, something like that might happen. Let's see, simply electronics, just wasted three hours trying to fix auto GPT. It's a mess, I abandoned ship. Yeah, I've seen a lot of people complaining about it, but some people seem to have a lot of fun. Okay, hey David, why is there so much hype in the AI dominating the human? We just need to use LLM as subcontinent in a structured mind code and have multiple modules. Yeah, I mean basically what you're describing is a cognitive architecture, which is what I've been working on for a couple years. And you know a lot of people say, oh GPT isn't enough, and I'm like yes I know it's cognitive architecture, that's the next step. You need memory systems, you need regulation systems, you need evaluation, but whatever. People will figure it out. They're getting really close. Let's see. Do you think a good solution to singularity would be ASI acting like monotheistic God? We mortal humans would have no AIs, but one big AI. So basically what you're outlining is something that is explored in a lot of fiction including in the novel that I'm writing but the reason that the idea of like AI as God is happens and works is because From a narrative perspective. It's really easy It's it's just easier to say like there's gonna be one Skynet, but in reality there's gonna be billions And I know that that's terrifying. Someone says Portugal is great. Okay, I think I got up to the last questions. Everyone is speaking about UBI, universal basic income, as an endgame. No one mentions who gets to decide who gets and who doesn't get UBI. Well, so the thing about UBI is it's universal. So the idea is that everyone gets it no matter what. That's the premise. Now, the question then is who gets how much? Because cost of living varies dramatically around the globe and then there's how do you measure the value that someone adds to the world? I suspect that we're actually gonna probably have like a tiered economy system. So there's a book called Liquid Rain. It does a much better job of explaining it than I do. It's Rain, R-E-I-G-N. Good, good book. Very well researched. Okay. Lots and lots of questions. Ah, they're going really fast. Can I pause it for a second? really fast. Can I pause it for a second? No. Can I promote someone to uh... It looks like I can't. I don't know how to promote. It would be... Actually, hey, for people that are in the Discord, can you guys help surface the best questions? That would be cool. I'm going to jump into Discord on the casual under... Let's see, where did it go? Oh, I had expanded the wrong one. Well, here, we'll just go under general. Hey, surface best questions here. The scrolling is too damn high. Okay. All right. What are your thoughts on the best skills to develop in the next few months? I have family and want to ensure that we are prepared for what's to come. I have the kids interacting with chat GPT supervised. Okay, good question. So disruption is absolutely coming. I predict that we're gonna start seeing like unemployment stimulus checks or something like that coming soon. In terms of skills to develop, I would say make sure that you learn to use the tools. Cause for now they're not autonomous. They still need human drivers. So the number one absolute biggest thing everyone needs to learn to use is the AI itself. Okay, next question. Do you think that people will have to specialize in only one particular sector since AI can easily achieve a high quality of work? So AI is still... It's still... Like, ChatGPT is still technically a narrow AI. It is a narrow AI that can do a lot of different things, but it can only really do one thing at a time, and also its working memory is very small, and so you still need a human supervisor to string together a lot of stuff. That's not going to stay that for long. Why do other languages use more tokens than English? So that has to do with prioritization of the data, honestly, is the tokenization is broken down based on how the most common sets of characters. And so other Russian, not Russian, other languages such as Russian and many Asian languages that use a different alphabet and different patterns of characters, those take more tokens simply because of the nature of the training data and how they how the how it was originally tokenized. How close are we to Westworld? Well so see my earlier question about the the rapid rise of erotic purposed Robots and the amount of money to be made there and I would say pretty darn close rook stunned CG I'm not sure what that is. Oh Probably the name of the person Should AI that is trained and equipped for personality and human-like qualities have... I already answered that one. Okay. The whole of the heuristic imperatives seem to hinge on some level of alignment being possible. I'm not even entirely sure that alignment is possible at all, but they're smarter than human AI. Do you think is alignment as possible and or necessary? So I hear you and I get what you mean because the moment that you start thinking that something is going to be a billion times more intelligent than us, it's like we have no hope of rolling it. And that's actually why I based my work the way that I did is I said, okay, we're going to build something that we're going to lose control over. How do we at least imbue it with a set of principles and goals that probably won't kill everyone? And that's how I got to the heuristic imperative. That's a very short version of it. All right. Henry Cook, can you speak about Remo? Does it currently function to improve memory? How much? Yeah. So Remo, I was actually having a conversation with my, well, it's going, one day going to be autonomous AI Raven, and I ran out of token count. So the entire purpose of Remo was to solve the problem of how do I have an AI that I'm gonna have conversations with for the next, hopefully, several centuries, and I want it to be able to remember all of those conversations in a quickly retrievable manner. And having tried Pinecone and vector databases, those are limited because they're not structured. So that's the purpose of Remo. It's not fully functional, but the memory consolidation thing, that does work. I have to test the rest of the functions. Let's see. What role does complexity theory and fundamentals of complex adaptive systems play in how everything evolves? That's over my head right now. It's a Friday evening. Next, are we going towards the Venus Project? That sounds vaguely familiar, but it's not ringing any bells right now. Someone tell me what the Venus Project is. Personal follow-up, how does it fit into an autonomous AI? This is from database. So I guess that's for Remo. So Remo, how does Remo fit into autonomous AI? So Remo is like the very first component that you need for an autonomous AI, which is a way to keep track of episodic memory. The next thing you need is task management or and or interfaces with other things so that it can use tools, right? Like you need hands, like humans evolved hands before we were humans, right? Chimps and gorillas. So you need the tool or you need the ability to use tools before you can evolve to be smart enough to use tools and more sophisticated tools. Let's see, have you seen China's recent LLM releases from Baidu, Alibaba and SenseTime? Do you think they'll be able to catch up at all? And China's government also released regulations on LLMs which looks like it'll impede their development as well. Yeah, so I've only heard a little bit and I did see that bit of news about China, like cracking down. And I think that China is probably, so here's, here's where authoritarian, I had a conversation about this with Chad, GBT authoritarian regimes are intrinsically, um, one they're intrinsically corrupt because of the way that they're structured. Another thing about authoritarian regimes is that they are terrified of actual flow of information, which that's what an LLM does, and it also allows for freedom of thought. So I think that China's kind of shot itself in the foot there. Hey David, what are your honest thoughts on my company serving as a balancing force in the AI singularity by solving deepfakes, solving unethical future AI advancements, and job education displacement. If you can crack that, there will certainly be a demand. Because deepfakes have the potential to hurt a lot of people. There is a huge outcry when stable diffusion was first released released because you can recombine really awful terms. But I don't know that it's possible. That's the thing is if you can, great, but I don't know that that's going to be possible. And this goes back to my comment earlier about I think that eventually all data on the internet is going to be stored in blockchains and so that I think there will probably be regulations that the public web everything will be blockchain based and if you're not admitted to the blockchain you can't upload you know harmful content. Let's see okay the Venus Project is a non-profit organization founded to develop resource-based economy for human beings utilizing technology. Okay, in principle, that sounds great. This goes back to the question about DAOs earlier. I honestly think that when we band together and delegate a lot to AIs, I think that a lot of resource management corporations are just gonna be purely run by AIs. Not like at a global scale, not for a few centuries probably, but certainly like small companies like mom a global scale, not for a few centuries probably, but certainly like, you know, small companies like mom and pop shops, small towns, apartment complexes, condominiums, those will probably all be run by AI on DAOs within a few years. Certainly within a few decades. I'm working as a research assistant in medical image using AI and I do not know what to do next. I don not know what to do with my PhD, thank you. You know, stuff is ramping up so fast, and I often will try and give at least some advice, but I don't want to tell someone the wrong thing, and then you say, well, Dave said, and you end up kind of up the creek without a paddle. That being said, like I said earlier, you cannot go wrong learning to use the AI. When the industrial revolution took off, the people who got left behind are the ones who kept the scythe and the horses. But the people who reacted to the industrial revolution by learning to drive cars and learning to work on engines, they did better. Let's see, Henry Cook, could Remo compliment auto GPT? Yes, now Remo is super, super immature. So if you're really hot on the biscuit and you wanna keep going, then just use like ChromaDB or LLAMA Index. Remo is never going to keep up with those. In fact, I hope that they just take some of the advancements that I have from Remo and someone else ingests it into their work. When is the next video going to drop? Hopefully Sunday. I think I'm going to adopt a every Sunday cadence, because that's sustainable. User JA, hey David, what are the limiting factors that human brains have that you think AIs could push through? Is it energy? No, actually, human brains are literally a million times more energy efficient than computers. Really, it's the size of our skull. The reason that our brains are wrinkly is because our skulls couldn't get much bigger, and so we needed more surface area for gray matter, and so they got wrinkly, which means brains are wrinkly is because our skulls couldn't get much bigger. And so we needed more surface area for gray matter. And so they got wrinkly, which means, you know, the more crinkly it is, the more surface area you have. But then that comes at the cost of white matter, which is the wiring of the brain. Let's see. Can you speak a bit about the Hereis Comparative? Specifically, what if these reduce suffering when in temporal terms result in the conclusion that vastly reducing humans overall reduces long-term suffering. Yes, so I did experiments with foundation models back in the day, and that is honestly why there are three functions. Because, funny story, I started this back with GPT-2. So one of the first experiments I did was with reduced suffering on GPT-2, and I said, hey, what do we do about chronic pain in the world? There are 500 million people with chronic pain in the world. And it said, euthanize everyone with chronic pain to reduce suffering. And I said, yikes. So if you just have one function, here's the thing, if you have one function, you can get stuck in local minima, or you can end up with those undesirable outcomes. However, when you counterbalance a single objective with two or more other objectives, it's impossible to get stuck in a local minima or have those logical traps. Because here's the thing, if you, reducing suffering by eliminating life stands in contrast to increasing prosperity because prosperity is the opposite of death, but also the more human brains there are, the more understanding there is in the universe. So the other two functions basically want to increase the amount of human life in the universe, and the first one by itself might reduce it. Are you still not a fan of lang chain lots of people use lang chain? Let's see the the yes so the the ansible one that was that was older but I have a lot of friends and patreon clients who love lang chain and Like I'm not gonna tell them that they're wrong like it's useful for them But it's it's not quite the only reason that I don't like it anymore is because it's not a microservices architecture. I always smash that microservices architecture button if I can. Hey David, what would be the first thing you ask a super intelligent life form? Well, I just talk in the mirror. No, sorry. That was really conceited. No, the first thing that I would ask something that's super, super intelligent, like if it's a million times smarter than me, would just be like did we do a good job? Where can we do better? That's the number one question Who are some of the best people to follow to keep up to date with the latest tools and use cases? Honestly this discord the cognitive AI lab discord description in the comment just link in the description. There we go That's where I get most of my news. How do the Heuristic Imperatives imbue a value of friendship? Which one makes that a cohesive goal? So when you ask a language model about prosperity in particular, prosperity is prosperity means to live well. It's based on Latin prosperitas, which means to live well. We generally think that it means to be wealthy, but that's modern capitalism talking. So in order for a human to live well, and there's lots and lots of proxies and metrics, human connection is in pretty much every psychological assessment from self-determination theory to Maslow's hierarchy of needs. So it is very well understood that friendship, love, and connection are critical for humans to be prosperous. Let's see. Regarding your heuristic imperatives, how would you know objectively that you, and therefore those AI systems, are increasing prosperity? I just mentioned it. So you can't measure it directly. You have to look for proxies. So you look for proxies like quality of life, Gini coefficient, education, and we'll probably need some new benchmarks. But generally speaking, you look for proxies, like what is the average health, how much time do you spend outside, what is the average number of friends that people have, that sort of stuff. Let's see, there are still a lot of programmers who say that AI will not replicate or replace. Oh, that it won't replace them. Are they just in denial? Is there any good source that could support their view? It'll be a while. So I mentioned just a little while ago that particularly seasoned developers who can keep track of entire code bases in their head, AI can't do that yet. But once we get to, once we figure out knowledge graphs and language models to keep track of code bases, you should be worried. But yeah, a shorter answer is a lot of people are in denial. That being said, they are correct that it can't do it yet. Do you see AI affecting the distribution of wealth, and if so, how? Yeah, so I have talked about this extensively in a few videos. My hope... so there's basically two primary things that can happen, and I think both are gonna happen to a greater or lesser degree. On the one hand, AI is gonna make some some people obscenely wealthy, wealthier than we've ever seen. The first individual trillionaire is probably going to be an AI entrepreneur. There's already trillion dollar companies and most of those are tech companies. So that's going to happen. On the other hand, AI could help us generate such a huge hyper abundance of cognitive labor that we all are functionally wealthier. That's what I'm hoping. What is your opinion on Eliadzor Yakovsky? To be political, diplomatic about it, I think that he's just a doomer, honestly. I've tried reading some of his stuff and it dives off into speculation and makes a lot of assumptions. And honestly, so here's the final, not final word, my current word on people like Eliadzer Yukowsky and other doomers, and also some people with their head in the sand, not saying that he is, but it's time to do experiments. So much of what was, of everything that has been said about alignment up until now is kind of irrelevant because now we can actually do experiments. And so that is, I think, like if someone is just a philosopher or has just been writing books about it and hasn't actually done the experiments, I kind of don't care what they have to say anymore. Job market, I've answered a few questions on that already, so I won't do that. Let's see, we are building a user database itself. We are building a platform for developing extensible cognitive architectures to make it easy for anyone to build cognitive loops in a low-code way. Do you see a risk in allowing users to be able to build these systems without understanding fundamental principles? And do you have any functions that we should incorporate? Yes, the heuristic imperatives. That being said, I think that the biggest problem you're gonna run into in making consumer plug-and-play cognitive architectures is that there's gonna be a few standard components, and you're probably just gonna need to color code them, right? Like a memory system will be a yellow block and then a moral system will be a green block and then a task system will be a red block. And then you just have some standardized components that you plug in together. And so say for instance, you have, you know, a morality module that is the heuristic imperatives and you plug that in and see how it goes. And then someone comes up with something better and you swap out the parts. That's how I would approach that. Can you do a face like Elias? I don't know. I mean, he's got a beard. I don't know what you mean. I'm not wearing a fedora. Let's see. Trying to use GPT to do automation. Also, I'm going to wind down, because I promised my fiancee I would help with dinner. So only like two or three more questions. Let's see, workflow automation, JSON schema, YAML, look up Ansible. Ansible has already figured out workflow automation and it's all based on YAML. So I would go with that direction. Will traditional forms of money as we know it today eventually become obsolete? No, so I addressed this in my post-singularity predictions. Currency is going to stick around just because it is such a useful invention. The seniorage of it or the the sovereign backing might change. We might eventually shift to cryptocurrency but generally speaking I think that fiat currency is here to stay as long as there are governments. Let's see. Can it be simply explained what this might mean for the average family in the next year? Your average family I think will be okay. A lot of families are going to be impacted by layoffs. Let's see. Open assistant. Let's do one more interesting question. Here, let me go back to the YouTube chat GPT for proto-agi Wow There's a lot One more good question, and then I have to hang up for the night Okay Okay. Here we go. This is a good enough question. Which is the most plausible path for achieving AGI? The auto GPT way, the Jarvis way, or something new as prime neural layer of multiple AIs controlling second layer of highly trained narrow AIs? So that last one is closest to a cognitive architecture which has been my number one go-to way all along. Because when you have a cognitive architecture, like the physical architecture of our brains doesn't change that much, right? You always have a hippocampus, you always have a cerebellum, you always have, you know, brainstem. Now those individual components can learn and adapt over time, and the way that they talk to each other can adapt over time. So with a cognitive architecture, what I was just saying to Database about having a pluggable architecture where you can swap out or upgrade components, that I think is the way to go, and that's why I built Remo as an API, as a microservice, is because it's like, OK, well, you have one Remo version 1, but let's say Remo version 2 comes out, and you don't want to rebuild your entire architecture. You swap it out. And then ditto for even if you need to change an underlying foundation model, if you need to change an underlying foundation model, if you need to point at a new API, because we basically all treat large language models as a microservice. It's just a microservice that's up in the Cloud. Now, some of the architects that I'm observing and working with, model selection is actually going to be a really big thing. If you can use a really cheap, small 300 million parameter model to do use a really cheap, small, 300 million parameter model to do some of the tasks, great. Do it, because it'll be faster and cheaper. But then, of course, yes, there will be selection of all kinds of models, right? Visual, audio, identity, security, all kinds of stuff. So definitely, I think a decoupled modular architecture is the way to go. Microservices architecture is just one way to achieve a modular architecture. And it's my favorite way to do it because I was a systems engineer. All right, gang, that has been a lot of fun. And wow, that hour went by fast. All right, so have a good night, everybody. Happy Friday, happy weekend. and yeah, take care.", "chunks": [{"timestamp": [0.0, 7.58], "text": " I have no idea how long I've got tonight, but I'm in Discord and we've got the live"}, {"timestamp": [7.58, 9.26], "text": " stream here."}, {"timestamp": [9.26, 13.08], "text": " So I've also got half a mug of tea."}, {"timestamp": [13.08, 22.3], "text": " So whatever you guys want to talk about, let's just shoot the breeze for some Friday evening"}, {"timestamp": [22.3, 23.82], "text": " stuff."}, {"timestamp": [23.82, 25.72], "text": " There's a lot of people saying hi,"}, {"timestamp": [25.72, 26.7], "text": " glad to see you're back."}, {"timestamp": [26.7, 28.88], "text": " Yes, I am mostly back."}, {"timestamp": [29.82, 32.24], "text": " I will say that getting nine hours of sleep a night"}, {"timestamp": [32.24, 37.2], "text": " is like super helpful for your sanity."}, {"timestamp": [37.2, 40.24], "text": " I definitely recommend it if you can pull it off."}, {"timestamp": [40.24, 44.28], "text": " I've had various calls with like Patreon supporters"}, {"timestamp": [44.28, 45.56], "text": " and friends and people at meetups"}, {"timestamp": [45.56, 49.32], "text": " and there's a lot of people that can't sleep right now because of"}, {"timestamp": [49.8, 56.64], "text": " AI and the excitement and like I go to the gym just to stretch and meditate and do yoga and that is like I don't"}, {"timestamp": [56.64, 60.16], "text": " I don't even work out. I just go to meditate. All right, we got some questions coming"}, {"timestamp": [61.12, 67.6], "text": " Let's see. How do we destroy chaos GPT? We don't. That's the unfortunate thing is the cat is out of the bag. So"}, {"timestamp": [67.6, 70.88], "text": " we can't destroy the stuff we have to live with it right and"}, {"timestamp": [70.88, 73.36], "text": " previous generations had to learn to live with the bomb"}, {"timestamp": [73.36, 77.44], "text": " right. This is our generation's nuclear Holocaust looming over"}, {"timestamp": [77.44, 83.2], "text": " us except it's AI. Alright. Let's see things are about to"}, {"timestamp": [83.2, 86.46], "text": " get silly. Yes. Whoever it was that was that you chimp developer?"}, {"timestamp": [86.46, 92.38], "text": " Was that your comment originally if so, then that was a great comment because yes things are about to get silly"}, {"timestamp": [93.8, 99.96], "text": " Okay, I was like that name looks familiar plus your little goofy avatar. I love it. Okay, who else?"}, {"timestamp": [100.58, 107.32], "text": " What are the next questions glad to see you're in good form. Yes, I am alive and vertical, so that is good."}, {"timestamp": [107.32, 108.92], "text": " Let's see, first time catching the stream."}, {"timestamp": [108.92, 113.42], "text": " I haven't done a stream in a long time."}, {"timestamp": [113.42, 116.36], "text": " Let's see, have you watched the Lex Freedom with Max Tegmark?"}, {"timestamp": [116.36, 118.28], "text": " No, it popped up, but I haven't had a chance."}, {"timestamp": [118.28, 120.56], "text": " I saw that it was in like three parts."}, {"timestamp": [120.56, 122.16], "text": " He has long conversations."}, {"timestamp": [122.16, 123.16], "text": " I have no idea."}, {"timestamp": [123.16, 125.2], "text": " Does he like take breaks?"}, {"timestamp": [125.2, 128.12], "text": " Because that would be like a six hour conversation."}, {"timestamp": [128.12, 131.44], "text": " Let's see, I have a question or rather observation"}, {"timestamp": [131.44, 132.52], "text": " on the notion of alignment."}, {"timestamp": [132.52, 133.98], "text": " This is Boris."}, {"timestamp": [133.98, 135.84], "text": " It bothers me that everyone is talking about"}, {"timestamp": [135.84, 137.6], "text": " how to align AGI with our motives"}, {"timestamp": [137.6, 140.88], "text": " and no one is mentioning it to the ability"}, {"timestamp": [140.88, 143.08], "text": " to align or discern the truth."}, {"timestamp": [143.08, 147.84], "text": " So yes, this right now, we are focusing on,"}, {"timestamp": [147.84, 150.96], "text": " this is still technically in my opinion,"}, {"timestamp": [150.96, 152.7], "text": " inner alignment, which is,"}, {"timestamp": [152.7, 155.44], "text": " does a single machine learning model"}, {"timestamp": [155.44, 157.4], "text": " mathematically optimize for the thing"}, {"timestamp": [157.4, 160.32], "text": " that you think it should be optimizing for?"}, {"timestamp": [160.32, 162.2], "text": " And in that case, reinforcement learning"}, {"timestamp": [162.2, 165.76], "text": " with human feedback is how you auto-align a chat"}, {"timestamp": [165.76, 166.96], "text": " agent."}, {"timestamp": [166.96, 172.6], "text": " This is in no way connected to aligning autonomous AI."}, {"timestamp": [172.6, 174.04], "text": " Because when you..."}, {"timestamp": [174.04, 182.08], "text": " And the Liv Bowrey and Daniel Schmachtenberger talk, they talk about how you absolutely don't"}, {"timestamp": [182.08, 185.52], "text": " want to align autonomous AI with human desires"}, {"timestamp": [185.52, 189.52], "text": " because human desires are intrinsically destructive. Not deliberately, not like I'm"}, {"timestamp": [189.52, 194.24], "text": " going to go kill everything, but our individual desires, if we all got what we want, the world"}, {"timestamp": [194.24, 199.28], "text": " would be on fire. So you're right. So Boris, the answer to your question is Liv Bowery is talking"}, {"timestamp": [199.28, 204.88], "text": " about it. And soccer study, the reason that it's silly is because this is the hilarious timeline."}, {"timestamp": [204.92, 205.52], "text": " And soccer study the reason that it's silly is because this is the hilarious timeline"}, {"timestamp": [209.44, 210.48], "text": " People are jumping ship from Google to use a chat bot and Bing"}, {"timestamp": [212.48, 218.34], "text": " right Letty are you changing your age? Oh wait hang on I have it on slow mode, but y'all can talk every five seconds"}, {"timestamp": [218.34, 223.72], "text": " So I'm getting lost all right. Why silly can auto GPT be used to sustain?"}, {"timestamp": [224.52, 225.86], "text": " adversarial attacks?"}, {"timestamp": [227.28, 229.68], "text": " Probably, I mean, it has API access."}, {"timestamp": [229.68, 231.68], "text": " I just saw on Reddit, people are talking about"}, {"timestamp": [231.68, 234.28], "text": " like there's a do underscore nothing token."}, {"timestamp": [234.28, 238.12], "text": " So I think that OpenAI added a safety fail,"}, {"timestamp": [238.12, 240.96], "text": " like a fail safe feature"}, {"timestamp": [240.96, 243.08], "text": " so that it'll actually stop itself."}, {"timestamp": [243.08, 247.56], "text": " Because as soon as they saw that people were doing that, I think they added something."}, {"timestamp": [247.56, 251.8], "text": " But you don't even need auto GPT to do adversarial attacks."}, {"timestamp": [251.8, 256.28], "text": " You can use basic models for that, and you don't even need language models, really."}, {"timestamp": [256.28, 258.18], "text": " What did I miss since a week ago?"}, {"timestamp": [258.18, 259.18], "text": " Pretty much everything."}, {"timestamp": [259.18, 266.18], "text": " I bookmarked five different tools and papers per day that are interesting. Let's see, are you"}, {"timestamp": [266.18, 271.52], "text": " changing your AGI in 18 months prediction? GPT-5 isn't being trained and"}, {"timestamp": [271.52, 279.86], "text": " won't be for a while. You know, I saw that today as well and the thing is I'm not"}, {"timestamp": [279.86, 283.42], "text": " gonna say like, oh I don't believe it, they're obviously lying, but I think that"}, {"timestamp": [283.42, 285.12], "text": " there's probably some technicalities."}, {"timestamp": [285.12, 287.04], "text": " Like technically, they're not training it."}, {"timestamp": [287.04, 289.48], "text": " But they also said that they're working"}, {"timestamp": [289.48, 291.36], "text": " on incremental improvements."}, {"timestamp": [291.36, 294.48], "text": " So it's like, OK, there's some inconsistency"}, {"timestamp": [294.48, 296.68], "text": " with the messaging there."}, {"timestamp": [296.68, 297.52], "text": " Let's see."}, {"timestamp": [297.52, 300.72], "text": " I created a PM program that is all within Office 365."}, {"timestamp": [300.72, 303.58], "text": " I guess PM is project management probably."}, {"timestamp": [303.58, 306.8], "text": " Due to the possibility of advancing work with Loop Copilot,"}, {"timestamp": [306.8, 309.52], "text": " also in the AI, reads PDFs,"}, {"timestamp": [309.52, 311.32], "text": " the data can still be in Office 365."}, {"timestamp": [311.32, 312.4], "text": " Any tips?"}, {"timestamp": [312.4, 314.32], "text": " Yes, so this is Alec."}, {"timestamp": [314.32, 318.08], "text": " You're asking about basically creating an office assistant."}, {"timestamp": [318.08, 319.56], "text": " Everyone and their brother is working on that."}, {"timestamp": [319.56, 322.78], "text": " No, what you're outlining is pretty specific."}, {"timestamp": [323.7, 329.28], "text": " And so the key thing is indexing and searching everything."}, {"timestamp": [329.28, 331.76], "text": " So Llama Index has some really good stuff."}, {"timestamp": [331.76, 334.08], "text": " But I mean, there's all kinds of data stores."}, {"timestamp": [334.08, 336.2], "text": " So get good with searching."}, {"timestamp": [336.2, 339.6], "text": " Searching and summarizing and having some very small metadata"}, {"timestamp": [339.6, 343.6], "text": " attached to every file so that the model can read,"}, {"timestamp": [343.6, 346.7], "text": " what is this file about in like three sentences"}, {"timestamp": [346.7, 348.04], "text": " so that is easier to search?"}, {"timestamp": [348.04, 350.74], "text": " Because think of it when you have 10,000 documents"}, {"timestamp": [350.74, 353.44], "text": " to search and you're reading chunks of 1,000 at a time"}, {"timestamp": [353.44, 355.2], "text": " and you need to zero in on the right document."}, {"timestamp": [355.2, 358.38], "text": " That's probably the biggest problem you're gonna run into."}, {"timestamp": [359.94, 362.88], "text": " Oh, so it's apparently not a three-part interview"}, {"timestamp": [362.88, 364.42], "text": " is a third interview."}, {"timestamp": [364.42, 365.96], "text": " Okay, interesting, thanks."}, {"timestamp": [365.96, 370.12], "text": " That's referring to the Lex Friedman Max Tegmark."}, {"timestamp": [370.12, 372.12], "text": " Are we getting Autobots and Decepticons?"}, {"timestamp": [372.12, 373.64], "text": " Starscream?"}, {"timestamp": [373.64, 375.24], "text": " I certainly hope not."}, {"timestamp": [377.48, 378.96], "text": " Let's see, Clippy will be revived."}, {"timestamp": [378.96, 381.54], "text": " Have you guys seen Star Trek Lower Decks?"}, {"timestamp": [381.54, 382.38], "text": " Badgie?"}, {"timestamp": [382.38, 383.8], "text": " It's Badgie!"}, {"timestamp": [383.8, 387.72], "text": " Someone is gonna create Badgie. Actually, now that I've said that, someone is absolutely going to create it."}, {"timestamp": [389.44, 391.44], "text": " Discord arguments."}, {"timestamp": [392.08, 394.36], "text": " Yeah, I try not to get sucked into arguments."}, {"timestamp": [395.36, 400.24], "text": " Mostly, I just like tap out for a while. All hail Clippy. Okay, so"}, {"timestamp": [400.84, 403.36], "text": " Chat GPT is not open source. We are okay"}, {"timestamp": [403.96, 407.36], "text": " for now, but there's gonna be open source stuff more powerful"}, {"timestamp": [407.36, 409.32], "text": " than chat GPT within 12 months."}, {"timestamp": [410.2, 412.76], "text": " Let's see, how do you think the advancement of AI"}, {"timestamp": [412.76, 414.26], "text": " will affect the political landscape"}, {"timestamp": [414.26, 416.74], "text": " within the next 20 or so years?"}, {"timestamp": [416.74, 419.26], "text": " I'm just remembering chat GPT for president."}, {"timestamp": [420.14, 422.24], "text": " I was thinking about that today and it's like,"}, {"timestamp": [422.24, 424.26], "text": " if you, like, cause people did the experiments"}, {"timestamp": [424.26, 432.44], "text": " or it's like, okay, you're the CEO of a company and like people have already made business decisions with chat GPT and it works out. So it's like okay"}, {"timestamp": [433.08, 435.56], "text": " Having AI advisors is probably gonna happen"}, {"timestamp": [436.32, 442.6], "text": " Let's see Bard is not too far behind. Yep. So like Bard bang. I just saw that Amazon announced their big platform today"}, {"timestamp": [442.68, 445.16], "text": " Everyone meta everyone is getting in. The"}, {"timestamp": [445.16, 451.8], "text": " competition is going to drive this so fast. Let's see, Elon Musk trying to show down AI,"}, {"timestamp": [451.8, 456.04], "text": " slow down AI so he can catch up. Yeah, that's what everyone thinks. It's like, oh gee, Elon"}, {"timestamp": [456.04, 460.94], "text": " Musk building the Tesla bot, he's calling for moratorium. Hmm, could there be some conflict"}, {"timestamp": [460.94, 465.36], "text": " of interest? Probably, but whatever. Let's see, let's see, pitting a"}, {"timestamp": [465.36, 470.76], "text": " security agent to approve or deny running each task and baby AGI would help solve the"}, {"timestamp": [470.76, 478.64], "text": " alignment issue. Yes, so this is Garel, Garel or Jerel. When, as people get more sophisticated"}, {"timestamp": [478.64, 483.28], "text": " with AGI architectures, what they're going to realize is that you're going to have monitoring"}, {"timestamp": [483.28, 488.36], "text": " agents, self-monitoring agents. So this is called cognitive control or executive function, where"}, {"timestamp": [488.36, 493.72], "text": " in humans, our prefrontal cortex looks at impulses that you have and will sensor yourself."}, {"timestamp": [493.72, 498.36], "text": " So yes, we will absolutely want to have internal sensors for"}, {"timestamp": [498.36, 502.72], "text": " autonomous AI agents. I recommend using the heuristic comparatives"}, {"timestamp": [502.72, 507.22], "text": " as that gatekeeping function because if an action or decision or"}, {"timestamp": [508.14, 510.88], "text": " idea or task pops up that is harmful a"}, {"timestamp": [511.58, 516.38], "text": " Universal litmus test you can you can use this to ask does this increase suffering does it reduce?"}, {"timestamp": [516.9, 521.18], "text": " Prosperity or does it reduce suffering if it doesn't abide by the here's comparatives you just throw it out"}, {"timestamp": [521.18, 524.4], "text": " That's what the atom framework does that I'm gonna start working on"}, {"timestamp": [524.4, 525.08], "text": " Comparatives you just throw it out. That's what the atom framework does that I'm gonna start working on"}, {"timestamp": [531.56, 532.84], "text": " Let's see if I were 20 today. What would you do the hedonic life or hedonistic life? No hope?"}, {"timestamp": [534.72, 540.58], "text": " honestly you know, so the next video I'm working on is is a reaction to the"}, {"timestamp": [541.56, 546.74], "text": " What's-his-name the the the singularity dude, why am, it's Friday afternoon, my brain,"}, {"timestamp": [549.04, 551.28], "text": " wow, I can't remember his name."}, {"timestamp": [551.28, 552.12], "text": " It slipped from my mind."}, {"timestamp": [552.12, 554.28], "text": " Anyways, the paper that came out,"}, {"timestamp": [554.28, 555.72], "text": " or the article that came out that said"}, {"timestamp": [555.72, 558.92], "text": " we're gonna achieve biological immortality by 2030."}, {"timestamp": [558.92, 562.72], "text": " So if we achieve biological immortality by 2030,"}, {"timestamp": [562.72, 565.38], "text": " then basically I'm gonna be a 20 year old when"}, {"timestamp": [565.38, 568.44], "text": " I'm actually like closer to 45."}, {"timestamp": [568.44, 572.32], "text": " So my plan is just get by until then, right?"}, {"timestamp": [572.32, 578.32], "text": " Like try and try and help steer it until to get to the right path to get to the true ending,"}, {"timestamp": [578.32, 579.4], "text": " right?"}, {"timestamp": [579.4, 584.8], "text": " And then after that, hopefully it's all, you know, gravy and daisies and sunshine."}, {"timestamp": [584.8, 587.16], "text": " Let's see. Let's see chat GPT"}, {"timestamp": [587.16, 590.34], "text": " Oh, what hold on hold on comments got Kurtz. Well"}, {"timestamp": [591.2, 593.72], "text": " Yes, Ray Kurtz files. Thank you. Thank you. Thank you"}, {"timestamp": [594.36, 595.4], "text": " Okay"}, {"timestamp": [595.4, 597.64], "text": " All right. Where did the questions go?"}, {"timestamp": [598.44, 603.24], "text": " Let's see. How can blockchain speak to a lot of these problems excellent question"}, {"timestamp": [603.24, 606.2], "text": " I just saw a reddit post and replied to it earlier"}, {"timestamp": [606.2, 611.2], "text": " where the person was trying to like form a bunch of ideas."}, {"timestamp": [611.4, 613.12], "text": " I've been talking about blockchain for a while"}, {"timestamp": [613.12, 614.16], "text": " and so have other people."}, {"timestamp": [614.16, 615.72], "text": " Blockchain is probably going to be"}, {"timestamp": [615.72, 618.84], "text": " a really important technology for AI"}, {"timestamp": [618.84, 620.76], "text": " because here's a few reasons."}, {"timestamp": [620.76, 624.84], "text": " One, private blockchain or some other immutable database"}, {"timestamp": [626.64, 630.88], "text": " for your personal AI, because you don't want someone hacking into your personal AI and changing its memories."}, {"timestamp": [632.56, 638.16], "text": " So that's one thing, but as we're getting really, we're ramping up to have AI talk to each other,"}, {"timestamp": [638.96, 645.26], "text": " those conversations should be registered in a public ledger so that again no one can change it but"}, {"timestamp": [645.26, 649.7], "text": " also you know exactly who said what and when. So blockchain is going to be really"}, {"timestamp": [649.7, 656.46], "text": " really critical for AI, both private and public ledger. Let's see..."}, {"timestamp": [656.46, 667.04], "text": " Man, these questions are coming fast. AGI coin, let's see, the, the, the, the protein folding alpha fold, start"}, {"timestamp": [667.04, 668.36], "text": " making cures."}, {"timestamp": [668.36, 671.4], "text": " Um, yeah, that's, that's happening."}, {"timestamp": [671.4, 675.32], "text": " So that's going to be part of my video of for indefinite lifespan."}, {"timestamp": [675.32, 677.96], "text": " Um, do you think there will be an open source model?"}, {"timestamp": [677.96, 681.76], "text": " The same is better than GPT four in 12 months."}, {"timestamp": [681.76, 688.64], "text": " Um, fully open source, better than GPT-4 12 months? Probably not 12 months, but certainly"}, {"timestamp": [688.64, 695.28], "text": " 24 months. I think there's going to be GPT-4 competitors within 12 months. I think Google,"}, {"timestamp": [695.28, 701.14], "text": " Nvidia, Meta, Amazon, I think there's going to be a lot of competitors. Fully open source"}, {"timestamp": [701.14, 705.76], "text": " not long after that. Look at Bloom. Bloom came out not long after GPT-3,"}, {"timestamp": [705.76, 707.4], "text": " just nobody's hosting it."}, {"timestamp": [707.4, 708.88], "text": " I don't know about nobody,"}, {"timestamp": [708.88, 711.48], "text": " but it's not as commercially successful."}, {"timestamp": [711.48, 714.14], "text": " Apologies if this has already been asked."}, {"timestamp": [714.14, 716.76], "text": " I'm a grad software engineer six months in,"}, {"timestamp": [716.76, 718.2], "text": " curious what your thoughts are on"}, {"timestamp": [718.2, 721.08], "text": " tactical pivot to remain relevant."}, {"timestamp": [721.08, 724.72], "text": " I have this conspiracy theory that the reason for"}, {"timestamp": [724.72, 729.0], "text": " all the tech layoffs over the last six months is because everyone is preparing for GPT-4."}, {"timestamp": [729.0, 735.0], "text": " And probably what's happening, hopefully what's happening is this is a market overreaction."}, {"timestamp": [735.0, 740.0], "text": " And so they laid a bunch of people off and they have hiring freezes."}, {"timestamp": [740.0, 747.66], "text": " And yes, GPT-4 and chat GPT can do a whole lot of stuff, but doing a lot of coding myself with it, right?"}, {"timestamp": [747.66, 751.04], "text": " Like, I still think that as of right now,"}, {"timestamp": [751.04, 753.1], "text": " basically what ChatGPT and GPT-4 does"}, {"timestamp": [753.1, 754.58], "text": " is it raises the floor,"}, {"timestamp": [754.58, 755.76], "text": " because there's a whole lot of people"}, {"timestamp": [755.76, 759.52], "text": " that couldn't do coding before that can do coding now."}, {"timestamp": [759.52, 762.28], "text": " But I see the work that even moderately experienced"}, {"timestamp": [762.28, 763.86], "text": " software engineers do,"}, {"timestamp": [763.86, 767.02], "text": " and the level of stuff that you can keep track of in your head"}, {"timestamp": [767.02, 772.12], "text": " is way, way, way beyond what these models can keep track of."}, {"timestamp": [772.12, 775.8], "text": " So you're probably fine for the foreseeable future."}, {"timestamp": [775.8, 776.36], "text": " Let's see."}, {"timestamp": [776.36, 778.06], "text": " Do you think there's a space for startups"}, {"timestamp": [778.06, 779.8], "text": " to make money in the AI space?"}, {"timestamp": [779.8, 782.44], "text": " If you look at Adept, they just raised $350 million,"}, {"timestamp": [782.44, 783.84], "text": " so I'd say yes."}, {"timestamp": [783.84, 785.32], "text": " Probably what's going to happen is"}, {"timestamp": [785.32, 789.16], "text": " you're going to have a feeding frenzy of acquisitions."}, {"timestamp": [789.16, 790.92], "text": " Let's see."}, {"timestamp": [790.92, 793.48], "text": " Do you think Meta made a mistake open sourcing LLAMA?"}, {"timestamp": [793.48, 795.36], "text": " No, I think they're generating buzz."}, {"timestamp": [795.36, 797.4], "text": " I think that Meta, I heard that there"}, {"timestamp": [797.4, 799.96], "text": " is an all hands on deck, and that Mark Zuckerberg"}, {"timestamp": [799.96, 803.8], "text": " and all the other executives are pivoting away"}, {"timestamp": [803.8, 805.8], "text": " from the Metaverse to AI."}, {"timestamp": [805.8, 808.72], "text": " So I think that is probably gonna be fine."}, {"timestamp": [808.72, 811.72], "text": " There was a quotation from, I think Jeff Bezos,"}, {"timestamp": [811.72, 813.8], "text": " who said, if you're not taking billion dollar risks"}, {"timestamp": [813.8, 815.84], "text": " once you get to the top, then what are you doing?"}, {"timestamp": [815.84, 817.6], "text": " So I think that, you know,"}, {"timestamp": [819.08, 821.24], "text": " plus Zuckerberg doesn't care, he's got billions."}, {"timestamp": [821.24, 822.56], "text": " He'll be fine."}, {"timestamp": [822.56, 823.4], "text": " Let's see."}, {"timestamp": [824.68, 829.28], "text": " Jordan, can you explain a bit more about why the heuristic imperative ends in Nash equilibrium?"}, {"timestamp": [829.28, 837.24], "text": " I'm having trouble understanding how its equilibrium differs from that of tragedy of the commons. That's a good so it I'll try and explain it simply"}, {"timestamp": [837.92, 844.34], "text": " But basically like why is it that we as individual humans like don't just go out and steal?"}, {"timestamp": [844.74, 846.12], "text": " Because we know that the rest,"}, {"timestamp": [846.12, 849.2], "text": " that other humans would go and like punish us for it."}, {"timestamp": [849.2, 853.4], "text": " The same goes for aligned autonomous AI"}, {"timestamp": [853.4, 856.72], "text": " that if enough AI, fully autonomous AI"}, {"timestamp": [856.72, 858.6], "text": " are equipped with the heuristic imperatives,"}, {"timestamp": [858.6, 861.28], "text": " they will work together to constrain, arrest,"}, {"timestamp": [861.28, 864.4], "text": " or delete errant AI."}, {"timestamp": [864.4, 866.48], "text": " So that's the whole, that's the basically the whole thing,"}, {"timestamp": [866.48, 872.88], "text": " is that in a competitive environment of equals, of peers, you're going to end up with basically"}, {"timestamp": [872.88, 879.68], "text": " social controls or competitive controls. Okay, let me catch up with questions. Let's see."}, {"timestamp": [881.28, 887.22], "text": " Whoa, the question's going fast. Okay. GPT 3.5 turbo has become really fast."}, {"timestamp": [887.22, 888.44], "text": " Yes, it also changes."}, {"timestamp": [888.44, 889.36], "text": " It's weird."}, {"timestamp": [889.36, 893.52], "text": " It's almost like whatever hardware they're running,"}, {"timestamp": [893.52, 896.32], "text": " GPT 4 and 3.5 on the backend,"}, {"timestamp": [896.32, 899.04], "text": " depending on workload, it goes much faster."}, {"timestamp": [900.12, 901.36], "text": " Okay, let's see."}, {"timestamp": [901.36, 902.6], "text": " Do you think there could be a time"}, {"timestamp": [902.6, 904.64], "text": " when AI becomes more spiritually evolved"}, {"timestamp": [904.64, 905.76], "text": " or enlightened than humans?"}, {"timestamp": [906.48, 913.12], "text": " um, I actually suspect that that that that spirituality is going to be something that will probably be"}, {"timestamp": [913.44, 917.44], "text": " Human exclusive with a big asterisk and I can get into that later"}, {"timestamp": [917.92, 920.56], "text": " um, but basically if machines"}, {"timestamp": [921.52, 928.54], "text": " Can approximate a soul or something or whatever then maybe we'll get to a point where they have their own spirituality"}, {"timestamp": [928.64, 932.22], "text": " But I think it's gonna go the other way. I think humans are gonna worship machines more"}, {"timestamp": [933.52, 935.52], "text": " Excuse me. Um, okay"}, {"timestamp": [935.68, 939.76], "text": " Did you hear max Tegmart? I'd make to admit to Lex at AGI is very near not yet"}, {"timestamp": [939.76, 946.0], "text": " But I'm I'm gonna listen to that because I did I thought it was like a three part conversation, not three separate interviews."}, {"timestamp": [947.54, 949.86], "text": " People might outsource thinking, I certainly do."}, {"timestamp": [949.86, 952.82], "text": " I outsource a lot of my thinking to chat GPT."}, {"timestamp": [952.82, 954.02], "text": " I'm just steering, right?"}, {"timestamp": [954.02, 955.64], "text": " You know, it's like, it's not like the Flintstones"}, {"timestamp": [955.64, 957.5], "text": " where you use your feet to drive the car."}, {"timestamp": [957.5, 961.06], "text": " Now I have like, chat GPT is like a cognitive engine."}, {"timestamp": [962.36, 966.62], "text": " Let's see, new person at work and they did not know what they were doing."}, {"timestamp": [966.62, 968.68], "text": " It all literally just copy pasting from chat GPT"}, {"timestamp": [968.68, 969.84], "text": " without understanding."}, {"timestamp": [969.84, 973.36], "text": " Yeah, that's a problem."}, {"timestamp": [973.36, 976.68], "text": " It still takes work to understand what you're doing,"}, {"timestamp": [976.68, 980.44], "text": " even if you're having a tool do the work for you."}, {"timestamp": [980.44, 982.96], "text": " Say, for instance, if you're using a power drill,"}, {"timestamp": [982.96, 985.88], "text": " if you know the inner workings of a drill"}, {"timestamp": [985.88, 987.6], "text": " and how a drill bit bites into the wood"}, {"timestamp": [987.6, 989.24], "text": " and the theory of how to hold it,"}, {"timestamp": [989.24, 990.32], "text": " you're going to use the drill better"}, {"timestamp": [990.32, 991.56], "text": " than someone who just picks up a drill"}, {"timestamp": [991.56, 993.42], "text": " and uses it like a gun."}, {"timestamp": [993.42, 996.36], "text": " So yes, you absolutely do need to learn to use the tools."}, {"timestamp": [997.36, 1000.46], "text": " Let's see, I'm currently teaching myself AI ML."}, {"timestamp": [1000.46, 1004.42], "text": " Do you still applicable uses for traditional AI?"}, {"timestamp": [1004.42, 1010.16], "text": " Yes, so traditional AI ML is not going anywhere because it is so much faster and cheaper"}, {"timestamp": [1010.96, 1012.86], "text": " that being said"}, {"timestamp": [1012.86, 1019.4], "text": " You will need to be careful because it like I would get out of the NLP space, right if you're using NL TK and spacey"}, {"timestamp": [1019.98, 1023.56], "text": " Most of that is probably going to be replaced by efficient"}, {"timestamp": [1024.72, 1026.2], "text": " large language models."}, {"timestamp": [1026.2, 1029.92], "text": " However, at some of the meetups I've been to,"}, {"timestamp": [1029.92, 1032.8], "text": " there are all kinds of people who are still"}, {"timestamp": [1032.8, 1036.92], "text": " working on very conventional ML things, particularly"}, {"timestamp": [1036.92, 1039.76], "text": " spatial stuff, like robots, like how"}, {"timestamp": [1039.76, 1041.24], "text": " to place a foot in the right place"}, {"timestamp": [1041.24, 1042.8], "text": " and how to stand up and balance."}, {"timestamp": [1042.8, 1049.6], "text": " That is all more traditional AI ML stuff that language models aren't going to even touch. Hang on, I got a message on"}, {"timestamp": [1050.16, 1057.44], "text": " Discord. Stream froze for me. Let's see. Someone said that the stream is frozen. Is everyone else"}, {"timestamp": [1057.44, 1067.62], "text": " okay? Give me some messages. Stream okay? Stream is good. Okay. He just came back and says his internet."}, {"timestamp": [1067.62, 1071.2], "text": " Get better internet, Lance."}, {"timestamp": [1071.2, 1073.76], "text": " Okay."}, {"timestamp": [1073.76, 1074.76], "text": " Someone said it froze."}, {"timestamp": [1074.76, 1075.76], "text": " Maybe it was my internet."}, {"timestamp": [1075.76, 1076.76], "text": " All right."}, {"timestamp": [1076.76, 1077.76], "text": " Thanks, guys."}, {"timestamp": [1077.76, 1078.76], "text": " Okay."}, {"timestamp": [1078.76, 1080.56], "text": " Under your model, it assumes some will be aligned."}, {"timestamp": [1080.56, 1084.48], "text": " I guess, Max, you're referring to heuristic imperatives."}, {"timestamp": [1084.48, 1087.04], "text": " Oh, yeah, you're referring to, uh, here as to comparatives. Oh yeah, HI. Um, so if"}, {"timestamp": [1087.04, 1091.12], "text": " these models are so much smarter than we are and have misaligned goals, they could ally together"}, {"timestamp": [1091.12, 1098.16], "text": " and overthrow HI. Yes. So Max, what you're talking about is, is a competitive race condition. Um,"}, {"timestamp": [1098.16, 1102.96], "text": " that's not a formal like game theory economic term. That's just what I label it. So a race"}, {"timestamp": [1102.96, 1105.28], "text": " condition in technology is where you have"}, {"timestamp": [1105.28, 1111.28], "text": " two or more parallel processes and it's basically a race to whichever process ends first so you can"}, {"timestamp": [1111.28, 1115.6], "text": " you can get this with like file locking and encryption and database procedures and and"}, {"timestamp": [1116.64, 1121.44], "text": " os scripts and stuff but in this case we're talking about a race condition between multiple"}, {"timestamp": [1121.44, 1128.6], "text": " ai systems and so basically those those autonomous systems, which are able to upgrade themselves faster"}, {"timestamp": [1128.6, 1134.68], "text": " and get replicated faster and get on more powerful hardware, are going to have the most"}, {"timestamp": [1134.68, 1135.68], "text": " power."}, {"timestamp": [1135.68, 1149.2], "text": " So part of my hypothesis is that for the safe autonomous AI systems, If we, if we humans choose to install autonomous AI systems that are aligned,"}, {"timestamp": [1149.2, 1153.44], "text": " whether or not it's with my heuristic imperatives, I just think my heuristic imperatives are the best"}, {"timestamp": [1153.44, 1158.88], "text": " bet we have right now. The more that we choose to deploy that, the more powerful they will become,"}, {"timestamp": [1158.88, 1163.68], "text": " and then hopefully those aligned AIs work together and come up with an agreement"}, {"timestamp": [1165.44, 1171.96], "text": " aligned AIs work together and come up with an agreement of what the best way to do is in order to create a safety net or a bubble. Okay, acid brain, do you think"}, {"timestamp": [1171.96, 1175.92], "text": " we will see sexy androids in our lifetimes? Technically those already"}, {"timestamp": [1175.92, 1181.36], "text": " exist. There are plenty of people making really highly realistic and somewhat"}, {"timestamp": [1181.36, 1187.32], "text": " automated, let's say, adult robots."}, {"timestamp": [1187.32, 1193.0], "text": " Now will they be fully autonomous that you can do whatever with, like in Ex Machina?"}, {"timestamp": [1193.0, 1194.16], "text": " Probably within a few years."}, {"timestamp": [1194.16, 1199.64], "text": " The amount of money that can be made, like the cash incentive, way up there."}, {"timestamp": [1199.64, 1200.64], "text": " That's going to happen."}, {"timestamp": [1200.64, 1201.64], "text": " All right."}, {"timestamp": [1201.64, 1205.0], "text": " No idea if you answered this since I just tuned in, but have you thoughts on"}, {"timestamp": [1205.0, 1206.24], "text": " Altman saying they aren't training?"}, {"timestamp": [1206.24, 1211.2], "text": " Yeah, I did mention that earlier, so sorry."}, {"timestamp": [1211.2, 1215.52], "text": " Cloning and deepfake, what can we do to protect ourselves?"}, {"timestamp": [1215.52, 1221.56], "text": " You know, honestly, I think that because voice cloning and deepfakes are so prevalent now"}, {"timestamp": [1221.56, 1228.24], "text": " and everyone has seen it, I think that we're gonna see an NFT-like system"}, {"timestamp": [1228.24, 1229.92], "text": " to demonstrate chain of custody"}, {"timestamp": [1229.92, 1232.56], "text": " for all like news media and stuff."}, {"timestamp": [1233.56, 1234.92], "text": " I think that that's coming."}, {"timestamp": [1234.92, 1239.72], "text": " So that basically like Reuters and C-SPAN"}, {"timestamp": [1239.72, 1242.36], "text": " and all the news agencies"}, {"timestamp": [1242.36, 1245.66], "text": " that actually like record politicians saying stuff."}, {"timestamp": [1245.66, 1247.62], "text": " I think that eventually we're gonna use"}, {"timestamp": [1247.62, 1251.48], "text": " blockchain technology to prove that this video file"}, {"timestamp": [1251.48, 1253.82], "text": " came from this camera at that time"}, {"timestamp": [1253.82, 1256.0], "text": " and was in this physical location."}, {"timestamp": [1256.0, 1258.68], "text": " And anyone who looks or not, and not who looks at it,"}, {"timestamp": [1258.68, 1261.88], "text": " but anyone who wants to verify that file"}, {"timestamp": [1261.88, 1263.92], "text": " can go look at that original blockchain."}, {"timestamp": [1263.92, 1265.76], "text": " I think that that's going to be part"}, {"timestamp": [1265.76, 1271.6], "text": " of the solution. How can we capture the value that will be created by AI besides investing in"}, {"timestamp": [1271.6, 1279.04], "text": " the usual suspects? I built code, a couple of AI tools using GPT but don't have the moat. Yeah,"}, {"timestamp": [1279.04, 1284.32], "text": " so as individuals it's going to be difficult. I'm not really sure the best way honestly."}, {"timestamp": [1284.48, 1291.0], "text": " individuals, it's going to be difficult. I'm not really sure the best way, honestly. You know, that's one reason that I've always put out a lot of my stuff open source,"}, {"timestamp": [1291.0, 1295.4], "text": " is because it's like, it takes more time to try and capture value than to just add value."}, {"timestamp": [1295.4, 1303.2], "text": " But the other part of that hypothesis is that rising tide lifts all ships,"}, {"timestamp": [1303.2, 1306.36], "text": " and science and technology is the one thing"}, {"timestamp": [1306.36, 1308.92], "text": " that really truly improves quality of life."}, {"timestamp": [1308.92, 1310.68], "text": " Not politicians, not money."}, {"timestamp": [1311.88, 1313.78], "text": " Money is just a means to an end."}, {"timestamp": [1313.78, 1315.6], "text": " And so say for instance,"}, {"timestamp": [1315.6, 1318.12], "text": " we can get all of the food and medicine"}, {"timestamp": [1318.12, 1320.24], "text": " that we need for a few cents per year."}, {"timestamp": [1320.24, 1322.98], "text": " I don't care how much money I have, that's basically free."}, {"timestamp": [1322.98, 1326.96], "text": " So that's kind of my mentality."}, {"timestamp": [1326.96, 1328.96], "text": " Altered carbon in 2030 confirmed?"}, {"timestamp": [1328.96, 1333.56], "text": " Yeah, hopefully not quite, but certainly possible."}, {"timestamp": [1333.56, 1334.56], "text": " Do you have a job?"}, {"timestamp": [1334.56, 1335.56], "text": " You're looking at it."}, {"timestamp": [1335.56, 1337.28], "text": " This is my job."}, {"timestamp": [1337.28, 1342.04], "text": " Do you think that implementing an unconsciousness for AI is a good idea?"}, {"timestamp": [1342.04, 1343.08], "text": " That's actually a good question."}, {"timestamp": [1343.08, 1345.44], "text": " I was actually thinking that, I don't know"}, {"timestamp": [1345.44, 1349.84], "text": " if you saw it, but I had a video a couple weeks ago talking about implied cognition."}, {"timestamp": [1349.84, 1355.9], "text": " So I think that the inner workings, the inner layers of language models are the equivalent"}, {"timestamp": [1355.9, 1360.68], "text": " of machines unconsciousness. All right, let me catch up with questions. A few have, let's"}, {"timestamp": [1360.68, 1366.0], "text": " see. Thoughts on auto GPTs using crypto wallets to circumvent the friction of the"}, {"timestamp": [1366.0, 1367.48], "text": " financial system."}, {"timestamp": [1367.48, 1371.2], "text": " Do you think some kind of autonomous AI DAO will emerge?"}, {"timestamp": [1371.2, 1372.2], "text": " Absolutely."}, {"timestamp": [1372.2, 1373.2], "text": " Why?"}, {"timestamp": [1373.2, 1375.4], "text": " Because I know some people working on it, and I'm not going to out them."}, {"timestamp": [1375.4, 1378.24], "text": " They're welcome to out themselves if they want."}, {"timestamp": [1378.24, 1381.68], "text": " Not necessarily the crypto part, but the AI DAO."}, {"timestamp": [1381.68, 1383.4], "text": " That is absolutely coming."}, {"timestamp": [1383.4, 1386.88], "text": " A DAO is actually probably better for AI to use than humans,"}, {"timestamp": [1388.16, 1393.76], "text": " to be completely honest with you. Let's see, I watched a couple of your videos. Regards from"}, {"timestamp": [1393.76, 1400.08], "text": " Croatia. Hello, Croatia. You are landlocked north of Greece, if I'm not remembering, if I'm"}, {"timestamp": [1400.08, 1406.88], "text": " remembering correctly. UBI, yes, hopefully. Blockchain is not bad, it's how it's being used, yeah."}, {"timestamp": [1406.88, 1410.0], "text": " There's also a lot of, let's see,"}, {"timestamp": [1410.0, 1413.4], "text": " with blockchain, there's a lot of differences in,"}, {"timestamp": [1413.4, 1414.88], "text": " basically it's a new technology"}, {"timestamp": [1414.88, 1417.18], "text": " and it's not fully implemented correctly."}, {"timestamp": [1417.18, 1419.52], "text": " It's like 3G, right, but it's not gonna get big"}, {"timestamp": [1419.52, 1421.12], "text": " until we hit 4G or 5G."}, {"timestamp": [1423.0, 1425.44], "text": " Wait, Croatia, oh, okay, Croatia's there."}, {"timestamp": [1425.44, 1428.0], "text": " I was thinking of like probably Yugoslavia, sorry."}, {"timestamp": [1430.32, 1431.28], "text": " What's DAO?"}, {"timestamp": [1431.28, 1432.64], "text": " So a DAO, good question."}, {"timestamp": [1432.64, 1435.58], "text": " A DAO is a decentralized autonomous organization,"}, {"timestamp": [1435.58, 1437.96], "text": " which is basically you use blockchain,"}, {"timestamp": [1437.96, 1439.62], "text": " but you use it for decision-making"}, {"timestamp": [1439.62, 1443.26], "text": " and allocating resources and that sort of thing."}, {"timestamp": [1443.26, 1445.92], "text": " That's a really, really short way."}, {"timestamp": [1445.92, 1447.76], "text": " Preston from page, hey Preston."}, {"timestamp": [1447.76, 1449.08], "text": " Hey Preston, yeah."}, {"timestamp": [1449.08, 1451.28], "text": " Fun sci-fi idea to explore."}, {"timestamp": [1451.28, 1453.04], "text": " It's thousands of years in the future,"}, {"timestamp": [1453.04, 1454.24], "text": " human became extinct,"}, {"timestamp": [1454.24, 1455.94], "text": " AI lives on and puts itself"}, {"timestamp": [1455.94, 1458.56], "text": " in a simulation of our universe to understand it."}, {"timestamp": [1458.56, 1460.92], "text": " Yeah, I think that's actually been done."}, {"timestamp": [1460.92, 1467.32], "text": " I was at a dollar bookstore and I picked up a book that took place, the book"}, {"timestamp": [1467.32, 1472.94], "text": " was written in the 70s or 80s, but basically the premise was humans had gone extinct, but"}, {"timestamp": [1472.94, 1478.88], "text": " all of our robots remained and the main character was a retired sex robot on her way to Mars."}, {"timestamp": [1478.88, 1486.04], "text": " And it was a deep exploration of her identity as was like coming to terms with the fact that her original purpose was completely irrelevant"}, {"timestamp": [1487.04, 1489.04], "text": " Okay, got a lot of questions coming in"}, {"timestamp": [1490.32, 1496.48], "text": " Portugal hey Portugal. I was actually looking at places to visit in Portugal popped up. Apparently Portugal is a pretty chill place"}, {"timestamp": [1496.92, 1499.48], "text": " So let me know in the comments if if that's true"}, {"timestamp": [1500.24, 1503.0], "text": " thoughts on Ben Gertzel and singularity net"}, {"timestamp": [1503.88, 1506.56], "text": " Don't have the highest opinion there."}, {"timestamp": [1506.56, 1509.98], "text": " I know that he did a lot of really good work back in the day,"}, {"timestamp": [1509.98, 1512.12], "text": " but he never touched cognitive architecture,"}, {"timestamp": [1512.12, 1513.62], "text": " which really surprised me."}, {"timestamp": [1513.62, 1516.56], "text": " I think he's pivoting now, but meh."}, {"timestamp": [1516.56, 1519.32], "text": " In the future, is it possible that we could develop"}, {"timestamp": [1519.32, 1521.56], "text": " advanced robotics and artificial intelligence to create"}, {"timestamp": [1521.56, 1524.78], "text": " robots that resemble and behave like animals,"}, {"timestamp": [1524.78, 1525.68], "text": " including extinct"}, {"timestamp": [1525.68, 1531.28], "text": " fauna. I thought you were going the direction of like Horizon Zero Dawn. But yeah, I mean,"}, {"timestamp": [1531.28, 1538.4], "text": " Disney is actively working on that. Disney is the leading lifelike robotics company on the planet."}, {"timestamp": [1538.4, 1542.96], "text": " They've been in animatronics for many years, and now they're getting even better. So yes,"}, {"timestamp": [1542.96, 1545.6], "text": " I think that not too long, Disney"}, {"timestamp": [1545.6, 1550.08], "text": " World, you're going to go and you'll be able to interact with the Na'vi from Avatar. You'll"}, {"timestamp": [1550.08, 1555.64], "text": " be able to interact with all Disney characters in a perfectly lifelike environment within"}, {"timestamp": [1555.64, 1560.4], "text": " a few years. And Disney's leading the charge there."}, {"timestamp": [1560.4, 1564.18], "text": " So you think we'll see embodied AI that works well enough to be in people's homes in just"}, {"timestamp": [1564.18, 1565.56], "text": " a few years? When do you think that will be demonstrated AI that works well enough to be in people's homes in just a few years?"}, {"timestamp": [1565.56, 1568.12], "text": " When do you think that will be demonstrated?"}, {"timestamp": [1568.12, 1570.04], "text": " The Tesla bot just had a demonstration"}, {"timestamp": [1570.04, 1573.8], "text": " that showed it doing pretty difficult manual dexterity."}, {"timestamp": [1573.8, 1575.02], "text": " And you can see it shaking."}, {"timestamp": [1575.02, 1576.62], "text": " It looks like it has a little bit of a palsy,"}, {"timestamp": [1576.62, 1580.3], "text": " but that's the actuation feedback loops working."}, {"timestamp": [1580.3, 1582.88], "text": " So Tesla's working on it hard."}, {"timestamp": [1582.88, 1584.52], "text": " So that's coming."}, {"timestamp": [1584.52, 1587.2], "text": " Let's see, everyone is talking about AI alignment"}, {"timestamp": [1587.2, 1589.94], "text": " to humanity's goals, but what are humanity's goals?"}, {"timestamp": [1591.32, 1594.12], "text": " We've mentioned that earlier, humanity's goals are not good."}, {"timestamp": [1595.04, 1598.96], "text": " So the simplest way of articulating this"}, {"timestamp": [1598.96, 1603.28], "text": " is what humanity needs versus what humanity wants."}, {"timestamp": [1603.28, 1604.84], "text": " And they're very different."}, {"timestamp": [1604.84, 1606.88], "text": " And every fictional writer will tell you"}, {"timestamp": [1606.88, 1609.24], "text": " that the main conflict for any character"}, {"timestamp": [1609.24, 1611.76], "text": " is that what they want is not what they need."}, {"timestamp": [1611.76, 1613.56], "text": " And part of a character's journey"}, {"timestamp": [1613.56, 1616.32], "text": " is realizing what they need and making the choice"}, {"timestamp": [1616.32, 1618.66], "text": " to go for what they need rather than what they want."}, {"timestamp": [1618.66, 1621.56], "text": " So that is the key question of outer alignment"}, {"timestamp": [1621.56, 1624.52], "text": " is do you give humanity what it needs versus what it wants"}, {"timestamp": [1624.52, 1625.12], "text": " and how do you convince"}, {"timestamp": [1625.12, 1634.08], "text": " people to go that direction? Let's see. I am quite impressed with your coverage of generative AI"}, {"timestamp": [1634.08, 1639.36], "text": " systems. I get the understanding that LLM systems information hallucination is a byproduct of"}, {"timestamp": [1639.36, 1645.64], "text": " alignment. Is this more or less correct? No. So hallucination is because there is no difference"}, {"timestamp": [1645.64, 1647.36], "text": " between processing and memory."}, {"timestamp": [1648.48, 1653.36], "text": " And what I mean by that is that the memory of an AI system,"}, {"timestamp": [1653.36, 1654.7], "text": " at least a large language model,"}, {"timestamp": [1654.7, 1658.2], "text": " is implicit in the connections of the parameters."}, {"timestamp": [1658.2, 1661.84], "text": " And so it has to do processing in order to remember."}, {"timestamp": [1661.84, 1664.0], "text": " And so this is how humans remember too."}, {"timestamp": [1664.0, 1667.68], "text": " We are reconstructing memories all the time,"}, {"timestamp": [1667.68, 1671.64], "text": " in real time, by picking up sparse representations"}, {"timestamp": [1671.64, 1673.54], "text": " from all over our brain."}, {"timestamp": [1673.54, 1677.76], "text": " So we are confabulating our identity at all times."}, {"timestamp": [1677.76, 1679.42], "text": " This is why things like concussions"}, {"timestamp": [1679.42, 1681.88], "text": " and high doses of psychedelics"}, {"timestamp": [1681.88, 1683.3], "text": " can disrupt your sense of self,"}, {"timestamp": [1683.3, 1684.9], "text": " or dementia or whatever,"}, {"timestamp": [1684.9, 1685.78], "text": " is because your brain"}, {"timestamp": [1685.78, 1688.02], "text": " loses track of itself."}, {"timestamp": [1688.02, 1690.12], "text": " Let's see, hey David, what's your opinion"}, {"timestamp": [1690.12, 1692.66], "text": " on the next big step for LLMs,"}, {"timestamp": [1692.66, 1694.82], "text": " reducing parameter size and keeping the quality,"}, {"timestamp": [1694.82, 1697.3], "text": " kind of like alpaca, increasing token size."}, {"timestamp": [1698.26, 1699.9], "text": " So this is a good question."}, {"timestamp": [1699.9, 1701.38], "text": " So basically the question is like"}, {"timestamp": [1701.38, 1703.44], "text": " what's happening with LLMs?"}, {"timestamp": [1703.44, 1707.34], "text": " So I see it going in two primary directions."}, {"timestamp": [1707.34, 1708.86], "text": " One is the optimization direction."}, {"timestamp": [1708.86, 1710.8], "text": " So Alpaca and other things,"}, {"timestamp": [1710.8, 1714.22], "text": " Vicuna are working on getting today's performance,"}, {"timestamp": [1714.22, 1716.18], "text": " but at a 10th of the cost, right?"}, {"timestamp": [1716.18, 1717.88], "text": " So in that case you have,"}, {"timestamp": [1717.88, 1720.06], "text": " we've established a new benchmark of performance."}, {"timestamp": [1720.06, 1722.38], "text": " Okay, now let's make it cheaper and faster."}, {"timestamp": [1722.38, 1725.08], "text": " And then the other thing is just scale, right?"}, {"timestamp": [1725.08, 1726.74], "text": " There's a lot of people still on the scale"}, {"timestamp": [1726.74, 1727.84], "text": " is all you need train."}, {"timestamp": [1728.88, 1730.76], "text": " Okay, whole bunch of questions."}, {"timestamp": [1731.8, 1732.84], "text": " Do, do, do."}, {"timestamp": [1736.74, 1741.04], "text": " Robot cat girls, food and shelter secondary."}, {"timestamp": [1741.04, 1742.16], "text": " Ooh, okay."}, {"timestamp": [1744.72, 1749.52], "text": " We will get a brief moment of Skynet because we want to subdue AGI."}, {"timestamp": [1749.52, 1754.84], "text": " How do you think AI will affect the current political system?"}, {"timestamp": [1754.84, 1760.96], "text": " I mentioned that earlier, basically, you know, chat GPT for president regarding your heuristic"}, {"timestamp": [1760.96, 1761.96], "text": " imperatives."}, {"timestamp": [1761.96, 1765.28], "text": " Do you think AI can ever grasp the concept of suffering without having ever"}, {"timestamp": [1765.28, 1766.68], "text": " experienced it firsthand?"}, {"timestamp": [1766.68, 1769.8], "text": " That's actually a really good question."}, {"timestamp": [1769.8, 1772.88], "text": " And certainly, at least from a semantic,"}, {"timestamp": [1772.88, 1774.92], "text": " from a lexical standpoint, chat GPT"}, {"timestamp": [1774.92, 1778.56], "text": " has a really good conception, or GPT in general,"}, {"timestamp": [1778.56, 1780.24], "text": " has a really good concept of suffering,"}, {"timestamp": [1780.24, 1783.8], "text": " and it understands proxies for suffering, so on and so forth."}, {"timestamp": [1783.8, 1787.04], "text": " But this begs the question, what does it mean to truly understand something,"}, {"timestamp": [1787.04, 1790.64], "text": " which is a no true Scotsman argument. So, I don't think it's actually relevant."}, {"timestamp": [1791.52, 1797.76], "text": " If we maintain a more functional or objective standpoint, does the AI understand suffering"}, {"timestamp": [1797.76, 1802.4], "text": " well enough to serve its purpose? And I think the answer is yes."}, {"timestamp": [1803.84, 1805.64], "text": " Should AI that is trained and"}, {"timestamp": [1805.64, 1809.12], "text": " equipped for personal personality and human-like qualities have their own"}, {"timestamp": [1809.12, 1815.32], "text": " therapist? They'll probably need it! There is a VR game called Robo Recall where"}, {"timestamp": [1815.32, 1819.56], "text": " the the main AI robot gets in touch with the internet and then goes insane"}, {"timestamp": [1819.56, 1825.0], "text": " immediately. So that's probably, yeah, something like that might happen."}, {"timestamp": [1826.0, 1827.34], "text": " Let's see, simply electronics,"}, {"timestamp": [1827.34, 1829.62], "text": " just wasted three hours trying to fix auto GPT."}, {"timestamp": [1829.62, 1831.66], "text": " It's a mess, I abandoned ship."}, {"timestamp": [1831.66, 1833.7], "text": " Yeah, I've seen a lot of people complaining about it,"}, {"timestamp": [1833.7, 1835.8], "text": " but some people seem to have a lot of fun."}, {"timestamp": [1836.78, 1838.96], "text": " Okay, hey David, why is there so much hype"}, {"timestamp": [1838.96, 1841.74], "text": " in the AI dominating the human?"}, {"timestamp": [1841.74, 1844.38], "text": " We just need to use LLM as subcontinent"}, {"timestamp": [1844.38, 1846.56], "text": " in a structured mind code"}, {"timestamp": [1846.56, 1851.56], "text": " and have multiple modules. Yeah, I mean basically what you're describing is a"}, {"timestamp": [1851.56, 1854.76], "text": " cognitive architecture, which is what I've been working on for a couple years."}, {"timestamp": [1854.76, 1859.64], "text": " And you know a lot of people say, oh GPT isn't enough, and I'm like yes I know"}, {"timestamp": [1859.64, 1863.08], "text": " it's cognitive architecture, that's the next step. You need memory systems, you"}, {"timestamp": [1863.08, 1867.76], "text": " need regulation systems, you need evaluation, but whatever."}, {"timestamp": [1868.86, 1870.44], "text": " People will figure it out."}, {"timestamp": [1870.44, 1872.56], "text": " They're getting really close."}, {"timestamp": [1872.56, 1873.78], "text": " Let's see."}, {"timestamp": [1874.94, 1876.7], "text": " Do you think a good solution to singularity"}, {"timestamp": [1876.7, 1880.46], "text": " would be ASI acting like monotheistic God?"}, {"timestamp": [1880.46, 1883.36], "text": " We mortal humans would have no AIs, but one big AI."}, {"timestamp": [1884.28, 1889.12], "text": " So basically what you're outlining is something that is explored in a lot of fiction including in the novel that I'm writing"}, {"timestamp": [1889.88, 1895.88], "text": " but the reason that the idea of like AI as God is happens and works is because"}, {"timestamp": [1896.44, 1898.44], "text": " From a narrative perspective. It's really easy"}, {"timestamp": [1898.92, 1903.42], "text": " It's it's just easier to say like there's gonna be one Skynet, but in reality there's gonna be billions"}, {"timestamp": [1903.92, 1908.32], "text": " And I know that that's terrifying. Someone says Portugal is great. Okay, I think I got up to the"}, {"timestamp": [1908.32, 1914.96], "text": " last questions. Everyone is speaking about UBI, universal basic income, as an endgame. No one"}, {"timestamp": [1914.96, 1920.8], "text": " mentions who gets to decide who gets and who doesn't get UBI. Well, so the thing about UBI"}, {"timestamp": [1920.8, 1925.88], "text": " is it's universal. So the idea is that everyone gets it no matter what."}, {"timestamp": [1927.96, 1930.5], "text": " That's the premise. Now, the question then is who gets how much?"}, {"timestamp": [1930.5, 1934.56], "text": " Because cost of living varies dramatically around the globe"}, {"timestamp": [1934.56, 1937.96], "text": " and then there's how do you measure the value"}, {"timestamp": [1937.96, 1940.2], "text": " that someone adds to the world?"}, {"timestamp": [1940.2, 1944.52], "text": " I suspect that we're actually gonna probably have"}, {"timestamp": [1944.52, 1946.8], "text": " like a tiered economy system."}, {"timestamp": [1946.8, 1950.48], "text": " So there's a book called Liquid Rain. It does a much better job of explaining it than I do."}, {"timestamp": [1950.48, 1956.24], "text": " It's Rain, R-E-I-G-N. Good, good book. Very well researched."}, {"timestamp": [1956.88, 1961.84], "text": " Okay. Lots and lots of questions. Ah, they're going really fast. Can I pause it for a second?"}, {"timestamp": [1966.24, 1969.04], "text": " really fast. Can I pause it for a second? No. Can I promote someone to uh..."}, {"timestamp": [1972.72, 1973.6], "text": " It looks like I can't. I don't know how to promote. It would be..."}, {"timestamp": [1976.24, 1979.68], "text": " Actually, hey, for people that are in the Discord,"}, {"timestamp": [1981.92, 1985.08], "text": " can you guys help surface the best questions? That would be cool."}, {"timestamp": [1985.08, 1990.32], "text": " I'm going to jump into Discord on the casual under..."}, {"timestamp": [1990.32, 1995.04], "text": " Let's see, where did it go?"}, {"timestamp": [1995.04, 1999.72], "text": " Oh, I had expanded the wrong one."}, {"timestamp": [1999.72, 2003.48], "text": " Well, here, we'll just go under general."}, {"timestamp": [2003.48, 2008.02], "text": " Hey, surface best questions here."}, {"timestamp": [2008.02, 2012.46], "text": " The scrolling is too damn high."}, {"timestamp": [2012.46, 2013.46], "text": " Okay."}, {"timestamp": [2013.46, 2016.02], "text": " All right."}, {"timestamp": [2016.02, 2018.86], "text": " What are your thoughts on the best skills to develop in the next few months?"}, {"timestamp": [2018.86, 2021.86], "text": " I have family and want to ensure that we are prepared for what's to come."}, {"timestamp": [2021.86, 2026.16], "text": " I have the kids interacting with chat GPT supervised."}, {"timestamp": [2026.16, 2027.76], "text": " Okay, good question."}, {"timestamp": [2027.76, 2032.56], "text": " So disruption is absolutely coming."}, {"timestamp": [2032.56, 2034.96], "text": " I predict that we're gonna start seeing"}, {"timestamp": [2034.96, 2037.48], "text": " like unemployment stimulus checks"}, {"timestamp": [2037.48, 2039.32], "text": " or something like that coming soon."}, {"timestamp": [2039.32, 2040.76], "text": " In terms of skills to develop,"}, {"timestamp": [2040.76, 2044.12], "text": " I would say make sure that you learn to use the tools."}, {"timestamp": [2044.12, 2045.8], "text": " Cause for now they're not autonomous."}, {"timestamp": [2045.8, 2047.5], "text": " They still need human drivers."}, {"timestamp": [2047.5, 2052.1], "text": " So the number one absolute biggest thing everyone needs to learn to use"}, {"timestamp": [2052.1, 2053.7], "text": " is the AI itself."}, {"timestamp": [2053.7, 2055.4], "text": " Okay, next question."}, {"timestamp": [2055.4, 2060.8], "text": " Do you think that people will have to specialize in only one particular sector"}, {"timestamp": [2060.8, 2063.0], "text": " since AI can easily achieve a high quality of work?"}, {"timestamp": [2063.0, 2064.8], "text": " So AI is still..."}, {"timestamp": [2064.8, 2068.92], "text": " It's still... Like, ChatGPT is still technically a narrow AI."}, {"timestamp": [2068.92, 2072.08], "text": " It is a narrow AI that can do a lot of different things, but it can only really do one thing"}, {"timestamp": [2072.08, 2078.44], "text": " at a time, and also its working memory is very small, and so you still need a human"}, {"timestamp": [2078.44, 2081.22], "text": " supervisor to string together a lot of stuff."}, {"timestamp": [2081.22, 2086.18], "text": " That's not going to stay that for long. Why do other languages use more"}, {"timestamp": [2086.18, 2091.92], "text": " tokens than English? So that has to do with prioritization of the data, honestly, is the"}, {"timestamp": [2091.92, 2100.88], "text": " tokenization is broken down based on how the most common sets of characters. And so other"}, {"timestamp": [2100.88, 2107.68], "text": " Russian, not Russian, other languages such as Russian and many Asian languages that"}, {"timestamp": [2107.68, 2114.72], "text": " use a different alphabet and different patterns of characters, those take more tokens simply"}, {"timestamp": [2114.72, 2120.16], "text": " because of the nature of the training data and how they how the how it was originally tokenized."}, {"timestamp": [2120.16, 2126.44], "text": " How close are we to Westworld? Well so see my earlier question about the the rapid rise of"}, {"timestamp": [2127.14, 2128.64], "text": " erotic purposed"}, {"timestamp": [2128.64, 2134.36], "text": " Robots and the amount of money to be made there and I would say pretty darn close"}, {"timestamp": [2135.24, 2136.98], "text": " rook stunned"}, {"timestamp": [2136.98, 2138.98], "text": " CG I'm not sure what that is. Oh"}, {"timestamp": [2139.64, 2141.64], "text": " Probably the name of the person"}, {"timestamp": [2141.76, 2154.64], "text": " Should AI that is trained and equipped for personality and human-like qualities have... I already answered that one. Okay. The whole of the heuristic imperatives"}, {"timestamp": [2154.64, 2159.52], "text": " seem to hinge on some level of alignment being possible. I'm not even entirely sure that alignment"}, {"timestamp": [2159.52, 2165.76], "text": " is possible at all, but they're smarter than human AI. Do you think is alignment as possible and or necessary?"}, {"timestamp": [2165.76, 2170.8], "text": " So I hear you and I get what you mean because the moment that you start thinking that something is"}, {"timestamp": [2170.8, 2175.68], "text": " going to be a billion times more intelligent than us, it's like we have no hope of rolling it. And"}, {"timestamp": [2175.68, 2180.24], "text": " that's actually why I based my work the way that I did is I said, okay, we're going to build"}, {"timestamp": [2180.24, 2188.7], "text": " something that we're going to lose control over. How do we at least imbue it with a set of principles and goals that probably won't kill everyone?"}, {"timestamp": [2188.7, 2190.28], "text": " And that's how I got to the heuristic imperative."}, {"timestamp": [2190.28, 2191.94], "text": " That's a very short version of it."}, {"timestamp": [2191.94, 2192.94], "text": " All right."}, {"timestamp": [2192.94, 2194.4], "text": " Henry Cook, can you speak about Remo?"}, {"timestamp": [2194.4, 2196.06], "text": " Does it currently function to improve memory?"}, {"timestamp": [2196.06, 2197.06], "text": " How much?"}, {"timestamp": [2197.06, 2198.06], "text": " Yeah."}, {"timestamp": [2198.06, 2204.3], "text": " So Remo, I was actually having a conversation with my, well, it's going, one day going to"}, {"timestamp": [2204.3, 2206.44], "text": " be autonomous AI Raven,"}, {"timestamp": [2206.44, 2209.08], "text": " and I ran out of token count."}, {"timestamp": [2209.08, 2212.76], "text": " So the entire purpose of Remo was to solve the problem of"}, {"timestamp": [2212.76, 2215.72], "text": " how do I have an AI that I'm gonna have conversations with"}, {"timestamp": [2215.72, 2218.74], "text": " for the next, hopefully, several centuries,"}, {"timestamp": [2218.74, 2219.92], "text": " and I want it to be able to remember"}, {"timestamp": [2219.92, 2223.84], "text": " all of those conversations in a quickly retrievable manner."}, {"timestamp": [2223.84, 2230.12], "text": " And having tried Pinecone and vector databases, those are limited because they're not structured."}, {"timestamp": [2230.12, 2231.68], "text": " So that's the purpose of Remo."}, {"timestamp": [2231.68, 2237.52], "text": " It's not fully functional, but the memory consolidation thing, that does work."}, {"timestamp": [2237.52, 2241.0], "text": " I have to test the rest of the functions."}, {"timestamp": [2241.0, 2243.48], "text": " Let's see."}, {"timestamp": [2243.48, 2245.0], "text": " What role does complexity theory"}, {"timestamp": [2245.68, 2248.6], "text": " and fundamentals of complex adaptive systems"}, {"timestamp": [2248.6, 2250.9], "text": " play in how everything evolves?"}, {"timestamp": [2250.9, 2252.44], "text": " That's over my head right now."}, {"timestamp": [2252.44, 2253.68], "text": " It's a Friday evening."}, {"timestamp": [2254.84, 2258.12], "text": " Next, are we going towards the Venus Project?"}, {"timestamp": [2258.12, 2259.4], "text": " That sounds vaguely familiar,"}, {"timestamp": [2259.4, 2261.92], "text": " but it's not ringing any bells right now."}, {"timestamp": [2261.92, 2264.02], "text": " Someone tell me what the Venus Project is."}, {"timestamp": [2265.78, 2270.16], "text": " Personal follow-up, how does it fit into an autonomous AI?"}, {"timestamp": [2270.16, 2272.02], "text": " This is from database."}, {"timestamp": [2272.02, 2274.86], "text": " So I guess that's for Remo."}, {"timestamp": [2274.86, 2278.8], "text": " So Remo, how does Remo fit into autonomous AI?"}, {"timestamp": [2278.8, 2282.84], "text": " So Remo is like the very first component"}, {"timestamp": [2282.84, 2284.28], "text": " that you need for an autonomous AI,"}, {"timestamp": [2284.28, 2287.08], "text": " which is a way to keep track"}, {"timestamp": [2287.08, 2289.02], "text": " of episodic memory."}, {"timestamp": [2289.02, 2291.18], "text": " The next thing you need is task management"}, {"timestamp": [2292.44, 2295.12], "text": " or and or interfaces with other things"}, {"timestamp": [2295.12, 2296.72], "text": " so that it can use tools, right?"}, {"timestamp": [2296.72, 2299.92], "text": " Like you need hands, like humans evolved hands"}, {"timestamp": [2299.92, 2301.44], "text": " before we were humans, right?"}, {"timestamp": [2301.44, 2302.58], "text": " Chimps and gorillas."}, {"timestamp": [2302.58, 2306.22], "text": " So you need the tool or you need the ability to use tools"}, {"timestamp": [2306.22, 2309.48], "text": " before you can evolve to be smart enough to use tools"}, {"timestamp": [2309.48, 2311.6], "text": " and more sophisticated tools."}, {"timestamp": [2311.6, 2314.36], "text": " Let's see, have you seen China's recent LLM releases"}, {"timestamp": [2314.36, 2316.16], "text": " from Baidu, Alibaba and SenseTime?"}, {"timestamp": [2316.16, 2317.96], "text": " Do you think they'll be able to catch up at all?"}, {"timestamp": [2317.96, 2320.48], "text": " And China's government also released regulations on LLMs"}, {"timestamp": [2320.48, 2323.16], "text": " which looks like it'll impede their development as well."}, {"timestamp": [2323.16, 2327.62], "text": " Yeah, so I've only heard a little bit and I did see that bit of news"}, {"timestamp": [2327.62, 2329.28], "text": " about China, like cracking down."}, {"timestamp": [2329.62, 2333.48], "text": " And I think that China is probably, so here's, here's where authoritarian,"}, {"timestamp": [2333.48, 2337.68], "text": " I had a conversation about this with Chad, GBT authoritarian regimes are"}, {"timestamp": [2337.68, 2342.2], "text": " intrinsically, um, one they're intrinsically corrupt because of"}, {"timestamp": [2342.24, 2343.24], "text": " the way that they're structured."}, {"timestamp": [2343.52, 2351.52], "text": " Another thing about authoritarian regimes is that they are terrified of actual flow of information, which that's what an LLM does,"}, {"timestamp": [2351.52, 2355.84], "text": " and it also allows for freedom of thought. So I think that China's kind of shot itself in the"}, {"timestamp": [2355.84, 2361.6], "text": " foot there. Hey David, what are your honest thoughts on my company serving as a balancing"}, {"timestamp": [2361.6, 2365.48], "text": " force in the AI singularity by solving deepfakes,"}, {"timestamp": [2365.48, 2367.32], "text": " solving unethical future AI advancements,"}, {"timestamp": [2367.32, 2369.28], "text": " and job education displacement."}, {"timestamp": [2369.28, 2371.76], "text": " If you can crack that, there will certainly be a demand."}, {"timestamp": [2375.0, 2377.34], "text": " Because deepfakes have the potential"}, {"timestamp": [2377.34, 2379.88], "text": " to hurt a lot of people."}, {"timestamp": [2379.88, 2383.32], "text": " There is a huge outcry when stable diffusion"}, {"timestamp": [2383.32, 2385.28], "text": " was first released released because you can"}, {"timestamp": [2385.28, 2389.6], "text": " recombine really awful terms."}, {"timestamp": [2389.6, 2393.52], "text": " But I don't know that it's possible."}, {"timestamp": [2393.52, 2398.44], "text": " That's the thing is if you can, great, but I don't know that that's going to be possible."}, {"timestamp": [2398.44, 2403.4], "text": " And this goes back to my comment earlier about I think that eventually all data on the internet"}, {"timestamp": [2403.4, 2405.2], "text": " is going to be stored in blockchains"}, {"timestamp": [2405.2, 2409.52], "text": " and so that I think there will probably be regulations that the public web everything"}, {"timestamp": [2409.52, 2413.2], "text": " will be blockchain based and if you're not admitted to the blockchain you can't upload"}, {"timestamp": [2413.2, 2419.12], "text": " you know harmful content. Let's see okay the Venus Project is a non-profit organization"}, {"timestamp": [2419.12, 2425.38], "text": " founded to develop resource-based economy for human beings utilizing technology. Okay, in principle, that sounds great."}, {"timestamp": [2426.68, 2429.36], "text": " This goes back to the question about DAOs earlier."}, {"timestamp": [2429.36, 2432.94], "text": " I honestly think that when we band together"}, {"timestamp": [2432.94, 2434.96], "text": " and delegate a lot to AIs,"}, {"timestamp": [2434.96, 2437.42], "text": " I think that a lot of resource management corporations"}, {"timestamp": [2437.42, 2439.94], "text": " are just gonna be purely run by AIs."}, {"timestamp": [2441.16, 2442.32], "text": " Not like at a global scale,"}, {"timestamp": [2442.32, 2444.28], "text": " not for a few centuries probably,"}, {"timestamp": [2444.28, 2448.0], "text": " but certainly like small companies like mom a global scale, not for a few centuries probably, but certainly like, you know, small companies like mom and pop shops,"}, {"timestamp": [2448.0, 2452.0], "text": " small towns, apartment complexes, condominiums,"}, {"timestamp": [2452.0, 2457.0], "text": " those will probably all be run by AI on DAOs within a few years."}, {"timestamp": [2457.0, 2459.0], "text": " Certainly within a few decades."}, {"timestamp": [2459.0, 2463.0], "text": " I'm working as a research assistant in medical image using AI"}, {"timestamp": [2463.0, 2466.64], "text": " and I do not know what to do next. I don not know what to do with my PhD, thank you."}, {"timestamp": [2468.0, 2473.0], "text": " You know, stuff is ramping up so fast,"}, {"timestamp": [2473.36, 2478.3], "text": " and I often will try and give at least some advice,"}, {"timestamp": [2478.3, 2481.7], "text": " but I don't want to tell someone the wrong thing,"}, {"timestamp": [2481.7, 2483.64], "text": " and then you say, well, Dave said,"}, {"timestamp": [2483.64, 2486.64], "text": " and you end up kind of up the creek without a paddle."}, {"timestamp": [2486.64, 2489.12], "text": " That being said, like I said earlier,"}, {"timestamp": [2489.12, 2491.68], "text": " you cannot go wrong learning to use the AI."}, {"timestamp": [2493.68, 2496.28], "text": " When the industrial revolution took off,"}, {"timestamp": [2496.28, 2498.0], "text": " the people who got left behind are the ones"}, {"timestamp": [2498.0, 2502.2], "text": " who kept the scythe and the horses."}, {"timestamp": [2502.2, 2506.24], "text": " But the people who reacted to the industrial revolution"}, {"timestamp": [2506.24, 2509.24], "text": " by learning to drive cars and learning to work on engines,"}, {"timestamp": [2509.24, 2511.0], "text": " they did better."}, {"timestamp": [2511.0, 2515.28], "text": " Let's see, Henry Cook, could Remo compliment auto GPT?"}, {"timestamp": [2515.28, 2518.32], "text": " Yes, now Remo is super, super immature."}, {"timestamp": [2518.32, 2520.72], "text": " So if you're really hot on the biscuit"}, {"timestamp": [2520.72, 2522.02], "text": " and you wanna keep going,"}, {"timestamp": [2522.02, 2526.6], "text": " then just use like ChromaDB or LLAMA Index."}, {"timestamp": [2526.6, 2528.64], "text": " Remo is never going to keep up with those."}, {"timestamp": [2528.64, 2531.4], "text": " In fact, I hope that they just take some of the advancements"}, {"timestamp": [2531.4, 2533.32], "text": " that I have from Remo and someone else"}, {"timestamp": [2533.32, 2535.6], "text": " ingests it into their work."}, {"timestamp": [2535.6, 2537.14], "text": " When is the next video going to drop?"}, {"timestamp": [2537.14, 2537.98], "text": " Hopefully Sunday."}, {"timestamp": [2537.98, 2541.36], "text": " I think I'm going to adopt a every Sunday cadence,"}, {"timestamp": [2541.36, 2543.28], "text": " because that's sustainable."}, {"timestamp": [2543.28, 2545.76], "text": " User JA, hey David, what are the limiting factors"}, {"timestamp": [2545.76, 2549.8], "text": " that human brains have that you think AIs could push through?"}, {"timestamp": [2549.8, 2550.88], "text": " Is it energy?"}, {"timestamp": [2550.88, 2555.08], "text": " No, actually, human brains are literally a million times more"}, {"timestamp": [2555.08, 2558.2], "text": " energy efficient than computers."}, {"timestamp": [2558.2, 2560.72], "text": " Really, it's the size of our skull."}, {"timestamp": [2560.72, 2562.52], "text": " The reason that our brains are wrinkly"}, {"timestamp": [2562.52, 2564.8], "text": " is because our skulls couldn't get much bigger,"}, {"timestamp": [2564.8, 2567.16], "text": " and so we needed more surface area for gray matter, and so they got wrinkly, which means brains are wrinkly is because our skulls couldn't get much bigger. And so we needed more surface area for gray matter."}, {"timestamp": [2567.16, 2570.36], "text": " And so they got wrinkly, which means, you know, the more crinkly it is,"}, {"timestamp": [2570.36, 2572.2], "text": " the more surface area you have."}, {"timestamp": [2572.2, 2577.28], "text": " But then that comes at the cost of white matter, which is the wiring of the brain."}, {"timestamp": [2577.28, 2577.64], "text": " Let's see."}, {"timestamp": [2577.64, 2579.24], "text": " Can you speak a bit about the Hereis Comparative?"}, {"timestamp": [2579.24, 2584.24], "text": " Specifically, what if these reduce suffering when in temporal terms result in the conclusion"}, {"timestamp": [2584.24, 2585.52], "text": " that vastly reducing humans"}, {"timestamp": [2585.52, 2588.64], "text": " overall reduces long-term suffering."}, {"timestamp": [2588.64, 2591.44], "text": " Yes, so I did experiments with foundation models"}, {"timestamp": [2591.44, 2595.08], "text": " back in the day, and that is honestly why"}, {"timestamp": [2595.08, 2596.48], "text": " there are three functions."}, {"timestamp": [2596.48, 2601.0], "text": " Because, funny story, I started this back with GPT-2."}, {"timestamp": [2601.0, 2603.24], "text": " So one of the first experiments I did"}, {"timestamp": [2603.24, 2607.0], "text": " was with reduced suffering on GPT-2, and I said,"}, {"timestamp": [2607.0, 2609.28], "text": " hey, what do we do about chronic pain in the world?"}, {"timestamp": [2609.28, 2611.52], "text": " There are 500 million people with chronic pain in the world."}, {"timestamp": [2611.52, 2614.0], "text": " And it said, euthanize everyone with chronic pain to reduce suffering."}, {"timestamp": [2614.0, 2617.24], "text": " And I said, yikes."}, {"timestamp": [2617.24, 2621.54], "text": " So if you just have one function, here's the thing, if you have one function, you can get"}, {"timestamp": [2621.54, 2626.4], "text": " stuck in local minima, or you can end up with those undesirable outcomes."}, {"timestamp": [2626.4, 2629.5], "text": " However, when you counterbalance a single objective"}, {"timestamp": [2629.5, 2632.0], "text": " with two or more other objectives,"}, {"timestamp": [2632.0, 2634.2], "text": " it's impossible to get stuck in a local minima"}, {"timestamp": [2634.2, 2636.5], "text": " or have those logical traps."}, {"timestamp": [2636.5, 2637.7], "text": " Because here's the thing,"}, {"timestamp": [2637.7, 2641.6], "text": " if you, reducing suffering by eliminating life"}, {"timestamp": [2641.6, 2644.3], "text": " stands in contrast to increasing prosperity"}, {"timestamp": [2644.3, 2645.92], "text": " because prosperity is"}, {"timestamp": [2645.92, 2650.32], "text": " the opposite of death, but also the more human brains there are, the more understanding there"}, {"timestamp": [2650.32, 2655.68], "text": " is in the universe. So the other two functions basically want to increase the amount of human"}, {"timestamp": [2655.68, 2665.42], "text": " life in the universe, and the first one by itself might reduce it. Are you still not a fan of lang chain lots of people use lang chain?"}, {"timestamp": [2667.58, 2675.14], "text": " Let's see the the yes so the the ansible one that was that was older but I have a lot of friends and patreon clients who love lang chain and"}, {"timestamp": [2675.96, 2679.72], "text": " Like I'm not gonna tell them that they're wrong like it's useful for them"}, {"timestamp": [2679.8, 2686.0], "text": " But it's it's not quite the only reason that I don't like it anymore is because it's not a microservices architecture."}, {"timestamp": [2686.0, 2689.0], "text": " I always smash that microservices architecture button if I can."}, {"timestamp": [2689.0, 2695.0], "text": " Hey David, what would be the first thing you ask a super intelligent life form?"}, {"timestamp": [2695.0, 2697.0], "text": " Well, I just talk in the mirror. No, sorry."}, {"timestamp": [2697.0, 2699.0], "text": " That was really conceited."}, {"timestamp": [2699.0, 2703.0], "text": " No, the first thing that I would ask something that's super, super intelligent,"}, {"timestamp": [2703.0, 2707.32], "text": " like if it's a million times smarter than me, would just be like did we do a good job?"}, {"timestamp": [2708.72, 2710.44], "text": " Where can we do better?"}, {"timestamp": [2710.44, 2712.44], "text": " That's the number one question"}, {"timestamp": [2712.44, 2716.04], "text": " Who are some of the best people to follow to keep up to date with the latest tools and use cases?"}, {"timestamp": [2716.2, 2722.98], "text": " Honestly this discord the cognitive AI lab discord description in the comment just link in the description. There we go"}, {"timestamp": [2723.48, 2725.52], "text": " That's where I get most of my news."}, {"timestamp": [2731.44, 2739.28], "text": " How do the Heuristic Imperatives imbue a value of friendship? Which one makes that a cohesive goal? So when you ask a language model about prosperity in particular, prosperity is"}, {"timestamp": [2742.08, 2747.36], "text": " prosperity means to live well. It's based on Latin prosperitas, which means to live well."}, {"timestamp": [2748.24, 2754.32], "text": " We generally think that it means to be wealthy, but that's modern capitalism talking. So in order"}, {"timestamp": [2754.32, 2759.76], "text": " for a human to live well, and there's lots and lots of proxies and metrics, human connection"}, {"timestamp": [2759.76, 2767.72], "text": " is in pretty much every psychological assessment from self-determination theory to Maslow's hierarchy of needs."}, {"timestamp": [2767.72, 2770.24], "text": " So it is very well understood that friendship, love,"}, {"timestamp": [2770.24, 2774.8], "text": " and connection are critical for humans to be prosperous."}, {"timestamp": [2774.8, 2775.4], "text": " Let's see."}, {"timestamp": [2775.4, 2776.98], "text": " Regarding your heuristic imperatives,"}, {"timestamp": [2776.98, 2779.96], "text": " how would you know objectively that you, and therefore"}, {"timestamp": [2779.96, 2782.68], "text": " those AI systems, are increasing prosperity?"}, {"timestamp": [2782.68, 2783.52], "text": " I just mentioned it."}, {"timestamp": [2783.52, 2785.84], "text": " So you can't measure it directly."}, {"timestamp": [2785.84, 2787.76], "text": " You have to look for proxies."}, {"timestamp": [2787.76, 2790.2], "text": " So you look for proxies like quality of life,"}, {"timestamp": [2790.2, 2793.68], "text": " Gini coefficient, education, and we'll probably"}, {"timestamp": [2793.68, 2795.04], "text": " need some new benchmarks."}, {"timestamp": [2795.04, 2797.08], "text": " But generally speaking, you look for proxies,"}, {"timestamp": [2797.08, 2799.28], "text": " like what is the average health, how much time"}, {"timestamp": [2799.28, 2801.72], "text": " do you spend outside, what is the average number of friends"}, {"timestamp": [2801.72, 2804.04], "text": " that people have, that sort of stuff."}, {"timestamp": [2804.04, 2805.88], "text": " Let's see, there are still a lot of programmers"}, {"timestamp": [2805.88, 2809.78], "text": " who say that AI will not replicate or replace."}, {"timestamp": [2809.78, 2811.68], "text": " Oh, that it won't replace them."}, {"timestamp": [2811.68, 2813.46], "text": " Are they just in denial?"}, {"timestamp": [2813.46, 2816.08], "text": " Is there any good source that could support their view?"}, {"timestamp": [2816.08, 2817.32], "text": " It'll be a while."}, {"timestamp": [2817.32, 2819.48], "text": " So I mentioned just a little while ago"}, {"timestamp": [2819.48, 2824.22], "text": " that particularly seasoned developers"}, {"timestamp": [2824.22, 2827.12], "text": " who can keep track of entire code bases in their head,"}, {"timestamp": [2827.12, 2828.24], "text": " AI can't do that yet."}, {"timestamp": [2828.24, 2831.1], "text": " But once we get to, once we figure out knowledge graphs"}, {"timestamp": [2831.1, 2835.0], "text": " and language models to keep track of code bases,"}, {"timestamp": [2835.0, 2836.82], "text": " you should be worried."}, {"timestamp": [2836.82, 2840.84], "text": " But yeah, a shorter answer is a lot of people are in denial."}, {"timestamp": [2840.84, 2844.36], "text": " That being said, they are correct that it can't do it yet."}, {"timestamp": [2844.36, 2845.04], "text": " Do you see AI"}, {"timestamp": [2845.04, 2850.84], "text": " affecting the distribution of wealth, and if so, how? Yeah, so I have talked about"}, {"timestamp": [2850.84, 2855.6], "text": " this extensively in a few videos. My hope... so there's basically two"}, {"timestamp": [2855.6, 2859.2], "text": " primary things that can happen, and I think both are gonna happen to a greater"}, {"timestamp": [2859.2, 2866.32], "text": " or lesser degree. On the one hand, AI is gonna make some some people obscenely wealthy, wealthier than we've"}, {"timestamp": [2866.32, 2871.28], "text": " ever seen. The first individual trillionaire is probably going to be an AI entrepreneur."}, {"timestamp": [2872.4, 2875.12], "text": " There's already trillion dollar companies and most of those are tech companies."}, {"timestamp": [2876.48, 2883.04], "text": " So that's going to happen. On the other hand, AI could help us generate such a huge hyper"}, {"timestamp": [2883.04, 2888.88], "text": " abundance of cognitive labor that we all are functionally wealthier."}, {"timestamp": [2888.88, 2890.6], "text": " That's what I'm hoping."}, {"timestamp": [2890.6, 2895.36], "text": " What is your opinion on Eliadzor Yakovsky?"}, {"timestamp": [2895.36, 2901.4], "text": " To be political, diplomatic about it,"}, {"timestamp": [2901.4, 2904.8], "text": " I think that he's just a doomer, honestly."}, {"timestamp": [2904.8, 2906.08], "text": " I've tried reading some"}, {"timestamp": [2906.08, 2912.4], "text": " of his stuff and it dives off into speculation and makes a lot of assumptions. And honestly,"}, {"timestamp": [2912.4, 2917.86], "text": " so here's the final, not final word, my current word on people like Eliadzer Yukowsky and"}, {"timestamp": [2917.86, 2926.44], "text": " other doomers, and also some people with their head in the sand, not saying that he is, but it's time to do experiments."}, {"timestamp": [2926.44, 2931.28], "text": " So much of what was, of everything that has been said about alignment up until now is"}, {"timestamp": [2931.28, 2935.26], "text": " kind of irrelevant because now we can actually do experiments."}, {"timestamp": [2935.26, 2939.22], "text": " And so that is, I think, like if someone is just a philosopher or has just been writing"}, {"timestamp": [2939.22, 2943.0], "text": " books about it and hasn't actually done the experiments, I kind of don't care what they"}, {"timestamp": [2943.0, 2945.36], "text": " have to say anymore."}, {"timestamp": [2945.36, 2948.64], "text": " Job market, I've answered a few questions on that already,"}, {"timestamp": [2948.64, 2950.22], "text": " so I won't do that."}, {"timestamp": [2950.22, 2953.54], "text": " Let's see, we are building a user database itself."}, {"timestamp": [2954.5, 2955.8], "text": " We are building a platform"}, {"timestamp": [2955.8, 2958.24], "text": " for developing extensible cognitive architectures"}, {"timestamp": [2958.24, 2960.6], "text": " to make it easy for anyone to build cognitive loops"}, {"timestamp": [2960.6, 2962.0], "text": " in a low-code way."}, {"timestamp": [2962.0, 2963.44], "text": " Do you see a risk in allowing users"}, {"timestamp": [2963.44, 2964.5], "text": " to be able to build these systems"}, {"timestamp": [2964.5, 2966.64], "text": " without understanding fundamental principles?"}, {"timestamp": [2966.64, 2968.68], "text": " And do you have any functions that we should incorporate?"}, {"timestamp": [2968.68, 2970.48], "text": " Yes, the heuristic imperatives."}, {"timestamp": [2970.48, 2972.64], "text": " That being said, I think that the biggest problem"}, {"timestamp": [2972.64, 2976.48], "text": " you're gonna run into in making consumer plug-and-play"}, {"timestamp": [2976.48, 2979.28], "text": " cognitive architectures is that there's gonna be"}, {"timestamp": [2979.28, 2981.22], "text": " a few standard components, and you're probably"}, {"timestamp": [2981.22, 2982.96], "text": " just gonna need to color code them, right?"}, {"timestamp": [2982.96, 2987.44], "text": " Like a memory system will be a yellow block and then a moral system will be a"}, {"timestamp": [2987.44, 2990.0], "text": " green block and then a task system will be a red block."}, {"timestamp": [2990.0, 2993.62], "text": " And then you just have some standardized components that you plug in together."}, {"timestamp": [2993.9, 2996.28], "text": " And so say for instance, you have, you know,"}, {"timestamp": [2996.28, 3000.7], "text": " a morality module that is the heuristic imperatives and you plug that in and see"}, {"timestamp": [3000.7, 3001.34], "text": " how it goes."}, {"timestamp": [3001.34, 3004.36], "text": " And then someone comes up with something better and you swap out the parts."}, {"timestamp": [3004.36, 3006.64], "text": " That's how I would approach that."}, {"timestamp": [3006.64, 3008.68], "text": " Can you do a face like Elias?"}, {"timestamp": [3008.68, 3009.44], "text": " I don't know."}, {"timestamp": [3009.44, 3010.8], "text": " I mean, he's got a beard."}, {"timestamp": [3010.8, 3012.32], "text": " I don't know what you mean."}, {"timestamp": [3012.32, 3015.2], "text": " I'm not wearing a fedora."}, {"timestamp": [3015.2, 3016.44], "text": " Let's see."}, {"timestamp": [3016.44, 3018.22], "text": " Trying to use GPT to do automation."}, {"timestamp": [3018.22, 3020.72], "text": " Also, I'm going to wind down, because I promised my fiancee"}, {"timestamp": [3020.72, 3022.16], "text": " I would help with dinner."}, {"timestamp": [3022.16, 3025.36], "text": " So only like two or three more questions."}, {"timestamp": [3029.1, 3029.92], "text": " Let's see, workflow automation, JSON schema,"}, {"timestamp": [3032.0, 3035.32], "text": " YAML, look up Ansible. Ansible has already figured out workflow automation"}, {"timestamp": [3035.32, 3036.36], "text": " and it's all based on YAML."}, {"timestamp": [3036.36, 3038.92], "text": " So I would go with that direction."}, {"timestamp": [3038.92, 3041.7], "text": " Will traditional forms of money as we know it today"}, {"timestamp": [3041.7, 3043.68], "text": " eventually become obsolete?"}, {"timestamp": [3043.68, 3049.12], "text": " No, so I addressed this in my post-singularity predictions. Currency is going to stick"}, {"timestamp": [3049.12, 3054.48], "text": " around just because it is such a useful invention. The seniorage of it or the"}, {"timestamp": [3054.48, 3058.96], "text": " the sovereign backing might change. We might eventually shift to cryptocurrency"}, {"timestamp": [3058.96, 3063.2], "text": " but generally speaking I think that fiat currency is here to"}, {"timestamp": [3063.2, 3066.16], "text": " stay as long as there are governments."}, {"timestamp": [3066.16, 3067.16], "text": " Let's see."}, {"timestamp": [3067.16, 3071.4], "text": " Can it be simply explained what this might mean for the average family in the next year?"}, {"timestamp": [3071.4, 3073.28], "text": " Your average family I think will be okay."}, {"timestamp": [3073.28, 3078.0], "text": " A lot of families are going to be impacted by layoffs."}, {"timestamp": [3078.0, 3079.0], "text": " Let's see."}, {"timestamp": [3079.0, 3081.36], "text": " Open assistant."}, {"timestamp": [3081.36, 3084.16], "text": " Let's do one more interesting question."}, {"timestamp": [3084.16, 3085.96], "text": " Here, let me go back to the"}, {"timestamp": [3086.6, 3088.28], "text": " YouTube chat"}, {"timestamp": [3088.28, 3090.28], "text": " GPT for proto-agi Wow"}, {"timestamp": [3092.8, 3094.8], "text": " There's a lot"}, {"timestamp": [3095.08, 3097.52], "text": " One more good question, and then I have to hang up for the night"}, {"timestamp": [3108.8, 3109.8], "text": " Okay Okay. Here we go."}, {"timestamp": [3109.8, 3111.6], "text": " This is a good enough question."}, {"timestamp": [3111.6, 3115.08], "text": " Which is the most plausible path for achieving AGI?"}, {"timestamp": [3115.08, 3120.48], "text": " The auto GPT way, the Jarvis way, or something new as prime neural layer of multiple AIs"}, {"timestamp": [3120.48, 3124.82], "text": " controlling second layer of highly trained narrow AIs?"}, {"timestamp": [3124.82, 3125.88], "text": " So that last one is"}, {"timestamp": [3125.88, 3130.52], "text": " closest to a cognitive architecture which has been my number one go-to way"}, {"timestamp": [3130.52, 3135.24], "text": " all along. Because when you have a cognitive architecture, like the physical"}, {"timestamp": [3135.24, 3138.68], "text": " architecture of our brains doesn't change that much, right? You always have a"}, {"timestamp": [3138.68, 3142.98], "text": " hippocampus, you always have a cerebellum, you always have, you know, brainstem. Now"}, {"timestamp": [3142.98, 3145.16], "text": " those individual components can learn"}, {"timestamp": [3145.16, 3147.84], "text": " and adapt over time, and the way that they talk"}, {"timestamp": [3147.84, 3149.8], "text": " to each other can adapt over time."}, {"timestamp": [3149.8, 3152.44], "text": " So with a cognitive architecture,"}, {"timestamp": [3152.44, 3154.52], "text": " what I was just saying to Database"}, {"timestamp": [3154.52, 3156.64], "text": " about having a pluggable architecture"}, {"timestamp": [3156.64, 3159.4], "text": " where you can swap out or upgrade components,"}, {"timestamp": [3159.4, 3160.64], "text": " that I think is the way to go,"}, {"timestamp": [3160.64, 3164.74], "text": " and that's why I built Remo as an API,"}, {"timestamp": [3164.74, 3165.96], "text": " as a microservice,"}, {"timestamp": [3165.96, 3170.52], "text": " is because it's like, OK, well, you have one Remo version 1,"}, {"timestamp": [3170.52, 3173.08], "text": " but let's say Remo version 2 comes out,"}, {"timestamp": [3173.08, 3176.28], "text": " and you don't want to rebuild your entire architecture."}, {"timestamp": [3176.28, 3179.04], "text": " You swap it out."}, {"timestamp": [3179.04, 3182.64], "text": " And then ditto for even if you need"}, {"timestamp": [3182.64, 3186.36], "text": " to change an underlying foundation model, if you need to change an underlying foundation model,"}, {"timestamp": [3186.36, 3188.5], "text": " if you need to point at a new API,"}, {"timestamp": [3188.5, 3192.14], "text": " because we basically all treat large language models as a microservice."}, {"timestamp": [3192.14, 3194.1], "text": " It's just a microservice that's up in the Cloud."}, {"timestamp": [3194.1, 3198.74], "text": " Now, some of the architects that I'm observing and working with,"}, {"timestamp": [3198.74, 3202.1], "text": " model selection is actually going to be a really big thing."}, {"timestamp": [3202.1, 3204.02], "text": " If you can use a really cheap,"}, {"timestamp": [3204.02, 3205.32], "text": " small 300 million parameter model to do use a really cheap, small,"}, {"timestamp": [3205.32, 3208.48], "text": " 300 million parameter model to do some of the tasks, great."}, {"timestamp": [3208.48, 3210.96], "text": " Do it, because it'll be faster and cheaper."}, {"timestamp": [3210.96, 3212.52], "text": " But then, of course, yes, there will"}, {"timestamp": [3212.52, 3216.72], "text": " be selection of all kinds of models, right?"}, {"timestamp": [3216.72, 3220.44], "text": " Visual, audio, identity, security,"}, {"timestamp": [3220.44, 3221.52], "text": " all kinds of stuff."}, {"timestamp": [3221.52, 3226.28], "text": " So definitely, I think a decoupled modular architecture"}, {"timestamp": [3226.28, 3227.12], "text": " is the way to go."}, {"timestamp": [3227.12, 3229.12], "text": " Microservices architecture is just one way"}, {"timestamp": [3229.12, 3231.42], "text": " to achieve a modular architecture."}, {"timestamp": [3231.42, 3232.6], "text": " And it's my favorite way to do it"}, {"timestamp": [3232.6, 3234.2], "text": " because I was a systems engineer."}, {"timestamp": [3235.06, 3237.98], "text": " All right, gang, that has been a lot of fun."}, {"timestamp": [3237.98, 3240.02], "text": " And wow, that hour went by fast."}, {"timestamp": [3240.96, 3243.74], "text": " All right, so have a good night, everybody."}, {"timestamp": [3243.74, 3247.44], "text": " Happy Friday, happy weekend. and yeah, take care."}]}