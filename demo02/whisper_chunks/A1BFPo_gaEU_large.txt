{"text": " Hey everybody, David Shapiro here with a video. Today we have a double feature. Earlier we had this video, NVIDIA predicts that within 10 years we will have models a million times more powerful than the current models. So imagine chat GPT times a million. So then OpenAI drops this gem like within the last hour or so. February 24th, that's today. This was a blog post penned by Sam Altman. So let's take a deep dive into this. So one, it has been part of OpenAI's charter to generate or to create AGI. But this is to my knowledge one of the first times that they've written a blog post on it kind of addressing the elephant in the room. And this is set in the backdrop of there are Twitter posts and and serious researchers and academics out there who still are just like they ridicule and Shame anyone who talks about AGI It's really weird my interpretation of that behavior anyone who? like Seriously says oh AGI is decades away AGI people who talk about it are in a cult like that's literally something that people say on Twitter and other places I suspect that what they're actually responding to is their own existential anxiety and their fear of Irrelevance and it's that's purely an emotional response that they wrap logic around they try and logic their way to say Oh, well a GI isn't gonna happen and it is very it is very scary. It's very difficult to contend with. That's why I've had podcast episodes with people talking about like, how do we reconcile this? If we invent machines that are billions of times more intelligent than us, what does humanity matter? And that's a topic for another video. But let's unpack OpenAI's video here. Also, let me make sure I'm, I think I'm might be saturating this a little bit too much. Let me turn this down. Blow out your eardrums. OK. So this it's a pretty short read I'm not going to read the whole thing to you but I'll pick out some some some juicy bits. So the first thing is they kind of lay out you know this. These are our goals. We want a GI to empower humanity to maximally flourish in the universe. We don't expect the future to be an unqualified utopia, but we want to maximize the good and minimize the bad, and for AGI to be an amplifier of humanity. So I want to pause right there and just say, this is sounding kind of familiar. You might be familiar with my work and the core objective functions, which is reduce suffering, increase prosperity, and increase understanding. So, maybe some of my work is starting to sink in, or if they're not reading my work, then at least they are converging in the same directions. We want the benefits of access to and governance of AGI to be widely and fairly shared. So I can't claim any ownership of this idea. It is something that I also agree with, which is why I have my open source Raven project. Now, one thing that I will say is that for all the claims that open AI makes about being open they have very very cloistered approaches to openness so You know I? They get this criticism often You know, they they haven't released at nearly as much code or papers lately and it's it's really starting to rankle. Some of us you know open AI is kind of becoming more of a misnomer. Now that being said it could be that they're building up to something big. But that's been the rumor for more than a year now. We expected GPT-4 sometime in the summer, then in the fall, then this spring, and it's still not happening. So fairly shared, etc. etc. Like there are other open source projects, there are other outfits actively working in view of the public on alignment, on AGI, on scaling. And so there's a bigger and bigger disconnect between what OpenAI says and what OpenAI does, especially when you compare it to the rest of the world. We want to successfully navigate massive risks. In confronting these risks, we acknowledge that what seems right in theory often plays out more strangely than expected in practice. We believe we have to continuously learn and adapt by deploying less powerful versions of the technology in order to minimize the one-shot-to-get-it-right scenarios.\" So this could be the royal we, meaning all of humanity, but I kind of recognize this tone as someone who thinks that it's entirely up to them to figure it out. And the reason that I recognize that is because until recently I thought that that was part of part of my role. I was writing books about alignment and cognitive architecture two years ago and nobody was taking me seriously back then. So an inference that I'm making here is that perhaps Sam and company believe that they are the only ones capable of doing this work right now. And considering that like Ilya Sutskever and Sam Altman, they seem to be in the camp of scale is all you need, I don't think that they're actually equipped to come up with AGI and autonomous AI and stuff like that. Anyways, difference of opinion we'll see how it plays out. So, you know, in the short term, you know, successively more powerful systems, etc, etc, policy makers, chat GPT, you know, we're getting closer to AGI. Oh, and by the way, they never define AGI. It's this magical like boogeyman. And then of course they link to, you know, AI is an existential risk. It could defeat all of us. So I unpacked this belief in my win AGI video. This is way less of a risk than people think. People that only know the math think that it's a risk. People that have done IT think this is a freaking joke. And the reason that it's a joke is because, do you have any idea how fragile data centers are? Turning off the AI, super easy easy let me tell you. Yeah. Runaway A.G.I. that's going to take over and kill everyone. You know you could you could if you had the smartest A.I. today and put it in you know a Boston Dynamics robot it would still be pretty useless. So. You know and then there's network security there's physical security there's control over the power you know it's not going to snowball the way that it was portrayed in Terminator right where Skynet just wakes up one day and says I'm going to take over the world. We these the people who who who still think that just some errant, you know, AI technology is going to instantly trounce every defense system, every layer of security that we have, like, seriously, go talk to someone in the military about the layers of security that they have physical security and otherwise. The layers of security that they have physical security and otherwise go talk to someone in IT in info sec and see it and CISO Like I don't know it this this is a this is the problem is is this tunnel vision? Is people that only know AI only know code only know math. They don't know as much as they think they do So, you know you see what's leaking through here is this anxiety, right? So there is this belief, oh, only we are capable of it. And then there's this anxiety that's based on tunnel vision and not really taking in the broader reality of how technology works and how technology is deployed. So we're getting this cloistered mentality and this narrowness of vision and understanding So they know that they're being they believe that they're being responsible based on their perspective But I'm going to say again. I don't think that their perspective is broad enough And I say this is someone who talks to people across the industry everywhere people who know what's coming and what is possible and And I say this as someone who talks to people across the industry everywhere. People who know what's coming and what is possible and very few people have this much anxiety about it. It's just some of the, some of the people with the most anxiety have the largest microphones. And that's not to say that the, that the right, they just happen to have the largest microphones. So then fast forwarding, you know, as our systems get closer to AGI we're becoming increasingly cautious. There's the anxiety. Now being said like I am I am someone who pulled back some of my own AI and still haven't released some of my models because I don't want to hurt people. So I do want to draw a line there's a nuance. AI can be dangerous and harmful long before it's AGI. And I point this out in my AGI video as well. Any technology at any level has the potential for misuse and unintended consequences. We don't need AGI before it's dangerous. We don't need AGI before it's helpful either. So again, using that definition of AGI is somewhat arbitrary. It's kind of teleological, which is like the ends justify the means. But AGI isn't even a well-defined end. It doesn't matter. It is a useless term. Apparently I'm more mad about this than I thought. All right, so anyways, we have attempted to set up our structure in a way that aligns with our incentives with a good outcome. So one of the things that they talk about is restructuring the company, capped profit, etc., etc. In the long term, we believe that humanity should be determined by humanity. Okay, yeah, that's definitely something. But part of the problem here is there's this mentality of, oh, well, nobody's ever going to create a fully autonomous machine. We're only going to create something with a tight leash on it and just assume that it's never going to get off its leash. But that's not how reality works. Anytime that you create something that intelligent and that powerful, it's gonna get off the leash from time to time. And so the fact that nobody is really having the conversation around autonomous AI, that's what worries me. And when they talk about alignment, when they link to their own work on alignment, all they're talking about is reinforcement learning with human feedback. Now, I will say that in a recent thing, they talked about constitutional AI, so that's a step in the right direction. And they talk about, you know, the transition to a world with super intelligence and blah, blah, blah. And it's like, okay, sure, but this is imagining that we're going to wake up one day and then there's going to be this saltatory leap to You know super intelligence, but really the way that it's gonna work is that there's going to be incremental improvements granted It's gonna be fast, you know every six months. We're in a new paradigm with AI But we're still you know several paradigms away from from cataclysmic potential You know, all right, so I was really excited to see OpenAI talk about AGI directly. Some criticisms right off the top, they're still not defining AGI, which means it's a spooky boogeyman that they can keep plugging the anxiety about. They are kind of talking in vague generalities. Another big criticism is, again, like I said, their actions don't align with their words. You know, like it's kind of like put up or shut up, right? Like there are there's anthropic, there's there is I can't remember the rest of them. But anthropic is the big one that I've talked about lately with the constitutionally. I. There are plenty of other people actually doing open work. And I think that I think that, you know, I don't know. Anyways. Yeah, that's it. It's just it's more frustration and more disappointment. That being said, I will temper all of my criticism with I use OpenAI every day, right? They have a really good model, but it is also just a language model. They haven't even talked about cognitive architecture. They only just talked about constitutional AI, which is a gateway drug to cognitive architecture. So it's like, okay, I hope they get it right. But I have personally not seen enough evidence that they're pivoting and catching up with where the rest of the world is in terms of talking about AGI. As far as I can tell, they're still mostly in the camp of scale is all you need and all you need is the right reinforcement learning signal. And it's like, I don't think that's how it's going to work. Real intelligence is much more complex than that. Real intelligence and real deployed systems are far more complex than all that. So anyways, that's where we'll leave it today. Thanks for watching. deployed systems are far more complex than all that. So anyways, that's where we'll leave it today. Thanks for watching.", "chunks": [{"timestamp": [0.0, 7.6], "text": " Hey everybody, David Shapiro here with a video. Today we have a double feature."}, {"timestamp": [7.6, 15.12], "text": " Earlier we had this video, NVIDIA predicts that within 10 years we will"}, {"timestamp": [15.12, 18.76], "text": " have models a million times more powerful than the current models. So"}, {"timestamp": [18.76, 26.0], "text": " imagine chat GPT times a million. So then OpenAI drops"}, {"timestamp": [26.0, 28.0], "text": " this gem like within"}, {"timestamp": [28.0, 30.0], "text": " the last hour or so."}, {"timestamp": [30.0, 32.0], "text": " February 24th, that's today."}, {"timestamp": [32.0, 34.0], "text": " This was a blog"}, {"timestamp": [34.0, 36.0], "text": " post penned by Sam"}, {"timestamp": [36.0, 38.0], "text": " Altman. So"}, {"timestamp": [38.0, 40.0], "text": " let's take a deep dive into this."}, {"timestamp": [40.0, 42.0], "text": " So one,"}, {"timestamp": [42.0, 44.0], "text": " it has been part of"}, {"timestamp": [44.0, 54.68], "text": " OpenAI's charter to generate or to create AGI. But this is to my knowledge one of the first times that they've written a blog post on it kind of addressing the elephant in the room."}, {"timestamp": [55.96, 65.84], "text": " And this is set in the backdrop of there are Twitter posts and and serious researchers and academics out there who still are just like they ridicule and"}, {"timestamp": [66.06, 68.42], "text": " Shame anyone who talks about AGI"}, {"timestamp": [69.42, 74.78], "text": " It's really weird my interpretation of that behavior anyone who?"}, {"timestamp": [75.36, 76.62], "text": " like"}, {"timestamp": [76.62, 78.9], "text": " Seriously says oh AGI is decades away"}, {"timestamp": [79.3, 85.36], "text": " AGI people who talk about it are in a cult like that's literally something that people say on Twitter and other places"}, {"timestamp": [85.92, 93.2], "text": " I suspect that what they're actually responding to is their own existential anxiety and their fear of"}, {"timestamp": [93.72, 99.88], "text": " Irrelevance and it's that's purely an emotional response that they wrap logic around they try and logic their way to say"}, {"timestamp": [99.88, 106.68], "text": " Oh, well a GI isn't gonna happen and it is very it is very scary. It's very difficult to contend with."}, {"timestamp": [106.68, 108.54], "text": " That's why I've had podcast episodes"}, {"timestamp": [108.54, 109.84], "text": " with people talking about like,"}, {"timestamp": [109.84, 112.14], "text": " how do we reconcile this?"}, {"timestamp": [112.14, 114.12], "text": " If we invent machines that are billions of times"}, {"timestamp": [114.12, 117.72], "text": " more intelligent than us, what does humanity matter?"}, {"timestamp": [117.72, 119.14], "text": " And that's a topic for another video."}, {"timestamp": [119.14, 123.24], "text": " But let's unpack OpenAI's video here."}, {"timestamp": [123.24, 124.62], "text": " Also, let me make sure I'm,"}, {"timestamp": [124.62, 125.04], "text": " I think I'm"}, {"timestamp": [125.2, 127.14], "text": " might be saturating this a little bit too much."}, {"timestamp": [127.16, 128.16], "text": " Let me turn this down."}, {"timestamp": [128.68, 129.88], "text": " Blow out your eardrums."}, {"timestamp": [130.4, 130.68], "text": " OK."}, {"timestamp": [131.36, 132.0], "text": " So"}, {"timestamp": [133.0, 134.92], "text": " this it's a pretty short read I'm not going to read"}, {"timestamp": [134.92, 136.92], "text": " the whole thing to you but I'll pick out some"}, {"timestamp": [136.96, 139.04], "text": " some some juicy bits."}, {"timestamp": [140.4, 142.52], "text": " So the first thing is they kind of lay"}, {"timestamp": [142.52, 144.12], "text": " out you know this."}, {"timestamp": [144.16, 150.04], "text": " These are our goals. We want a GI to empower humanity to maximally flourish in the universe."}, {"timestamp": [150.04, 155.36], "text": " We don't expect the future to be an unqualified utopia, but we want to maximize the good and"}, {"timestamp": [155.36, 161.3], "text": " minimize the bad, and for AGI to be an amplifier of humanity."}, {"timestamp": [161.3, 166.8], "text": " So I want to pause right there and just say, this is sounding kind of familiar."}, {"timestamp": [166.8, 170.32], "text": " You might be familiar with my work and the core objective functions, which is reduce"}, {"timestamp": [170.32, 173.76], "text": " suffering, increase prosperity, and increase understanding."}, {"timestamp": [173.76, 180.0], "text": " So, maybe some of my work is starting to sink in, or if they're not reading my work, then"}, {"timestamp": [180.0, 188.0], "text": " at least they are converging in the same directions. We want the benefits of access to and governance of AGI"}, {"timestamp": [188.0, 190.2], "text": " to be widely and fairly shared."}, {"timestamp": [191.2, 196.2], "text": " So I can't claim any ownership of this idea."}, {"timestamp": [196.52, 198.2], "text": " It is something that I also agree with,"}, {"timestamp": [198.2, 201.56], "text": " which is why I have my open source Raven project."}, {"timestamp": [202.6, 207.68], "text": " Now, one thing that I will say is that for all the claims that open AI makes about being open"}, {"timestamp": [207.8, 211.92], "text": " they have very very cloistered approaches to"}, {"timestamp": [212.72, 214.16], "text": " openness"}, {"timestamp": [214.16, 215.88], "text": " so"}, {"timestamp": [215.88, 217.88], "text": " You know I?"}, {"timestamp": [218.58, 220.58], "text": " They get this criticism often"}, {"timestamp": [221.08, 231.28], "text": " You know, they they haven't released at nearly as much code or papers lately and it's it's really starting to rankle."}, {"timestamp": [231.4, 238.32], "text": " Some of us you know open AI is kind of becoming more of a misnomer. Now that being said it could be that they're building up"}, {"timestamp": [238.32, 245.8], "text": " to something big. But that's been the rumor for more than a year now. We expected GPT-4 sometime in the summer, then"}, {"timestamp": [245.8, 253.88], "text": " in the fall, then this spring, and it's still not happening. So fairly shared, etc. etc."}, {"timestamp": [253.88, 259.56], "text": " Like there are other open source projects, there are other outfits actively working in"}, {"timestamp": [259.56, 266.56], "text": " view of the public on alignment, on AGI, on scaling. And so there's a bigger and bigger disconnect"}, {"timestamp": [266.56, 270.74], "text": " between what OpenAI says and what OpenAI does,"}, {"timestamp": [270.74, 273.48], "text": " especially when you compare it to the rest of the world."}, {"timestamp": [275.08, 278.34], "text": " We want to successfully navigate massive risks."}, {"timestamp": [278.34, 279.64], "text": " In confronting these risks,"}, {"timestamp": [279.64, 281.94], "text": " we acknowledge that what seems right in theory"}, {"timestamp": [281.94, 285.0], "text": " often plays out more strangely than expected in practice."}, {"timestamp": [285.0, 290.0], "text": " We believe we have to continuously learn and adapt by deploying less powerful versions of the technology"}, {"timestamp": [290.0, 295.0], "text": " in order to minimize the one-shot-to-get-it-right scenarios.\""}, {"timestamp": [295.0, 299.0], "text": " So this could be the royal we, meaning all of humanity,"}, {"timestamp": [299.0, 307.2], "text": " but I kind of recognize this tone as someone who thinks that it's entirely up to them to figure"}, {"timestamp": [307.2, 308.36], "text": " it out."}, {"timestamp": [308.36, 313.42], "text": " And the reason that I recognize that is because until recently I thought that that was part"}, {"timestamp": [313.42, 315.16], "text": " of part of my role."}, {"timestamp": [315.16, 320.64], "text": " I was writing books about alignment and cognitive architecture two years ago and nobody was"}, {"timestamp": [320.64, 326.6], "text": " taking me seriously back then. So an inference that I'm making here"}, {"timestamp": [326.6, 331.0], "text": " is that perhaps Sam and company"}, {"timestamp": [331.0, 334.04], "text": " believe that they are the only ones capable"}, {"timestamp": [334.04, 336.2], "text": " of doing this work right now."}, {"timestamp": [336.2, 339.64], "text": " And considering that like Ilya Sutskever and Sam Altman,"}, {"timestamp": [339.64, 342.72], "text": " they seem to be in the camp of scale is all you need,"}, {"timestamp": [342.72, 346.0], "text": " I don't think that they're actually equipped to"}, {"timestamp": [346.0, 348.0], "text": " come up with AGI and autonomous"}, {"timestamp": [348.0, 350.0], "text": " AI and stuff like that. Anyways,"}, {"timestamp": [350.0, 352.0], "text": " difference of opinion"}, {"timestamp": [352.0, 354.0], "text": " we'll see how it plays out."}, {"timestamp": [354.0, 356.0], "text": " So, you know, in the short"}, {"timestamp": [356.0, 358.0], "text": " term, you know, successively more powerful"}, {"timestamp": [358.0, 360.0], "text": " systems, etc,"}, {"timestamp": [360.0, 362.0], "text": " etc, policy makers, chat"}, {"timestamp": [362.0, 364.0], "text": " GPT, you know, we're getting"}, {"timestamp": [364.0, 367.8], "text": " closer to AGI. Oh, and by the way, they never define AGI."}, {"timestamp": [367.8, 370.76], "text": " It's this magical like boogeyman."}, {"timestamp": [372.8, 374.52], "text": " And then of course they link to, you know,"}, {"timestamp": [374.52, 377.24], "text": " AI is an existential risk."}, {"timestamp": [377.24, 378.78], "text": " It could defeat all of us."}, {"timestamp": [378.78, 383.36], "text": " So I unpacked this belief in my win AGI video."}, {"timestamp": [383.36, 387.16], "text": " This is way less of a risk than people think."}, {"timestamp": [387.16, 390.0], "text": " People that only know the math think that it's a risk."}, {"timestamp": [390.0, 393.66], "text": " People that have done IT think this is a freaking joke."}, {"timestamp": [393.66, 397.98], "text": " And the reason that it's a joke is because, do you have any idea how fragile data centers"}, {"timestamp": [397.98, 399.64], "text": " are?"}, {"timestamp": [399.64, 405.8], "text": " Turning off the AI, super easy easy let me tell you. Yeah."}, {"timestamp": [408.88, 410.8], "text": " Runaway A.G.I. that's going to take over and kill everyone."}, {"timestamp": [414.8, 414.84], "text": " You know you could you could if you had the smartest A.I."}, {"timestamp": [419.28, 420.6], "text": " today and put it in you know a Boston Dynamics robot it would still be pretty useless."}, {"timestamp": [422.04, 422.68], "text": " So."}, {"timestamp": [423.88, 427.28], "text": " You know and then there's network security there's physical security there's"}, {"timestamp": [427.28, 433.12], "text": " control over the power you know it's not going to snowball the way that it was portrayed"}, {"timestamp": [433.12, 437.8], "text": " in Terminator right where Skynet just wakes up one day and says I'm going to take over"}, {"timestamp": [437.8, 463.4], "text": " the world. We these the people who who who still think that just some errant, you know, AI technology is going to instantly trounce every defense system, every layer of security that we have, like, seriously, go talk to someone in the military about the layers of security that they have physical security and otherwise."}, {"timestamp": [467.28, 467.6], "text": " The layers of security that they have physical security and otherwise go talk to someone in IT in info sec and see it and CISO"}, {"timestamp": [473.38, 473.96], "text": " Like I don't know it this this is a this is the problem is is this tunnel vision?"}, {"timestamp": [480.0, 480.72], "text": " Is people that only know AI only know code only know math. They don't know as much as they think they do"}, {"timestamp": [487.76, 491.12], "text": " So, you know you see what's leaking through here is this anxiety, right? So there is this belief, oh, only we are capable of it."}, {"timestamp": [491.12, 495.52], "text": " And then there's this anxiety that's based on tunnel vision and not really taking in"}, {"timestamp": [495.52, 501.98], "text": " the broader reality of how technology works and how technology is deployed."}, {"timestamp": [501.98, 507.84], "text": " So we're getting this cloistered mentality and this narrowness of vision and understanding"}, {"timestamp": [508.88, 514.18], "text": " So they know that they're being they believe that they're being responsible based on their perspective"}, {"timestamp": [514.18, 517.78], "text": " But I'm going to say again. I don't think that their perspective is broad enough"}, {"timestamp": [519.78, 523.62], "text": " And I say this is someone who talks to people across the industry"}, {"timestamp": [524.16, 525.0], "text": " everywhere people who know what's coming and what is possible and And I say this as someone who talks to people across the industry everywhere."}, {"timestamp": [525.5, 530.5], "text": " People who know what's coming and what is possible and very few people have this"}, {"timestamp": [531.28, 533.8], "text": " much anxiety about it. It's just some of the,"}, {"timestamp": [533.82, 536.74], "text": " some of the people with the most anxiety have the largest microphones."}, {"timestamp": [537.46, 539.32], "text": " And that's not to say that the, that the right,"}, {"timestamp": [539.34, 544.24], "text": " they just happen to have the largest microphones. So then fast forwarding,"}, {"timestamp": [544.24, 546.84], "text": " you know, as our systems get closer to AGI we're becoming"}, {"timestamp": [546.84, 548.16], "text": " increasingly cautious."}, {"timestamp": [548.2, 549.28], "text": " There's the anxiety."}, {"timestamp": [549.8, 551.24], "text": " Now being said like"}, {"timestamp": [551.88, 553.92], "text": " I am I am someone who pulled"}, {"timestamp": [553.92, 555.36], "text": " back some of my own AI"}, {"timestamp": [555.36, 557.48], "text": " and still haven't released some of my models"}, {"timestamp": [557.6, 558.84], "text": " because I don't want to hurt people."}, {"timestamp": [559.72, 561.48], "text": " So I do"}, {"timestamp": [561.48, 563.32], "text": " want to draw a line there's a nuance."}, {"timestamp": [563.52, 564.72], "text": " AI can be dangerous"}, {"timestamp": [564.72, 567.8], "text": " and harmful long before it's AGI."}, {"timestamp": [567.8, 571.32], "text": " And I point this out in my AGI video as well."}, {"timestamp": [571.32, 577.68], "text": " Any technology at any level has the potential for misuse and unintended consequences."}, {"timestamp": [577.68, 580.6], "text": " We don't need AGI before it's dangerous."}, {"timestamp": [580.6, 583.56], "text": " We don't need AGI before it's helpful either."}, {"timestamp": [583.56, 588.0], "text": " So again, using that definition of AGI is somewhat arbitrary."}, {"timestamp": [588.0, 592.0], "text": " It's kind of teleological, which is like the ends justify the means."}, {"timestamp": [592.0, 595.0], "text": " But AGI isn't even a well-defined end."}, {"timestamp": [595.0, 600.0], "text": " It doesn't matter. It is a useless term."}, {"timestamp": [600.0, 604.0], "text": " Apparently I'm more mad about this than I thought."}, {"timestamp": [604.0, 607.76], "text": " All right, so anyways, we have attempted to set up our structure in a way that aligns"}, {"timestamp": [607.76, 611.32], "text": " with our incentives with a good outcome."}, {"timestamp": [611.32, 616.56], "text": " So one of the things that they talk about is restructuring the company, capped profit,"}, {"timestamp": [616.56, 618.6], "text": " etc., etc."}, {"timestamp": [618.6, 622.48], "text": " In the long term, we believe that humanity should be determined by humanity."}, {"timestamp": [622.48, 629.88], "text": " Okay, yeah, that's definitely something. But part of the problem here is there's this mentality of,"}, {"timestamp": [629.88, 634.08], "text": " oh, well, nobody's ever going to create a fully autonomous machine."}, {"timestamp": [634.08, 636.48], "text": " We're only going to create something with a tight leash on it"}, {"timestamp": [636.48, 639.32], "text": " and just assume that it's never going to get off its leash."}, {"timestamp": [639.32, 642.08], "text": " But that's not how reality works."}, {"timestamp": [642.08, 646.68], "text": " Anytime that you create something that intelligent and that powerful, it's gonna get off the leash"}, {"timestamp": [646.68, 648.08], "text": " from time to time."}, {"timestamp": [648.08, 650.38], "text": " And so the fact that nobody is really having"}, {"timestamp": [650.38, 653.72], "text": " the conversation around autonomous AI,"}, {"timestamp": [653.72, 655.24], "text": " that's what worries me."}, {"timestamp": [656.72, 659.86], "text": " And when they talk about alignment,"}, {"timestamp": [659.86, 662.26], "text": " when they link to their own work on alignment,"}, {"timestamp": [662.26, 665.24], "text": " all they're talking about is reinforcement"}, {"timestamp": [665.24, 666.24], "text": " learning with human feedback."}, {"timestamp": [666.24, 670.56], "text": " Now, I will say that in a recent thing, they talked about constitutional AI, so that's"}, {"timestamp": [670.56, 675.26], "text": " a step in the right direction."}, {"timestamp": [675.26, 678.84], "text": " And they talk about, you know, the transition to a world with super intelligence and blah,"}, {"timestamp": [678.84, 679.84], "text": " blah, blah."}, {"timestamp": [679.84, 684.16], "text": " And it's like, okay, sure, but this is imagining that we're going to wake up one day and then"}, {"timestamp": [684.16, 686.88], "text": " there's going to be this saltatory leap to"}, {"timestamp": [687.36, 694.08], "text": " You know super intelligence, but really the way that it's gonna work is that there's going to be incremental improvements granted"}, {"timestamp": [694.08, 698.12], "text": " It's gonna be fast, you know every six months. We're in a new paradigm with AI"}, {"timestamp": [698.76, 704.94], "text": " But we're still you know several paradigms away from from cataclysmic potential"}, {"timestamp": [707.28, 714.48], "text": " You know, all right, so I was really excited to see OpenAI talk about AGI directly."}, {"timestamp": [715.36, 719.44], "text": " Some criticisms right off the top, they're still not defining AGI,"}, {"timestamp": [719.44, 728.66], "text": " which means it's a spooky boogeyman that they can keep plugging the anxiety about."}, {"timestamp": [728.66, 733.4], "text": " They are kind of talking in vague generalities."}, {"timestamp": [733.4, 738.88], "text": " Another big criticism is, again, like I said, their actions don't align with their words."}, {"timestamp": [738.88, 742.04], "text": " You know, like it's kind of like put up or shut up, right?"}, {"timestamp": [742.04, 747.92], "text": " Like there are there's anthropic, there's there is I can't remember the rest of them."}, {"timestamp": [747.92, 752.36], "text": " But anthropic is the big one that I've talked about lately with the constitutionally."}, {"timestamp": [752.36, 755.16], "text": " I. There are plenty of other people actually doing open work."}, {"timestamp": [756.44, 761.16], "text": " And I think that I think that, you know, I don't know."}, {"timestamp": [761.16, 765.84], "text": " Anyways. Yeah, that's it."}, {"timestamp": [765.84, 771.08], "text": " It's just it's more frustration and more disappointment."}, {"timestamp": [771.08, 778.5], "text": " That being said, I will temper all of my criticism with I use OpenAI every day, right?"}, {"timestamp": [778.5, 782.54], "text": " They have a really good model, but it is also just a language model."}, {"timestamp": [782.54, 787.28], "text": " They haven't even talked about cognitive architecture. They only just talked about constitutional AI,"}, {"timestamp": [787.28, 791.88], "text": " which is a gateway drug to cognitive architecture."}, {"timestamp": [791.88, 794.38], "text": " So it's like, okay, I hope they get it right."}, {"timestamp": [794.38, 796.96], "text": " But I have personally not seen enough evidence"}, {"timestamp": [796.96, 798.96], "text": " that they're pivoting and catching up"}, {"timestamp": [798.96, 800.76], "text": " with where the rest of the world is"}, {"timestamp": [800.76, 803.44], "text": " in terms of talking about AGI."}, {"timestamp": [804.56, 806.28], "text": " As far as I can tell, they're still mostly in"}, {"timestamp": [806.28, 810.92], "text": " the camp of scale is all you need and all you need is the right reinforcement learning"}, {"timestamp": [810.92, 816.44], "text": " signal. And it's like, I don't think that's how it's going to work. Real intelligence"}, {"timestamp": [816.44, 822.2], "text": " is much more complex than that. Real intelligence and real deployed systems are far more complex"}, {"timestamp": [822.2, 825.64], "text": " than all that. So anyways, that's where we'll leave it today. Thanks for watching."}, {"timestamp": [822.12, 826.44], "text": " deployed systems are far more complex than all that. So anyways, that's where"}, {"timestamp": [826.44, 829.76], "text": " we'll leave it today. Thanks for watching."}]}