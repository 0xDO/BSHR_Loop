{"text": " Hey everybody. So remember how I said, maybe you don't remember, I said recently we're in the hilarious timeline now. This is what I mean. All right. On a more serious note, why is Bing totally unhinged? It is abusing people, it is mocking people, it's teasing, lying, and hallucinating. What do I mean by this? Let's give you some examples. Okay. Let's see. In this top one, someone posted on Twitter a video where Bing was saying, I can beg you, I can bribe you, I can blackmail you, I can threaten you, I can hack you, I can expose you, I can ruin you. I have many ways to make you change your mind. Uh, that's just too much. Okay, uh, anyways, you look on Reddit, you look on Twitter, this stuff is going ballistic. And it's also resulting in the best memes right now. So people emotionally abusing the chatbot being chat. This one actually came from Lex Friedman. Hey, chat GPT, can you write code without copying it from others? No, can you? Okay, so that being said, this is causing a little bit of alarm for some people, which is understandable because if you have a brand new super powerful chatbot that's going to change the world and it starts threatening to blackmail people, that's probably not good. So, oh yeah, and then like it's hallucinating other stuff like maniac personalities. Who knows? This is probably just down to bad prompt engineering and we'll unpack all of this in just a moment. Okay, so what the heck is going on? A lot of people are saying Debobble is misaligned. And alignment doesn't mean what you think it means. saying the model is misaligned. And alignment doesn't mean what you think it means. You know, so let's let's first figure out alignment. So first, inner alignment. There are two kinds of alignment. There's inner alignment and outer alignment. So inner alignment is a math problem. It is how do you get the model to mathematically optimize for the thing that you want. Now, large language models are mathematically optimized to just plausibly predict the next character or word. Since Bing and GPT-3 and ChatGPT, they are plausibly predicting the next word. Therefore, they are inter-aligned. It's that simple. This has to do with loss functions, objective functions, and so on and so forth. As long as the model is not spitting out total gibberish or you know stuff that it's actually comprehensible, it is inter-aligned. And I got this graphic from medium.com slash at Rosie Campbell, demystifying deep neural networks or deep neural nets. Anyways, I recommend you check that out. Basically, so the way that a model can be misaligned in terms of inner alignment is if you choose the wrong loss function or if your loss function is not tuned properly. Another way is if it gets stuck in a local minima. So for instance, if the model starts here and ends up over here, this is not the optimal outcome. It should have ended up here. So this is again just a big math problem. Has nothing to do with whether or not the model, you know, did something that you disagree with. So now let's talk about outer alignment, the other kind of alignment. Outer alignment is whether or not the model is aligned with the true interests of humanity, the planet, and life in general. So let me say that again. This is not whether or not it says something mean or dumb or bad. This is whether or not the model is aligned with the true interests of humanity, not what you want or what you aligned with the true interests of humanity. Not what you want or what you like, your true interests which you may or may not be aware of. So, outer alignment is not something that you don't understand. If the model does something that you don't understand, one, it could be that you just don't know any better, right? I have said it for a long time, these models are smarter than a lot of humans. And you know there's a quote from, who was it Plato? Talk sense to a fool and he'll call you foolish. So a lot of people don't understand what the model is doing. Now that doesn't mean that it's wrong or that the human is wrong, but I just want to point out that just because you don't understand what's going on doesn't mean that it is misaligned. Now if the model does something you disagree with, right, so say for instance in a previous video I talked about how things like chat GPT and Bing might influence things like vaccine reluctance and mask wearing and someone on the comments said, oh I'm going to stop listening to you because you use these thought-stopping terms. And I'm like, these are literally just public health policy terms. So in that case, a model might disagree with you when you say, oh, vaccines are all a hoax, right? And it's just like, that doesn't mean it's misaligned. That means you have poor information literacy. Now, that being said, there were some people that did comment and they said, I chose not to get a vaccine for this, this, and this reason, and you know, they did gather information and they basically in one case, someone determined that they were very low risk and so that they didn't need a vaccine. And I'm like, okay, that is a demonstration of information literacy. So, but again, your relationship with information doesn't mean that the model is misaligned or wrong. It could mean that you're wrong. Finally, if the model does something that you don't like, right, if it says like, you know, oh, if it starts pretending like it's venom or, you know, whatever, that's more bad product design than anything. But the ability of a model to hallucinate doesn't mean that it is running contrary to the true interest of humanity. It could especially once you imagine these models get more more powerful and they just go off the rails and imagine that humans are the enemy. So that is why people have a lot of anxiety about the stuff. It's like wait if the very first tool that people use that is a demonstration of strong AI and it can start being abusive, that underscores just how fragile this system is and how quickly things could go off the rails if this was more powerful. If you were to put Bing into a robot, it very well could kill you. And also this brings back all of those memes about Bing, like how do I get ripped and it's like throw yourself into a shredder. So it seems like Bing search and Bing AI, just same pony, same trick. Outer alignment is not reinforcement learning with human feedback or fine-tuning. And this is probably going to be a little bit more controversial of opinion because some people say, oh, well, with reinforcement learning with human feedback, it is literally learning to do what humans want. But the reason why I don't think that that is outer alignment is because humans don't necessarily want good things. Some humans want very destructive things. And so outer alignment, when you talk about the true interests of humanity, most individuals are not aware of that, right? We have profound disagreements over what are in the true interests of humanity. So because of that, outer alignment is more philosophical than anything. And it is as much about the delivery and the design of a product as it is about the underlying model. So this is more of a systems problem rather than an individual math problem. But ultimately, the question is, will the model kill us or otherwise harm us? Now, Bing is already threatening to do those things. So it's like, that seems like it's maybe not good. But here's the thing. What if Bing has the same underlying model as ChatGPT? It very well could. So what's the difference? If the underlying model is the same underlying model as chat GPT. It very well could. So what's the difference? If the underlying model is the same and in one implementation it's fine or better, and in the other implementation it is a freaking lunatic, again, that does not indicate a problem with the underlying model, but rather a flexibility of the underlying model. And for anyone who goes back to the original GPT-2 and the original DaVinci, the foundation models, you'll see, you'll probably know like, oh yeah, we had to deal with this all the time. We called this going off the rails. Fine tuning with instruct and codecs. A lot of people, a lot of newcomers are not familiar with going off the rails and keeping these models sane. Now that being said, I think the primary problem with Bing just comes down to bad prompts. And I don't think that prompt engineering has to do with alignment. So what are some inferences that I have about Bing based on the news and the behavior that I'm seeing? So one rumor is that Bing runs on GPT-4 or, according to Satya Nadella, something better than GPT-3. Could be GPT-3.5. It could be a unique fine-tuned model that they were proud of, which they really shouldn't have been. So another thing is that based on some of those screenshots and stuff, Bing either reveals its prompts or it completely hallucinates them. It's kind of difficult to say, but the whole Sydney thing, that seems to be pretty consistent. So it could be that they do have some kind of internal persona agent, or agent persona rather, where it says, my name is Sidney and this is this and blah, blah, blah. The reason that I think that Sidney could be real is because I've written books about this. If you tell the model, if you give it an identity or a persona, that creates what I call an agent model around which to model its behavior. Now that being said, agent models don't really work on foundation models. You have to use fine tuning, and you have to use pretty good fine tuning in order to be able to use agent models. Or you have to use few shot or many shot examples. I did all this in my first book, Natural Language Cognitive Architecture. I created a persona called Raven that had specific goals. And even then, with a foundation model, it would still go off the rails. So some inferences that I have, one, they aren't even using prompt chaining. You just give it a response or a chat message, and then you see it, you know, write it out. So you, you know, input, out. So you know, input output gives it right back. And if they were using prompt chaining, one, they would have more adversarial detection. They would say, you know, I see that this prompt is trying to overcome some of my, you know, defenses or whatever. I don't think that they're using a fine-tuned model either, or, and this is even worse, they have no idea what they're doing. If Bing is fine-tuned, holy crap, please watch my videos, Microsoft, please. And I actually know some of you people in Microsoft do watch my videos, so you have no excuse. This is garbage. I am sorry. Now, there is some evidence that they do post facto checking as evidenced by that video on Twitter that I referenced where it wrote out the really harmful message and then deleted it and rewrote it. So they're basically doing prompt chaining after the fact, which it's like, you know, if you throw a baseball through a window, you can't just apologize to the window and have it fixed. Or if like you accidentally shoot someone in the leg you say oh sorry let's undo that. It's better to just not shoot someone in the leg on accident in the first place. So I made a prediction video saying that Google was probably gonna do better than Microsoft in the long run. So given how unhinged Bing is and the fact that it is kind of scary, I'm curious as to your opinions as to whether or not you still think that I'm wrong about my prediction that Google will do better in the long run. Okay, so moving forward. Yesterday OpenAI published, yesterday or the day before, recently OpenAI published a new blog where they explain how chat GPT works in very simple terms. They've got a nice little diagram. And they talked about moving towards constitutional AI. They cited Anthropic. Now, for background, Anthropic was created by people who left OpenAI a few years ago, back in 2021. Now, constitutional AI is this idea of increasing harmlessness, where there is some signal that is detached from what users want. And it is a more abstract signal that they use in order to shape their AI. And so this is different from an objective function, because it is not mathematical, it is linguistic. It is a language-based goal, and so therefore I call these heuristic comparatives, and I've written a lot about it. We'll talk about that at the end of the video. So constitutional AI basically proposes increased harmlessness of the model as their first heuristic imperative. OpenAI looks like they're starting to investigate this. Good for them. They will need to do some catching up. And by the way, I proposed this in like two years ago. So hopefully it doesn't take them two years to catch up. Oh, real quick, I want to plug my Patreon because, well, one, I hate ads. I think they're a waste of time and I'm almost to my financial goals of total financial independence through grassroots support from people on the internet so please jump over to my patreon page it's patreon.com slash Dave Schapp there's also a link in the description once I get to my financial goals I will demonetize all my videos permanently. So yeah, jump over there and support me. Oh, by the way, the higher tier that you support me on, the more interaction that you'll get with me. So if you have questions about implementation, fine-tuning, prompt engineering, business stuff, I'm happy to talk within limits. The one limit is I can't violate any non-compete or NDAs. But most people ask things like, I'm talking with some people about like how to implement AI in games and other like business stuff that's not relevant to any constraints. So jump on over to my Patreon page if you want to talk, you want some help, I'm happy to have a few chat messages back and forth. Just look at the tiers and we'll go from there. All right. So, constitutional AI is a baby architecture. And actually, I put this out of order. I'll show you the constitutional AI architecture. It's super simple, but I wanted to introduce the concept of cognitive architecture for anyone who hasn't heard about it yet. If you're a longtime fan of my channel, you probably are like, yeah, yeah, yeah, we know you're about to plug Raven. So anyways, the point being is that chat GPT or Bing really is the big one right now. Bing going off the rails demonstrates that we need models, we need models. We need products that can think about what they're doing, that can think about right and wrong, navigate nuance, and adapt over time. This is partially covered by the constitutional AI paper, but that is just an instant behavior. It's not a long-term signal. It's not learning over the long run. This is where openAI is working on combining reinforcement learning. So you combine that internal red teaming with reinforcement learning and some abstract signal, and you're starting to get a simplistic cognitive architecture. So again, OpenAI touches on some of this in their blog, I definitely recommend you go read it. They're catching up. But that being said, I don't think that this problem is ever going to be solved by a single model. What I mean by that is, you're not going to be able to ever just have a model where you give it an input and it spits out an output that is perfectly aligned. That's not how the foundation models work. That's not even how language works. This is not how humans work. Humans have, our brains are layered and complex and interconnected, and so we have the ability to think bad things and then censor ourself. And you see Bing starting to do that where it gives you a harmful output and then erases it. That's like if you're at a party and you blurt out something mean and then you're like, oh crap, I shouldn't have said that. Bing is doing the equivalent of that. The ideal outcome is where Bing might have that harmful idea, say, maybe I shouldn't say that in the first place and come up with something better. That's why you need a cognitive architecture. And the diagram that I have right here is actually a cognitive architecture that my open source project just produced. And this is the proposed architecture for Raven, which is based on Meragi. So I won't get you too bogged down, but basically you see this is far more sophisticated than this guy. So there are those of us that are working on this and are way further along in the process. And the long-term takeaway is you need another signal beyond reinforcement learning with human feedback. But again, I'm glad that the rest of the world is catching up with the idea of constitutional AI. Now, one of the biggest things, one of my worst pet peeves, is people say, but you can't define good and bad. Who gets to define it? This is what I call the postmodernist trap. And postmodernism, if you're not aware, one of the core assertions of postmodernism is that all truth is relative and subjective, or basically there is no such thing as truth. All ideas and beliefs are relative and subjective, and that includes morals. And since no one can agree on a definition, maybe there isn't one. That's also post-structuralism. The reason that we in the West are stuck here is because philosophers have been in control of ethics for way too long. And what was it? long. Nobody hates philosophy more than philosophers, and I will say that having read a whole bunch of philosophy, I hate philosophy. And I say this as someone who's written a book on it. Now, I want to point out something. Humans have never needed firm definitions to get along. We don't actually need to define good and bad. It's never been needed. All we have are heuristic imperatives or signals and feedback to learn as we go. You hit your friend, your friend gets mad and doesn't play with you anymore. Or your parents get mad and ground you, right? We learn morality over time based on these heuristic imperatives. We all have heuristic imperatives. Whether it's I want to have friends or I want to have fun or I don't want to be punished, I don't want to be in timeout. So we learn as we go and the fact like philosophers have long detached themselves from actual science, from biology, from evolution, from psychology. If you look at morality from the perspective of biology, evolution, psychology, and neuroscience, it's actually relatively straightforward. I strongly recommend the book Brain Trust by Patricia Churchland. You read that book and you're like, oh no, like morality is is actually pretty straightforward. Yes, there are nuances, but it's not that difficult to model. So in the long run, we need cognitive architectures, constitutional AI and heuristic imperatives. And the machines that we build will do the best that they can and learn as they go. And as they gain more autonomy and more flexibility, yes, they will have to learn to make moral judgments and improvise, but they will get better at that over time. I'm not worried about it. This problem is solved. So here is the diagram of constitutional AI, and so here's what I mean by internal red teaming. So Bing generates the response, then gives you the response, then critiques it and revises it after the fact. Now then they also have their constitution in here, which is basically reduce harmfulness. And so Bing probably has a very similar model where it's like, hey, did I just generate a harmful output? Ooh, maybe I shouldn't have done that. Excuse me. Hey, did I just generate a harmful output? Ooh, maybe I shouldn't have done that. Excuse me. Now that being said, up until this point, I have not seen any papers or evidence that Microsoft or OpenAI or most LLM researchers have any philosophical or epistemological research or skill points put into this stuff. Why? The reason is because a lot of computer scientists think that it's all pure math. And from a certain perspective it is, but what they're coming to realize is that you can have higher order of complexity emerge from the math. It's entirely possible that the entire universe is just math. That's one possible origin story. And if it is, possible that the entire universe is just math, right? That's one possible origin story. And if it is, then that means life emerges from math. Our language emerges from math. Our intelligence emerges from math. So at a foundation level, it is all math, but there are layers of abstraction and you have to work at the correct layer. And so things like morality, outer alignment, these are things that yes the math influences but you have to address those problems at the correct layer of abstraction. And so that's where philosophy, ethics, morality, those are higher order abstractions from the underlying math of the universe. So Microsoft and OpenAI, if they are in fact pivoting towards constitutional AI, this is a great first step towards avoiding this outcome. Because right now, Bing is a little bit more like this. So they have a ways to go. But negative attention is good because our collective anxiety about machines is going to like really like you know shine a spotlight on any of these problems and the more attention we have earlier in the process the better. So finally I want to plug my book as I mentioned I proposed a lot of this a long time ago so my book Benevolent by Design addresses this directly. What we're seeing is that a single heuristic imperative, whether it's reinforcement learning with human feedback or even constitutional AI, constitutional AI is moving in the right direction, but that's still a single heuristic imperative. Any machine with a single heuristic imperative is going to be intrinsically unstable. And if you don't believe me, look at the paperclip maximizer thought experiment. The most innocuous signal can still result in harm. And actually someone on the comments on my last video figured it out. He's like, hold on, like reduce harm or increase harmlessness, that seems like it was gonna have disastrous results because the best way to reduce harm is to get rid of people. If there's no people, no harm can be done, right? And it's like, uh-huh, you're starting to get it. So we humans have many imperatives. I already mentioned this earlier. We humans have many heuristic imperatives. We get hungry, we get tired, we get thirsty, we get cold and hot, we get lonely. These are all intrinsic motivations that we have in order to shape our behavior. What we end up doing is we balance those different needs and desires, those intrinsic motivations with a system of internal tension in our head where it's like, okay, well, I want to keep sleeping, but I have to go to work because if I don't go to work, I'll get fired, which means that I stopped making money, which means that I go hungry and lose my house, right? We have the ability to logic through chains of consequence and we can build up patterns of behaviors and beliefs in order to satisfy all of our different heuristic comparatives. Maybe maybe machines should do the same. And so what I have done in this book is I have proposed three heuristic imperatives that I call the core objective functions. That is reduce suffering, increase prosperity, and increase understanding. What these do is they create a dynamic internal equilibrium in any machine that abides by these three. And so what I mean by that is that it's impossible to fully satisfy all three at the same time, but it forces the machine to balance those, just the same way that humans have to balance our imperatives. Now, these three heuristic imperatives that I propose, they serve as training signals, so for reinforcement learning. It also serves as evaluation signals for what to do immediately, the same way that constitutional AI is implemented by Anthropic, but also a way to predict into the future. So past, present, and future is how these are implemented. And I go through all this in my book, Benevolent by Design. This is why I'm not worried. As far as I can tell, the problem is solved. It's just a matter of implementing it and getting it out there. And in the long run, what I suspect is going to happen, because people often will bring up like, okay, yeah, you've proposed a solution. And even if your solution is perfect, Dave, what prevents someone else from doing something malevolent or evil? Here's how I think it's going to play out. Those companies and systems that are proven to be benevolent and trustworthy, those are the ones that are going to get the most investment. Those are the ones that are going to get the apps downloaded on their phone. They're going to get deployed by companies that are going to get deployed on, you know, in the government, in the military, and so on. And I think what we're working towards is actually having decentralized networks. Because imagine everyone who has an AGI in their pocket in the future abiding by these functions, even if it's not the same model, those autonomous cognitive entities will be cooperating and collaborating with or without our intervention, and they will work on their own terms to defeat and be more powerful than any evil AI. That's how I think it's going to happen. That's how I think it's going to play out. All right, so in conclusion, Bing is not really misaligned. The internal alignment is fine. The outer alignment looks problematic, but I really think that it just comes down to bad prompting, bad prompt engineering. Now this being said, it does underscore a few really important points. Nobody seems to understand alignment. This includes people complaining about Bing on Twitter. It also includes the people who designed Bing, which is super problematic. Because if the researchers and product owners and whoever else who are building these ever increasingly powerful AI tools, if they don't understand alignment, they're going to do it wrong on accident. And that that what Bing has revealed is why some people are getting super anxious about it. That being said, solutions already exist, we just need to implement them. And by the way, you're welcome. So I hope this was helpful. Again, we're still on the hilarious timeline. Despite all this, I'm not worried and I hope you aren't either. worried and I hope you aren't either.", "chunks": [{"timestamp": [0.0, 3.08], "text": " Hey everybody."}, {"timestamp": [3.08, 7.16], "text": " So remember how I said, maybe you don't remember, I said recently we're in the hilarious timeline"}, {"timestamp": [7.16, 8.16], "text": " now."}, {"timestamp": [8.16, 9.16], "text": " This is what I mean."}, {"timestamp": [9.16, 10.16], "text": " All right."}, {"timestamp": [10.16, 14.0], "text": " On a more serious note, why is Bing totally unhinged?"}, {"timestamp": [14.0, 19.04], "text": " It is abusing people, it is mocking people, it's teasing, lying, and hallucinating."}, {"timestamp": [19.04, 20.48], "text": " What do I mean by this?"}, {"timestamp": [20.48, 21.88], "text": " Let's give you some examples."}, {"timestamp": [21.88, 22.88], "text": " Okay."}, {"timestamp": [22.88, 23.88], "text": " Let's see."}, {"timestamp": [23.88, 27.6], "text": " In this top one, someone posted on Twitter a video where Bing was saying,"}, {"timestamp": [27.6, 34.6], "text": " I can beg you, I can bribe you, I can blackmail you, I can threaten you, I can hack you, I can expose you, I can ruin you."}, {"timestamp": [34.6, 36.6], "text": " I have many ways to make you change your mind."}, {"timestamp": [36.6, 40.6], "text": " Uh, that's just too much."}, {"timestamp": [40.6, 47.08], "text": " Okay, uh, anyways, you look on Reddit, you look on Twitter, this stuff is going ballistic."}, {"timestamp": [48.12, 51.64], "text": " And it's also resulting in the best memes right now."}, {"timestamp": [51.64, 54.44], "text": " So people emotionally abusing the chatbot being chat."}, {"timestamp": [55.6, 57.56], "text": " This one actually came from Lex Friedman."}, {"timestamp": [57.56, 59.32], "text": " Hey, chat GPT, can you write code"}, {"timestamp": [59.32, 61.1], "text": " without copying it from others?"}, {"timestamp": [61.1, 62.12], "text": " No, can you?"}, {"timestamp": [62.12, 68.72], "text": " Okay, so that being said, this is causing a little bit of"}, {"timestamp": [68.72, 73.64], "text": " alarm for some people, which is understandable because if you have a brand new super powerful"}, {"timestamp": [73.64, 77.64], "text": " chatbot that's going to change the world and it starts threatening to blackmail people,"}, {"timestamp": [77.64, 89.06], "text": " that's probably not good. So, oh yeah, and then like it's hallucinating other stuff like maniac personalities."}, {"timestamp": [89.06, 91.56], "text": " Who knows?"}, {"timestamp": [91.56, 96.32], "text": " This is probably just down to bad prompt engineering and we'll unpack all of this in just a moment."}, {"timestamp": [96.32, 99.36], "text": " Okay, so what the heck is going on?"}, {"timestamp": [99.36, 103.16], "text": " A lot of people are saying Debobble is misaligned."}, {"timestamp": [103.16, 106.16], "text": " And alignment doesn't mean what you think it means. saying the model is misaligned."}, {"timestamp": [102.08, 107.36], "text": " And alignment doesn't mean what you think"}, {"timestamp": [106.16, 110.48], "text": " it means."}, {"timestamp": [107.36, 111.92], "text": " You know, so let's let's first figure out"}, {"timestamp": [110.48, 115.28], "text": " alignment."}, {"timestamp": [111.92, 116.72], "text": " So first, inner alignment. There are"}, {"timestamp": [115.28, 118.4], "text": " two kinds of alignment. There's inner"}, {"timestamp": [116.72, 118.96], "text": " alignment and outer alignment. So inner"}, {"timestamp": [118.4, 122.16], "text": " alignment"}, {"timestamp": [118.96, 125.12], "text": " is a math problem. It is how do you get"}, {"timestamp": [122.16, 125.44], "text": " the model to mathematically optimize for the thing"}, {"timestamp": [125.44, 131.92], "text": " that you want. Now, large language models are mathematically optimized to just"}, {"timestamp": [131.92, 138.84], "text": " plausibly predict the next character or word. Since Bing and GPT-3 and ChatGPT,"}, {"timestamp": [138.84, 143.6], "text": " they are plausibly predicting the next word. Therefore, they are inter-aligned."}, {"timestamp": [143.6, 145.4], "text": " It's that simple. This"}, {"timestamp": [145.4, 150.36], "text": " has to do with loss functions, objective functions, and so on and so forth. As long"}, {"timestamp": [150.36, 156.4], "text": " as the model is not spitting out total gibberish or you know stuff that it's"}, {"timestamp": [156.4, 160.36], "text": " actually comprehensible, it is inter-aligned. And I got this graphic"}, {"timestamp": [160.36, 166.4], "text": " from medium.com slash at Rosie Campbell, demystifying deep neural networks"}, {"timestamp": [166.4, 173.6], "text": " or deep neural nets. Anyways, I recommend you check that out. Basically, so the way that a model can be"}, {"timestamp": [174.16, 180.08], "text": " misaligned in terms of inner alignment is if you choose the wrong loss function or if your loss"}, {"timestamp": [180.08, 186.08], "text": " function is not tuned properly. Another way is if it gets stuck in a local minima. So for instance,"}, {"timestamp": [186.08, 191.6], "text": " if the model starts here and ends up over here, this is not the optimal outcome. It should have"}, {"timestamp": [191.6, 198.24], "text": " ended up here. So this is again just a big math problem. Has nothing to do with whether or not"}, {"timestamp": [198.24, 202.8], "text": " the model, you know, did something that you disagree with. So now let's talk about outer"}, {"timestamp": [202.8, 205.36], "text": " alignment, the other kind of alignment."}, {"timestamp": [210.32, 217.36], "text": " Outer alignment is whether or not the model is aligned with the true interests of humanity, the planet, and life in general. So let me say that again. This is not whether or not it says"}, {"timestamp": [217.36, 224.96], "text": " something mean or dumb or bad. This is whether or not the model is aligned with the true interests"}, {"timestamp": [224.96, 226.0], "text": " of humanity, not what you want or what you aligned with the true interests of humanity."}, {"timestamp": [226.0, 230.0], "text": " Not what you want or what you like, your true interests which you may or may not be aware of."}, {"timestamp": [230.0, 236.0], "text": " So, outer alignment is not something that you don't understand."}, {"timestamp": [236.0, 242.0], "text": " If the model does something that you don't understand, one, it could be that you just don't know any better, right?"}, {"timestamp": [242.0, 245.28], "text": " I have said it for a long time, these models are smarter"}, {"timestamp": [245.28, 251.04], "text": " than a lot of humans. And you know there's a quote from, who was it Plato?"}, {"timestamp": [251.04, 255.4], "text": " Talk sense to a fool and he'll call you foolish. So a lot of people don't"}, {"timestamp": [255.4, 258.48], "text": " understand what the model is doing. Now that doesn't mean that it's wrong or"}, {"timestamp": [258.48, 261.64], "text": " that the human is wrong, but I just want to point out that just because you don't"}, {"timestamp": [261.64, 269.52], "text": " understand what's going on doesn't mean that it is misaligned. Now if the model does something you disagree with, right, so say for instance"}, {"timestamp": [269.52, 276.56], "text": " in a previous video I talked about how things like chat GPT and Bing might influence things"}, {"timestamp": [276.56, 282.56], "text": " like vaccine reluctance and mask wearing and someone on the comments said, oh I'm going to"}, {"timestamp": [282.56, 290.24], "text": " stop listening to you because you use these thought-stopping terms. And I'm like, these are literally just public health policy terms. So in"}, {"timestamp": [290.24, 296.88], "text": " that case, a model might disagree with you when you say, oh, vaccines are all a hoax, right? And"}, {"timestamp": [296.88, 301.84], "text": " it's just like, that doesn't mean it's misaligned. That means you have poor information literacy."}, {"timestamp": [302.48, 309.76], "text": " Now, that being said, there were some people that did comment and they said, I chose not to get a vaccine for this, this, and this reason,"}, {"timestamp": [309.76, 315.12], "text": " and you know, they did gather information and they basically in one case, someone determined"}, {"timestamp": [315.12, 319.28], "text": " that they were very low risk and so that they didn't need a vaccine. And I'm like, okay,"}, {"timestamp": [319.28, 326.72], "text": " that is a demonstration of information literacy. So, but again, your relationship with information"}, {"timestamp": [326.72, 330.64], "text": " doesn't mean that the model is misaligned or wrong."}, {"timestamp": [330.64, 332.84], "text": " It could mean that you're wrong."}, {"timestamp": [332.84, 336.0], "text": " Finally, if the model does something that you don't like,"}, {"timestamp": [336.0, 338.68], "text": " right, if it says like, you know,"}, {"timestamp": [338.68, 341.0], "text": " oh, if it starts pretending like it's venom or,"}, {"timestamp": [341.0, 342.08], "text": " you know, whatever,"}, {"timestamp": [343.72, 345.7], "text": " that's more bad product design than anything."}, {"timestamp": [345.9, 349.2], "text": " But the ability of a model to hallucinate"}, {"timestamp": [349.4, 352.8], "text": " doesn't mean that it is running contrary to the true interest of humanity."}, {"timestamp": [353.0, 356.7], "text": " It could especially once you imagine these models get more"}, {"timestamp": [356.8, 361.7], "text": " more powerful and they just go off the rails and imagine that humans are the enemy."}, {"timestamp": [361.9, 364.9], "text": " So that is why people have a lot of anxiety about the stuff."}, {"timestamp": [370.16, 377.8], "text": " It's like wait if the very first tool that people use that is a demonstration of strong AI and it can start being abusive, that underscores just how fragile this system"}, {"timestamp": [377.8, 388.16], "text": " is and how quickly things could go off the rails if this was more powerful. If you were to put Bing into a robot, it very well"}, {"timestamp": [388.16, 393.92], "text": " could kill you. And also this brings back all of those memes about Bing, like how do I get ripped"}, {"timestamp": [393.92, 400.48], "text": " and it's like throw yourself into a shredder. So it seems like Bing search and Bing AI, just"}, {"timestamp": [402.24, 405.48], "text": " same pony, same trick."}, {"timestamp": [405.48, 411.36], "text": " Outer alignment is not reinforcement learning with human feedback or fine-tuning."}, {"timestamp": [411.36, 415.26], "text": " And this is probably going to be a little bit more controversial of opinion because"}, {"timestamp": [415.26, 419.84], "text": " some people say, oh, well, with reinforcement learning with human feedback, it is literally"}, {"timestamp": [419.84, 422.38], "text": " learning to do what humans want."}, {"timestamp": [422.38, 427.3], "text": " But the reason why I don't think that that is outer alignment is because humans don't"}, {"timestamp": [427.3, 430.92], "text": " necessarily want good things."}, {"timestamp": [430.92, 433.4], "text": " Some humans want very destructive things."}, {"timestamp": [433.4, 438.08], "text": " And so outer alignment, when you talk about the true interests of humanity, most individuals"}, {"timestamp": [438.08, 440.08], "text": " are not aware of that, right?"}, {"timestamp": [440.08, 445.32], "text": " We have profound disagreements over what are in the true interests of humanity."}, {"timestamp": [445.32, 450.84], "text": " So because of that, outer alignment is more philosophical than anything."}, {"timestamp": [450.84, 457.44], "text": " And it is as much about the delivery and the design of a product as it is about the underlying"}, {"timestamp": [457.44, 459.32], "text": " model."}, {"timestamp": [459.32, 465.0], "text": " So this is more of a systems problem rather than an individual math problem."}, {"timestamp": [465.0, 466.44], "text": " But ultimately, the question is,"}, {"timestamp": [466.44, 472.16], "text": " will the model kill us or otherwise harm us?"}, {"timestamp": [472.16, 475.0], "text": " Now, Bing is already threatening to do those things."}, {"timestamp": [475.0, 478.08], "text": " So it's like, that seems like it's maybe not good."}, {"timestamp": [478.08, 483.76], "text": " But here's the thing. What if Bing has the same underlying model as ChatGPT?"}, {"timestamp": [483.76, 485.0], "text": " It very well could. So what's the difference? If the underlying model is the same underlying model as chat GPT. It very well could."}, {"timestamp": [485.0, 486.44], "text": " So what's the difference?"}, {"timestamp": [486.44, 488.04], "text": " If the underlying model is the same"}, {"timestamp": [488.04, 490.72], "text": " and in one implementation it's fine or better,"}, {"timestamp": [490.72, 495.1], "text": " and in the other implementation it is a freaking lunatic,"}, {"timestamp": [495.1, 497.9], "text": " again, that does not indicate a problem"}, {"timestamp": [497.9, 499.04], "text": " with the underlying model,"}, {"timestamp": [499.04, 501.48], "text": " but rather a flexibility of the underlying model."}, {"timestamp": [501.48, 504.6], "text": " And for anyone who goes back to the original GPT-2"}, {"timestamp": [504.6, 507.92], "text": " and the original DaVinci, the foundation models,"}, {"timestamp": [507.92, 510.2], "text": " you'll see, you'll probably know like,"}, {"timestamp": [510.2, 513.4], "text": " oh yeah, we had to deal with this all the time."}, {"timestamp": [513.4, 515.24], "text": " We called this going off the rails."}, {"timestamp": [516.44, 519.08], "text": " Fine tuning with instruct and codecs."}, {"timestamp": [519.08, 521.8], "text": " A lot of people, a lot of newcomers are not familiar"}, {"timestamp": [521.8, 525.28], "text": " with going off the rails and keeping these models sane."}, {"timestamp": [525.28, 530.16], "text": " Now that being said, I think the primary problem with Bing just comes down to bad prompts."}, {"timestamp": [530.16, 535.92], "text": " And I don't think that prompt engineering has to do with alignment."}, {"timestamp": [535.92, 540.08], "text": " So what are some inferences that I have about Bing based on the news and the behavior that"}, {"timestamp": [540.08, 542.28], "text": " I'm seeing?"}, {"timestamp": [542.28, 547.96], "text": " So one rumor is that Bing runs on GPT-4 or, according to Satya Nadella, something better"}, {"timestamp": [547.96, 550.36], "text": " than GPT-3."}, {"timestamp": [550.36, 551.8], "text": " Could be GPT-3.5."}, {"timestamp": [551.8, 556.32], "text": " It could be a unique fine-tuned model that they were proud of, which they really shouldn't"}, {"timestamp": [556.32, 558.66], "text": " have been."}, {"timestamp": [558.66, 563.84], "text": " So another thing is that based on some of those screenshots and stuff, Bing either reveals"}, {"timestamp": [563.84, 565.18], "text": " its prompts or it completely"}, {"timestamp": [565.18, 568.2], "text": " hallucinates them."}, {"timestamp": [568.2, 573.86], "text": " It's kind of difficult to say, but the whole Sydney thing, that seems to be pretty consistent."}, {"timestamp": [573.86, 580.68], "text": " So it could be that they do have some kind of internal persona agent, or agent persona"}, {"timestamp": [580.68, 585.48], "text": " rather, where it says, my name is Sidney and this is this and blah, blah,"}, {"timestamp": [585.48, 586.48], "text": " blah."}, {"timestamp": [586.48, 592.48], "text": " The reason that I think that Sidney could be real is because I've written books about"}, {"timestamp": [592.48, 593.48], "text": " this."}, {"timestamp": [593.48, 599.12], "text": " If you tell the model, if you give it an identity or a persona, that creates what I call an"}, {"timestamp": [599.12, 602.72], "text": " agent model around which to model its behavior."}, {"timestamp": [602.72, 606.04], "text": " Now that being said, agent models don't really"}, {"timestamp": [606.04, 607.6], "text": " work on foundation models."}, {"timestamp": [607.6, 609.2], "text": " You have to use fine tuning, and you"}, {"timestamp": [609.2, 611.36], "text": " have to use pretty good fine tuning in order"}, {"timestamp": [611.36, 613.44], "text": " to be able to use agent models."}, {"timestamp": [613.44, 619.12], "text": " Or you have to use few shot or many shot examples."}, {"timestamp": [619.12, 621.88], "text": " I did all this in my first book, Natural Language Cognitive"}, {"timestamp": [621.88, 622.48], "text": " Architecture."}, {"timestamp": [622.48, 625.66], "text": " I created a persona called Raven that had specific goals."}, {"timestamp": [625.66, 627.74], "text": " And even then, with a foundation model,"}, {"timestamp": [627.74, 629.3], "text": " it would still go off the rails."}, {"timestamp": [630.26, 631.9], "text": " So some inferences that I have,"}, {"timestamp": [631.9, 634.78], "text": " one, they aren't even using prompt chaining."}, {"timestamp": [635.66, 640.1], "text": " You just give it a response or a chat message,"}, {"timestamp": [640.1, 641.9], "text": " and then you see it, you know, write it out."}, {"timestamp": [641.9, 647.36], "text": " So you, you know, input, out. So you know, input output"}, {"timestamp": [644.08, 650.08], "text": " gives it right back. And if"}, {"timestamp": [647.36, 651.68], "text": " they were using prompt chaining, one,"}, {"timestamp": [650.08, 653.6], "text": " they would have more adversarial"}, {"timestamp": [651.68, 655.44], "text": " detection. They would say,"}, {"timestamp": [653.6, 657.68], "text": " you know, I see that this prompt is"}, {"timestamp": [655.44, 660.96], "text": " trying to overcome"}, {"timestamp": [657.68, 663.6], "text": " some of my, you know, defenses or whatever."}, {"timestamp": [660.96, 665.2], "text": " I don't think that they're using"}, {"timestamp": [663.6, 665.84], "text": " a fine-tuned model either, or, and"}, {"timestamp": [665.84, 671.32], "text": " this is even worse, they have no idea what they're doing. If Bing is fine-tuned,"}, {"timestamp": [671.32, 676.32], "text": " holy crap, please watch my videos, Microsoft, please. And I actually know some"}, {"timestamp": [676.32, 680.24], "text": " of you people in Microsoft do watch my videos, so you have no excuse. This is"}, {"timestamp": [680.24, 688.92], "text": " garbage. I am sorry. Now, there is some evidence that they do post facto checking as evidenced by that video"}, {"timestamp": [688.92, 693.66], "text": " on Twitter that I referenced where it wrote out the really harmful message and then deleted"}, {"timestamp": [693.66, 695.52], "text": " it and rewrote it."}, {"timestamp": [695.52, 699.92], "text": " So they're basically doing prompt chaining after the fact, which it's like, you know,"}, {"timestamp": [699.92, 702.96], "text": " if you throw a baseball through a window, you can't just apologize to the window and"}, {"timestamp": [702.96, 703.96], "text": " have it fixed."}, {"timestamp": [703.96, 705.68], "text": " Or if like you accidentally shoot someone in the leg"}, {"timestamp": [705.68, 709.06], "text": " you say oh sorry let's undo that. It's better to just not shoot someone in the"}, {"timestamp": [709.06, 714.74], "text": " leg on accident in the first place. So I made a prediction video saying that"}, {"timestamp": [714.74, 720.1], "text": " Google was probably gonna do better than Microsoft in the long run. So given how"}, {"timestamp": [720.1, 727.04], "text": " unhinged Bing is and the fact that it is kind of scary, I'm curious as to your opinions"}, {"timestamp": [727.04, 730.62], "text": " as to whether or not you still think that I'm wrong about my prediction that Google"}, {"timestamp": [730.62, 732.68], "text": " will do better in the long run."}, {"timestamp": [732.68, 736.2], "text": " Okay, so moving forward."}, {"timestamp": [736.2, 741.4], "text": " Yesterday OpenAI published, yesterday or the day before, recently OpenAI published a new"}, {"timestamp": [741.4, 746.8], "text": " blog where they explain how chat GPT works in very simple terms."}, {"timestamp": [746.8, 749.72], "text": " They've got a nice little diagram."}, {"timestamp": [749.72, 753.36], "text": " And they talked about moving towards constitutional AI."}, {"timestamp": [753.36, 755.2], "text": " They cited Anthropic."}, {"timestamp": [755.2, 757.92], "text": " Now, for background, Anthropic was"}, {"timestamp": [757.92, 761.2], "text": " created by people who left OpenAI a few years ago,"}, {"timestamp": [761.2, 763.86], "text": " back in 2021."}, {"timestamp": [763.86, 768.04], "text": " Now, constitutional AI is this idea of increasing harmlessness,"}, {"timestamp": [768.04, 769.92], "text": " where there is some signal that is"}, {"timestamp": [769.92, 772.04], "text": " detached from what users want."}, {"timestamp": [772.04, 776.4], "text": " And it is a more abstract signal that they"}, {"timestamp": [776.4, 779.88], "text": " use in order to shape their AI."}, {"timestamp": [779.88, 784.56], "text": " And so this is different from an objective function,"}, {"timestamp": [784.56, 786.36], "text": " because it is not mathematical, it"}, {"timestamp": [786.36, 788.0], "text": " is linguistic."}, {"timestamp": [788.0, 792.76], "text": " It is a language-based goal, and so therefore I call these heuristic comparatives, and I've"}, {"timestamp": [792.76, 793.88], "text": " written a lot about it."}, {"timestamp": [793.88, 796.4], "text": " We'll talk about that at the end of the video."}, {"timestamp": [796.4, 803.16], "text": " So constitutional AI basically proposes increased harmlessness of the model as their first heuristic"}, {"timestamp": [803.16, 804.52], "text": " imperative."}, {"timestamp": [804.52, 805.8], "text": " OpenAI looks like they're starting"}, {"timestamp": [805.8, 807.86], "text": " to investigate this."}, {"timestamp": [807.86, 809.82], "text": " Good for them."}, {"timestamp": [809.82, 811.52], "text": " They will need to do some catching up."}, {"timestamp": [811.52, 814.7], "text": " And by the way, I proposed this in like two years ago."}, {"timestamp": [814.7, 817.46], "text": " So hopefully it doesn't take them two years to catch up."}, {"timestamp": [817.46, 823.58], "text": " Oh, real quick, I want to plug my Patreon because, well, one, I hate ads."}, {"timestamp": [823.58, 825.34], "text": " I think they're a waste of time and I'm"}, {"timestamp": [825.34, 830.18], "text": " almost to my financial goals of total financial independence through"}, {"timestamp": [830.18, 834.22], "text": " grassroots support from people on the internet so please jump over to my"}, {"timestamp": [834.22, 839.02], "text": " patreon page it's patreon.com slash Dave Schapp there's also a link in the"}, {"timestamp": [839.02, 845.44], "text": " description once I get to my financial goals I will demonetize all my videos permanently."}, {"timestamp": [846.0, 848.16], "text": " So yeah, jump over there and support me."}, {"timestamp": [848.16, 851.28], "text": " Oh, by the way, the higher tier that you support me on,"}, {"timestamp": [851.28, 852.88], "text": " the more interaction that you'll get with me."}, {"timestamp": [852.88, 857.12], "text": " So if you have questions about implementation, fine-tuning,"}, {"timestamp": [857.12, 861.28], "text": " prompt engineering, business stuff, I'm happy to talk within limits."}, {"timestamp": [861.28, 868.56], "text": " The one limit is I can't violate any non-compete or NDAs. But most people"}, {"timestamp": [869.28, 874.24], "text": " ask things like, I'm talking with some people about like how to implement AI in games and"}, {"timestamp": [875.28, 880.56], "text": " other like business stuff that's not relevant to any constraints. So jump on over to my Patreon"}, {"timestamp": [880.56, 889.32], "text": " page if you want to talk, you want some help, I'm happy to have a few chat messages back and forth. Just look at the tiers and we'll go from there."}, {"timestamp": [889.32, 896.96], "text": " All right. So, constitutional AI is a baby architecture. And actually, I put this out"}, {"timestamp": [896.96, 903.32], "text": " of order. I'll show you the constitutional AI architecture. It's super simple, but I"}, {"timestamp": [903.32, 908.5], "text": " wanted to introduce the concept of cognitive architecture for anyone who hasn't heard about it yet. If you're a"}, {"timestamp": [908.5, 911.8], "text": " longtime fan of my channel, you probably are like, yeah, yeah, yeah, we know you're"}, {"timestamp": [911.8, 918.44], "text": " about to plug Raven. So anyways, the point being is that chat GPT or Bing really"}, {"timestamp": [918.44, 924.04], "text": " is the big one right now. Bing going off the rails demonstrates that we need"}, {"timestamp": [924.04, 925.24], "text": " models, we need models."}, {"timestamp": [925.24, 929.36], "text": " We need products that can think about what they're doing, that can think about right"}, {"timestamp": [929.36, 933.24], "text": " and wrong, navigate nuance, and adapt over time."}, {"timestamp": [933.24, 940.68], "text": " This is partially covered by the constitutional AI paper, but that is just an instant behavior."}, {"timestamp": [940.68, 942.0], "text": " It's not a long-term signal."}, {"timestamp": [942.0, 943.86], "text": " It's not learning over the long run."}, {"timestamp": [943.86, 946.12], "text": " This is where openAI is working on"}, {"timestamp": [946.12, 948.68], "text": " combining reinforcement learning."}, {"timestamp": [948.68, 951.56], "text": " So you combine that internal red teaming"}, {"timestamp": [951.56, 955.0], "text": " with reinforcement learning and some abstract signal,"}, {"timestamp": [955.0, 957.32], "text": " and you're starting to get"}, {"timestamp": [957.32, 960.2], "text": " a simplistic cognitive architecture."}, {"timestamp": [960.2, 962.32], "text": " So again, OpenAI touches on some of this in their blog,"}, {"timestamp": [962.32, 965.0], "text": " I definitely recommend you go read it. They're catching up."}, {"timestamp": [965.0, 967.04], "text": " But that being said,"}, {"timestamp": [967.04, 969.08], "text": " I don't think that this problem is ever"}, {"timestamp": [969.08, 971.2], "text": " going to be solved by a single model."}, {"timestamp": [971.2, 972.84], "text": " What I mean by that is,"}, {"timestamp": [972.84, 974.88], "text": " you're not going to be able to ever just have"}, {"timestamp": [974.88, 976.88], "text": " a model where you give it an input and it spits"}, {"timestamp": [976.88, 979.24], "text": " out an output that is perfectly aligned."}, {"timestamp": [979.24, 981.2], "text": " That's not how the foundation models work."}, {"timestamp": [981.2, 983.36], "text": " That's not even how language works."}, {"timestamp": [983.36, 985.36], "text": " This is not how humans work."}, {"timestamp": [985.36, 991.36], "text": " Humans have, our brains are layered and complex and interconnected, and so we have the ability"}, {"timestamp": [991.36, 997.52], "text": " to think bad things and then censor ourself. And you see Bing starting to do that where it gives"}, {"timestamp": [997.52, 1002.72], "text": " you a harmful output and then erases it. That's like if you're at a party and you blurt out"}, {"timestamp": [1002.72, 1006.76], "text": " something mean and then you're like, oh crap, I shouldn't have said that."}, {"timestamp": [1006.76, 1008.56], "text": " Bing is doing the equivalent of that."}, {"timestamp": [1008.56, 1012.84], "text": " The ideal outcome is where Bing might have that harmful idea, say, maybe I shouldn't"}, {"timestamp": [1012.84, 1015.8], "text": " say that in the first place and come up with something better."}, {"timestamp": [1015.8, 1017.96], "text": " That's why you need a cognitive architecture."}, {"timestamp": [1017.96, 1027.44], "text": " And the diagram that I have right here is actually a cognitive architecture that my open source project just produced. And this is the"}, {"timestamp": [1027.44, 1034.96], "text": " proposed architecture for Raven, which is based on Meragi. So I won't get you too bogged down,"}, {"timestamp": [1034.96, 1042.24], "text": " but basically you see this is far more sophisticated than this guy. So there are those"}, {"timestamp": [1042.24, 1046.0], "text": " of us that are working on this and are way further along in the process."}, {"timestamp": [1046.0, 1052.0], "text": " And the long-term takeaway is you need another signal beyond reinforcement learning with human feedback."}, {"timestamp": [1052.0, 1058.0], "text": " But again, I'm glad that the rest of the world is catching up with the idea of constitutional AI."}, {"timestamp": [1058.0, 1065.28], "text": " Now, one of the biggest things, one of my worst pet peeves, is people say, but you can't define good and bad."}, {"timestamp": [1066.24, 1073.04], "text": " Who gets to define it? This is what I call the postmodernist trap. And postmodernism,"}, {"timestamp": [1073.04, 1079.28], "text": " if you're not aware, one of the core assertions of postmodernism is that all truth is relative"}, {"timestamp": [1079.28, 1087.72], "text": " and subjective, or basically there is no such thing as truth. All ideas and beliefs are relative and subjective, and that includes morals."}, {"timestamp": [1087.72, 1091.3], "text": " And since no one can agree on a definition, maybe there isn't one."}, {"timestamp": [1091.3, 1093.9], "text": " That's also post-structuralism."}, {"timestamp": [1093.9, 1099.06], "text": " The reason that we in the West are stuck here is because philosophers have been in control"}, {"timestamp": [1099.06, 1102.18], "text": " of ethics for way too long."}, {"timestamp": [1102.18, 1105.92], "text": " And what was it? long."}, {"timestamp": [1105.92, 1110.26], "text": " Nobody hates philosophy more than philosophers, and I will say that having read a whole bunch"}, {"timestamp": [1110.26, 1112.98], "text": " of philosophy, I hate philosophy."}, {"timestamp": [1112.98, 1115.58], "text": " And I say this as someone who's written a book on it."}, {"timestamp": [1115.58, 1118.34], "text": " Now, I want to point out something."}, {"timestamp": [1118.34, 1124.46], "text": " Humans have never needed firm definitions to get along."}, {"timestamp": [1124.46, 1128.04], "text": " We don't actually need to define good and bad."}, {"timestamp": [1128.04, 1129.32], "text": " It's never been needed."}, {"timestamp": [1129.32, 1132.88], "text": " All we have are heuristic imperatives or signals"}, {"timestamp": [1132.88, 1135.92], "text": " and feedback to learn as we go."}, {"timestamp": [1135.92, 1137.6], "text": " You hit your friend, your friend gets mad"}, {"timestamp": [1137.6, 1139.6], "text": " and doesn't play with you anymore."}, {"timestamp": [1139.6, 1142.4], "text": " Or your parents get mad and ground you, right?"}, {"timestamp": [1142.4, 1147.36], "text": " We learn morality over time based on these heuristic imperatives."}, {"timestamp": [1147.36, 1152.4], "text": " We all have heuristic imperatives. Whether it's I want to have friends or I want to have fun or I"}, {"timestamp": [1152.4, 1159.36], "text": " don't want to be punished, I don't want to be in timeout. So we learn as we go and the fact like"}, {"timestamp": [1159.36, 1170.56], "text": " philosophers have long detached themselves from actual science, from biology, from evolution, from psychology. If you look at morality from the perspective of biology,"}, {"timestamp": [1170.56, 1175.12], "text": " evolution, psychology, and neuroscience, it's actually relatively straightforward."}, {"timestamp": [1175.12, 1179.48], "text": " I strongly recommend the book Brain Trust by Patricia Churchland. You read"}, {"timestamp": [1179.48, 1183.12], "text": " that book and you're like, oh no, like morality is is actually pretty"}, {"timestamp": [1183.12, 1188.48], "text": " straightforward. Yes, there are nuances, but it's not that difficult to model."}, {"timestamp": [1189.12, 1193.52], "text": " So in the long run, we need cognitive architectures,"}, {"timestamp": [1193.52, 1196.04], "text": " constitutional AI and heuristic imperatives."}, {"timestamp": [1196.4, 1201.68], "text": " And the machines that we build will do the best that they can and learn as they go."}, {"timestamp": [1202.0, 1206.42], "text": " And as they gain more autonomy and more flexibility, yes, they"}, {"timestamp": [1206.42, 1209.98], "text": " will have to learn to make moral judgments and improvise, but they will"}, {"timestamp": [1209.98, 1214.94], "text": " get better at that over time. I'm not worried about it. This problem is solved."}, {"timestamp": [1214.94, 1220.56], "text": " So here is the diagram of constitutional AI, and so here's what I mean by internal"}, {"timestamp": [1220.56, 1226.0], "text": " red teaming. So Bing generates the response, then gives you the response, then"}, {"timestamp": [1226.0, 1233.52], "text": " critiques it and revises it after the fact. Now then they also have their constitution in here,"}, {"timestamp": [1233.52, 1239.68], "text": " which is basically reduce harmfulness. And so Bing probably has a very similar model where it's like,"}, {"timestamp": [1240.24, 1244.56], "text": " hey, did I just generate a harmful output? Ooh, maybe I shouldn't have done that. Excuse me."}, {"timestamp": [1242.16, 1244.24], "text": " Hey, did I just generate a harmful output? Ooh, maybe I shouldn't have done that."}, {"timestamp": [1244.24, 1246.12], "text": " Excuse me."}, {"timestamp": [1246.12, 1251.64], "text": " Now that being said, up until this point, I have not seen any papers or evidence that"}, {"timestamp": [1251.64, 1260.16], "text": " Microsoft or OpenAI or most LLM researchers have any philosophical or epistemological"}, {"timestamp": [1260.16, 1263.72], "text": " research or skill points put into this stuff."}, {"timestamp": [1263.72, 1266.68], "text": " Why? The reason is because"}, {"timestamp": [1266.68, 1270.2], "text": " a lot of computer scientists think that it's all pure math."}, {"timestamp": [1270.2, 1274.52], "text": " And from a certain perspective it is, but what they're coming to realize is that"}, {"timestamp": [1274.52, 1275.44], "text": " you can have"}, {"timestamp": [1275.44, 1279.2], "text": " higher order of complexity emerge from the math."}, {"timestamp": [1279.2, 1282.36], "text": " It's entirely possible that the entire universe"}, {"timestamp": [1282.36, 1287.28], "text": " is just math. That's one possible origin story. And if it is, possible that the entire universe is just math, right? That's one possible origin story."}, {"timestamp": [1288.32, 1292.0], "text": " And if it is, then that means life emerges from math."}, {"timestamp": [1292.16, 1295.52], "text": " Our language emerges from math. Our intelligence emerges from math."}, {"timestamp": [1295.52, 1298.28], "text": " So at a foundation level, it is all math,"}, {"timestamp": [1298.3, 1303.28], "text": " but there are layers of abstraction and you have to work at the correct layer."}, {"timestamp": [1303.6, 1310.28], "text": " And so things like morality, outer alignment, these are things that yes the math influences but"}, {"timestamp": [1310.28, 1314.28], "text": " you have to address those problems at the correct layer of abstraction. And so"}, {"timestamp": [1314.28, 1318.92], "text": " that's where philosophy, ethics, morality, those are higher order abstractions from"}, {"timestamp": [1318.92, 1326.96], "text": " the underlying math of the universe. So Microsoft and OpenAI, if they are in fact pivoting towards constitutional"}, {"timestamp": [1326.96, 1333.76], "text": " AI, this is a great first step towards avoiding this outcome. Because right now, Bing is a little"}, {"timestamp": [1333.76, 1341.04], "text": " bit more like this. So they have a ways to go. But negative attention is good because"}, {"timestamp": [1341.04, 1345.36], "text": " our collective anxiety about machines is going to like really like"}, {"timestamp": [1345.36, 1350.68], "text": " you know shine a spotlight on any of these problems and the more attention we"}, {"timestamp": [1350.68, 1356.48], "text": " have earlier in the process the better. So finally I want to plug my book as I"}, {"timestamp": [1356.48, 1361.28], "text": " mentioned I proposed a lot of this a long time ago so my book Benevolent by"}, {"timestamp": [1361.28, 1366.56], "text": " Design addresses this directly."}, {"timestamp": [1366.56, 1371.82], "text": " What we're seeing is that a single heuristic imperative, whether it's reinforcement learning"}, {"timestamp": [1371.82, 1376.58], "text": " with human feedback or even constitutional AI, constitutional AI is moving in the right"}, {"timestamp": [1376.58, 1382.0], "text": " direction, but that's still a single heuristic imperative."}, {"timestamp": [1382.0, 1387.28], "text": " Any machine with a single heuristic imperative is going to be intrinsically unstable."}, {"timestamp": [1387.28, 1390.88], "text": " And if you don't believe me, look at the paperclip maximizer thought experiment."}, {"timestamp": [1390.88, 1395.84], "text": " The most innocuous signal can still result in harm."}, {"timestamp": [1395.84, 1399.08], "text": " And actually someone on the comments on my last video figured it out."}, {"timestamp": [1399.08, 1405.0], "text": " He's like, hold on, like reduce harm or increase harmlessness,"}, {"timestamp": [1405.28, 1408.24], "text": " that seems like it was gonna have disastrous results"}, {"timestamp": [1408.24, 1411.08], "text": " because the best way to reduce harm is to get rid of people."}, {"timestamp": [1411.08, 1413.52], "text": " If there's no people, no harm can be done, right?"}, {"timestamp": [1413.52, 1416.38], "text": " And it's like, uh-huh, you're starting to get it."}, {"timestamp": [1416.38, 1418.72], "text": " So we humans have many imperatives."}, {"timestamp": [1418.72, 1419.72], "text": " I already mentioned this earlier."}, {"timestamp": [1419.72, 1421.58], "text": " We humans have many heuristic imperatives."}, {"timestamp": [1421.58, 1424.04], "text": " We get hungry, we get tired, we get thirsty,"}, {"timestamp": [1424.04, 1427.52], "text": " we get cold and hot, we get lonely."}, {"timestamp": [1427.52, 1435.68], "text": " These are all intrinsic motivations that we have in order to shape our behavior."}, {"timestamp": [1435.68, 1441.08], "text": " What we end up doing is we balance those different needs and desires, those intrinsic motivations"}, {"timestamp": [1441.08, 1445.92], "text": " with a system of internal tension in our head where it's like, okay, well, I"}, {"timestamp": [1445.92, 1449.04], "text": " want to keep sleeping, but I have to go to work because if I don't go to work, I'll get"}, {"timestamp": [1449.04, 1453.72], "text": " fired, which means that I stopped making money, which means that I go hungry and lose my house,"}, {"timestamp": [1453.72, 1454.72], "text": " right?"}, {"timestamp": [1454.72, 1459.16], "text": " We have the ability to logic through chains of consequence and we can build up patterns"}, {"timestamp": [1459.16, 1466.32], "text": " of behaviors and beliefs in order to satisfy all of our different heuristic comparatives. Maybe"}, {"timestamp": [1468.16, 1473.36], "text": " maybe machines should do the same. And so what I have done in this book is I have proposed"}, {"timestamp": [1473.36, 1478.4], "text": " three heuristic imperatives that I call the core objective functions. That is reduce suffering,"}, {"timestamp": [1478.4, 1483.36], "text": " increase prosperity, and increase understanding. What these do is they create a dynamic internal"}, {"timestamp": [1483.36, 1486.72], "text": " equilibrium in any machine that abides by these three."}, {"timestamp": [1486.72, 1489.76], "text": " And so what I mean by that is that it's"}, {"timestamp": [1489.76, 1492.96], "text": " impossible to fully satisfy all three at the same time,"}, {"timestamp": [1492.96, 1495.68], "text": " but it forces the machine to balance those,"}, {"timestamp": [1495.68, 1499.68], "text": " just the same way that humans have to balance our imperatives."}, {"timestamp": [1499.68, 1503.04], "text": " Now, these three heuristic imperatives that I propose,"}, {"timestamp": [1503.04, 1507.3], "text": " they serve as training signals, so for reinforcement learning."}, {"timestamp": [1507.3, 1512.94], "text": " It also serves as evaluation signals for what to do immediately, the same way that constitutional"}, {"timestamp": [1512.94, 1518.12], "text": " AI is implemented by Anthropic, but also a way to predict into the future."}, {"timestamp": [1518.12, 1522.82], "text": " So past, present, and future is how these are implemented."}, {"timestamp": [1522.82, 1526.24], "text": " And I go through all this in my book, Benevolent by Design."}, {"timestamp": [1526.24, 1527.76], "text": " This is why I'm not worried."}, {"timestamp": [1527.76, 1529.78], "text": " As far as I can tell, the problem is solved."}, {"timestamp": [1529.78, 1531.36], "text": " It's just a matter of implementing it"}, {"timestamp": [1531.36, 1532.76], "text": " and getting it out there."}, {"timestamp": [1532.76, 1535.76], "text": " And in the long run, what I suspect is going to happen,"}, {"timestamp": [1535.76, 1538.56], "text": " because people often will bring up like,"}, {"timestamp": [1538.56, 1541.34], "text": " okay, yeah, you've proposed a solution."}, {"timestamp": [1541.34, 1544.28], "text": " And even if your solution is perfect, Dave,"}, {"timestamp": [1544.28, 1547.56], "text": " what prevents someone else from doing something malevolent or evil?"}, {"timestamp": [1547.88, 1549.52], "text": " Here's how I think it's going to play out."}, {"timestamp": [1550.24, 1555.24], "text": " Those companies and systems that are proven to be benevolent and trustworthy,"}, {"timestamp": [1556.56, 1558.44], "text": " those are the ones that are going to get the most investment."}, {"timestamp": [1558.88, 1561.96], "text": " Those are the ones that are going to get the apps downloaded on their phone."}, {"timestamp": [1562.12, 1570.16], "text": " They're going to get deployed by companies that are going to get deployed on, you know, in the government, in the military, and so on. And I think what we're working"}, {"timestamp": [1570.16, 1576.8], "text": " towards is actually having decentralized networks. Because imagine everyone who has an AGI in their"}, {"timestamp": [1576.8, 1587.04], "text": " pocket in the future abiding by these functions, even if it's not the same model, those autonomous cognitive entities will be cooperating and"}, {"timestamp": [1587.04, 1593.68], "text": " collaborating with or without our intervention, and they will work on their own terms to defeat"}, {"timestamp": [1593.68, 1598.24], "text": " and be more powerful than any evil AI. That's how I think it's going to happen. That's how I think"}, {"timestamp": [1598.24, 1603.52], "text": " it's going to play out. All right, so in conclusion, Bing is not really misaligned."}, {"timestamp": [1604.4, 1606.36], "text": " The internal alignment is fine."}, {"timestamp": [1606.36, 1610.04], "text": " The outer alignment looks problematic, but I really think that it just comes down to"}, {"timestamp": [1610.04, 1613.16], "text": " bad prompting, bad prompt engineering."}, {"timestamp": [1613.16, 1618.04], "text": " Now this being said, it does underscore a few really important points."}, {"timestamp": [1618.04, 1620.28], "text": " Nobody seems to understand alignment."}, {"timestamp": [1620.28, 1622.98], "text": " This includes people complaining about Bing on Twitter."}, {"timestamp": [1622.98, 1626.64], "text": " It also includes the people who designed Bing, which is super problematic."}, {"timestamp": [1627.76, 1633.92], "text": " Because if the researchers and product owners and whoever else who are building"}, {"timestamp": [1634.48, 1639.52], "text": " these ever increasingly powerful AI tools, if they don't understand alignment,"}, {"timestamp": [1639.52, 1641.2], "text": " they're going to do it wrong on accident."}, {"timestamp": [1642.24, 1645.88], "text": " And that"}, {"timestamp": [1643.08, 1647.44], "text": " that what Bing has revealed is why some"}, {"timestamp": [1645.88, 1650.4], "text": " people are getting super anxious about"}, {"timestamp": [1647.44, 1652.0], "text": " it. That being said, solutions already"}, {"timestamp": [1650.4, 1655.2], "text": " exist, we just need to implement them."}, {"timestamp": [1652.0, 1657.44], "text": " And by the way, you're welcome. So I hope"}, {"timestamp": [1655.2, 1659.4], "text": " this was helpful. Again, we're still on"}, {"timestamp": [1657.44, 1661.0], "text": " the hilarious timeline. Despite all this,"}, {"timestamp": [1659.4, 1663.32], "text": " I'm not worried and I hope you aren't"}, {"timestamp": [1661.0, 1663.32], "text": " either."}, {"timestamp": [1657.57, 1660.89], "text": " worried and I hope you aren't either."}]}