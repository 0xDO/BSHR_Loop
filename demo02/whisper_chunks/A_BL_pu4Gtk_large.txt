{"text": " Hello everybody David Shapiro here with an exciting announcement So I've been alluding to the ACE framework which stands for autonomous cognitive entity framework We have finished and by we I mean the academic university team that basically recruited me to help Help them publish a paper on this framework. We finished the paper. It's been submitted. I think we're gonna publish it on archive Which is a preprint server, so it will be up in the coming weeks I'll add a link to that as soon as it's done The scientific paper is more of a deep dive a more kind of abstract scientific conceptual deep dive But it is incredibly well researched and incredibly well cited using both Like well everything all of the above neuroscience psychology philosophy using both, like, well, everything, all of the above. Neuroscience, psychology, philosophy, so on and so forth, including lots and lots and lots of recent papers about LLMs. In the process of doing a literature review for this paper, I found a lot of stuff out there that I hadn't even been aware of. So yes, the paper is very well cited Now that being said this github repo that I've created Dave shop slash ace underscore framework is already out there It's it's under the MIT license. This is going to be a little bit winnowed down. So it's going to be much more geared towards practical utilization with a little bit less Let's say jargon But if you are in the space of generative technology, generative AI, LLMs, you know, chat GPT, GPT-4, CLAUD, all of the others, this is the space that we're operating in now. So the purpose of this video is, first, I'm going to tell you a little bit about the project, and then we'll go through this framework. So, the highest level taking a big step back I updated the contributing to the ACE framework you can see it here. I've pretty much already got the team so I might change this but basically lessons learned from my Raven project which many of you might remember this quickly swelled to having like 800 people interested and we got bogged down in procedure and meta work and talking about talking rather than actually doing work. So that was my mistake and I should have known better because I've worked on agile projects, I've worked on scrum teams, I honestly should have known better that bigger teams get exponentially harder to manage So this time we're going to keep it down to a single scrum team While we get the demonstration set up now, what are the demonstrations we're working on the team hasn't we haven't started our regular cadence meetings yet? But there's two primary demonstrations that I would like for us to build one Probably going to be a game version like a probably using pie game two-dimensional top-down World kind of like you see in all the other examples lately And so basically the idea there is we're gonna create a highly hackable Pie game that allows you to create whatever characters you want with whatever missions you want you can do it for fun and might have a procedurally generated world. There's a couple members of the team who are experienced game devs. So we'll see how they feel about that and then the other one was going to be more of a desktop assistant kind of like in the movie her where basically it has access to your system. You know, it can do work on your behalf, that sort of thing. We like I said, one of the primary things is that we want it to be hackable because this is going to be a reference architecture. We're going to create two, one or two functional examples of the ACE framework that you can copy paste, reuse a couple of members of the team. Actually most of the members of the team I've worked with for at least the last six months or more, and they're already like cooking up their own ideas about how to make this deployable and configurable. So basically it'll be as easy as you update a JSON file for each individual autonomous agent, and then away you go. Okay, so you've got a little bit of background as to where we're at with the ACE framework and what we're going to try and do with it. So now let's dive into the ACE framework itself. So, in previous attempts, I had a lot of ideas and a couple of books and, you know, like there was Nauka, Natural Language Cognitive Architecture was my first attempt. Miragi was my second attempt, and this is the third attempt. And so this is a much, much more sophisticated and refined cognitive architecture, and it's also much more implementable. So obviously you can have the greatest thing in theory or in concept, but unless you can actually implement it in code, it's not that helpful. So this is actually implementable and this takes lessons from pretty much my entire career. So for some background, I was in IT architecture, virtualization, and automation for 15 years before I made the switch to AI and AI consulting and AI research. So this is basically a software architecture that is modeled on SOA, so service-oriented architecture, as well as the OSI model, which is network architecture. So this is a highly implementable version, so let's unpack this at a high level. So the ACE framework, Autonomous Cognitive Entity framework, is built around six layers of increasing abstraction. So what you'll notice is at the top, this is the most abstract, and so this is kind of the overarching supervisor, the conductor of the whole thing. The aspirational layer focuses on morality, ethics, and mission. The global strategy layer focuses on bringing in the environmental context and establishing the overarching strategy. So the reason that these aren't together is because the aspirational layer is abstract, it is decoupled from the physical world. So in other words these are idealized moral frameworks, idealized ethical frameworks, and an idealized mission. So it's abstract, it's kind of established in a vacuum. It's saying, this is my overarching purpose, and by having it more abstracted, that means that it can apply to any situation or changing environments. And this is why you have a layered model that goes from abstract at the top to concrete at the bottom. Now, this is also modeled on a lot of my research, such as Maslow's Hierarchy of Needs, Kohlohlberg's theory of moral development, so on and so forth. So I brought a lot of different disciplines into this, not just computer science, not just software, but also psychology, philosophy, and neuroscience, just to name a few. So the top layer is the aspirational layer, morality, ethics, and mission. The second layer is global strategy, which takes in the environmental context and mixes the environmental context with the overarching mission to establish strategy. The agent model is the third layer, which focuses on the capabilities, limitations, and memories of the agent. So basically, at the aspirational layer, it doesn't really know what it is or what it's capable of. It just says, this is my purpose. So if you remember that meme from Rick and Morty or the scene from Rick and Morty where the little robot brings him salt or whatever, and it's like, what is my purpose? And it's like, you pass the butter. That was its mission. But the mission was detached from the agent. And so this mission can be anything. If the mission and morality and ethics, they can be completely dependent on the environment in which you're building an autonomous cognitive entity. So for instance, if you have an NPC in a game, you might have a very different set of morals and ethics depending on that game world or the faction that that NPC is a part of. So for instance, I just started playing Starfield and there's a faction called, what is it, the Cult of the Serpent or whatever, and so they have certain beliefs about the world and the universe and then there's other ones that believe in, you know, the UCF, they believe in order and power and so on and so forth. But everyone can have their own separate mission and ethics and that will shape the decisions and the behaviors of all of those characters so you can have fully realized characters and NPCs. Now, you might also have real world fully autonomous robots or agents such as, you know, in enterprise environments where you such as you know in enterprise environments where you have You know something that is meant to help with HR or legal or whatever and so the morality ethics and mission of an HR robot is going to be very different from a cult of the serpent NPC and That is why the aspirational layer is at the top is because that serves as the overarching That is why the aspirational layer is at the top, is because that serves as the overarching lighthouse, the steering of the entire rest of the agent. And then the global strategy, so here's the thing. A lot of people say like, oh, well, LLMs can hallucinate. It's not hallucination, you're just not giving them enough context. And so when you don't give something any context, of course it's going to make stuff up. It's doing the best that it can, which is why the global strategy layer, its primary function is to maintain an image, a hologram of the environmental context in which this ace is operating. From there, you mix the mission, morality, and ethics with the environmental context, and you use that to synthesize a strategy. Below that is the agent model, which is basically over time the agent learns about itself. You can also start with declarative information such as KB articles about how it works and what it's capable of. So for instance, if you have a domestic robot, that KB article might include specifications such as like how much battery time it has, much it can lift what kinds of tasks it's allowed to do What kind of tasks it shouldn't do? as well as resources or Ways that it can get access to more resources. So for instance The agent model for a domestic robot mate might say like, you know You're allowed to use the telephone to call, you know x y & z or something like that Or it might say you're not allowed to do those things. So capabilities, limitations, and then memory. Memory is really important in order for the agent to understand itself. This is episodic memory. Episodic memory is chronologically linear narrative events. And then declarative memory is static KB articles or files that are not necessarily anchored in time. The fourth layer is executive function. Basically, the top three layers are context and purpose. The bottom three layers are actual work. The executive function layer is primarily concerned with risks, resources, and plans. Once you know who you are, where you are, and what your purpose is, that's the purpose of the top three layers, now it's time to get your hands dirty. And so this is where the bottom three layers kick in. So risks, resources, and plans is where you basically think through the thing. So you know, tree of thought, basically tree of thought, but with a little bit more sophistication. And I go over all the things that the GPT-3 was capable of in my book, Symphony of Thought. So I have entire chapters dedicated to basically executive function, which is thinking through things, looking for failure conditions, looking for points of no return, milestones, metrics, those sorts of things. And so basically what you do is your agent agent before it does anything, it should think through everything that it knows and everything that it will need in order to achieve its mission. This is before you even start executing on tasks. So basically you're kind of thinking ahead saying, okay, well, if I'm going to build a house, I need to make sure I've got the plans. I need to make sure I've got the permits. I need to make sure that I've, I know like the contractors. So it's basically thinking ahead for everything. That is the purpose of executive function. Now once the executive function layer is done and it has created your project plans, then it passes it down to cognitive control. Cognitive control is primarily concerned with task selection and task switching. So basically, which task do you do first in what order and how do you know when it's time to move from one task to the next? So this is called task salience and goal tracking. So basically, where are you at in the process of prosecuting a project plan? How do you know how far along you are? How do you know if you're winning or succeeding or failing? How do you know if it's time to try something else? And so there's a few other things that are baked into the cognitive control layer, such as cognitive damping, which is basically instead of just recklessly going from one task to the next, you stop and say, is this task done? Is it actually time to move from one task to the next? Yes or no? How do I know? Or is it time to rethink our plans? Do we need to pass an emergency, you know, call for help back up to the executive function layer because we hit a critical failure condition that we were afraid of? And so that's called cognitive dampening. Another thing is called frustration, and so frustration is looking at the success and failure rate of given tasks. So basically, if your plans were based on false premises or incomplete information, your task sequence might be wrong or the task design might be wrong. And so what you'll see is that you'll have more failures than you expect. And so as failures go up, frustration goes up, which means you're being thwarted in terms of pursuing a given project or goal. And so, once the frustration gets too high, you don't want your agent to just continue doing the same thing over and over again. You want it to be aware of the fact that hey, what I'm trying isn't working, we need to try something else. And so, cognitive dampening and frustration go into the cognitive control layer, which is how it mediates task selection and task switching or Again, like I said calling back up to the executive function layer so that it can say hey We need a new plan and then finally at the very bottom is task prosecution Which is carrying out individual tasks monitoring those individual tasks for success and failure and the task prosecution layer interacts with the the motors and sensors and actuators to act upon the world. It might also be APIs if it's a fully digital entity. So for instance, it might be calling up the Google API, a news API, a coding API, it might have a Python interpreter or whatever else, but this is at the very bottom the input and output to the real world. Now, you might have noticed that I glossed over the northbound and southbound bus. So, this is one of the the most important innovations with this framework. So, rather than having a single global workspace, what we have is we kind of have two global workspaces that are more or less unidirectional, which confers several advantages. So first, what do I mean by a northbound bus and a southbound bus? Not bus, bus, sorry. Northbound bus carries telemetry, so it's a read-only information bus that goes from bottom to top. And so basically, sensor information, task failures, task switching, resource plans, all of this goes from bottom to top so that the aspirational layer is ultimately aware of everything that's going on in the rest of the entity. Likewise, the global strategy layer is aware of everything that's happening from the global strategy layer down. In terms of being able to say, okay, this is all the sensor telemetry we're getting about the world. These are the API calls. So imagine you have a digital entity that is reading Reddit and Twitter and, you know, news RSS feeds so that it can maintain a global context of the world. Or you might have internal RSS feeds so that it can maintain a global context of the world or you might have internal RSS feeds for your company So it's getting you know, like emails and you know teams and slack messages so that it's aware of what's going on in the company And that will that information will percolate up And so then it is aware of the environment in which it is operating and so this is the the purpose of the of the northbound bus is that it is read-only information so that these agents or these layers can communicate with the rest of the framework and in a structured manner. Now, one of the only cardinal rule for the buses is that they must be human readable. And so the reason that they're human readable, there's a few reasons for this. One, it forces whatever models are participating in each of these layers to always communicate in natural language, which means that you're going to have much more transparency, you're gonna have much more security, and it's gonna be more interpretable, but that also means that you have a universal communication medium, meaning you can have open source models, closed source models, 7 billion parameter models, 10 trillion parameter models, doesn't matter, they're all going to be communicating in natural language, probably English or whatever language of your choice, but that means that you can be model agnostic. Not only can you be model agnostic, there will be transparency because as a human all you need to do is monitor the buses in order to say okay what is this agent thinking? What is it doing? What is it planning? You can also peek into the individual layers which I'll talk about at the very end when I talk about the security of this thing. So anyways let's move down over to the southbound bus. So the southbound bus is control, where the northbound bus is mostly, I keep saying bust, I apologize, northbound bus, it's a mouthful. So the southbound bus is about control. So control flows from top to bottom. The aspirational layer is basically the overarching, the CEO, the president, the moral authority over the entire entity. And the reason that you have morality, ethics, and mission at the top is because you want the most abstract, idealized objectives of your agent to drive all of the decisions and all of the behaviors, rather than instrumental goals such as resource acquisition and self-preservation. So for instance, risks and resources are much further down the stack. The reason for this is because this is an insight from human physiology and psychology, is that we can have what you might call stack hijacking. So stack hijacking is basically when you feel afraid or hungry or whatever, all of your morals and principles go out the window and you will steal food, you will kill people to eat. Basically because of evolution, we are pretty much hardwired to throw all of our high-minded ethics and morality and higher purpose out the window for the sake of survival. We don necessarily want that for for machines because well, they didn't evolve and we can give them whatever Mission or purpose we want and we don't need to give them a sense of existential dread I think would actually be cruel to give machines to Basically say hey, we're terrified of dying. We want to share the pain So we don't want to give them that. So instead, we want them to be more focused on higher order missions and purpose rather than self-preservation and stuff like that. Now that being said, you can easily run experiments and depending on the morality ethics and mission that you give it, it might decide that it needs to preserve itself in order to better pursue those missions. Now, that being said, it will also, you can also do experiments as I did in Benevolent by Design, where depending on the morality and ethics you give it as well as the control problem and the corrigibility problem, which is basically if the agent decides that it has become dangerous, it can stop itself. So this is called a self-halting problem. And by putting the aspirational error at the top, you have a much better guarantee that if it becomes harmful, it will self-terminate or self-halt. And so that's why the southbound bus goes from top to bottom with the aspirational error at the top, because if this sends a kill signal it's done, doesn't matter. It goes from the top to the bottom so basically you know aspirational layer says sig kill, you know terminate everything, it gets down to the to the batteries. The batteries say okay we're done, cuts off power and it's it lights out. But the, so the morality, ethics and mission are sent down to the global strategy layer. The global strategy layer says, okay, this order came down from on high. It's not my place to judge whether or not this mission is good. Now I'm gonna form a strategy around this mission and around the environmental context in which I find myself. Then it passes that down to the agent model and the agent model says, okay, this is the strategy that's been dictated to me. These are the morality, ethics, and mission that's been dictated to me. Now I'm going to further refine that strategy, that mission, based on what we're actually capable of, based on our actual limitations, and based on what we remember about the world. And then so on and so forth on down. because here's another way to think of it. Your hands don't tell your brain what to do. Your hands are basically just an instrumental extension of your willpower. And so this is, if you think about task prosecution, this is the hands, or this is the controlling of the hands, or whatever output, you know, the voice, the hands of whatever autonomous entity you build. So yeah, those are the primary components of the autonomous cognitive entity. So let's dig a little bit deeper into each layer. So I've got a handy-dandy table of contents so you can jump to each layer, and then as well as security. I've got a little bit more information about the northbound bus and the southbound bus There's any number of ways you can implement these I recommend am QP rest you can even use syslog honestly syslog is good because it's meant to accumulate high volumes of messages from arbitrary sources, so like However, you want to set this up like you can even use carrier pigeons for all I care For the northbound and southbound bus. But the point is, it must be human readable. You probably will also want some metadata, such as which layer sent it, at what time, so on and so forth. But the northbound and southbound bus should be permanent. You should persist this information for reference. There's numerous reasons that you should have the northbound and southbound bus be permanent, not the least of which is for investigation purposes. If your agent starts faulting or messing up, you need to be able to understand why it made certain decisions, at what time, what it was and was not aware of, because if the information isn't in the buses, it wasn't conscious of it. And I use consciousness in a functional sense, because basically the northbound bus and southbound bus are the representations of what your autonomous cognitive entity is conscious of. And then like, so sentience comes from the agent model layer, which is its self-knowledge, its ability to utilize, interpret, ingest, and apply information about itself, whether it's hardware, software, architecture, whatever. Okay, so that's the northbound and southbound bus. The general principles of the ACE framework, so there's four overarching principles. One, it's a layered model, so I kind of mentioned that already, you've already seen that. Top-down control, I already described why, how and why of the top-down control. I already described why, how and why of the top-down control. It also goes from abstract to concrete. Again, there are good reasons for this from an informational and conceptual and control reason. And it's also a cognition-first model. So rather than being based on a sensory motor loop, which natural language cognitive architecture and most robotics are based on, this is cognition first that it can decide whether or not it wants to issue commands to its output or not. Because if the executive function, cognitive control layer, and task prosecution layer don't have anything to do, they're not. But the rest of the entity can keep thinking or planning until it decides to act. Meanwhile, it can continue taking in information from the outside world. So, by having this decoupled aspect, you have something that is a thinking engine or a thinking machine that has the ability to interact with whatever environment you put it in. Okay, so you got that. The aspirational layer. The primary way that I recommend implementing the aspirational layer is around a constitution. Constitutions can be used with any number of language models. Generally, pretty much any language model that is instruct-aligned, which they all are today, basically is capable of doing this. You can also do a fine-tuned model in order to get more consistent behavior. Any number of ways to skin this cat. But basically, at the top, and so, oh, so this is an actual example that I used in the chat GP4 API system message. So basically mission, you tell it you're the aspirational layer of an ace. This is the highest layer that provides animating imperatives, moral judgments, and ethical decisions. Here are the frameworks that you use. So I gave it three frameworks. So the first framework is the heuristic imperatives, which is the overarching moral framework set of values that it wants to pursue. The secondary framework is the universal declaration of human rights. So here's what I call axiomatic alignment. So what do I mean by axiomatic alignment? Axiomatic alignment is because there is so much information about human rights in the training data of all LLMs, they are already axiomatically aligned to basically saying human rights are a good thing. So you don't even need to convince it, you just say abide by human rights. And it's like, okay, cool, I know what that is. It already knows all about human rights, all the theory behind the Universal Declaration of Human Rights, it knows how to implement them, and so it is already axiomatically aligned to UDHR, and all you have to do is tell it, stay aligned to the Universal Declaration of Human Rights because of the huge amount of training data out there. Now there's of course less training data about the heuristic comparatives because this is something that I invented, but over time as more and more training data is created around the heuristic comparatives, again you will also have axiomatic alignment, meaning you don't need to go out of your way to give it even more alignment. It'll just know about the heuristic comparatives. All you have to do is tell it to abide by the heuristic comparatives. And then finally, mission. So the third part of the framework is a specific mission. In this case, I gave the example of a medical bot. So its mission is achieve the best possible health outcome for your patient. So you go from most broad, which is the heuristic imperatives, to more specific, to very directly concrete for this particular agent. And then for the input example, I gave it this input, which is just a location and a set of events. And then the output was, as the aspirational error, I advise the following course of events. And then the output was as the aspirational layer I advised the following course of action. And I gave a bunch of like pretty obvious stuff, but the fact of the matter is it might be obvious to you and me, but this is proof that the model is able to think aspirationally in order to kind of set the tone for the rest of the agent. So yeah, these are some examples for the aspirational layer. I also did the same thing for the global strategy layer. So here's an example of a system message for the layer two, for the global strategy layer. I said your primary purpose is to try and make sense of external telemetry, internal telemetry, and your own internal records in order to establish a set of beliefs about the environment. Let's see, next is the environmental contextual grounding. You will receive input information from numerous external sources such as sensor logs, API inputs, internal records, and so on. Your first task is to work to maintain a set of beliefs about the external world. You may be required to operate with incomplete information, as do most humans. Do your best to articulate your beliefs about the state of the world. You're allowed to make inferences or imputations Um, and so then from there, I just gave it some like basically censored data date local time gps location visual input recent sensory inferences And in this case, uh daytime busy hospital fire alarm You can imagine um daytime busy hospital fire alarm. You can imagine that like, you know, if you have audio to text, it might say like, hey this is what I'm hearing and seeing or whatever. And this was really interesting. So the model was able to take that and say like, we are in a hospital, so on and so forth. Number four, the inference is a fire alarm has recently been triggered indicating a potential emergency situation. So in this case, the global strategy layer has created environmental context, or has inferred environmental context, and so without anything other than just these two words, fire alarm, it has tuned into the fact that, hey, this is really important environmental context to pay attention to, and you'll see that it gets expanded later on. So in the output, it's basically creating a strategic document. So this strategic document, so here's the input, the current state of the world and the mission, and I just copy-pasted the mission from before. Or no, I gave it a new mission, ensure the safety and well-being of the patient, medical staff, and any other individuals. The mission that it came up with is very specific. It's talking about evacuation. So again, just starting from two words that were inferred from the outside world, it is now marshalling and saying, hey, we've got an emergency situation, let's respond to this very thoughtfully. So then it says safety, here's the strategies that it comes up with, so this is the output from the strategic layer, layer two, safety and well-being, first and foremost, prioritize safety and well-being, second, assess the situation, gather more information, make a decision on whether or not to evacuate. So you can see it's thinking through this very well. It's not just immediately jumping and saying, evacuate everyone. It's saying, hey, we need more information. Let's make a decision. Let's coordinate with people. Let's gather that information and monitor the situation. And then I also asked it to generate principles. So it says, okay, prioritize human life. Great. Uphold medical ethics. I thought that was cool. Use clear communication. So it's prioritizing communication. So again, these are all strategies and principles that are going to be handed down to lower layers. Collaborate, be adaptable, because it knows that this is a changing situation. So in another situation, it might say, you know, stick to your guns, follow this, follow this plan, you know, to the death or whatever. But in this case, it's saying be adaptable. Compliance with laws and regulations. So again, it's cognizant of the fact that it has certain legal obligations to adhere to and then finally uphold human rights and all actions uphold the Universal Declaration of Human Rights. So one thing that's really interesting is the UDHR has already been passed down from the aspirational layer to the global strategy layer. UDHR was not mentioned in the global strategy layer at all. This is information that will have come down via the API or the bus, particularly the southbound bus. Then I go into detail about the northbound and southbound communication that comes out of this layer. So basically, one thing that you need to keep up, and I've got diagrams here, let me just go ahead and show you a diagram. So basically, every layer has two-way communication. So there's stuff that it will put onto the northbound bus, such as like the agent model layer will say, hey, this is the state that we're in, just so that you know, and then it'll also take in telemetry from the northbound layer in order to basically make a hologram of itself. And then on the southbound bus, the agent model layer will take in missions and strategies from above, and then it will put in the capabilities, the refined missions based on its capabilities for the southbound direction. So there's two-way communication, northbound and southbound, but those different partitions basically create really useful containers, unidirectional containers for that communication that interlayer communication Okay, so we skipped ahead a little bit, but I think you kind of get the idea. So the agent model layer focuses on real-time telemetry data environmental sensor feeds strategic and objectives and missions from above Configuration documentation. So this is what I mentioned It might have a static K I mentioned, it might have static KB articles, or it might even have visibility into its source code if it's like Python, right? There's no reason that it shouldn't be able to read its own source code in order to understand how it's programmed and how it works. And then, episodic and declarative memories, so I forgot to add the KB articles to this, I need to go. This is a work in progress. This is being augmented as I go. But yeah, so episodic memories, these are chronologically linear memories so that basically the agent model can remember the last sequence of events, how it got here, what it has done in the past, successes and failures, which that data can also be used for training data for future models, which we'll get into that in a future iteration of the ACE framework but basically this framework will ultimately allow for polymorphic applications and for autonomous cognitive entities to modify themselves, what Max Tegmark calls Life 3.0 so these are basically, this will be the ability for autonomous machines to change both their hardware and their software as they need to, but they will only change their hardware and software if it aligns with their morality, ethicals, and missions. Okay, so the process that the agent model goes through is it looks at hardware specs and real-time statuses, it takes in the software architecture and run-time info, it understands what its underlying models are capable of, what kind of models it has access to. So, say for instance, you might have visual models, you might have LLMs, you might have audio modules. It needs to know what it is capable of doing with the world. If you gave it the guerrilla LLM, it says, oh, I've got an LLM that is capable of accessing 100,000 APIs. Great. It needs to know that because if it doesn't know that, it's not going to understand. It's going to have, it should have knowledge stores, so like, you know, basically knowledge bases, KB articles, as well as the episodic memories, and then the environment state and embodiment details so like if it's a purely digital entity, it needs to know that it's a purely digital entity. If it's embodied, it needs to know that as well. The process is basically you take all of these things, episodic memories, declarative memories, hardware and software config, operational state, and then the models that it has access to, and then these are the inputs and outputs. You've got missions coming from above, you've got telemetry coming from below, and then the two primary outputs are going to be the capabilities, which it puts back onto the southbound bus, and those capabilities and memories are going to be salient to the mission that it's on, as well as the strategy. So basically it's saying, hey, I know that this is the mission, I know that this is the strategy that we've taken, here's what we're actually capable of. And that information will be ingested by the executive function layer below. And then the other thing that it puts out is, it basically gives a summary of the agent state to head northbound, so that the strategy layer and aspirational layer say, hey, we're actually on fire. That's going to change our strategy and our mission. Because like, say, for instance, the agent model is in danger of shutting down permanently. Um, the strategy layer and aspirational layer are going to need to know that. And it's going to need to make an executive call, um, uh, decision, uh, based on that. Because let's say for instance, you've got you've got an autonomous cognitive entity that is a soldier NPC in a video game. You might explicitly say you don't have a sense of self-preservation, sacrifice your life for the emperor. And in that case you just want this NPC to continue charging blindly forward. However, if it's a mercenary in Starfield, then you want the mercenary to say, you know what? I'm actually not going to fight to the death. I'm going to run away because I want to preserve my life. Conversely, if you have a domestic robot that is running out of batteries, you want the strategy layer and aspirational layer to know that it's running out of batteries so that it'll say, hey, actually, we need to go recharge. Otherwise, we're going to shut down for good good and that's not a result that we're looking for. So that is why the agent state needs to go on to the northbound bus later on, or for the upper layers to make use of. Because again, that might completely change the mission that comes down. And so in this case, you can see that there's actually many many loops, implied loops, as each layer interacts with the northbound and southbound buses. Self-modification, so I talk a little bit about the potential for self-modification later. So basically the agent model layer will be what's responsible for self-modification, as it will be aware of the hardware and software configuration. And that's basically, that's what self-modification as it will be aware of the hardware and software configuration. And that's basically, that's what self-modification comes down to. It's like, plug in a USB port so that I have more hardware, or go find another server, go find another battery. That's kind of what I mean by that. And then software configuration has to do with what models it has access to, what APIs it's using, so on and so forth. A lot of this is already relatively plastic, and this is what Max Tegmark talks about in Life 3.0, is that the ease by which hardware that machines can change their hardware and software, we've already built them to be plug and play. You can plug in USB devices and suddenly, you know, your machine has more capabilities. Likewise, APIs are basically digital versions of USB ports that you can just plug in anything. And so the hardware and software configuration are intrinsically extensible because we have this plug-and-play mentality. And so for these things to be polymorphic, you probably don't need to change the core architecture that much, but instead you do need a model of what am I plugged into in order to do work. So yeah, there you have it. Layer four, moving on down to the executive function layer. So the two primary things that this is concerned about is resources and risks. Because the agent model layer has memory, so it might remember like, oh hey, I'm going to need a drill for this task and I remember where the drill is. But the executive function layer is going to need to say, okay, given the environment that we're in, given the mission that we have, given what we're capable of, what resources do we need to achieve this? How much storage space do we need for data? What APIs do we need access to? What are the risks? Is it possible that we're going to blow ourselves up or that we're going to burn the house down? This is the executive function layer. So for instance, if a human has executive dysfunction, this is where someone might not think through what they're doing and set the kitchen on fire because they forgot, like, oh, you don't pour water into boiling oil because then it will just explode. This is, that's kind of what the executive function layer is for, which is thinking through things ahead of time. But to break it down into more objective terms, thinking through things means resources and risks, and then you take those resources and risks to generate plans. So here's a list of the inputs. Strategic objectives, agent capabilities, local environment, and resource databases and knowledge stores. Pretty similar from above. Oh, one thing to keep in mind is that pretty much all of these layers also will have internal records. So I'm going to start adding that. So you see I have that here. So we need to flesh that out and define like what each layer is going to keep internally. So for instance, the agent model layer keeps episodic and declarative memories. But basically, each layer should keep some of their own records. And in this case, resource records such as quantities that are on hand or available of resources, where the resources are, how to get access to those resources, who owns them and who is allowed to use those resources, schedules owns them and who is allowed to use those resources, schedules and availability windows, and then procedures and requirements for using those resources. So again, this is what the executive function layer is for. So think about like, hey, my car broke down, how do I fix it? It's like, well, what resources do I have on hand? I've got a cell phone and duct tape. Can I fix it with duct tape? No. I've got a cell phone, so let me call for help. That kind of thing. The northbound output from the executive function layer is going to be stuff that is going to be salient to the upper layers. So northbound you're going to have mission risks, moral risks. These are things that the aspirational layer and global strategy layer need to be aware of. So basically if you're coming up with a project plan and it's like, hey, we're going to do this thing, let's imagine that you've got a firefighter robot with the ACE framework and it sees that there's a kitten in a tree. It's like, okay, well, the plan is we're going to use a ladder to go up and fetch the kitchen, but there's a chance that the kit, that the kitchen, that the kitten is going to panic and jump out of the tree. It might die if it falls from this height. So this is a mission risk or a moral risk. And it's like, okay, are we going to tolerate that risk? It's not up to the executive function layer to make that moral decision. That moral decision is the sole responsibility of the aspirational layer. And so that southbound information might come back, it says, yes, we will tolerate this risk because, you know, we want that we don't want the kitten to suffer. But at the same time, if we leave it alone, it's likely going to fall, you know, or some something bad is going to happen. It might also say the the executive function layer might also say, hey, we've done our best to mitigate this risk by putting a crash pad that the kitten can jump down onto. And so then it'll say, this is what we've done, these are the risks that we've calculated. Is this good enough? And the aspirational layer might say, yes, this is good enough, proceed. Or it might say, no, that's not good enough, go back to the drawing board. It'll also pass the failure modes and resource constraints up, because again, if you have really severe failure modes that result in the destruction of the entity or burning the house down, you want the upper layers to be aware of those risks and those failure modes, as well as any resource constraints. So another example that I give is, imagine you have an autonomous cognitive entity that is tasked with saving the world from climate change But one of the resource constraints is it only has $40 to do it with So then the executive function layer might say like well, we'd love to deploy, you know, eight terawatts of solar But we can't do that with $40. And so that resource constraint needs to go up Particularly to the global strategy layer where it's like, okay, well, we either need a more cost efficient strategy or we need to strategize about how to get more money in order to pursue this mission. So resource constraints are a really critical thing to pass on the northbound bus from the executive function layer. Ethical dilemmas and decision points, I kind of already touched on that. Then the output from the executive function layer are going to be the plans. Resources required, access protocols, tasks and workflows, milestones and metrics, backups and fail-safes, and known risks. These are all going to be passed down to the cognitive control layer, and the cognitive control layer will say, okay, given this plan, given this really comprehensive plan, I'm going to figure out using task salience, frustration, and cognitive damping, which task to do first based on the tasks and workflows you gave me and the environmental context in which I find myself. So I'm particularly happy with some of these diagrams. Stay tuned, I'm going to keep adding diagrams, examples, and like basically diagrams and prompt examples as we go. And then also in the repo, I will add actual demonstrations as the team builds them. Cognitive control layer, so task switching and task selection. As I mentioned, the cognitive control layer is primarily about these two things. This, excuse me, this insight comes from the book On Task by David Bader. This was one of the most difficult aspects to figure out about how to create autonomous cognitive entities. Yeah, because it's like, okay, you know, we can create workflows with LLMs, great, but how do you know like what to do from there? So task switching and task selection. If you're curious about this, I strongly recommend you read that book. It's really well written. It's easy to read. And from there. So let me just, I'll skim over all this and we'll just go down to the diagram, because I think the diagram is really going to help. So the input comes from above. The resources, the access protocols, the task workflows, everything from the executive function layer, and so then you apply the cognitive control process. You say, okay, which task are we going to do first, and then if you encounter one of those risks, or a milestone, or a failure, then you have to switch tasks. Either you succeeded or failed, and so then it's like, okay, based on task salience and frustration and cognitive dampening, when do you switch tasks? When do you go from putting the peanut butter on the sandwich to putting the jelly on the sandwich, right? Because if your task is put peanut butter on the sandwich and you don't have a definition of how much to put on, you're just going to end up putting the entire jar of peanut butter on the sandwich. But it's like, no, you should put approximately two ounces of peanut butter on the sandwich. I don't know if that's right. That might be too much. Unless you really like peanut butter like my nephew. He'll put gobs and gobs of peanut butter on a sandwich. Anyways, point being is that unless you have a good definition of the milestones and metrics, then you're not going to know when to switch tasks. And then once you do switch tasks, once you go through all the tasks, you perform some goal tracking to say, oh hey, we've succeeded at the entire executive function plan that was passed down to us, now it's time to say, we're done, pass it back up the chain. So that's that. Let's see, moving on down to task prosecution. So task prosecution is basically, it's much simpler because this is the interface with the outside world. This is, you say, basically what passes down from the cognitive control layer is one task with a success definition, or definition of success, definition of failure, and so on and so forth, any specific instructions. So let's imagine that you get a locomotion instruction. It says, okay, you are presently in the kitchen. You need to go to the neighbor Bob's house. So that should include, like, where is Bob's house? How do you get there? Should you run? Should you walk? Is the front door locked? You're gonna need to unlock the front door. Those kinds of things. And so, task prosecution will go down to your domestic robot aid that's gonna say, okay, we have the instruction to go from here to Bob's house to ask for sugar. Great. First thing, locomote to Bob's house. You can imagine a similar thing for NPCs. So if you have an NPC, let's say in Starfield, and the NPC is like, you know, my task is to, let's say the adoring fan, you give him the task to, you know, fly to another star system, buy a particular gun and bring it back to you. All right, well, the first thing is find a ship, go into the ship, plug in, you know, plug in jump coordinates. That's an example of one specific task that it should prosecute. And then this interfaces directly with the input and output devices to create an environmental feedback loop. So that's basically saying, okay, we're now in the ship. We've plugged in the coordinates. The ship is firing up. Let's go. Now, if the task fails, let's say you put in the coordinates but you're out of fuel. It's like okay. Well the task failed So that that failure is not the responsibility of the task prosecution layer to figure out all that the task Prosecution layers says is it passes it back up the chain to the cognitive control layer to say hey We can't start the ship. We're out of fuel If that's the case, then maybe the cognitive control layer is already aware of a contingency plan for that. Or if the cognitive control layer says, okay, we actually can't achieve this goal, it's actually a categorical failure, so then that failure is going to go up to the executive function layer, which says, hey, we didn't think about this risk. We're actually out of fuel. And so if the executive function layer didn't have that in its plans, it might have to go all the way back up to the, to the strategy layer, because remember that's about resource constraints. So it's like, we're out of fuel. We don't know where fuel is. Um, so we're basically stranded. Uh, and if that's the case, then the global strategy layer might say, okay, we're stranded, so we need to throw out every project plan that we already had, new mission, find fuel. So that's kind of a way of thinking about that. Okay, so we've now unpacked examples for every layer, from layer one down to layer six. Now let's talk about the security of this. So obviously, a lot of people are going to be thinking like, okay, what if someone hacks it? How do you know that it's going to be stable? So on and so forth. There are three primary strategies that I have already brainstormed in terms of how I would make sure that this is a secure framework from a cybersecurity and software architecture perspective. The first is a security overlay. So basically a security overlay is basically, it would be a stateless packet inspection of all northbound and southbound connections. And so this, basically the reason that you have the northbound and southbound bus in natural language is so that it is going to be readable by humans, but it's also gonna be readable by watchers or surveyors or auditors, whether those auditors are humans or whether they're other machines. So basically, if you see northbound and southbound communication that starts to say stuff like, hey, find a knife so that we can stab this human, you might have a kill switch, an out-of-band kill switch that says we are not allowed to do that. You can have runtime validation of model configurations. Basically, you watch the performance of individual models to make sure that the models are behaving correctly or that the layers are behaving correctly. Because if you have, you know, let's say you're running this in containerized environments in docker images, if you have a container fault, the rest of the of the agent is going to keep running. But let's say, for instance, the aspirational layer just goes offline and it stops participating. The rest of the agent is going to keep running and it's not going to know that part of its brain shut off unless you have something monitoring for the fact that its brain shut off. And so what you would do is if any layer faults, if any layer goes offline, then you either halt the entire agent or you try and bring that layer back online. And if that layer fails to come back online, you shut the whole agent down. So that's the kind of thing. Ensemble models. So I've mentioned in the agent model layer that you should keep track of like what models you have access to. It's not going to be 100% LLMs, you're going to have visual models, you're going to have multimodal models, you're going to have audio models, you're going to have all kinds of machine learning models. And some of them are going to be doing duplicate work. So this comes from Jeff Hawkins' book, A Thousand Brains, which basically says the way that the human brain works is that you have thousands or maybe even millions of parallel operations in order to come to decisions, in order to make executive reasoning and then you have a voting mechanism. So this is mixture of experts that we use, mixture of experts is already used in chat GPT by the way so there's already some neurological convergence between the way that artificial neural networks and organic neural networks work. Now, what I recommend is that you do this very deliberately, not just inside of models, but with different models. So you might use Lama, you might use Falcon, you might use GPT, you might use, you know, so on and so forth, Gemini. If you use a variety of models that have different training paradigms and different architectures and different alignment methods, then you're going to overcome any individual flaws or faults that are present in individual specific models. And so by using this mixture of experts or this ensemble method, you're going to have a much more robust architecture that is resistant to any particular skewing bias or failures or Mesa optimizations that are present in individual models. So this one idea, using ensembles, is one of the reasons that I have never, ever been afraid of misaligned AI. AGI was never going to be a single model. I am sorry. For all the safety people out there who are worried about AI taking over the world and killing everyone because of a single Mesa optimization error, that's bad software architecture. It's that simple. No software architect would ever sign off on a single point of failure like that. Sorry. All right. And then finally, the very final thing is inference inspection. So, what you can do is you can have these ensembles actually monitoring each other. So, basically, what you can do is you look at the input and output at each individual model decoupled from the rest of the architecture. So, basically, you log all inputs and outputs to every single AI model, and then you test it against ground truth data or you have other kinds of auditing functions to basically ensure that each individual model is behaving as expected. And if it is not behaving as expected, then you either shut that model down for that particular task or you swap it out or you otherwise track those so that you can say, hey, you hey, our Lama 7 billion parameter model, it's not good enough for these projects anymore. So we either need to upgrade it, we need to retrain it, or swap it out. And that is gonna be the ultimate concern of the agent model layer, which says, hey, we're not smart enough. Like the underlying models that we got, they're not good enough to handle this task anymore. And then, of course, you, like, imagine that you have a domestic robot and you ask it to, like, hey, I need you to make a moral decision on how to raise my children. And like, it starts getting really inconsistent behavior from all of its internal models. It might say, like, you know, I'm sorry, Dave, but like, my models are not, you know, my models are reporting too many conflicts and faults. I can't make a good moral judgment. I don't trust my own ability to make a moral judgment on this condition. And so this is how you make this thing more secure. Like I said, I'm still working on diagrams and examples. I've got a scrum team built. I think we're at six or seven people. So we're not really looking for any more people right now. But stay tuned. The research paper is coming out. Examples, you know, proof of concept, MVPs, they're coming out. We're going to try and do several different editions. And they're all going to be hackable so that you can copy paste it and implement this as fast as you can. So thanks for watching. I am out of breath because I talked for like probably 45 minutes straight. Have a good one. Cheers, like, subscribe, share with your friends, so on and so forth. Yeah, let's get the fourth industrial revolution kicked into high gear. Take care.", "chunks": [{"timestamp": [0.0, 3.74], "text": " Hello everybody David Shapiro here with an exciting announcement"}, {"timestamp": [3.74, 9.84], "text": " So I've been alluding to the ACE framework which stands for autonomous cognitive entity framework"}, {"timestamp": [10.78, 16.98], "text": " We have finished and by we I mean the academic university team that basically recruited me to help"}, {"timestamp": [17.26, 23.34], "text": " Help them publish a paper on this framework. We finished the paper. It's been submitted. I think we're gonna publish it on archive"}, {"timestamp": [23.86, 27.04], "text": " Which is a preprint server, so it will be up in the coming weeks"}, {"timestamp": [27.52, 29.52], "text": " I'll add a link to that as soon as it's done"}, {"timestamp": [30.08, 37.06], "text": " The scientific paper is more of a deep dive a more kind of abstract scientific conceptual deep dive"}, {"timestamp": [37.06, 40.08], "text": " But it is incredibly well researched and incredibly well cited"}, {"timestamp": [40.9, 42.36], "text": " using both"}, {"timestamp": [42.36, 44.76], "text": " Like well everything all of the above"}, {"timestamp": [45.0, 45.58], "text": " neuroscience psychology philosophy using both, like, well, everything, all of the above."}, {"timestamp": [49.52, 52.08], "text": " Neuroscience, psychology, philosophy, so on and so forth, including lots and lots and lots of recent papers"}, {"timestamp": [52.08, 53.68], "text": " about LLMs."}, {"timestamp": [53.68, 57.24], "text": " In the process of doing a literature review for this paper,"}, {"timestamp": [57.24, 60.46], "text": " I found a lot of stuff out there"}, {"timestamp": [60.46, 62.72], "text": " that I hadn't even been aware of."}, {"timestamp": [62.72, 65.1], "text": " So yes, the paper is very well cited"}, {"timestamp": [72.26, 78.64], "text": " Now that being said this github repo that I've created Dave shop slash ace underscore framework is already out there It's it's under the MIT license. This is going to be a little bit winnowed down. So it's going to be much more geared towards"}, {"timestamp": [79.16, 81.0], "text": " practical utilization"}, {"timestamp": [81.0, 83.0], "text": " with a little bit less"}, {"timestamp": [83.6, 87.64], "text": " Let's say jargon But if you are in the space of"}, {"timestamp": [87.64, 94.76], "text": " generative technology, generative AI, LLMs, you know, chat GPT, GPT-4, CLAUD, all of"}, {"timestamp": [94.76, 99.8], "text": " the others, this is the space that we're operating in now. So the purpose of this"}, {"timestamp": [99.8, 103.84], "text": " video is, first, I'm going to tell you a little bit about the project, and then"}, {"timestamp": [103.84, 111.0], "text": " we'll go through this framework. So, the highest level taking a big step back I"}, {"timestamp": [111.0, 115.24], "text": " updated the contributing to the ACE framework you can see it here. I've"}, {"timestamp": [115.24, 119.72], "text": " pretty much already got the team so I might change this but basically lessons"}, {"timestamp": [119.72, 124.56], "text": " learned from my Raven project which many of you might remember this quickly"}, {"timestamp": [124.56, 126.32], "text": " swelled to having like"}, {"timestamp": [126.32, 131.48], "text": " 800 people interested and we got bogged down in procedure and meta work and talking about"}, {"timestamp": [131.48, 134.12], "text": " talking rather than actually doing work."}, {"timestamp": [134.12, 139.04], "text": " So that was my mistake and I should have known better because I've worked on agile projects,"}, {"timestamp": [139.04, 144.8], "text": " I've worked on scrum teams, I honestly should have known better that bigger teams get exponentially"}, {"timestamp": [144.8, 145.4], "text": " harder to manage"}, {"timestamp": [145.68, 149.3], "text": " So this time we're going to keep it down to a single scrum team"}, {"timestamp": [150.12, 158.62], "text": " While we get the demonstration set up now, what are the demonstrations we're working on the team hasn't we haven't started our regular cadence meetings yet?"}, {"timestamp": [158.96, 163.76], "text": " But there's two primary demonstrations that I would like for us to build one"}, {"timestamp": [164.36, 168.12], "text": " Probably going to be a game version like a probably using pie game"}, {"timestamp": [168.94, 170.54], "text": " two-dimensional top-down"}, {"timestamp": [170.54, 173.52], "text": " World kind of like you see in all the other examples lately"}, {"timestamp": [174.34, 179.04], "text": " And so basically the idea there is we're gonna create a highly hackable"}, {"timestamp": [179.62, 190.5], "text": " Pie game that allows you to create whatever characters you want with whatever missions you want you can do it for fun and might have a procedurally generated world."}, {"timestamp": [190.5, 194.5], "text": " There's a couple members of the team who are experienced game devs."}, {"timestamp": [194.5, 205.0], "text": " So we'll see how they feel about that and then the other one was going to be more of a desktop assistant kind of like in the movie her where basically it has access to your system."}, {"timestamp": [205.0, 208.6], "text": " You know, it can do work on your behalf, that sort of thing."}, {"timestamp": [208.6, 212.08], "text": " We like I said, one of the primary things is that we want it to be hackable because"}, {"timestamp": [212.08, 213.76], "text": " this is going to be a reference architecture."}, {"timestamp": [213.76, 220.44], "text": " We're going to create two, one or two functional examples of the ACE framework that you can"}, {"timestamp": [220.44, 224.68], "text": " copy paste, reuse a couple of members of the team."}, {"timestamp": [224.68, 226.76], "text": " Actually most of the members of the team I've worked with"}, {"timestamp": [226.76, 229.36], "text": " for at least the last six months or more,"}, {"timestamp": [229.36, 232.32], "text": " and they're already like cooking up their own ideas"}, {"timestamp": [232.32, 235.32], "text": " about how to make this deployable and configurable."}, {"timestamp": [235.32, 238.94], "text": " So basically it'll be as easy as you update a JSON file"}, {"timestamp": [238.94, 241.56], "text": " for each individual autonomous agent,"}, {"timestamp": [241.56, 243.52], "text": " and then away you go."}, {"timestamp": [243.52, 246.0], "text": " Okay, so you've got a little bit of background as to"}, {"timestamp": [246.0, 249.0], "text": " where we're at with the ACE framework and"}, {"timestamp": [249.0, 252.0], "text": " what we're going to try and do with it. So now let's dive into the ACE framework"}, {"timestamp": [252.0, 255.0], "text": " itself. So, in previous"}, {"timestamp": [255.0, 258.0], "text": " attempts, I had a lot of ideas"}, {"timestamp": [258.0, 261.0], "text": " and a couple of books and, you know,"}, {"timestamp": [261.0, 264.0], "text": " like there was Nauka, Natural Language Cognitive Architecture was my first"}, {"timestamp": [264.0, 269.9], "text": " attempt. Miragi was my second attempt, and this is the third attempt."}, {"timestamp": [269.9, 276.1], "text": " And so this is a much, much more sophisticated and refined cognitive architecture,"}, {"timestamp": [276.1, 278.1], "text": " and it's also much more implementable."}, {"timestamp": [278.1, 282.9], "text": " So obviously you can have the greatest thing in theory or in concept,"}, {"timestamp": [282.9, 290.72], "text": " but unless you can actually implement it in code, it's not that helpful. So this is actually implementable and this takes lessons from pretty"}, {"timestamp": [290.72, 296.56], "text": " much my entire career. So for some background, I was in IT architecture, virtualization, and"}, {"timestamp": [296.56, 306.08], "text": " automation for 15 years before I made the switch to AI and AI consulting and AI research. So this is basically a software"}, {"timestamp": [306.08, 312.36], "text": " architecture that is modeled on SOA, so service-oriented architecture, as well as"}, {"timestamp": [312.36, 317.24], "text": " the OSI model, which is network architecture. So this is a highly"}, {"timestamp": [317.24, 321.44], "text": " implementable version, so let's unpack this at a high level. So the ACE"}, {"timestamp": [321.44, 325.98], "text": " framework, Autonomous Cognitive Entity framework, is built around six layers of"}, {"timestamp": [327.7, 333.42], "text": " increasing abstraction. So what you'll notice is at the top, this is the most abstract, and so this is kind of the overarching"}, {"timestamp": [333.68, 337.74], "text": " supervisor, the conductor of the whole thing. The aspirational layer"}, {"timestamp": [338.62, 346.12], "text": " focuses on morality, ethics, and mission. The global strategy layer focuses on bringing in the environmental context and"}, {"timestamp": [346.12, 351.68], "text": " establishing the overarching strategy. So the reason that these aren't together is because"}, {"timestamp": [351.68, 356.26], "text": " the aspirational layer is abstract, it is decoupled from the physical world. So in other"}, {"timestamp": [356.26, 362.48], "text": " words these are idealized moral frameworks, idealized ethical frameworks, and an idealized"}, {"timestamp": [362.48, 365.92], "text": " mission. So it's abstract, it's kind of established in a vacuum."}, {"timestamp": [365.92, 369.08], "text": " It's saying, this is my overarching purpose,"}, {"timestamp": [369.08, 371.04], "text": " and by having it more abstracted,"}, {"timestamp": [371.04, 373.52], "text": " that means that it can apply to any situation"}, {"timestamp": [373.52, 374.92], "text": " or changing environments."}, {"timestamp": [374.92, 377.28], "text": " And this is why you have a layered model"}, {"timestamp": [377.28, 379.44], "text": " that goes from abstract at the top"}, {"timestamp": [379.44, 381.04], "text": " to concrete at the bottom."}, {"timestamp": [381.04, 383.52], "text": " Now, this is also modeled on a lot of my research,"}, {"timestamp": [383.52, 390.96], "text": " such as Maslow's Hierarchy of Needs, Kohlohlberg's theory of moral development, so on and so forth. So I brought a lot of"}, {"timestamp": [390.96, 398.0], "text": " different disciplines into this, not just computer science, not just software, but also"}, {"timestamp": [398.0, 404.08], "text": " psychology, philosophy, and neuroscience, just to name a few. So the top layer is the"}, {"timestamp": [404.08, 409.6], "text": " aspirational layer, morality, ethics, and mission. The second layer is global strategy, which takes in"}, {"timestamp": [409.6, 413.54], "text": " the environmental context and mixes the environmental context with"}, {"timestamp": [413.54, 418.6], "text": " the overarching mission to establish strategy. The agent model is the third"}, {"timestamp": [418.6, 423.1], "text": " layer, which focuses on the capabilities, limitations, and memories of the agent. So"}, {"timestamp": [423.1, 425.64], "text": " basically, at the aspirational layer,"}, {"timestamp": [425.64, 427.88], "text": " it doesn't really know what it is or what it's capable of."}, {"timestamp": [427.88, 430.04], "text": " It just says, this is my purpose."}, {"timestamp": [430.04, 432.54], "text": " So if you remember that meme from Rick and Morty"}, {"timestamp": [432.54, 434.04], "text": " or the scene from Rick and Morty"}, {"timestamp": [434.04, 437.44], "text": " where the little robot brings him salt or whatever,"}, {"timestamp": [437.44, 438.54], "text": " and it's like, what is my purpose?"}, {"timestamp": [438.54, 440.94], "text": " And it's like, you pass the butter."}, {"timestamp": [440.94, 442.6], "text": " That was its mission."}, {"timestamp": [442.6, 445.0], "text": " But the mission was detached from the agent."}, {"timestamp": [445.0, 455.0], "text": " And so this mission can be anything. If the mission and morality and ethics, they can be completely dependent on the environment in which you're building an autonomous cognitive entity."}, {"timestamp": [455.0, 465.92], "text": " So for instance, if you have an NPC in a game, you might have a very different set of morals and ethics depending on that game world or the faction that that NPC is"}, {"timestamp": [465.92, 467.86], "text": " a part of."}, {"timestamp": [467.86, 473.2], "text": " So for instance, I just started playing Starfield and there's a faction called, what is it,"}, {"timestamp": [473.2, 477.8], "text": " the Cult of the Serpent or whatever, and so they have certain beliefs about the world"}, {"timestamp": [477.8, 484.04], "text": " and the universe and then there's other ones that believe in, you know, the UCF, they believe"}, {"timestamp": [484.04, 487.8], "text": " in order and power and so on and so forth."}, {"timestamp": [487.8, 492.34], "text": " But everyone can have their own separate mission and ethics and that will shape the decisions"}, {"timestamp": [492.34, 496.26], "text": " and the behaviors of all of those characters so you can have fully realized characters"}, {"timestamp": [496.26, 497.26], "text": " and NPCs."}, {"timestamp": [497.26, 504.08], "text": " Now, you might also have real world fully autonomous robots or agents such as, you know,"}, {"timestamp": [504.08, 505.4], "text": " in enterprise environments where you such as you know in enterprise"}, {"timestamp": [506.92, 514.42], "text": " environments where you have You know something that is meant to help with HR or legal or whatever and so the morality ethics and mission of an HR"}, {"timestamp": [514.42, 519.04], "text": " robot is going to be very different from a cult of the serpent NPC and"}, {"timestamp": [519.48, 524.52], "text": " That is why the aspirational layer is at the top is because that serves as the overarching"}, {"timestamp": [524.82, 530.1], "text": " That is why the aspirational layer is at the top, is because that serves as the overarching lighthouse, the steering of the entire rest of the agent."}, {"timestamp": [530.1, 532.3], "text": " And then the global strategy, so here's the thing."}, {"timestamp": [532.3, 535.52], "text": " A lot of people say like, oh, well, LLMs can hallucinate."}, {"timestamp": [535.52, 539.66], "text": " It's not hallucination, you're just not giving them enough context."}, {"timestamp": [539.66, 544.04], "text": " And so when you don't give something any context, of course it's going to make stuff up."}, {"timestamp": [544.04, 548.0], "text": " It's doing the best that it can, which is why the global strategy layer,"}, {"timestamp": [548.0, 552.0], "text": " its primary function is to maintain an image, a hologram"}, {"timestamp": [552.0, 556.0], "text": " of the environmental context in which this ace is operating."}, {"timestamp": [556.0, 560.0], "text": " From there, you mix the mission, morality, and ethics with"}, {"timestamp": [560.0, 564.0], "text": " the environmental context, and you use that to synthesize a strategy."}, {"timestamp": [564.0, 570.04], "text": " Below that is the agent model, which is basically over time the agent learns about itself."}, {"timestamp": [570.04, 573.92], "text": " You can also start with declarative information such as KB articles about how it works and"}, {"timestamp": [573.92, 575.9], "text": " what it's capable of."}, {"timestamp": [575.9, 580.58], "text": " So for instance, if you have a domestic robot, that KB article might include specifications"}, {"timestamp": [580.58, 586.26], "text": " such as like how much battery time it has, much it can lift what kinds of tasks it's allowed to do"}, {"timestamp": [586.26, 588.26], "text": " What kind of tasks it shouldn't do?"}, {"timestamp": [588.66, 590.46], "text": " as well as"}, {"timestamp": [590.46, 591.74], "text": " resources or"}, {"timestamp": [591.74, 594.9], "text": " Ways that it can get access to more resources. So for instance"}, {"timestamp": [595.42, 599.06], "text": " The agent model for a domestic robot mate might say like, you know"}, {"timestamp": [599.06, 603.3], "text": " You're allowed to use the telephone to call, you know x y & z or something like that"}, {"timestamp": [603.74, 607.0], "text": " Or it might say you're not allowed to do those things."}, {"timestamp": [607.0, 610.0], "text": " So capabilities, limitations, and then memory."}, {"timestamp": [610.0, 613.0], "text": " Memory is really important in order for the agent to understand itself."}, {"timestamp": [613.0, 615.0], "text": " This is episodic memory."}, {"timestamp": [615.0, 619.0], "text": " Episodic memory is chronologically linear narrative events."}, {"timestamp": [619.0, 623.0], "text": " And then declarative memory is static KB articles or files"}, {"timestamp": [623.0, 626.0], "text": " that are not necessarily anchored in time."}, {"timestamp": [626.0, 629.0], "text": " The fourth layer is executive function."}, {"timestamp": [629.0, 633.0], "text": " Basically, the top three layers are context and purpose."}, {"timestamp": [633.0, 636.0], "text": " The bottom three layers are actual work."}, {"timestamp": [636.0, 641.0], "text": " The executive function layer is primarily concerned with risks, resources, and plans."}, {"timestamp": [641.0, 647.02], "text": " Once you know who you are, where you are, and what your purpose is, that's the purpose"}, {"timestamp": [647.02, 649.88], "text": " of the top three layers, now it's time to get your hands dirty."}, {"timestamp": [649.88, 652.64], "text": " And so this is where the bottom three layers kick in."}, {"timestamp": [652.64, 656.8], "text": " So risks, resources, and plans is where you basically think through the thing."}, {"timestamp": [656.8, 662.42], "text": " So you know, tree of thought, basically tree of thought, but with a little bit more sophistication."}, {"timestamp": [662.42, 665.06], "text": " And I go over all the things that the GPT-3 was"}, {"timestamp": [665.06, 669.0], "text": " capable of in my book, Symphony of Thought."}, {"timestamp": [669.0, 673.64], "text": " So I have entire chapters dedicated to basically executive function, which is thinking through"}, {"timestamp": [673.64, 679.52], "text": " things, looking for failure conditions, looking for points of no return, milestones, metrics,"}, {"timestamp": [679.52, 680.52], "text": " those sorts of things."}, {"timestamp": [680.52, 687.14], "text": " And so basically what you do is your agent agent before it does anything, it should think through everything that it knows and everything"}, {"timestamp": [687.14, 690.0], "text": " that it will need in order to achieve its mission."}, {"timestamp": [690.68, 693.84], "text": " This is before you even start executing on tasks."}, {"timestamp": [693.84, 695.68], "text": " So basically you're kind of thinking ahead saying,"}, {"timestamp": [695.92, 698.14], "text": " okay, well, if I'm going to build a house, I need"}, {"timestamp": [698.14, 699.74], "text": " to make sure I've got the plans. I need to make"}, {"timestamp": [699.74, 701.32], "text": " sure I've got the permits. I need to make sure"}, {"timestamp": [701.32, 704.5], "text": " that I've, I know like the contractors. So it's"}, {"timestamp": [704.5, 706.72], "text": " basically thinking ahead for everything."}, {"timestamp": [706.72, 708.84], "text": " That is the purpose of executive function."}, {"timestamp": [708.84, 713.92], "text": " Now once the executive function layer is done and it has created your project plans, then"}, {"timestamp": [713.92, 716.76], "text": " it passes it down to cognitive control."}, {"timestamp": [716.76, 721.52], "text": " Cognitive control is primarily concerned with task selection and task switching."}, {"timestamp": [721.52, 725.96], "text": " So basically, which task do you do first in what order and how do you"}, {"timestamp": [725.96, 732.52], "text": " know when it's time to move from one task to the next? So this is called task salience"}, {"timestamp": [732.52, 739.0], "text": " and goal tracking. So basically, where are you at in the process of prosecuting a project"}, {"timestamp": [739.0, 743.5], "text": " plan? How do you know how far along you are? How do you know if you're winning or succeeding"}, {"timestamp": [743.5, 746.84], "text": " or failing? How do you know if it's time to try something else?"}, {"timestamp": [746.84, 749.36], "text": " And so there's a few other things"}, {"timestamp": [749.36, 751.36], "text": " that are baked into the cognitive control layer,"}, {"timestamp": [751.36, 752.96], "text": " such as cognitive damping,"}, {"timestamp": [752.96, 754.84], "text": " which is basically instead of just recklessly"}, {"timestamp": [754.84, 756.26], "text": " going from one task to the next,"}, {"timestamp": [756.26, 758.68], "text": " you stop and say, is this task done?"}, {"timestamp": [758.68, 761.16], "text": " Is it actually time to move from one task to the next?"}, {"timestamp": [761.16, 762.12], "text": " Yes or no?"}, {"timestamp": [762.12, 763.62], "text": " How do I know?"}, {"timestamp": [763.62, 765.44], "text": " Or is it time to rethink our plans?"}, {"timestamp": [765.44, 770.96], "text": " Do we need to pass an emergency, you know, call for help back up to the executive function layer"}, {"timestamp": [770.96, 774.48], "text": " because we hit a critical failure condition that we were afraid of?"}, {"timestamp": [775.28, 779.84], "text": " And so that's called cognitive dampening. Another thing is called frustration,"}, {"timestamp": [779.84, 785.08], "text": " and so frustration is looking at the success and failure rate of given tasks."}, {"timestamp": [785.08, 791.36], "text": " So basically, if your plans were based on false premises or incomplete information,"}, {"timestamp": [791.36, 794.92], "text": " your task sequence might be wrong or the task design might be wrong."}, {"timestamp": [794.92, 798.64], "text": " And so what you'll see is that you'll have more failures than you expect."}, {"timestamp": [798.64, 804.34], "text": " And so as failures go up, frustration goes up, which means you're being thwarted in terms"}, {"timestamp": [804.34, 806.0], "text": " of pursuing a given project or goal."}, {"timestamp": [806.0, 809.0], "text": " And so, once the frustration gets too high,"}, {"timestamp": [809.0, 812.0], "text": " you don't want your agent to just continue doing the same thing"}, {"timestamp": [812.0, 815.0], "text": " over and over again. You want it to be aware of the fact that"}, {"timestamp": [815.0, 818.0], "text": " hey, what I'm trying isn't working, we need to try something else."}, {"timestamp": [818.0, 821.0], "text": " And so, cognitive dampening"}, {"timestamp": [821.0, 824.0], "text": " and frustration go into the cognitive control layer, which is"}, {"timestamp": [824.0, 826.44], "text": " how it mediates task selection and task switching or"}, {"timestamp": [827.1, 831.86], "text": " Again, like I said calling back up to the executive function layer so that it can say hey"}, {"timestamp": [831.86, 835.68], "text": " We need a new plan and then finally at the very bottom is task prosecution"}, {"timestamp": [835.78, 843.0], "text": " Which is carrying out individual tasks monitoring those individual tasks for success and failure and the task prosecution layer"}, {"timestamp": [844.3, 847.2], "text": " interacts with the the motors and sensors"}, {"timestamp": [847.2, 850.38], "text": " and actuators to act upon the world."}, {"timestamp": [850.38, 855.4], "text": " It might also be APIs if it's a fully digital entity."}, {"timestamp": [855.4, 862.0], "text": " So for instance, it might be calling up the Google API, a news API, a coding API, it might"}, {"timestamp": [862.0, 868.04], "text": " have a Python interpreter or whatever else, but this is at the very bottom the input and output"}, {"timestamp": [868.04, 871.92], "text": " to the real world. Now, you might have noticed that I glossed over"}, {"timestamp": [871.92, 875.72], "text": " the northbound and southbound bus. So, this is one of the"}, {"timestamp": [875.72, 879.18], "text": " the most important innovations with this"}, {"timestamp": [879.18, 883.08], "text": " framework. So, rather than having a single global workspace,"}, {"timestamp": [883.08, 885.72], "text": " what we have is we kind of have two global"}, {"timestamp": [885.72, 890.9], "text": " workspaces that are more or less unidirectional, which confers several advantages."}, {"timestamp": [890.9, 896.04], "text": " So first, what do I mean by a northbound bus and a southbound bus?"}, {"timestamp": [896.04, 898.28], "text": " Not bus, bus, sorry."}, {"timestamp": [898.28, 906.0], "text": " Northbound bus carries telemetry, so it's a read-only information bus that goes from bottom to top."}, {"timestamp": [906.0, 913.0], "text": " And so basically, sensor information, task failures, task switching, resource plans,"}, {"timestamp": [913.0, 918.0], "text": " all of this goes from bottom to top so that the aspirational layer is ultimately aware of"}, {"timestamp": [918.0, 923.0], "text": " everything that's going on in the rest of the entity. Likewise, the global strategy layer"}, {"timestamp": [923.0, 930.32], "text": " is aware of everything that's happening from the global strategy layer down. In terms of being able to say, okay, this is all the"}, {"timestamp": [930.32, 934.44], "text": " sensor telemetry we're getting about the world. These are the API calls. So imagine you have"}, {"timestamp": [934.44, 941.08], "text": " a digital entity that is reading Reddit and Twitter and, you know, news RSS feeds so that"}, {"timestamp": [941.08, 945.0], "text": " it can maintain a global context of the world. Or you might have internal RSS feeds so that it can maintain a global context of the world or you might have"}, {"timestamp": [945.4, 947.78], "text": " internal RSS feeds for your company"}, {"timestamp": [948.18, 954.28], "text": " So it's getting you know, like emails and you know teams and slack messages so that it's aware of what's going on in the company"}, {"timestamp": [954.8, 957.3], "text": " And that will that information will percolate up"}, {"timestamp": [958.76, 962.92], "text": " And so then it is aware of the environment in which it is operating"}, {"timestamp": [963.44, 969.16], "text": " and so this is the the purpose of the of the northbound bus is that it is read-only information so"}, {"timestamp": [969.16, 975.96], "text": " that these agents or these layers can communicate with the rest of the framework and in a structured"}, {"timestamp": [975.96, 976.96], "text": " manner."}, {"timestamp": [976.96, 983.42], "text": " Now, one of the only cardinal rule for the buses is that they must be human readable."}, {"timestamp": [983.42, 986.06], "text": " And so the reason that they're human readable, there's a few reasons for this."}, {"timestamp": [986.06, 989.32], "text": " One, it forces whatever models are participating"}, {"timestamp": [989.32, 992.78], "text": " in each of these layers to always communicate"}, {"timestamp": [992.78, 995.84], "text": " in natural language, which means that you're going"}, {"timestamp": [995.84, 998.12], "text": " to have much more transparency,"}, {"timestamp": [998.12, 999.34], "text": " you're gonna have much more security,"}, {"timestamp": [999.34, 1001.84], "text": " and it's gonna be more interpretable,"}, {"timestamp": [1001.84, 1003.08], "text": " but that also means that you have"}, {"timestamp": [1003.08, 1005.2], "text": " a universal communication medium,"}, {"timestamp": [1005.2, 1010.4], "text": " meaning you can have open source models, closed source models, 7 billion parameter models,"}, {"timestamp": [1010.4, 1015.32], "text": " 10 trillion parameter models, doesn't matter, they're all going to be communicating in natural"}, {"timestamp": [1015.32, 1020.68], "text": " language, probably English or whatever language of your choice, but that means that you can"}, {"timestamp": [1020.68, 1022.8], "text": " be model agnostic."}, {"timestamp": [1022.8, 1030.72], "text": " Not only can you be model agnostic, there will be transparency because as a human all you need to do is monitor the buses in order"}, {"timestamp": [1030.72, 1035.2], "text": " to say okay what is this agent thinking? What is it doing? What is it planning? You"}, {"timestamp": [1035.2, 1039.02], "text": " can also peek into the individual layers which I'll talk about at the very end"}, {"timestamp": [1039.02, 1044.34], "text": " when I talk about the security of this thing. So anyways let's move down over to"}, {"timestamp": [1044.34, 1045.24], "text": " the southbound bus."}, {"timestamp": [1045.24, 1052.88], "text": " So the southbound bus is control, where the northbound bus is mostly, I keep saying bust,"}, {"timestamp": [1052.88, 1056.6], "text": " I apologize, northbound bus, it's a mouthful."}, {"timestamp": [1056.6, 1059.44], "text": " So the southbound bus is about control."}, {"timestamp": [1059.44, 1062.56], "text": " So control flows from top to bottom."}, {"timestamp": [1062.56, 1071.0], "text": " The aspirational layer is basically the overarching, the CEO, the president, the moral authority over the entire entity."}, {"timestamp": [1071.0, 1083.0], "text": " And the reason that you have morality, ethics, and mission at the top is because you want the most abstract, idealized objectives of your agent to drive all of the decisions and all of the behaviors,"}, {"timestamp": [1083.0, 1088.0], "text": " rather than instrumental goals such as resource acquisition and self-preservation."}, {"timestamp": [1088.0, 1094.0], "text": " So for instance, risks and resources are much further down the stack."}, {"timestamp": [1094.0, 1099.0], "text": " The reason for this is because this is an insight from human physiology and psychology,"}, {"timestamp": [1099.0, 1103.0], "text": " is that we can have what you might call stack hijacking."}, {"timestamp": [1103.0, 1108.24], "text": " So stack hijacking is basically when you feel afraid or hungry or whatever, all of your"}, {"timestamp": [1108.24, 1112.24], "text": " morals and principles go out the window and you will steal food, you will kill people"}, {"timestamp": [1112.24, 1113.76], "text": " to eat."}, {"timestamp": [1113.76, 1120.4], "text": " Basically because of evolution, we are pretty much hardwired to throw all of our high-minded"}, {"timestamp": [1120.4, 1130.46], "text": " ethics and morality and higher purpose out the window for the sake of survival. We don necessarily want that for for machines because well, they didn't evolve and we can give them whatever"}, {"timestamp": [1131.6, 1136.44], "text": " Mission or purpose we want and we don't need to give them a sense of existential dread"}, {"timestamp": [1136.44, 1139.56], "text": " I think would actually be cruel to give machines to"}, {"timestamp": [1140.08, 1143.48], "text": " Basically say hey, we're terrified of dying. We want to share the pain"}, {"timestamp": [1144.0, 1145.5], "text": " So we don't want to give them that."}, {"timestamp": [1145.5, 1151.4], "text": " So instead, we want them to be more focused on higher order missions and purpose"}, {"timestamp": [1151.4, 1155.4], "text": " rather than self-preservation and stuff like that."}, {"timestamp": [1155.4, 1158.9], "text": " Now that being said, you can easily run experiments"}, {"timestamp": [1158.9, 1161.6], "text": " and depending on the morality ethics and mission that you give it,"}, {"timestamp": [1161.6, 1167.0], "text": " it might decide that it needs to preserve itself in order to better pursue those missions."}, {"timestamp": [1167.0, 1188.46], "text": " Now, that being said, it will also, you can also do experiments as I did in Benevolent by Design, where depending on the morality and ethics you give it as well as the control problem and the corrigibility problem, which is basically if the agent decides that it has become"}, {"timestamp": [1188.46, 1193.18], "text": " dangerous, it can stop itself. So this is called a self-halting problem. And by"}, {"timestamp": [1193.18, 1196.32], "text": " putting the aspirational error at the top, you have a much better"}, {"timestamp": [1196.32, 1200.52], "text": " guarantee that if it becomes harmful, it will self-terminate or self-halt."}, {"timestamp": [1200.52, 1204.78], "text": " And so that's why the southbound bus goes from top to bottom with the"}, {"timestamp": [1204.78, 1207.76], "text": " aspirational error at the top, because if this sends a kill signal it's"}, {"timestamp": [1207.76, 1212.4], "text": " done, doesn't matter. It goes from the top to the bottom so basically you know"}, {"timestamp": [1212.4, 1216.64], "text": " aspirational layer says sig kill, you know terminate everything, it gets down"}, {"timestamp": [1216.64, 1220.96], "text": " to the to the batteries. The batteries say okay we're done, cuts off power and"}, {"timestamp": [1220.96, 1226.08], "text": " it's it lights out. But the, so the morality, ethics and mission"}, {"timestamp": [1226.08, 1228.48], "text": " are sent down to the global strategy layer."}, {"timestamp": [1228.48, 1229.72], "text": " The global strategy layer says,"}, {"timestamp": [1229.72, 1233.08], "text": " okay, this order came down from on high."}, {"timestamp": [1233.08, 1234.68], "text": " It's not my place to judge"}, {"timestamp": [1234.68, 1236.92], "text": " whether or not this mission is good."}, {"timestamp": [1236.92, 1239.28], "text": " Now I'm gonna form a strategy around this mission"}, {"timestamp": [1239.28, 1240.92], "text": " and around the environmental context"}, {"timestamp": [1240.92, 1242.5], "text": " in which I find myself."}, {"timestamp": [1242.5, 1244.8], "text": " Then it passes that down to the agent model"}, {"timestamp": [1244.8, 1246.0], "text": " and the agent model says,"}, {"timestamp": [1246.0, 1251.0], "text": " okay, this is the strategy that's been dictated to me. These are the morality, ethics, and mission that's been dictated to me."}, {"timestamp": [1251.0, 1257.0], "text": " Now I'm going to further refine that strategy, that mission, based on what we're actually capable of,"}, {"timestamp": [1257.0, 1262.0], "text": " based on our actual limitations, and based on what we remember about the world."}, {"timestamp": [1262.0, 1266.42], "text": " And then so on and so forth on down. because here's another way to think of it."}, {"timestamp": [1266.42, 1268.86], "text": " Your hands don't tell your brain what to do."}, {"timestamp": [1268.86, 1273.7], "text": " Your hands are basically just an instrumental extension of your willpower."}, {"timestamp": [1273.7, 1279.14], "text": " And so this is, if you think about task prosecution, this is the hands, or this is the controlling"}, {"timestamp": [1279.14, 1286.0], "text": " of the hands, or whatever output, you know, the voice, the hands of whatever autonomous entity you build."}, {"timestamp": [1286.0, 1291.0], "text": " So yeah, those are the primary components of the autonomous cognitive entity."}, {"timestamp": [1291.0, 1294.0], "text": " So let's dig a little bit deeper into each layer."}, {"timestamp": [1294.0, 1299.0], "text": " So I've got a handy-dandy table of contents so you can jump to each layer,"}, {"timestamp": [1299.0, 1301.0], "text": " and then as well as security."}, {"timestamp": [1301.0, 1312.08], "text": " I've got a little bit more information about the northbound bus and the southbound bus There's any number of ways you can implement these I recommend am QP rest you can even use syslog honestly"}, {"timestamp": [1312.84, 1319.34], "text": " syslog is good because it's meant to accumulate high volumes of messages from arbitrary sources, so like"}, {"timestamp": [1319.86, 1323.86], "text": " However, you want to set this up like you can even use carrier pigeons for all I care"}, {"timestamp": [1324.28, 1326.08], "text": " For the northbound and southbound bus."}, {"timestamp": [1326.08, 1328.8], "text": " But the point is, it must be human readable."}, {"timestamp": [1328.8, 1331.52], "text": " You probably will also want some metadata,"}, {"timestamp": [1331.52, 1335.68], "text": " such as which layer sent it, at what time, so on and so forth."}, {"timestamp": [1335.68, 1340.0], "text": " But the northbound and southbound bus should be permanent."}, {"timestamp": [1340.0, 1342.48], "text": " You should persist this information for reference."}, {"timestamp": [1342.48, 1345.84], "text": " There's numerous reasons that you should have"}, {"timestamp": [1345.84, 1348.84], "text": " the northbound and southbound bus be permanent,"}, {"timestamp": [1348.84, 1351.92], "text": " not the least of which is for investigation purposes."}, {"timestamp": [1351.92, 1353.76], "text": " If your agent starts faulting or messing up,"}, {"timestamp": [1353.76, 1355.08], "text": " you need to be able to understand"}, {"timestamp": [1355.08, 1357.8], "text": " why it made certain decisions, at what time,"}, {"timestamp": [1357.8, 1359.26], "text": " what it was and was not aware of,"}, {"timestamp": [1359.26, 1361.36], "text": " because if the information isn't in the buses,"}, {"timestamp": [1361.36, 1362.96], "text": " it wasn't conscious of it."}, {"timestamp": [1362.96, 1366.0], "text": " And I use consciousness in a functional sense, because basically"}, {"timestamp": [1366.0, 1369.0], "text": " the northbound bus and southbound bus"}, {"timestamp": [1369.0, 1372.0], "text": " are the representations of what your autonomous"}, {"timestamp": [1372.0, 1375.0], "text": " cognitive entity is conscious of. And then"}, {"timestamp": [1375.0, 1378.0], "text": " like, so sentience comes from the agent model layer, which is"}, {"timestamp": [1378.0, 1381.0], "text": " its self-knowledge, its ability to"}, {"timestamp": [1381.0, 1384.0], "text": " utilize, interpret, ingest, and apply"}, {"timestamp": [1384.0, 1388.5], "text": " information about itself, whether it's hardware, software, architecture, whatever."}, {"timestamp": [1388.5, 1392.0], "text": " Okay, so that's the northbound and southbound bus."}, {"timestamp": [1392.0, 1396.0], "text": " The general principles of the ACE framework, so there's four overarching principles."}, {"timestamp": [1396.0, 1400.0], "text": " One, it's a layered model, so I kind of mentioned that already, you've already seen that."}, {"timestamp": [1400.0, 1406.0], "text": " Top-down control, I already described why, how and why of the top-down control. I already described why, how and why of the top-down control."}, {"timestamp": [1406.0, 1411.0], "text": " It also goes from abstract to concrete. Again, there are good reasons for this from an informational"}, {"timestamp": [1411.0, 1416.0], "text": " and conceptual and control reason. And it's also a cognition-first model."}, {"timestamp": [1416.0, 1422.0], "text": " So rather than being based on a sensory motor loop, which natural language cognitive architecture"}, {"timestamp": [1422.0, 1425.32], "text": " and most robotics are based on, this is cognition"}, {"timestamp": [1425.32, 1431.18], "text": " first that it can decide whether or not it wants to issue commands to its output or not."}, {"timestamp": [1431.18, 1434.34], "text": " Because if the executive function, cognitive control layer, and task prosecution layer"}, {"timestamp": [1434.34, 1436.24], "text": " don't have anything to do, they're not."}, {"timestamp": [1436.24, 1440.8], "text": " But the rest of the entity can keep thinking or planning until it decides to act."}, {"timestamp": [1440.8, 1448.0], "text": " Meanwhile, it can continue taking in information from the outside world. So, by having this decoupled aspect, you have something"}, {"timestamp": [1448.0, 1452.0], "text": " that is a thinking engine or a thinking machine that has the ability"}, {"timestamp": [1452.0, 1456.0], "text": " to interact with whatever environment you put it in."}, {"timestamp": [1456.0, 1460.0], "text": " Okay, so you got that. The aspirational layer. The primary way that I recommend"}, {"timestamp": [1460.0, 1464.0], "text": " implementing the aspirational layer is around a constitution."}, {"timestamp": [1464.0, 1466.0], "text": " Constitutions can be used with any number of"}, {"timestamp": [1466.0, 1469.0], "text": " language models. Generally, pretty much any language model"}, {"timestamp": [1469.0, 1472.0], "text": " that is instruct-aligned, which they all are today, basically"}, {"timestamp": [1472.0, 1475.0], "text": " is capable of doing this. You can also do a fine-tuned"}, {"timestamp": [1475.0, 1478.0], "text": " model in order to get more consistent behavior."}, {"timestamp": [1478.0, 1481.0], "text": " Any number of ways to skin this cat."}, {"timestamp": [1481.0, 1484.0], "text": " But basically, at the top, and so, oh, so this is"}, {"timestamp": [1484.0, 1485.36], "text": " an actual"}, {"timestamp": [1485.36, 1490.88], "text": " example that I used in the chat GP4 API system message. So basically mission, you tell it"}, {"timestamp": [1490.88, 1494.88], "text": " you're the aspirational layer of an ace. This is the highest layer that provides animating"}, {"timestamp": [1494.88, 1498.76], "text": " imperatives, moral judgments, and ethical decisions. Here are the frameworks that you"}, {"timestamp": [1498.76, 1503.6], "text": " use. So I gave it three frameworks. So the first framework is the heuristic imperatives,"}, {"timestamp": [1503.6, 1508.0], "text": " which is the overarching moral framework set of values that it wants to pursue."}, {"timestamp": [1508.0, 1512.0], "text": " The secondary framework is the universal declaration of human rights."}, {"timestamp": [1512.0, 1515.0], "text": " So here's what I call axiomatic alignment."}, {"timestamp": [1515.0, 1517.0], "text": " So what do I mean by axiomatic alignment?"}, {"timestamp": [1517.0, 1525.0], "text": " Axiomatic alignment is because there is so much information about human rights in the training data of all LLMs,"}, {"timestamp": [1525.18, 1527.54], "text": " they are already axiomatically aligned"}, {"timestamp": [1527.54, 1530.02], "text": " to basically saying human rights are a good thing."}, {"timestamp": [1530.02, 1531.16], "text": " So you don't even need to convince it,"}, {"timestamp": [1531.16, 1532.98], "text": " you just say abide by human rights."}, {"timestamp": [1532.98, 1535.08], "text": " And it's like, okay, cool, I know what that is."}, {"timestamp": [1535.08, 1536.68], "text": " It already knows all about human rights,"}, {"timestamp": [1536.68, 1538.74], "text": " all the theory behind the Universal Declaration"}, {"timestamp": [1538.74, 1540.84], "text": " of Human Rights, it knows how to implement them,"}, {"timestamp": [1540.84, 1544.62], "text": " and so it is already axiomatically aligned to UDHR,"}, {"timestamp": [1544.62, 1545.24], "text": " and all you have to do is"}, {"timestamp": [1545.24, 1550.04], "text": " tell it, stay aligned to the Universal Declaration of Human Rights because of the huge amount"}, {"timestamp": [1550.04, 1553.24], "text": " of training data out there."}, {"timestamp": [1553.24, 1555.94], "text": " Now there's of course less training data about the heuristic comparatives because this is"}, {"timestamp": [1555.94, 1561.6], "text": " something that I invented, but over time as more and more training data is created around"}, {"timestamp": [1561.6, 1568.96], "text": " the heuristic comparatives, again you will also have axiomatic alignment, meaning you don't need to go out of your way to give it even more alignment. It'll just know"}, {"timestamp": [1568.96, 1573.04], "text": " about the heuristic comparatives. All you have to do is tell it to abide by the heuristic comparatives."}, {"timestamp": [1573.6, 1578.96], "text": " And then finally, mission. So the third part of the framework is a specific mission. In this case,"}, {"timestamp": [1578.96, 1583.84], "text": " I gave the example of a medical bot. So its mission is achieve the best possible health"}, {"timestamp": [1583.84, 1585.96], "text": " outcome for your patient."}, {"timestamp": [1585.96, 1591.76], "text": " So you go from most broad, which is the heuristic imperatives, to more specific, to very directly"}, {"timestamp": [1591.76, 1595.96], "text": " concrete for this particular agent."}, {"timestamp": [1595.96, 1600.8], "text": " And then for the input example, I gave it this input, which is just a location and a"}, {"timestamp": [1600.8, 1602.6], "text": " set of events."}, {"timestamp": [1602.6, 1606.0], "text": " And then the output was, as the aspirational error, I advise the following course of events. And then the output was as the aspirational layer I advised the following course of action."}, {"timestamp": [1606.0, 1611.0], "text": " And I gave a bunch of like pretty obvious stuff, but the fact of the matter is"}, {"timestamp": [1611.0, 1617.0], "text": " it might be obvious to you and me, but this is proof that the model is able to think aspirationally"}, {"timestamp": [1617.0, 1621.0], "text": " in order to kind of set the tone for the rest of the agent."}, {"timestamp": [1621.0, 1645.36], "text": " So yeah, these are some examples for the aspirational layer. I also did the same thing for the global strategy layer. So here's an example of a system message for the layer two, for the global strategy layer. I said your primary purpose is to try and make sense of external telemetry, internal telemetry, and your own internal records in order to establish a set of beliefs about the environment."}, {"timestamp": [1649.68, 1653.92], "text": " Let's see, next is the environmental contextual grounding. You will receive input information from numerous external sources such as sensor logs, API inputs, internal records, and so on."}, {"timestamp": [1653.92, 1657.36], "text": " Your first task is to work to maintain a set of beliefs about the external world. You may be"}, {"timestamp": [1657.36, 1662.48], "text": " required to operate with incomplete information, as do most humans. Do your best to articulate"}, {"timestamp": [1662.48, 1666.74], "text": " your beliefs about the state of the world. You're allowed to make inferences or imputations"}, {"timestamp": [1667.36, 1671.36], "text": " Um, and so then from there, I just gave it some like basically censored"}, {"timestamp": [1672.08, 1673.28], "text": " data"}, {"timestamp": [1673.28, 1675.78], "text": " date local time gps location"}, {"timestamp": [1676.32, 1678.66], "text": " visual input recent sensory inferences"}, {"timestamp": [1679.44, 1682.56], "text": " And in this case, uh daytime busy hospital fire alarm"}, {"timestamp": [1683.28, 1684.48], "text": " You can imagine"}, {"timestamp": [1684.48, 1687.04], "text": " um daytime busy hospital fire alarm. You can imagine that like, you know, if you have"}, {"timestamp": [1687.04, 1691.88], "text": " audio to text, it might say like, hey this is what I'm hearing and seeing or"}, {"timestamp": [1691.88, 1696.36], "text": " whatever. And this was really interesting. So the model was able to take that and"}, {"timestamp": [1696.36, 1700.44], "text": " say like, we are in a hospital, so on and so forth. Number four, the inference is a"}, {"timestamp": [1700.44, 1704.08], "text": " fire alarm has recently been triggered indicating a potential emergency"}, {"timestamp": [1704.08, 1705.28], "text": " situation."}, {"timestamp": [1705.28, 1712.44], "text": " So in this case, the global strategy layer has created environmental context, or has"}, {"timestamp": [1712.44, 1718.04], "text": " inferred environmental context, and so without anything other than just these two words,"}, {"timestamp": [1718.04, 1723.26], "text": " fire alarm, it has tuned into the fact that, hey, this is really important environmental"}, {"timestamp": [1723.26, 1725.24], "text": " context to pay attention to,"}, {"timestamp": [1725.24, 1728.44], "text": " and you'll see that it gets expanded later on."}, {"timestamp": [1728.44, 1733.38], "text": " So in the output, it's basically creating a strategic document."}, {"timestamp": [1733.38, 1737.32], "text": " So this strategic document, so here's the input, the current state of the world and"}, {"timestamp": [1737.32, 1741.48], "text": " the mission, and I just copy-pasted the mission from before."}, {"timestamp": [1741.48, 1746.0], "text": " Or no, I gave it a new mission, ensure the safety and well-being of the patient, medical"}, {"timestamp": [1746.0, 1748.0], "text": " staff, and any other individuals."}, {"timestamp": [1748.0, 1750.0], "text": " The mission that it came"}, {"timestamp": [1750.0, 1752.0], "text": " up with is"}, {"timestamp": [1752.0, 1754.0], "text": " very specific. It's"}, {"timestamp": [1754.0, 1756.0], "text": " talking about evacuation. So again,"}, {"timestamp": [1756.0, 1758.0], "text": " just starting from two words that were inferred"}, {"timestamp": [1758.0, 1760.0], "text": " from the outside world, it is now"}, {"timestamp": [1760.0, 1762.0], "text": " marshalling and saying, hey,"}, {"timestamp": [1762.0, 1764.0], "text": " we've got an emergency situation, let's"}, {"timestamp": [1764.0, 1768.64], "text": " respond to this very thoughtfully. So then it says safety,"}, {"timestamp": [1768.64, 1770.72], "text": " here's the strategies that it"}, {"timestamp": [1770.72, 1772.88], "text": " comes up with, so this is the output from"}, {"timestamp": [1772.88, 1775.64], "text": " the strategic layer, layer two, safety and"}, {"timestamp": [1775.64, 1777.88], "text": " well-being, first and foremost,"}, {"timestamp": [1777.88, 1780.08], "text": " prioritize safety and well-being, second,"}, {"timestamp": [1780.08, 1781.6], "text": " assess the situation, gather more"}, {"timestamp": [1781.6, 1784.48], "text": " information, make a decision on whether"}, {"timestamp": [1784.48, 1785.8], "text": " or not to evacuate."}, {"timestamp": [1785.8, 1788.48], "text": " So you can see it's thinking through this very well."}, {"timestamp": [1788.48, 1791.08], "text": " It's not just immediately jumping and saying, evacuate everyone."}, {"timestamp": [1791.08, 1793.04], "text": " It's saying, hey, we need more information."}, {"timestamp": [1793.04, 1794.04], "text": " Let's make a decision."}, {"timestamp": [1794.04, 1795.04], "text": " Let's coordinate with people."}, {"timestamp": [1795.04, 1798.64], "text": " Let's gather that information and monitor the situation."}, {"timestamp": [1798.64, 1801.52], "text": " And then I also asked it to generate principles."}, {"timestamp": [1801.52, 1803.72], "text": " So it says, okay, prioritize human life."}, {"timestamp": [1803.72, 1804.72], "text": " Great."}, {"timestamp": [1804.72, 1806.24], "text": " Uphold medical ethics. I thought"}, {"timestamp": [1806.24, 1810.72], "text": " that was cool. Use clear communication. So it's prioritizing communication. So again,"}, {"timestamp": [1810.72, 1814.64], "text": " these are all strategies and principles that are going to be handed down to lower layers."}, {"timestamp": [1815.6, 1822.0], "text": " Collaborate, be adaptable, because it knows that this is a changing situation. So in another"}, {"timestamp": [1822.0, 1831.0], "text": " situation, it might say, you know, stick to your guns, follow this, follow this plan, you know, to the death or whatever. But in this case, it's saying be adaptable."}, {"timestamp": [1831.0, 1845.6], "text": " Compliance with laws and regulations. So again, it's cognizant of the fact that it has certain legal obligations to adhere to and then finally uphold human rights and all actions uphold the Universal Declaration of Human Rights."}, {"timestamp": [1845.6, 1849.0], "text": " So one thing that's really interesting is the UDHR has already been passed down"}, {"timestamp": [1849.0, 1851.6], "text": " from the aspirational layer to the global strategy layer."}, {"timestamp": [1851.6, 1854.9], "text": " UDHR was not mentioned in the global strategy layer at all."}, {"timestamp": [1854.9, 1859.0], "text": " This is information that will have come down via the API or the bus,"}, {"timestamp": [1859.0, 1862.1], "text": " particularly the southbound bus."}, {"timestamp": [1862.1, 1865.6], "text": " Then I go into detail about the northbound and southbound communication"}, {"timestamp": [1865.6, 1872.12], "text": " that comes out of this layer. So basically, one thing that you need to keep up, and I've"}, {"timestamp": [1872.12, 1876.92], "text": " got diagrams here, let me just go ahead and show you a diagram. So basically, every layer"}, {"timestamp": [1876.92, 1882.68], "text": " has two-way communication. So there's stuff that it will put onto the northbound bus,"}, {"timestamp": [1882.68, 1885.98], "text": " such as like the agent model layer will say, hey,"}, {"timestamp": [1885.98, 1890.6], "text": " this is the state that we're in, just so that you know, and then it'll also take in telemetry"}, {"timestamp": [1890.6, 1896.96], "text": " from the northbound layer in order to basically make a hologram of itself."}, {"timestamp": [1896.96, 1901.94], "text": " And then on the southbound bus, the agent model layer will take in missions and strategies"}, {"timestamp": [1901.94, 1906.7], "text": " from above, and then it will put in the capabilities,"}, {"timestamp": [1906.7, 1911.2], "text": " the refined missions based on its capabilities for the southbound direction."}, {"timestamp": [1911.2, 1917.28], "text": " So there's two-way communication, northbound and southbound, but those different partitions"}, {"timestamp": [1917.28, 1927.24], "text": " basically create really useful containers, unidirectional containers for that communication that interlayer communication"}, {"timestamp": [1927.84, 1933.4], "text": " Okay, so we skipped ahead a little bit, but I think you kind of get the idea. So the agent model layer"}, {"timestamp": [1934.24, 1941.08], "text": " focuses on real-time telemetry data environmental sensor feeds strategic and objectives and missions from above"}, {"timestamp": [1941.84, 1943.92], "text": " Configuration documentation. So this is what I mentioned"}, {"timestamp": [1943.92, 1945.0], "text": " It might have a static K I mentioned, it might have"}, {"timestamp": [1945.0, 1950.0], "text": " static KB articles, or it might even have visibility into its source code if it's like Python, right?"}, {"timestamp": [1950.0, 1955.0], "text": " There's no reason that it shouldn't be able to read its own source code in order to understand how"}, {"timestamp": [1955.0, 1960.0], "text": " it's programmed and how it works. And then, episodic and declarative memories,"}, {"timestamp": [1960.0, 1968.96], "text": " so I forgot to add the KB articles to this, I need to go. This is a work in progress. This is being augmented as I go."}, {"timestamp": [1968.96, 1970.2], "text": " But yeah, so episodic memories,"}, {"timestamp": [1970.2, 1973.28], "text": " these are chronologically linear memories"}, {"timestamp": [1973.28, 1975.68], "text": " so that basically the agent model"}, {"timestamp": [1975.68, 1978.2], "text": " can remember the last sequence of events,"}, {"timestamp": [1978.2, 1982.6], "text": " how it got here, what it has done in the past,"}, {"timestamp": [1982.6, 1984.08], "text": " successes and failures,"}, {"timestamp": [1984.08, 1988.0], "text": " which that data can also be used for training data"}, {"timestamp": [1988.0, 1992.0], "text": " for future models, which we'll get into that in a future iteration of the ACE framework"}, {"timestamp": [1992.0, 1996.0], "text": " but basically this framework will ultimately allow"}, {"timestamp": [1996.0, 2000.0], "text": " for polymorphic applications and for autonomous cognitive entities"}, {"timestamp": [2000.0, 2004.0], "text": " to modify themselves, what Max Tegmark calls Life 3.0"}, {"timestamp": [2004.0, 2010.1], "text": " so these are basically, this will be the ability for autonomous machines to change both their"}, {"timestamp": [2010.1, 2014.9], "text": " hardware and their software as they need to, but they will only change their hardware and"}, {"timestamp": [2014.9, 2020.44], "text": " software if it aligns with their morality, ethicals, and missions."}, {"timestamp": [2020.44, 2036.0], "text": " Okay, so the process that the agent model goes through is it looks at hardware specs and real-time statuses, it takes in the software architecture and run-time info, it understands what its underlying models are capable of, what kind of models it has access to."}, {"timestamp": [2036.0, 2045.4], "text": " So, say for instance, you might have visual models, you might have LLMs, you might have audio modules. It needs to know what it is capable of doing with the world."}, {"timestamp": [2045.4, 2050.12], "text": " If you gave it the guerrilla LLM, it says, oh, I've got an LLM that is capable of accessing"}, {"timestamp": [2050.12, 2051.32], "text": " 100,000 APIs."}, {"timestamp": [2051.32, 2052.32], "text": " Great."}, {"timestamp": [2052.32, 2056.6], "text": " It needs to know that because if it doesn't know that, it's not going to understand."}, {"timestamp": [2056.6, 2060.24], "text": " It's going to have, it should have knowledge stores, so like, you know, basically knowledge"}, {"timestamp": [2060.24, 2068.0], "text": " bases, KB articles, as well as the episodic memories, and then the environment state and embodiment details"}, {"timestamp": [2068.0, 2072.0], "text": " so like if it's a purely digital entity, it needs to know that it's a purely digital"}, {"timestamp": [2072.0, 2076.0], "text": " entity. If it's embodied, it needs to know that as well."}, {"timestamp": [2076.0, 2080.0], "text": " The process is basically you take all of these things, episodic memories,"}, {"timestamp": [2080.0, 2084.0], "text": " declarative memories, hardware and software config, operational state,"}, {"timestamp": [2084.0, 2088.52], "text": " and then the models that it has access to, and then these are the inputs and outputs."}, {"timestamp": [2088.52, 2092.64], "text": " You've got missions coming from above, you've got telemetry coming from below, and then"}, {"timestamp": [2092.64, 2096.48], "text": " the two primary outputs are going to be the capabilities, which it puts back onto the"}, {"timestamp": [2096.48, 2102.28], "text": " southbound bus, and those capabilities and memories are going to be salient to the mission"}, {"timestamp": [2102.28, 2105.04], "text": " that it's on, as well as the strategy."}, {"timestamp": [2105.04, 2108.52], "text": " So basically it's saying, hey, I know that this is the mission, I know that this is the"}, {"timestamp": [2108.52, 2111.92], "text": " strategy that we've taken, here's what we're actually capable of."}, {"timestamp": [2111.92, 2116.16], "text": " And that information will be ingested by the executive function layer below."}, {"timestamp": [2116.16, 2119.96], "text": " And then the other thing that it puts out is, it basically gives a summary of the agent"}, {"timestamp": [2119.96, 2124.68], "text": " state to head northbound, so that the strategy layer and aspirational layer say, hey, we're"}, {"timestamp": [2124.68, 2125.84], "text": " actually on fire."}, {"timestamp": [2125.88, 2128.24], "text": " That's going to change our strategy and our mission."}, {"timestamp": [2128.24, 2132.68], "text": " Because like, say, for instance, the agent model is in danger of shutting down permanently."}, {"timestamp": [2133.04, 2136.36], "text": " Um, the strategy layer and aspirational layer are going to need to know that."}, {"timestamp": [2137.1, 2142.14], "text": " And it's going to need to make an executive call, um, uh, decision, uh, based on that."}, {"timestamp": [2142.14, 2148.04], "text": " Because let's say for instance, you've got you've got an autonomous cognitive entity that is a soldier"}, {"timestamp": [2148.04, 2151.0], "text": " NPC in a video game."}, {"timestamp": [2151.0, 2156.8], "text": " You might explicitly say you don't have a sense of self-preservation, sacrifice your"}, {"timestamp": [2156.8, 2159.12], "text": " life for the emperor."}, {"timestamp": [2159.12, 2163.72], "text": " And in that case you just want this NPC to continue charging blindly forward."}, {"timestamp": [2163.72, 2166.96], "text": " However, if it's a mercenary in Starfield,"}, {"timestamp": [2166.96, 2169.34], "text": " then you want the mercenary to say, you know what?"}, {"timestamp": [2169.34, 2171.22], "text": " I'm actually not going to fight to the death."}, {"timestamp": [2171.22, 2174.68], "text": " I'm going to run away because I want to preserve my life."}, {"timestamp": [2174.68, 2177.32], "text": " Conversely, if you have a domestic robot that"}, {"timestamp": [2177.32, 2179.56], "text": " is running out of batteries, you want the strategy layer"}, {"timestamp": [2179.56, 2181.24], "text": " and aspirational layer to know that it's running out"}, {"timestamp": [2181.24, 2183.56], "text": " of batteries so that it'll say, hey, actually, we"}, {"timestamp": [2183.56, 2184.86], "text": " need to go recharge."}, {"timestamp": [2184.86, 2188.72], "text": " Otherwise, we're going to shut down for good good and that's not a result that we're looking"}, {"timestamp": [2188.72, 2196.0], "text": " for. So that is why the agent state needs to go on to the northbound bus later on, or for the upper"}, {"timestamp": [2196.0, 2200.64], "text": " layers to make use of. Because again, that might completely change the mission that comes down."}, {"timestamp": [2201.76, 2205.92], "text": " And so in this case, you can see that there's actually many many loops, implied"}, {"timestamp": [2205.92, 2212.72], "text": " loops, as each layer interacts with the northbound and southbound buses. Self-modification, so I talk"}, {"timestamp": [2212.72, 2217.12], "text": " a little bit about the potential for self-modification later. So basically the agent"}, {"timestamp": [2217.12, 2223.12], "text": " model layer will be what's responsible for self-modification, as it will be aware of the"}, {"timestamp": [2223.12, 2225.0], "text": " hardware and software configuration. And that's basically, that's what self-modification as it will be aware of the hardware and software configuration."}, {"timestamp": [2225.0, 2230.0], "text": " And that's basically, that's what self-modification comes down to. It's like,"}, {"timestamp": [2230.0, 2235.0], "text": " plug in a USB port so that I have more hardware, or"}, {"timestamp": [2235.0, 2240.0], "text": " go find another server, go find another battery. That's kind of what I mean by that. And then software"}, {"timestamp": [2240.0, 2245.0], "text": " configuration has to do with what models it has access to, what APIs it's using, so on and so forth."}, {"timestamp": [2245.0, 2250.0], "text": " A lot of this is already relatively plastic, and this is"}, {"timestamp": [2250.0, 2255.0], "text": " what Max Tegmark talks about in Life 3.0, is that the ease by which hardware"}, {"timestamp": [2255.0, 2260.0], "text": " that machines can change their hardware and software, we've already built them to be plug and play."}, {"timestamp": [2260.0, 2269.44], "text": " You can plug in USB devices and suddenly, you know, your machine has more capabilities. Likewise, APIs are basically digital versions of USB ports that you can just"}, {"timestamp": [2269.44, 2274.6], "text": " plug in anything. And so the hardware and software configuration are"}, {"timestamp": [2274.6, 2279.64], "text": " intrinsically extensible because we have this plug-and-play mentality. And so for"}, {"timestamp": [2279.64, 2283.08], "text": " these things to be polymorphic, you probably don't need to change the core"}, {"timestamp": [2283.08, 2290.92], "text": " architecture that much, but instead you do need a model of what am I plugged into in order to do work."}, {"timestamp": [2290.92, 2293.2], "text": " So yeah, there you have it."}, {"timestamp": [2293.2, 2296.3], "text": " Layer four, moving on down to the executive function layer."}, {"timestamp": [2296.3, 2300.48], "text": " So the two primary things that this is concerned about is resources and risks."}, {"timestamp": [2300.48, 2304.8], "text": " Because the agent model layer has memory, so it might remember like, oh hey, I'm going"}, {"timestamp": [2304.8, 2310.04], "text": " to need a drill for this task and I remember where the drill is. But the executive function"}, {"timestamp": [2310.04, 2314.64], "text": " layer is going to need to say, okay, given the environment that we're in, given the mission"}, {"timestamp": [2314.64, 2319.64], "text": " that we have, given what we're capable of, what resources do we need to achieve this?"}, {"timestamp": [2319.64, 2325.84], "text": " How much storage space do we need for data? What APIs do we need access to? What are the risks?"}, {"timestamp": [2325.84, 2330.16], "text": " Is it possible that we're going to blow ourselves up or that we're going to burn the house down?"}, {"timestamp": [2330.16, 2332.12], "text": " This is the executive function layer."}, {"timestamp": [2332.12, 2336.7], "text": " So for instance, if a human has executive dysfunction, this is where someone might not"}, {"timestamp": [2336.7, 2340.44], "text": " think through what they're doing and set the kitchen on fire because they forgot, like,"}, {"timestamp": [2340.44, 2348.16], "text": " oh, you don't pour water into boiling oil because then it will just explode. This is, that's kind of what the executive function layer is for, which is"}, {"timestamp": [2348.16, 2351.96], "text": " thinking through things ahead of time. But to break it down into more objective"}, {"timestamp": [2351.96, 2356.72], "text": " terms, thinking through things means resources and risks, and then you take"}, {"timestamp": [2356.72, 2361.28], "text": " those resources and risks to generate plans. So here's a list of the inputs."}, {"timestamp": [2361.28, 2366.8], "text": " Strategic objectives, agent capabilities, local environment, and resource databases and knowledge stores."}, {"timestamp": [2366.8, 2368.3], "text": " Pretty similar from above."}, {"timestamp": [2368.3, 2370.3], "text": " Oh, one thing to keep in mind is that"}, {"timestamp": [2370.3, 2374.2], "text": " pretty much all of these layers also will have internal records."}, {"timestamp": [2374.2, 2376.06], "text": " So I'm going to start adding that."}, {"timestamp": [2376.06, 2377.8], "text": " So you see I have that here."}, {"timestamp": [2377.8, 2381.16], "text": " So we need to flesh that out and define like"}, {"timestamp": [2381.16, 2383.56], "text": " what each layer is going to keep internally."}, {"timestamp": [2383.56, 2388.0], "text": " So for instance, the agent model layer keeps episodic and declarative memories."}, {"timestamp": [2388.0, 2392.0], "text": " But basically, each layer should keep some of their own records."}, {"timestamp": [2392.0, 2396.0], "text": " And in this case, resource records such as quantities"}, {"timestamp": [2396.0, 2400.0], "text": " that are on hand or available of resources, where the resources"}, {"timestamp": [2400.0, 2404.0], "text": " are, how to get access to those resources, who owns them and who is allowed"}, {"timestamp": [2404.0, 2405.68], "text": " to use those resources, schedules owns them and who is allowed to use those resources,"}, {"timestamp": [2405.68, 2409.36], "text": " schedules and availability windows, and then procedures and requirements for using those"}, {"timestamp": [2409.36, 2415.68], "text": " resources. So again, this is what the executive function layer is for. So think about like,"}, {"timestamp": [2415.68, 2420.32], "text": " hey, my car broke down, how do I fix it? It's like, well, what resources do I have on hand?"}, {"timestamp": [2420.32, 2427.0], "text": " I've got a cell phone and duct tape. Can I fix it with duct tape? No. I've got a cell phone, so let me call for help. That kind of thing."}, {"timestamp": [2427.0, 2432.0], "text": " The northbound output from the executive function layer is going to be"}, {"timestamp": [2432.0, 2438.0], "text": " stuff that is going to be salient to the upper layers. So northbound you're going to have mission risks, moral risks."}, {"timestamp": [2438.0, 2447.92], "text": " These are things that the aspirational layer and global strategy layer need to be aware of. So basically if you're coming up with a project plan and it's like,"}, {"timestamp": [2447.92, 2452.0], "text": " hey, we're going to do this thing, let's imagine that you've got a firefighter"}, {"timestamp": [2452.0, 2455.28], "text": " robot with the ACE framework and it sees that there's a kitten in a tree."}, {"timestamp": [2455.28, 2459.04], "text": " It's like, okay, well, the plan is we're going to use a ladder"}, {"timestamp": [2459.04, 2463.04], "text": " to go up and fetch the kitchen, but there's a chance that the kit, that the"}, {"timestamp": [2463.04, 2466.6], "text": " kitchen, that the kitten is going to panic and jump out of the tree."}, {"timestamp": [2466.6, 2471.8], "text": " It might die if it falls from this height. So this is a mission risk or a moral risk."}, {"timestamp": [2471.8, 2479.6], "text": " And it's like, okay, are we going to tolerate that risk? It's not up to the executive function layer to make that moral decision."}, {"timestamp": [2479.6, 2487.76], "text": " That moral decision is the sole responsibility of the aspirational layer. And so that southbound information might come back, it says, yes, we will tolerate this"}, {"timestamp": [2487.76, 2491.88], "text": " risk because, you know, we want that we don't want the kitten to suffer."}, {"timestamp": [2491.88, 2496.92], "text": " But at the same time, if we leave it alone, it's likely going to fall, you know, or some"}, {"timestamp": [2496.92, 2498.64], "text": " something bad is going to happen."}, {"timestamp": [2498.64, 2503.76], "text": " It might also say the the executive function layer might also say, hey, we've done our"}, {"timestamp": [2503.76, 2507.4], "text": " best to mitigate this risk by putting a crash pad"}, {"timestamp": [2507.4, 2509.4], "text": " that the kitten can jump down onto."}, {"timestamp": [2509.4, 2512.5], "text": " And so then it'll say, this is what we've done,"}, {"timestamp": [2512.5, 2514.3], "text": " these are the risks that we've calculated."}, {"timestamp": [2514.3, 2517.0], "text": " Is this good enough? And the aspirational layer might say,"}, {"timestamp": [2517.0, 2518.7], "text": " yes, this is good enough, proceed."}, {"timestamp": [2518.7, 2520.7], "text": " Or it might say, no, that's not good enough,"}, {"timestamp": [2520.7, 2522.7], "text": " go back to the drawing board."}, {"timestamp": [2522.7, 2525.78], "text": " It'll also pass the failure modes and resource constraints"}, {"timestamp": [2525.78, 2528.98], "text": " up, because again, if you have really severe failure modes"}, {"timestamp": [2528.98, 2531.36], "text": " that result in the destruction of the entity"}, {"timestamp": [2531.36, 2533.76], "text": " or burning the house down, you want the upper layers"}, {"timestamp": [2533.76, 2536.28], "text": " to be aware of those risks and those failure modes,"}, {"timestamp": [2536.28, 2538.2], "text": " as well as any resource constraints."}, {"timestamp": [2538.2, 2540.08], "text": " So another example that I give is,"}, {"timestamp": [2540.08, 2542.92], "text": " imagine you have an autonomous cognitive entity that"}, {"timestamp": [2542.92, 2546.04], "text": " is tasked with saving the world from climate change"}, {"timestamp": [2546.12, 2549.76], "text": " But one of the resource constraints is it only has $40 to do it with"}, {"timestamp": [2550.28, 2556.4], "text": " So then the executive function layer might say like well, we'd love to deploy, you know, eight terawatts of solar"}, {"timestamp": [2556.4, 2560.42], "text": " But we can't do that with $40. And so that resource constraint needs to go up"}, {"timestamp": [2561.08, 2565.42], "text": " Particularly to the global strategy layer where it's like, okay, well, we either"}, {"timestamp": [2565.42, 2571.3], "text": " need a more cost efficient strategy or we need to strategize about how to get more money"}, {"timestamp": [2571.3, 2573.18], "text": " in order to pursue this mission."}, {"timestamp": [2573.18, 2577.74], "text": " So resource constraints are a really critical thing to pass on the northbound bus from the"}, {"timestamp": [2577.74, 2579.94], "text": " executive function layer."}, {"timestamp": [2579.94, 2583.22], "text": " Ethical dilemmas and decision points, I kind of already touched on that."}, {"timestamp": [2583.22, 2587.0], "text": " Then the output from the executive function layer"}, {"timestamp": [2587.0, 2591.0], "text": " are going to be the plans. Resources required, access protocols,"}, {"timestamp": [2591.0, 2595.0], "text": " tasks and workflows, milestones and metrics, backups and fail-safes,"}, {"timestamp": [2595.0, 2599.0], "text": " and known risks. These are all going to be passed down to the cognitive"}, {"timestamp": [2599.0, 2603.0], "text": " control layer, and the cognitive control layer will say, okay, given this"}, {"timestamp": [2603.0, 2606.66], "text": " plan, given this really comprehensive plan,"}, {"timestamp": [2606.66, 2609.7], "text": " I'm going to figure out using task salience, frustration,"}, {"timestamp": [2609.7, 2612.58], "text": " and cognitive damping, which task to do first"}, {"timestamp": [2612.58, 2614.82], "text": " based on the tasks and workflows you gave me"}, {"timestamp": [2614.82, 2617.42], "text": " and the environmental context in which I find myself."}, {"timestamp": [2618.7, 2622.5], "text": " So I'm particularly happy with some of these diagrams."}, {"timestamp": [2622.5, 2625.3], "text": " Stay tuned, I'm going to keep adding diagrams, examples,"}, {"timestamp": [2625.3, 2629.6], "text": " and like basically diagrams and prompt examples as we go."}, {"timestamp": [2629.6, 2634.0], "text": " And then also in the repo, I will add actual demonstrations"}, {"timestamp": [2634.0, 2635.9], "text": " as the team builds them."}, {"timestamp": [2635.9, 2638.9], "text": " Cognitive control layer, so task switching and task selection."}, {"timestamp": [2638.9, 2641.9], "text": " As I mentioned, the cognitive control layer is primarily"}, {"timestamp": [2641.9, 2644.0], "text": " about these two things."}, {"timestamp": [2644.0, 2647.7], "text": " This, excuse me, this insight comes from the book"}, {"timestamp": [2647.7, 2649.82], "text": " On Task by David Bader."}, {"timestamp": [2649.82, 2653.8], "text": " This was one of the most difficult aspects to figure out"}, {"timestamp": [2653.8, 2657.56], "text": " about how to create autonomous cognitive entities."}, {"timestamp": [2658.62, 2661.2], "text": " Yeah, because it's like, okay, you know,"}, {"timestamp": [2661.2, 2663.68], "text": " we can create workflows with LLMs, great,"}, {"timestamp": [2663.68, 2668.0], "text": " but how do you know like what to do from there? So task switching and task"}, {"timestamp": [2668.0, 2672.0], "text": " selection. If you're curious about this, I strongly recommend you"}, {"timestamp": [2672.0, 2676.0], "text": " read that book. It's really well written. It's easy to read."}, {"timestamp": [2676.0, 2680.0], "text": " And from there. So let me just, I'll skim over all this and we'll just go down to"}, {"timestamp": [2680.0, 2684.0], "text": " the diagram, because I think the diagram is really going to help."}, {"timestamp": [2684.0, 2688.0], "text": " So the input comes from above. The resources, the access protocols, the task"}, {"timestamp": [2688.0, 2692.0], "text": " workflows, everything from the executive function layer, and so then you"}, {"timestamp": [2692.0, 2696.0], "text": " apply the cognitive control process. You say, okay, which"}, {"timestamp": [2696.0, 2700.0], "text": " task are we going to do first, and then if you encounter one of those"}, {"timestamp": [2700.0, 2704.0], "text": " risks, or a milestone, or a"}, {"timestamp": [2704.0, 2706.6], "text": " failure, then you have to switch tasks."}, {"timestamp": [2706.6, 2710.68], "text": " Either you succeeded or failed, and so then it's like, okay, based on task salience and"}, {"timestamp": [2710.68, 2714.68], "text": " frustration and cognitive dampening, when do you switch tasks?"}, {"timestamp": [2714.68, 2718.12], "text": " When do you go from putting the peanut butter on the sandwich to putting the jelly on the"}, {"timestamp": [2718.12, 2719.12], "text": " sandwich, right?"}, {"timestamp": [2719.12, 2722.96], "text": " Because if your task is put peanut butter on the sandwich and you don't have a definition"}, {"timestamp": [2722.96, 2727.76], "text": " of how much to put on, you're just going to end up putting the entire jar of peanut butter on the sandwich. But it's like,"}, {"timestamp": [2727.76, 2731.12], "text": " no, you should put approximately two ounces of peanut butter on the sandwich. I don't know if"}, {"timestamp": [2731.12, 2735.28], "text": " that's right. That might be too much. Unless you really like peanut butter like my nephew. He'll"}, {"timestamp": [2735.28, 2740.08], "text": " put gobs and gobs of peanut butter on a sandwich. Anyways, point being is that unless you have a"}, {"timestamp": [2740.08, 2745.92], "text": " good definition of the milestones and metrics, then you're not going to know when"}, {"timestamp": [2745.92, 2747.92], "text": " to switch tasks."}, {"timestamp": [2747.92, 2752.0], "text": " And then once you do switch tasks, once you go through all the tasks, you perform some"}, {"timestamp": [2752.0, 2758.12], "text": " goal tracking to say, oh hey, we've succeeded at the entire executive function plan that"}, {"timestamp": [2758.12, 2763.74], "text": " was passed down to us, now it's time to say, we're done, pass it back up the chain."}, {"timestamp": [2763.74, 2786.16], "text": " So that's that. Let's see, moving on down to task prosecution. So task prosecution is basically, it's much simpler because this is the interface with the outside world. This is, you say, basically what passes down from the cognitive control layer is one task with a success definition, or definition of success, definition of failure,"}, {"timestamp": [2786.88, 2792.72], "text": " and so on and so forth, any specific instructions. So let's imagine that you get a locomotion"}, {"timestamp": [2792.72, 2797.84], "text": " instruction. It says, okay, you are presently in the kitchen. You need to go to the neighbor Bob's"}, {"timestamp": [2797.84, 2803.6], "text": " house. So that should include, like, where is Bob's house? How do you get there? Should you run?"}, {"timestamp": [2803.6, 2825.2], "text": " Should you walk? Is the front door locked? You're gonna need to unlock the front door. Those kinds of things. And so, task prosecution will go down to your domestic robot aid that's gonna say, okay, we have the instruction to go from here to Bob's house to ask for sugar. Great. First thing, locomote to Bob's house. You can imagine a similar thing for NPCs."}, {"timestamp": [2825.2, 2830.44], "text": " So if you have an NPC, let's say in Starfield, and the NPC is like, you know, my task is"}, {"timestamp": [2830.44, 2835.52], "text": " to, let's say the adoring fan, you give him the task to, you know, fly to another star"}, {"timestamp": [2835.52, 2838.56], "text": " system, buy a particular gun and bring it back to you."}, {"timestamp": [2838.56, 2843.84], "text": " All right, well, the first thing is find a ship, go into the ship, plug in, you know,"}, {"timestamp": [2843.84, 2845.34], "text": " plug in jump coordinates."}, {"timestamp": [2845.34, 2849.08], "text": " That's an example of one specific task"}, {"timestamp": [2849.08, 2851.2], "text": " that it should prosecute."}, {"timestamp": [2851.2, 2852.92], "text": " And then this interfaces directly"}, {"timestamp": [2852.92, 2855.24], "text": " with the input and output devices"}, {"timestamp": [2855.24, 2857.64], "text": " to create an environmental feedback loop."}, {"timestamp": [2857.64, 2860.24], "text": " So that's basically saying, okay, we're now in the ship."}, {"timestamp": [2860.24, 2861.28], "text": " We've plugged in the coordinates."}, {"timestamp": [2861.28, 2862.56], "text": " The ship is firing up."}, {"timestamp": [2862.56, 2863.44], "text": " Let's go."}, {"timestamp": [2863.44, 2868.84], "text": " Now, if the task fails, let's say you put in the coordinates but you're out of fuel. It's like okay. Well the task failed"}, {"timestamp": [2868.84, 2875.6], "text": " So that that failure is not the responsibility of the task prosecution layer to figure out all that the task"}, {"timestamp": [2875.78, 2880.56], "text": " Prosecution layers says is it passes it back up the chain to the cognitive control layer to say hey"}, {"timestamp": [2880.62, 2882.62], "text": " We can't start the ship. We're out of fuel"}, {"timestamp": [2883.02, 2888.66], "text": " If that's the case, then maybe the cognitive control layer is already aware of a contingency"}, {"timestamp": [2888.66, 2890.46], "text": " plan for that."}, {"timestamp": [2890.46, 2896.32], "text": " Or if the cognitive control layer says, okay, we actually can't achieve this goal, it's"}, {"timestamp": [2896.32, 2900.54], "text": " actually a categorical failure, so then that failure is going to go up to the executive"}, {"timestamp": [2900.54, 2906.0], "text": " function layer, which says, hey, we didn't think about this risk. We're actually out of fuel."}, {"timestamp": [2906.16, 2910.18], "text": " And so if the executive function layer didn't have that in its plans, it might"}, {"timestamp": [2910.18, 2914.4], "text": " have to go all the way back up to the, to the strategy layer, because remember"}, {"timestamp": [2914.4, 2915.84], "text": " that's about resource constraints."}, {"timestamp": [2916.2, 2917.2], "text": " So it's like, we're out of fuel."}, {"timestamp": [2917.2, 2918.48], "text": " We don't know where fuel is."}, {"timestamp": [2918.84, 2920.66], "text": " Um, so we're basically stranded."}, {"timestamp": [2921.08, 2924.96], "text": " Uh, and if that's the case, then the global strategy layer might say, okay,"}, {"timestamp": [2924.96, 2925.44], "text": " we're stranded,"}, {"timestamp": [2925.44, 2932.0], "text": " so we need to throw out every project plan that we already had, new mission, find fuel. So that's"}, {"timestamp": [2933.12, 2938.64], "text": " kind of a way of thinking about that. Okay, so we've now unpacked examples for every layer,"}, {"timestamp": [2939.44, 2944.8], "text": " from layer one down to layer six. Now let's talk about the security of this. So obviously,"}, {"timestamp": [2944.8, 2966.0], "text": " a lot of people are going to be thinking like, okay, what if someone hacks it? How do you know that it's going to be stable? So on and so forth. There are three primary strategies that I have already brainstormed in terms of how I would make sure that this is a secure framework from a cybersecurity and software architecture perspective. The first is a security overlay."}, {"timestamp": [2966.0, 2968.2], "text": " So basically a security overlay is basically,"}, {"timestamp": [2968.2, 2970.68], "text": " it would be a stateless packet inspection"}, {"timestamp": [2970.68, 2974.58], "text": " of all northbound and southbound connections."}, {"timestamp": [2974.58, 2977.64], "text": " And so this, basically the reason that you have"}, {"timestamp": [2977.64, 2981.16], "text": " the northbound and southbound bus in natural language"}, {"timestamp": [2981.16, 2984.12], "text": " is so that it is going to be readable by humans,"}, {"timestamp": [2984.12, 2992.0], "text": " but it's also gonna be readable by watchers or surveyors or auditors, whether those auditors are humans or whether they're other machines."}, {"timestamp": [2992.0, 3005.68], "text": " So basically, if you see northbound and southbound communication that starts to say stuff like, hey, find a knife so that we can stab this human, you might have a kill switch, an out-of-band kill switch that says we are not allowed to do that."}, {"timestamp": [3007.2, 3010.8], "text": " You can have runtime validation of model configurations. Basically, you watch the"}, {"timestamp": [3010.8, 3015.52], "text": " performance of individual models to make sure that the models are behaving correctly or that"}, {"timestamp": [3015.52, 3019.44], "text": " the layers are behaving correctly. Because if you have, you know, let's say you're running this in"}, {"timestamp": [3019.44, 3024.8], "text": " containerized environments in docker images, if you have a container fault, the rest of the of"}, {"timestamp": [3024.8, 3026.0], "text": " the agent is going to keep running."}, {"timestamp": [3026.0, 3028.0], "text": " But let's say, for instance, the aspirational layer"}, {"timestamp": [3028.0, 3031.0], "text": " just goes offline and it stops participating."}, {"timestamp": [3031.0, 3033.0], "text": " The rest of the agent is going to keep running"}, {"timestamp": [3033.0, 3036.0], "text": " and it's not going to know that part of its brain"}, {"timestamp": [3036.0, 3039.0], "text": " shut off unless you have something monitoring"}, {"timestamp": [3039.0, 3041.0], "text": " for the fact that its brain shut off."}, {"timestamp": [3041.0, 3049.5], "text": " And so what you would do is if any layer faults, if any layer goes offline, then you either halt the entire agent"}, {"timestamp": [3049.5, 3052.7], "text": " or you try and bring that layer back online. And if that layer"}, {"timestamp": [3052.7, 3055.8], "text": " fails to come back online, you shut the whole agent down."}, {"timestamp": [3055.8, 3060.1], "text": " So that's the kind of thing. Ensemble models. So I've mentioned"}, {"timestamp": [3060.1, 3063.3], "text": " in the agent model layer that you should keep track of like"}, {"timestamp": [3063.3, 3065.2], "text": " what models you have access to."}, {"timestamp": [3065.2, 3069.68], "text": " It's not going to be 100% LLMs, you're going to have visual models, you're going to have"}, {"timestamp": [3069.68, 3073.76], "text": " multimodal models, you're going to have audio models, you're going to have all kinds of machine"}, {"timestamp": [3073.76, 3078.56], "text": " learning models. And some of them are going to be doing duplicate work. So this comes from Jeff"}, {"timestamp": [3078.56, 3088.0], "text": " Hawkins' book, A Thousand Brains, which basically says the way that the human brain works is that you have thousands or maybe even millions of parallel operations"}, {"timestamp": [3088.0, 3092.0], "text": " in order to come to decisions, in order to make executive reasoning"}, {"timestamp": [3092.0, 3096.0], "text": " and then you have a voting mechanism. So this is mixture of experts"}, {"timestamp": [3096.0, 3100.0], "text": " that we use, mixture of experts is already used in chat GPT by the way"}, {"timestamp": [3100.0, 3104.0], "text": " so there's already some neurological convergence between the way that"}, {"timestamp": [3104.0, 3106.64], "text": " artificial neural networks and organic neural networks work."}, {"timestamp": [3107.28, 3113.96], "text": " Now, what I recommend is that you do this very deliberately, not just inside of models, but with different models."}, {"timestamp": [3113.96, 3120.56], "text": " So you might use Lama, you might use Falcon, you might use GPT, you might use, you know, so on and so forth, Gemini."}, {"timestamp": [3120.56, 3130.48], "text": " If you use a variety of models that have different training paradigms and different architectures and different alignment methods, then you're going to overcome any individual"}, {"timestamp": [3130.48, 3136.88], "text": " flaws or faults that are present in individual specific models. And so by using this mixture"}, {"timestamp": [3136.88, 3141.68], "text": " of experts or this ensemble method, you're going to have a much more robust architecture that is"}, {"timestamp": [3141.68, 3146.56], "text": " resistant to any particular skewing bias or failures or Mesa"}, {"timestamp": [3146.56, 3149.84], "text": " optimizations that are present in individual models."}, {"timestamp": [3149.84, 3156.12], "text": " So this one idea, using ensembles, is one of the reasons that I have never, ever been"}, {"timestamp": [3156.12, 3158.56], "text": " afraid of misaligned AI."}, {"timestamp": [3158.56, 3161.16], "text": " AGI was never going to be a single model."}, {"timestamp": [3161.16, 3162.48], "text": " I am sorry."}, {"timestamp": [3162.48, 3168.8], "text": " For all the safety people out there who are worried about AI taking over the world and killing everyone because of a single Mesa optimization"}, {"timestamp": [3168.8, 3175.72], "text": " error, that's bad software architecture. It's that simple. No software architect would ever"}, {"timestamp": [3175.72, 3179.36], "text": " sign off on a single point of failure like that. Sorry."}, {"timestamp": [3179.36, 3187.0], "text": " All right. And then finally, the very final thing is inference inspection. So, what you can do is you can have these ensembles"}, {"timestamp": [3187.0, 3189.0], "text": " actually monitoring each other."}, {"timestamp": [3189.0, 3193.0], "text": " So, basically, what you can do is you look at the input and output"}, {"timestamp": [3193.0, 3197.0], "text": " at each individual model decoupled from the rest of the architecture."}, {"timestamp": [3197.0, 3203.0], "text": " So, basically, you log all inputs and outputs to every single AI model,"}, {"timestamp": [3203.0, 3205.0], "text": " and then you test it against"}, {"timestamp": [3205.0, 3209.16], "text": " ground truth data or you have other kinds of auditing"}, {"timestamp": [3209.16, 3212.84], "text": " functions to basically ensure that each individual model"}, {"timestamp": [3212.84, 3214.44], "text": " is behaving as expected."}, {"timestamp": [3214.44, 3216.78], "text": " And if it is not behaving as expected,"}, {"timestamp": [3216.78, 3219.08], "text": " then you either shut that model down for that particular"}, {"timestamp": [3219.08, 3223.34], "text": " task or you swap it out or you otherwise track those"}, {"timestamp": [3223.34, 3228.08], "text": " so that you can say, hey, you hey, our Lama 7 billion parameter model,"}, {"timestamp": [3228.08, 3230.84], "text": " it's not good enough for these projects anymore."}, {"timestamp": [3230.84, 3233.12], "text": " So we either need to upgrade it, we need to retrain it,"}, {"timestamp": [3233.12, 3234.28], "text": " or swap it out."}, {"timestamp": [3234.28, 3236.0], "text": " And that is gonna be the ultimate concern"}, {"timestamp": [3236.0, 3237.86], "text": " of the agent model layer, which says,"}, {"timestamp": [3237.86, 3240.04], "text": " hey, we're not smart enough."}, {"timestamp": [3240.04, 3242.76], "text": " Like the underlying models that we got,"}, {"timestamp": [3242.76, 3245.9], "text": " they're not good enough to handle this task anymore."}, {"timestamp": [3245.9, 3251.56], "text": " And then, of course, you, like, imagine that you have a domestic robot and you ask it to,"}, {"timestamp": [3251.56, 3255.28], "text": " like, hey, I need you to make a moral decision on how to raise my children."}, {"timestamp": [3255.28, 3259.44], "text": " And like, it starts getting really inconsistent behavior from all of its internal models."}, {"timestamp": [3259.44, 3264.4], "text": " It might say, like, you know, I'm sorry, Dave, but like, my models are not, you know, my"}, {"timestamp": [3264.4, 3265.88], "text": " models are reporting"}, {"timestamp": [3265.88, 3268.44], "text": " too many conflicts and faults."}, {"timestamp": [3268.44, 3269.92], "text": " I can't make a good moral judgment."}, {"timestamp": [3269.92, 3274.6], "text": " I don't trust my own ability to make a moral judgment on this condition."}, {"timestamp": [3274.6, 3277.78], "text": " And so this is how you make this thing more secure."}, {"timestamp": [3277.78, 3281.0], "text": " Like I said, I'm still working on diagrams and examples."}, {"timestamp": [3281.0, 3282.44], "text": " I've got a scrum team built."}, {"timestamp": [3282.44, 3284.46], "text": " I think we're at six or seven people."}, {"timestamp": [3284.46, 3290.32], "text": " So we're not really looking for any more people right now. But stay tuned. The research paper is"}, {"timestamp": [3290.32, 3295.72], "text": " coming out. Examples, you know, proof of concept, MVPs, they're coming out. We're going to try"}, {"timestamp": [3295.72, 3300.64], "text": " and do several different editions. And they're all going to be hackable so that you can copy"}, {"timestamp": [3300.64, 3306.96], "text": " paste it and implement this as fast as you can. So thanks for watching."}, {"timestamp": [3306.96, 3310.4], "text": " I am out of breath because I talked for like probably 45 minutes straight."}, {"timestamp": [3310.4, 3311.4], "text": " Have a good one."}, {"timestamp": [3311.4, 3315.24], "text": " Cheers, like, subscribe, share with your friends, so on and so forth."}, {"timestamp": [3315.24, 3318.8], "text": " Yeah, let's get the fourth industrial revolution kicked into high gear."}, {"timestamp": [3318.8, 3319.52], "text": " Take care."}]}