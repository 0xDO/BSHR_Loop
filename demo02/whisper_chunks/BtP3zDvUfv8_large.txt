{"text": " Feintuning und in-Context-Learning. Das sind zwei Dinge, die man verwenden kann, um ein Large-Language-Model so anzupassen, damit es den Output generiert, den wir gerne haben wollen. MemGPT will irgendwas dazwischen versuchen. Ich bin der Meinung, das funktioniert nicht wirklich gut. Am Ende des Videos dazu mehr. Wenn wir einen sehr, sehr spezifischen Output haben wollen, dann ist Feinting das Ding, das wir verwenden sollen. Das sehen wir uns in diesem Video an. Wenn wir ein Large Language Model trainieren wollen an sehr, sehr viel Daten, dann ist in Context Learning das Ding, das wir verwenden sollten. Auch das werden wir uns in diesem Video ansehen. In Context Learning kannst du dir so vorstellen, dass man ganz einfach nur spezifische Daten dem Large Language Model gibt und das Modell kann sich die Dinge danach dazudenken. Insgesamt gibt es wirklich nur eine Handvoll Methoden, die man verwenden kann. Wie gesagt, das Fine Tuning, das In Context Learning, man kann auch einen Chatbot machen und Vector Database dazwischen schalten. Das haben wir uns schon hier angesehen. Alle haben ihre Vor- und Nachteile. Fine Tuning ist schon relativ cool, wenn man einen sehr, sehr spezifischen Output haben will. Aber meiner Meinung nach das Ding, das am meisten \u00fcbersehen wird, ist das In-Context-Learning. In-Context-Learning ist besonders praktisch, weil es so einfach funktioniert. Man kann Chezhipiti ganz einfach Stichpunkte geben und er kann sich Dinge dazu denken, auch wenn er an diesen Dingen nicht trainiert wurde. Das ist wirklich enorm cool. Lasst uns erst mal ganz kurz den Artikel von OpeMeYi ansehen, warum man Fine-Ting eigentlich machen sollte? Im Groben haben sie hier drei Punkte gesagt. Improved Stability, Reliable Output Formatting und Custom Tone. Was soll das bedeuten? Bei Improved Stability wollen sie dir einfach nur sagen, dass du dem Modell immer sagen kannst, hey, du sollst so antworten oder du sollst meinetwegen immer Deutsch sprechen. Reliable Output Formatting will ganz einfach nur hei\u00dfen, dass du immer zum Beispiel JSON-Files bekommst, wenn du das denn haben willst. Und Custom Tone soll ganz einfach nur hei\u00dfen, dass du ganz einfach dem Modell mitteilen kannst. Hey, sei immer sarkastisch, sei immer witzig oder sonst was. Generell gibt es nat\u00fcrlich noch eine andere Methode au\u00dfer dem Fine-Tuning, was wir uns danach auch noch ansehen, aber das Fine-Tuning ist schon etwas, das wirklich relativ cool ist. Und in drei Schritten zeigen sie dir auch, wie du dein Fine-Tuning umsetzen kannst. Generell musst du nat\u00fcrlich deine Datasets richtig eingeben. Aber keine Sorge, wir verwenden ein Colab Notebook, damit wir nichts selbst programmieren m\u00fcssen. Also musst du das nicht mal unbedingt ganz genau wissen. Du musst ganz einfach deine Daten in das richtige Format bringen. Als n\u00e4chstes musst du deine Daten nat\u00fcrlich alle hochladen. Daf\u00fcr verwenden wir die OpenAI API. Wir m\u00fcssen danach nat\u00fcrlich unsere API Keys eintragen. Aber auch das machen wir alles in Google Colab. Und danach wird OpenAI die Magie machen. Sie werden ein Modell eigens f\u00fcr dich trainieren, von dem du danach deine Outputs bekommst. Wieder nat\u00fcrlich mit der API. Und wie gesagt, wir machen alles ganz, ganz einfach in einem Colab Notebook, damit wir nichts selbst programmieren m\u00fcssen. Und zum Schluss k\u00f6nnen wir nat\u00fcrlich unser eigenes Modell verwenden und das war's. Das hier ist das Colab Notebook. Es wurde zusammengesetzt von Matt Schumer. Hier kannst du ihn nat\u00fcrlich auch auf Twitter folgen. Und hier k\u00f6nnen wir sogar unsere Datasets machen. Das ist alles wirklich, wirklich einfach. Wir scrollen runter, klicken ganz einfach Play bei dem ersten Button und wir m\u00fcssen trotzdem ausf\u00fchren. Das Colab Notebook wird uns sogar unsere Datasets erstellen. Wir werden hier folgendes machen. Wir machen ein Modell, das ganz einfach den Custom Tone macht. Wir wollen einfach ein Modell haben, das witzig ist. Wir gehen wieder zur\u00fcck ins Colab Notebook und hier bei diesem Prompt m\u00fcssen wir ganz einfach sagen, was wir haben wollen. Zum Beispiel, a model that takes in a puzzle like bla bla blup und so weiter, response out in spanisch. Wir wollen das aber in deutsch haben und wir geben hier ganz einfach unseren Prompt ein. Ich habe hier den Prompt ganz einfach gemacht, dass wir ein Modell haben wollen, das sehr sarkastisch ist, mit viel Humor. Es soll Twitter Beitr\u00e4ge schreiben \u00fcber k\u00fcnstliche Intelligenz. Sie sollen sehr kurz sein und den Output wollen wir in Deutsch haben. Generell k\u00f6nntest du ein Modell nat\u00fcrlich genauso wie auf der Webseite trainieren und hier deine eigenen Dateien f\u00fcttern. Du k\u00f6nntest ganze PDFs hochladen, jedoch m\u00fcsste man das in Python machen. Das ist etwas aufwendiger. Ich zeige hier die einfachste Methode, um ganz einfach den Ton anzupassen. Und sp\u00e4ter im Video zeige ich noch eine coole weitere Methode, wie wir ohne Fine Tuning ein Modell dazu bringen k\u00f6nnen, die Outputs genauso zu bekommen. Wir k\u00f6nnen ein Modell auch an unseren Daten trainieren und richtig, richtig viele Token sparen und das alles ohne ein Vector Dataset. Das werden wir uns sp\u00e4ter nat\u00fcrlich noch genauer ansehen. Also wir haben hier unseren Prompt gesetzt, jetzt klicken wir hier Play und wir m\u00fcssen trotzdem ausf\u00fchren. Die Temperatur kannst du \u00fcbrigens so einstellen wie du magst. Wenn du etwas sehr kreatives haben willst, wie wir hier zum Beispiel, k\u00f6nnte man die auch hoch machen auf 0,6 und wenn du etwas sehr logisches machen willst, zum Beispiel Coder stellen, dann w\u00fcrde ich die Temperatur auf 0 machen. Number of examples 50, das hei\u00dft wir bekommen ganz einfach 50 Beispiele hier, um danach unser Modell zu trainieren. Wir k\u00f6nnen das auch runtersetzen auf 30, aber zumindest 20 sollten wir machen. Als n\u00e4chstes m\u00fcssen wir die Modelle installieren. Wir klicken ganz einfach Play bei dieser Zelle. Das dauert nur wenige Sekunden. Als n\u00e4chstes brauchen wir nat\u00fcrlich einen API Key. Das habe ich schon einige Male gemacht. Also Create Key. Wir geben mit dem einen Namen. Lasst es meinetwegen Tune sein und wir klicken Create Secret Key, den m\u00fcssen wir danach kopieren. Und wir kopieren das, keine Sorge, den Key l\u00f6sche ich danach ganz einfach wieder. Diesen Key m\u00fcssen wir jetzt genau hier reingeben und zwar bei Enter your API Key, hier. Jetzt haben wir also hier unseren Key eingegeben, wir klicken ganz einfach Play und jetzt werden unsere Datasets erstellt. Das dauert eine kleine Weile. Generell k\u00f6nnte man mit diesem Colab Notebook nat\u00fcrlich die verschiedensten Dinge machen, wie gesagt. Und wenn du ein Modell fine-tunen willst an deinen eigenen Daten, dann musst du das nat\u00fcrlich ein klein wenig anders machen. Dann musst du das tats\u00e4chlich in Python machen, aber wie gesagt, am Ende des Videos zeige ich eine Methode, wie man das noch einfacher, noch effizienter und noch g\u00fcnstiger machen kann, als mit dem Fine-Tuning. Mittlerweile haben wir hier all unsere, ich habe es doch 50 werden lassen, also alle unsere 50 Beispiele gemacht. Und wir klicken hier unten jetzt Play bei der n\u00e4chsten Zelle, womit wir unseren System Prompt erstellen. Klicken wir ganz einfach Play bei dieser Zelle. Hier siehst du, was unser System Prompt danach ist. Das Modell hat ganz einfach automatisch die Aufgabe bekommen, dass es kurze Threads schreiben wird. Und die Threads sollen alle sarkastisch sein und in Deutsch. Wir haben jetzt also all unsere Daten hier erstellt. Wir haben einen System Prompt erstellt und nun m\u00fcssen wir nat\u00fcrlich all unsere Daten wie hier oben in das richtige Format bringen, wie wir nat\u00fcrlich auch schon im Blogartikel gesehen haben. Du hast hier gesehen, dass unser Dataset danach so formatiert sein soll und das macht uns auch die n\u00e4chste Zelle automatisch. Wir klicken einfach hier und wir bekommen danach JSON-Dateien und das geht wirklich unglaublich schnell. Wir haben jetzt all unsere 50 Dinger so umgewandelt, dass sie f\u00fcr uns funktionieren. Der Code macht alles automatisch in diesem Notebook. Als n\u00e4chstes m\u00fcssen wir unsere Files hochladen auf OpenAI und wir klicken ganz einfach Play bei dieser Zelle. Das geht auch im Nullkomma nichts. Und der letzte Schritt ist nat\u00fcrlich unser Modell zu trainieren. Und um unser Modell zu trainieren m\u00fcssen wir einfach nur noch hier Play klicken bei dieser Zelle und das sollte funktionieren. Das dauert meistens in etwa so um die 20 Minuten, aber wir k\u00f6nnen uns nat\u00fcrlich einfach alles ansehen. Wenn du hier play klickst, dann bekommst du hier die Updates. Wir sind aktuell also beim ersten Schritt von 150 Schritten und wenn du hier einfach ab und zu play klickst, dann siehst du einfach wie weit das Ding ist. Wir sind jetzt immer noch beim ersten Schritt von 150. Das wird jetzt in etwa 20 Minuten dauern. Also brauchen wir hier ein klein wenig Geduld und wir sehen uns sobald das hier alles trainiert worden ist. Und du siehst es geht mittlerweile viel viel schneller. Wir sind jetzt schon bei 21 von 150. Wie gesagt, wir sehen uns sobald das fertig ist. Okay und jetzt haben wir das Ganze gemacht. Nach in etwa 20 Minuten bekommst du das hier. The job has successfully completed. Du wirst auch eine Mail bekommen. OpenAI schreibt dir ganz einfach eine Mail zu, dass das ganze mittlerweile ausgef\u00fchrt ist. Und jetzt hast du hier sofort dein eigenes Modell. Um das Modell zu speichern, klicken wir einfach nur noch hier Play. Und danach sollte das ganze funktionieren. Du siehst unser personalisiertes Model hat diesen witzigen Namen und hier unten k\u00f6nnen wir das Modell jetzt testen. Hier bei Content k\u00f6nnen wir jetzt eingeben, was wir wollen. Lass uns mal den Default Prompt versuchen und hier Play klicken. Und hier haben wir jetzt unseren sarkastischen Tweet. Oh ja, k\u00fcnstliche Intelligenz ist total kreativ in der Musik. Wer will schon von echten Musikern oder Musikerinnen inspiriert werden, wenn wir stattdessen von Maschinen generierte Hits h\u00f6ren k\u00f6nnen? Und diese einzigartigen Melodien, wer braucht schon menschliche Emotionen und Leidenschaft, wenn wir stattdessen Algorithmen haben, die uns den gleichen Einheitspreis servieren? Es ist doch viel besser, wenn die Musikindustrie von Maschinen kontrolliert wird. Hashtag Sarcasmus Overlord. Also du siehst, das Modell funktioniert sehr, sehr gut. So k\u00f6nnte man wirklich in nu Haufenweise Tweets machen. Das funktioniert wirklich, wirklich gut. Und so k\u00f6nnte man ein solches Modell fine tunen. Das Modell, das wir gerade trainiert haben, ist \u00fcbrigens auch im OpenAI Playground vorhanden. Daf\u00fcr gehst du hier einfach in den Playground rein und bei Model kannst du hier das ganz einfach ausw\u00e4hlen. Geh auf Fine Tunes und hier kannst du dein Modell ausw\u00e4hlen, das wir gerade in unserem Google Colab trainiert haben. Hier kannst du jetzt arbeiten, so wie mit JetCPT. Du kannst hier oben deinen Prompt eingeben und los geht's. Eventuell kannst du sogar noch deinen System Prompt ein klein wenig anpassen. Das ist wirklich unglaublich einfach. Was ist also der Vorteil von diesem Fine Tuning? Hier kannst du mit sehr sehr g\u00fcnstigen Modellen wie zum Beispiel GPT 3.5 Turbo, wie wir hier laufen haben, das Modell sehr spezifisch machen und das Modell ist jetzt in diesen spezifischen Task sogar besser als GPT 4. Und f\u00fcr GPT 4 musst du nat\u00fcrlich bezahlen. Entweder du brauchst das Abo oder sonst was. Generell ist Fine Tuning etwas, das sehr sehr gut funktionieren kann. auch wenn du ein Modell zum Beispiel an deinen Firmen Daten trainieren willst, funktioniert das relativ gut. Es gibt jedoch ein paar Nachteile vom Fine Tuning. Nat\u00fcrlich wenn du an sehr sehr gro\u00dfen Daten trainierst, an Business Daten zum Beispiel, dann ist das Fine Tuning relativ teuer und um ehrlich zu sein ist es auch nicht so angenehm zu verwenden. Jedoch sehen wir uns jetzt eine richtig, richtig coole Alternative zum Fine Tuning an. Meiner Meinung nach eignet sich das Fine Tuning am besten f\u00fcr sowas. Du verwendest einfach irgendetwas, wo du das Modell spezifisch reagieren lassen willst und du willst GPT 3.5 Turbo verwenden f\u00fcr den Output. Lasst uns einfach mal ansehen, wie wir Chats GPT manipulieren k\u00f6nnen, damit wir richtig, richtig gute Outputs bekommen. David Shapiro hat hier etwas Cooles auf GitHub vorbereitet. Er sagt uns hier, dass es ganz einfach vier Dinge gibt, wie man ein Modell verbessern kann. Entweder Initial Bulk Training, das ist nat\u00fcrlich unglaublich teuer. Fine Tuning, so wie wir es gerade gemacht haben, das hat den Nachteil, dass man hier eventuell nicht zu viel Wissen speichern kann. Du hast gesehen, wir haben ein Modell, das jedoch einen sehr, sehr spezifischen Task machen kann. Also wenn du einen spezifischen Task machen willst, dann ist Feintuning gut. Online Learning, ja, das kann eventuell kommen. Und eventuell noch in Context Learning. Und das ist aktuell das Coolste. Worum geht es hier genau? Lasst uns erstmal kurz dar\u00fcber sprechen, warum und wie das Ganze funktioniert und worum es \u00fcberhaupt geht. Large Language Models sind associative, das heisst, die k\u00f6nnen ganz einfach Dinge dazu denken. Wenn wir den Modellen einfach nur spezifische Stichw\u00f6rter geben, dann wissen sie, an welchen Textdaten sie suchen sollen und was sie in etwa dazu spinnen sollten. Wenn ich zum Beispiel dir ein einziges Wort gebe, meinetwegen Bizeps, dann denkst du sofort an Muskeln, an Fitnessstudio, an was wei\u00df ich nicht alles. Du hast ganz einfach in deinem Hirn gewisse Dinge, die du mit dem Wort Bizeps verkn\u00fcpft hast. Und \u00e4hnlich funktionieren Large Language Models. Das Ding nennt sich ganz einfach Semantic Association. Das bedeutet, ich gebe dir ein Wort und du denkst sofort an eine F\u00fclle an W\u00f6rter, die dazu passen. Und Chats, GPT oder Large Language Models generell sind hier relativ \u00e4hnlich. Alle versuchen immer das Token Limit auszutricksen. Und man kann das machen, indem man Chatbots baut, indem man Vector Databases dazwischen schaltet und darauf Zeug speichert. Aber all das ist erstens ein klein wenig aufwendig, nicht so angenehm zu verwenden. Es werden Dinge gespeichert, die all das ist erstens ein klein wenig aufwendig, nicht so angenehm zu verwenden. Es werden Dinge gespeichert, die nicht unbedingt n\u00f6tig sind und wir k\u00f6nnen das alles sehr, sehr einfach und cool machen. Und zwar alles in Chatsheepitty. Du bekommst danach Custom Instructions zum Kopieren. Was bedeutet das aber alles f\u00fcr die Praxis? Das bedeutet, dass wenn du zum Beispiel riesige Daten hast, an denen du Chats GPT trainieren willst, dann kannst du die wichtigsten Stichw\u00f6rter aus diesen Daten extrahieren. Du gibst Chats GPT nur die Stichw\u00f6rter, dadurch wird das Token Limit nicht \u00fcberschritten und du kannst ganz einfach gut damit arbeiten. Und David Shapiro hat hier in diesem coolen GitHub-Repo alles zur Verf\u00fcgung gestellt. Alles was wir machen m\u00fcssen ist ganz einfach, das hier zu kopieren und in die Custom Instructions einzuf\u00fcgen. Danach k\u00f6nnen wir unseren Text in JCPT werfen und wir bekommen die Zusammenfassung. Damit k\u00f6nnen wir JCPT trainieren und sobald wir den Output haben, packen wir das in die Custom Instructions und das war's. Also wie funktioniert das? Wir klicken Copy Code, wir gehen in Chats GPT. In Chats GPT angekommen, klicken wir das 3-Punkte-Men\u00fc, gehen auf Custom Instructions und wir f\u00fcgen hier bei How would you like Chatshippity to respond das hier alles ein, was uns hier der gute David Shapiro zusammengeschrieben hat. Sobald das eingef\u00fcgt ist, klicken wir einfach nur noch Enable for new chats, wir klicken Save. Wir gehen in einen neuen Chat, wir verwenden hier mal Chatshippity 4 und zwar das Default Model und jetzt k\u00f6nnen wir hier ganz einfach Daten reinwerfen, die normalerweise f\u00fcr Chatshipti zu gro\u00df w\u00e4ren. Wir sollten Daten verwenden, die er in einmal verarbeiten kann. Das m\u00fcssen wir notfalls ein paar mal machen. Aber wir k\u00f6nnen sehr gro\u00dfe Daten runterbrechen auf die einfachsten Punkte. Lasst uns das ganz einfach mal testen. Custom Instructions sind also eingestellt. Alles was wir machen ist, wir m\u00fcssen ein PDF finden oder irgendwelche Daten, an denen wir Chatshippity trainieren wollen. Das hier ist ganz einfach eine PDF. Hier geht es jetzt ganz einfach mal um Diabetes und am Anfang werden uns ein paar Dinge erkl\u00e4rt, die Chatshippity eventuell nicht versteht. Gehen wir ganz einfach mal davon aus, dass das Daten sind, an denen du Chatshippeat trainieren willst und du musst ihn an ganzen PDF trainieren. Wie k\u00f6nnte man das machen? Man kopiert ganz einfach die Dinge, die f\u00fcr dich wichtig sind. Lass uns sagen einfach bis hierher. Danach gehen wir in Chatshippeat, wir f\u00fcgen das Zeug hier ein und wir senden es ab und jetzt bekommen wir Zusammenfassungen. Die Zusammenfassungen bekommen wir jetzt ganz, ganz genau so, wie sie f\u00fcr Chatshippe D danach sinnvoll sind, damit er alles richtig, richtig gut verarbeiten kann. Das ist wirklich, wirklich wichtig. Wir nehmen also diesen riesigen Text von der PDF. Wir fassen ihn so zusammen, damit er f\u00fcr Chatshippe D perfekt ist. Sieh dir an, wie viel k\u00fcrzer der Text wurde. Der ist wirklich unglaublich viel k\u00fcrzer. Und mit diesem Text k\u00f6nnen wir danach weiterarbeiten. Wenn du die gesamte PDF hier verwenden willst, um Chatshippity zu trainieren, dann musst du das nat\u00fcrlich in St\u00fccke machen. Wir haben Chatshippity jetzt jedoch erstmal an diesem Text trainiert. Das ist hier aktuell auf Englisch. K\u00f6nntest du nat\u00fcrlich auch auf Deutsch machen, wenn du magst. Und wenn du jetzt Output haben willst, der sich an diesem Text orientiert, der ganz einfach die Daten von diesem Text versteht, dann kopieren wir einfach das hier. Wir gehen in Chatshippity rein und wir m\u00fcssen ihn jetzt an unseren Daten trainieren. Zum Beispiel hier Info, wir f\u00fcgen das ein und jetzt m\u00fcssen wir noch unsere Custom Instructions so umstellen, damit das auch noch besser funktioniert. Daf\u00fcr gehen wir wieder auf die Custom Instructions. Wir k\u00f6nnen die alten Custom Instructions ganz einfach rausl\u00f6schen. Wir gehen wieder zur\u00fcck auf die coolen Dinge, die uns David Shapiro hier gemacht hat. Wir f\u00fcgen die neuen Custom Instructions an, der SPR Decompressor, wir kopieren das, zur\u00fcck in GPT. Wir f\u00fcgen sie ein bei unserem neuen Custom Instructions. Du siehst, jetzt ist er der Decompressor. Wir klicken hier Save. Und jetzt werden wir das wieder richtig, richtig gut entpacken. Und deshalb senden wir das jetzt ab. Und jetzt siehst du, wir haben Chatshippe.de ganz einfach diese Informationen gegeben, erkennt sich jetzt perfekt mit dem Thema aus und wir bekommen jetzt entpackt alles, was wichtig ist. Wir werden hier sogar weitergeleitet auf coole Webseiten, wo wir uns weiterbilden k\u00f6nnen. Und wir bekommen ganz einfach alles im \u00dcberblick. Die Rolle hier von, das ist jetzt wirklich ein Riesentext, wo ich sogar Stopp gedr\u00fcckt habe. Aber er wei\u00df ganz einfach Bescheid zu den spezifischen Daten, die wir ihm hier gegeben haben. Das ist die optimale L\u00f6sung, umatshippity zu trainieren. Man nimmt gro\u00dfe Texte, man l\u00e4sst sich die gro\u00dfen Texte zusammenfassen auf die wichtigsten Stichpunkte. Man nimmt sich hier ganz einfach das Grundkonzept von Large Language Models zu nutzen. Large Language Models k\u00f6nnen sich ganz einfach Dinge dazu denken. Indem du die wichtigsten W\u00f6rter gibst, wei\u00df Chatshippity ganz einfach wor\u00fcber du sprichst. So kannst du die wichtigsten W\u00f6rter gibst, wei\u00df Chatshpt ganz einfach, wor\u00fcber du sprichst. So kannst du das Token Limit unglaublich weit dr\u00fccken und du kannst dir unn\u00f6tiges Fine-Tuning sparen oder eventuell sogar das Bauen von einem Chatbot mit einem Vector Database. So solltest du Chatshpt trainieren, wenn du spezifische Daten und Outputs haben willst. Das ist die einfachste Methode. Verwende die Custom Instructions, lass dir gro\u00dfe Dinge so zusammenfassen, damit sie f\u00fcr Chats GPT perfekt sind. Verwende die n\u00e4chsten Custom Instructions, um die Dinge wieder so zu empacken, wie sie f\u00fcr dich perfekt sind. Das ist der riesengro\u00dfe Vorteil. Gerade wenn du Chatshippity an riesengro\u00dfen Daten trainieren willst, meinetwegen an Daten von deinem Business oder von sonst was, ist das meiner Meinung nach die L\u00f6sung, die man verwenden sollte. Alle versuchen immer das Token Limit auszutricksen. Ich bringe hier mal ein Beispiel, was alle versuchen zu machen. Alle versuchen in einen 5 Liter Kanister 20 Liter Wasser einzuf\u00fcllen. Das funktioniert nun mal nicht so gut. Ja, man kann Chatbots bauen. Ja, man kann Vector Databases mit einbauen. Ja, man kann hiermit arbeiten. Aber wenn du im ChatGPT arbeiten willst, dann ist das einfachste, du nimmst den Input und fasst ihn so kurz wie m\u00f6glich f\u00fcr Chatshpt zusammen. Wir nehmen uns danach ganz einfach das Ding zunutze, dass Chatshpt die Informationen danach wieder extrahieren kann und dass er sich die Dinge selbst dazu denken kann. So ist Chatshpt im Nullkommanichts an deinen Daten trainiert. Und das auch richtig. Wenn man mit Vector Databases arbeitet, in Chatbots usw. im Nullkomma nichts an deinen Daten trainiert und das auch richtig. Wenn man mit Vector Databases arbeitet, in Chatbots usw. hat man leider immer und immer wieder Nachteile. Das ist die einfachste und g\u00fcnstigste Methode. Im Groben kann man sagen, dass MemGPT versucht das Token Limit zu l\u00f6sen. Irgendwie schaffen sie es auch. Es funktioniert aber leider nicht so gut wie angenommen wird. Man versucht ganz einfach die Daten einzuspeisen sozusagen und die danach extern zu speichern. Du kannst dir das \u00e4hnlich vorstellen wie bei deinem Computer. In deinen RAM werden Dinge extern abgespeichert, die kann sich Chatshipp-IT dann immer wieder und wieder abrufen, sie werden nur ins Ged\u00e4chtnis geholt, wieder eingespeist und so weiter. Das wirft hier andauernd einen Kreis auf, der einigerma\u00dfen funktioniert leider nicht so gut. Wie gesagt, deshalb ist die bessere L\u00f6sung, wenn wir ganz einfach dieses sparse priming representation verwenden, so wie es uns der gute alte David Shapiro zusammengestellt hat. Das ist erstens ziemlich kompliziert und zweitens nicht wirklich n\u00f6tig und drittens wird h\u00f6chstwahrscheinlich das ganze abgel\u00f6st, sobald die Large Language Models gro\u00df genug werden. Irgendwann werden wir die Large Language Models auf bis zu eine Billion Token erh\u00f6hen k\u00f6nnen und dann wird all das irrelevant. Die Studie von Microsoft beweist, dass das m\u00f6glich sein wird. Bis dahin m\u00fcssen wir leider mit dem Token Limit arbeiten. Mempt GPT ist eine L\u00f6sung, die halbwegs funktioniert, jedoch nicht wirklich, wirklich gut. Also, so lange bis wir das Long Net Paper haben, ist wahrscheinlich die einfachste und effektivste L\u00f6sung ganz einfach das Sparse Priming Representation. Das hei\u00dft, wir fassen ganz einfach die Informationen so kurz wie m\u00f6glich zusammen. Wir fassen sie so zusammen, damit sich JetGPT perfekt versteht und wir arbeiten ganz einfach damit und sparen uns dieses Ding hier. Lass mich gerne wissen, was du von MemGPT und von Fine-Tuning h\u00e4ltst. Fine-Tuning hat vielleicht auch den ein oder anderen Anwendungszweck, wie zum Beispiel etwas sehr, sehr Spezifisches. Du hast gesehen, das Schreiben von Twitter-Threads k\u00f6nnte hier mit Fine-Tuning richtig, richtig gut funktionieren. Aber wenn du gro\u00dfe Dateien haben willst, wenn du Chats GPT an vielen Dingen trainieren willst, dann funktioniert meiner Meinung nach zumindest auch das Fine Tuning nicht perfekt, weil ihr hier ganz einfach das Token Limit in die Quere kommt. In dem Sinne, wenn du Chatshippity trainieren willst, kopiere einfach mal die Custom Instructions und teste das Ganze aus. Und wenn du dennoch lernen willst, wie du einen Chatbot machst, dann sieh dir das Video an. Und wenn du dennoch lernen willst, wie du einen Chatbot machst, dann sieh dir das Video an.", "chunks": [{"timestamp": [0.0, 2.76], "text": " Feintuning und in-Context-Learning."}, {"timestamp": [3.04, 6.8], "text": " Das sind zwei Dinge, die man verwenden kann, um ein Large-Language-Model"}, {"timestamp": [6.8, 11.12], "text": " so anzupassen, damit es den Output generiert, den wir gerne haben wollen."}, {"timestamp": [11.32, 14.32], "text": " MemGPT will irgendwas dazwischen versuchen."}, {"timestamp": [14.36, 17.36], "text": " Ich bin der Meinung, das funktioniert nicht wirklich gut."}, {"timestamp": [17.4, 19.68], "text": " Am Ende des Videos dazu mehr."}, {"timestamp": [19.68, 22.92], "text": " Wenn wir einen sehr, sehr spezifischen Output haben wollen,"}, {"timestamp": [23.08, 25.04], "text": " dann ist Feinting das Ding, das"}, {"timestamp": [25.04, 29.76], "text": " wir verwenden sollen. Das sehen wir uns in diesem Video an. Wenn wir ein Large Language"}, {"timestamp": [29.76, 35.64], "text": " Model trainieren wollen an sehr, sehr viel Daten, dann ist in Context Learning das Ding,"}, {"timestamp": [35.64, 41.08], "text": " das wir verwenden sollten. Auch das werden wir uns in diesem Video ansehen. In Context"}, {"timestamp": [41.08, 46.3], "text": " Learning kannst du dir so vorstellen, dass man ganz einfach nur spezifische Daten"}, {"timestamp": [46.3, 51.82], "text": " dem Large Language Model gibt und das Modell kann sich die Dinge danach dazudenken."}, {"timestamp": [51.82, 56.18], "text": " Insgesamt gibt es wirklich nur eine Handvoll Methoden, die man verwenden kann."}, {"timestamp": [56.18, 61.78], "text": " Wie gesagt, das Fine Tuning, das In Context Learning, man kann auch einen Chatbot machen"}, {"timestamp": [61.78, 66.04], "text": " und Vector Database dazwischen schalten. Das haben wir uns schon hier angesehen."}, {"timestamp": [66.04, 68.44], "text": " Alle haben ihre Vor- und Nachteile."}, {"timestamp": [68.44, 73.84], "text": " Fine Tuning ist schon relativ cool, wenn man einen sehr, sehr spezifischen Output haben will."}, {"timestamp": [73.84, 80.16], "text": " Aber meiner Meinung nach das Ding, das am meisten \u00fcbersehen wird, ist das In-Context-Learning."}, {"timestamp": [80.16, 86.0], "text": " In-Context-Learning ist besonders praktisch, weil es so einfach funktioniert."}, {"timestamp": [86.0, 92.0], "text": " Man kann Chezhipiti ganz einfach Stichpunkte geben und er kann sich Dinge dazu denken,"}, {"timestamp": [92.0, 97.0], "text": " auch wenn er an diesen Dingen nicht trainiert wurde. Das ist wirklich enorm cool."}, {"timestamp": [97.0, 101.0], "text": " Lasst uns erst mal ganz kurz den Artikel von OpeMeYi ansehen,"}, {"timestamp": [101.0, 107.0], "text": " warum man Fine-Ting eigentlich machen sollte? Im Groben haben sie hier drei Punkte gesagt."}, {"timestamp": [107.0, 112.0], "text": " Improved Stability, Reliable Output Formatting und Custom Tone."}, {"timestamp": [112.0, 114.0], "text": " Was soll das bedeuten?"}, {"timestamp": [114.0, 117.0], "text": " Bei Improved Stability wollen sie dir einfach nur sagen,"}, {"timestamp": [117.0, 119.0], "text": " dass du dem Modell immer sagen kannst,"}, {"timestamp": [119.0, 124.0], "text": " hey, du sollst so antworten oder du sollst meinetwegen immer Deutsch sprechen."}, {"timestamp": [124.0, 128.16], "text": " Reliable Output Formatting will ganz einfach nur hei\u00dfen, dass du immer"}, {"timestamp": [128.32, 131.64], "text": " zum Beispiel JSON-Files bekommst, wenn du das denn haben willst."}, {"timestamp": [131.88, 134.68], "text": " Und Custom Tone soll ganz einfach nur hei\u00dfen,"}, {"timestamp": [134.88, 137.36], "text": " dass du ganz einfach dem Modell mitteilen kannst."}, {"timestamp": [137.36, 140.68], "text": " Hey, sei immer sarkastisch, sei immer witzig oder sonst was."}, {"timestamp": [141.2, 145.32], "text": " Generell gibt es nat\u00fcrlich noch eine andere Methode au\u00dfer dem Fine-Tuning,"}, {"timestamp": [145.32, 149.46], "text": " was wir uns danach auch noch ansehen, aber das Fine-Tuning ist schon etwas, das wirklich"}, {"timestamp": [149.46, 153.2], "text": " relativ cool ist. Und in drei Schritten zeigen sie dir auch,"}, {"timestamp": [153.2, 159.16], "text": " wie du dein Fine-Tuning umsetzen kannst. Generell musst du nat\u00fcrlich deine Datasets richtig"}, {"timestamp": [159.16, 164.2], "text": " eingeben. Aber keine Sorge, wir verwenden ein Colab Notebook, damit wir nichts selbst"}, {"timestamp": [164.2, 165.4], "text": " programmieren m\u00fcssen."}, {"timestamp": [165.4, 168.5], "text": " Also musst du das nicht mal unbedingt ganz genau wissen."}, {"timestamp": [168.5, 172.5], "text": " Du musst ganz einfach deine Daten in das richtige Format bringen."}, {"timestamp": [172.5, 176.2], "text": " Als n\u00e4chstes musst du deine Daten nat\u00fcrlich alle hochladen."}, {"timestamp": [176.2, 179.4], "text": " Daf\u00fcr verwenden wir die OpenAI API."}, {"timestamp": [179.4, 183.1], "text": " Wir m\u00fcssen danach nat\u00fcrlich unsere API Keys eintragen."}, {"timestamp": [183.1, 186.0], "text": " Aber auch das machen wir alles in Google Colab."}, {"timestamp": [186.0, 189.0], "text": " Und danach wird OpenAI die Magie machen."}, {"timestamp": [189.0, 194.0], "text": " Sie werden ein Modell eigens f\u00fcr dich trainieren, von dem du danach deine Outputs bekommst."}, {"timestamp": [194.0, 196.0], "text": " Wieder nat\u00fcrlich mit der API."}, {"timestamp": [196.0, 201.0], "text": " Und wie gesagt, wir machen alles ganz, ganz einfach in einem Colab Notebook,"}, {"timestamp": [201.0, 204.0], "text": " damit wir nichts selbst programmieren m\u00fcssen."}, {"timestamp": [204.0, 208.2], "text": " Und zum Schluss k\u00f6nnen wir nat\u00fcrlich unser eigenes Modell verwenden und das war's."}, {"timestamp": [208.2, 213.1], "text": " Das hier ist das Colab Notebook. Es wurde zusammengesetzt von Matt Schumer."}, {"timestamp": [213.1, 215.7], "text": " Hier kannst du ihn nat\u00fcrlich auch auf Twitter folgen."}, {"timestamp": [215.7, 221.0], "text": " Und hier k\u00f6nnen wir sogar unsere Datasets machen. Das ist alles wirklich, wirklich einfach."}, {"timestamp": [221.0, 226.0], "text": " Wir scrollen runter, klicken ganz einfach Play bei dem ersten Button und wir m\u00fcssen trotzdem ausf\u00fchren."}, {"timestamp": [226.0, 230.0], "text": " Das Colab Notebook wird uns sogar unsere Datasets erstellen."}, {"timestamp": [230.0, 232.0], "text": " Wir werden hier folgendes machen."}, {"timestamp": [232.0, 237.0], "text": " Wir machen ein Modell, das ganz einfach den Custom Tone macht."}, {"timestamp": [237.0, 240.0], "text": " Wir wollen einfach ein Modell haben, das witzig ist."}, {"timestamp": [240.0, 246.8], "text": " Wir gehen wieder zur\u00fcck ins Colab Notebook und hier bei diesem Prompt m\u00fcssen wir ganz einfach sagen, was wir haben wollen."}, {"timestamp": [246.8, 252.4], "text": " Zum Beispiel, a model that takes in a puzzle like bla bla blup und so weiter, response"}, {"timestamp": [252.4, 253.4], "text": " out in spanisch."}, {"timestamp": [253.4, 257.48], "text": " Wir wollen das aber in deutsch haben und wir geben hier ganz einfach unseren Prompt"}, {"timestamp": [257.48, 258.48], "text": " ein."}, {"timestamp": [258.48, 262.08], "text": " Ich habe hier den Prompt ganz einfach gemacht, dass wir ein Modell haben wollen, das sehr"}, {"timestamp": [262.08, 264.46], "text": " sarkastisch ist, mit viel Humor."}, {"timestamp": [264.46, 268.0], "text": " Es soll Twitter Beitr\u00e4ge schreiben \u00fcber k\u00fcnstliche Intelligenz."}, {"timestamp": [268.0, 273.0], "text": " Sie sollen sehr kurz sein und den Output wollen wir in Deutsch haben."}, {"timestamp": [273.0, 278.0], "text": " Generell k\u00f6nntest du ein Modell nat\u00fcrlich genauso wie auf der Webseite trainieren"}, {"timestamp": [278.0, 280.0], "text": " und hier deine eigenen Dateien f\u00fcttern."}, {"timestamp": [280.0, 285.0], "text": " Du k\u00f6nntest ganze PDFs hochladen, jedoch m\u00fcsste man das in Python machen."}, {"timestamp": [285.0, 291.0], "text": " Das ist etwas aufwendiger. Ich zeige hier die einfachste Methode, um ganz einfach den Ton anzupassen."}, {"timestamp": [291.0, 300.0], "text": " Und sp\u00e4ter im Video zeige ich noch eine coole weitere Methode, wie wir ohne Fine Tuning ein Modell dazu bringen k\u00f6nnen, die Outputs genauso zu bekommen."}, {"timestamp": [300.0, 308.0], "text": " Wir k\u00f6nnen ein Modell auch an unseren Daten trainieren und richtig, richtig viele Token sparen und das alles ohne ein Vector Dataset."}, {"timestamp": [308.0, 311.0], "text": " Das werden wir uns sp\u00e4ter nat\u00fcrlich noch genauer ansehen."}, {"timestamp": [311.0, 317.0], "text": " Also wir haben hier unseren Prompt gesetzt, jetzt klicken wir hier Play und wir m\u00fcssen trotzdem ausf\u00fchren."}, {"timestamp": [317.0, 321.0], "text": " Die Temperatur kannst du \u00fcbrigens so einstellen wie du magst."}, {"timestamp": [321.0, 325.44], "text": " Wenn du etwas sehr kreatives haben willst, wie wir hier zum Beispiel,"}, {"timestamp": [325.44, 331.68], "text": " k\u00f6nnte man die auch hoch machen auf 0,6 und wenn du etwas sehr logisches machen willst,"}, {"timestamp": [331.68, 338.24], "text": " zum Beispiel Coder stellen, dann w\u00fcrde ich die Temperatur auf 0 machen. Number of examples 50,"}, {"timestamp": [338.24, 349.0], "text": " das hei\u00dft wir bekommen ganz einfach 50 Beispiele hier, um danach unser Modell zu trainieren. Wir k\u00f6nnen das auch runtersetzen auf 30, aber zumindest 20 sollten wir machen."}, {"timestamp": [349.0, 352.0], "text": " Als n\u00e4chstes m\u00fcssen wir die Modelle installieren."}, {"timestamp": [352.0, 354.0], "text": " Wir klicken ganz einfach Play bei dieser Zelle."}, {"timestamp": [354.0, 356.0], "text": " Das dauert nur wenige Sekunden."}, {"timestamp": [356.0, 359.0], "text": " Als n\u00e4chstes brauchen wir nat\u00fcrlich einen API Key."}, {"timestamp": [359.0, 361.0], "text": " Das habe ich schon einige Male gemacht."}, {"timestamp": [361.0, 364.0], "text": " Also Create Key. Wir geben mit dem einen Namen."}, {"timestamp": [364.0, 370.0], "text": " Lasst es meinetwegen Tune sein und wir klicken Create Secret Key, den m\u00fcssen wir danach kopieren."}, {"timestamp": [370.0, 374.32], "text": " Und wir kopieren das, keine Sorge, den Key l\u00f6sche ich danach ganz einfach wieder."}, {"timestamp": [374.32, 379.68], "text": " Diesen Key m\u00fcssen wir jetzt genau hier reingeben und zwar bei Enter your API Key,"}, {"timestamp": [379.68, 387.2], "text": " hier. Jetzt haben wir also hier unseren Key eingegeben, wir klicken ganz einfach Play und jetzt werden unsere Datasets erstellt."}, {"timestamp": [387.2, 389.2], "text": " Das dauert eine kleine Weile."}, {"timestamp": [389.2, 395.0], "text": " Generell k\u00f6nnte man mit diesem Colab Notebook nat\u00fcrlich die verschiedensten Dinge machen, wie gesagt."}, {"timestamp": [395.0, 399.0], "text": " Und wenn du ein Modell fine-tunen willst an deinen eigenen Daten,"}, {"timestamp": [399.0, 401.7], "text": " dann musst du das nat\u00fcrlich ein klein wenig anders machen."}, {"timestamp": [401.7, 406.92], "text": " Dann musst du das tats\u00e4chlich in Python machen, aber wie gesagt, am Ende des Videos zeige ich eine Methode,"}, {"timestamp": [406.92, 412.9], "text": " wie man das noch einfacher, noch effizienter und noch g\u00fcnstiger machen kann, als mit dem Fine-Tuning."}, {"timestamp": [413.68, 420.2], "text": " Mittlerweile haben wir hier all unsere, ich habe es doch 50 werden lassen, also alle unsere 50 Beispiele gemacht."}, {"timestamp": [420.2, 426.0], "text": " Und wir klicken hier unten jetzt Play bei der n\u00e4chsten Zelle, womit wir unseren System Prompt erstellen."}, {"timestamp": [426.0, 429.0], "text": " Klicken wir ganz einfach Play bei dieser Zelle."}, {"timestamp": [429.0, 432.0], "text": " Hier siehst du, was unser System Prompt danach ist."}, {"timestamp": [432.0, 438.0], "text": " Das Modell hat ganz einfach automatisch die Aufgabe bekommen, dass es kurze Threads schreiben wird."}, {"timestamp": [438.0, 443.0], "text": " Und die Threads sollen alle sarkastisch sein und in Deutsch."}, {"timestamp": [443.0, 445.12], "text": " Wir haben jetzt also all unsere Daten hier"}, {"timestamp": [445.12, 449.12], "text": " erstellt. Wir haben einen System Prompt erstellt und nun m\u00fcssen wir nat\u00fcrlich"}, {"timestamp": [449.12, 453.52], "text": " all unsere Daten wie hier oben in das richtige Format bringen, wie wir"}, {"timestamp": [453.52, 458.16], "text": " nat\u00fcrlich auch schon im Blogartikel gesehen haben. Du hast hier gesehen, dass"}, {"timestamp": [458.16, 463.16], "text": " unser Dataset danach so formatiert sein soll und das macht uns auch die n\u00e4chste"}, {"timestamp": [463.16, 466.32], "text": " Zelle automatisch. Wir klicken einfach hier und wir bekommen danach"}, {"timestamp": [466.56, 473.32], "text": " JSON-Dateien und das geht wirklich unglaublich schnell. Wir haben jetzt all unsere 50 Dinger so umgewandelt, dass sie f\u00fcr uns"}, {"timestamp": [473.68, 477.08], "text": " funktionieren. Der Code macht alles automatisch in diesem Notebook."}, {"timestamp": [477.6, 486.08], "text": " Als n\u00e4chstes m\u00fcssen wir unsere Files hochladen auf OpenAI und wir klicken ganz einfach Play bei dieser Zelle. Das geht auch im Nullkomma nichts."}, {"timestamp": [486.08, 492.4], "text": " Und der letzte Schritt ist nat\u00fcrlich unser Modell zu trainieren. Und um unser Modell zu trainieren"}, {"timestamp": [492.4, 497.64], "text": " m\u00fcssen wir einfach nur noch hier Play klicken bei dieser Zelle und das sollte funktionieren."}, {"timestamp": [497.64, 503.72], "text": " Das dauert meistens in etwa so um die 20 Minuten, aber wir k\u00f6nnen uns nat\u00fcrlich einfach alles"}, {"timestamp": [503.72, 505.56], "text": " ansehen. Wenn du hier play klickst,"}, {"timestamp": [505.56, 512.56], "text": " dann bekommst du hier die Updates. Wir sind aktuell also beim ersten Schritt von 150 Schritten und"}, {"timestamp": [512.56, 517.12], "text": " wenn du hier einfach ab und zu play klickst, dann siehst du einfach wie weit das Ding ist. Wir sind"}, {"timestamp": [517.12, 523.4], "text": " jetzt immer noch beim ersten Schritt von 150. Das wird jetzt in etwa 20 Minuten dauern. Also"}, {"timestamp": [523.4, 529.5], "text": " brauchen wir hier ein klein wenig Geduld und wir sehen uns sobald das hier alles trainiert worden ist."}, {"timestamp": [529.5, 532.5], "text": " Und du siehst es geht mittlerweile viel viel schneller."}, {"timestamp": [532.5, 536.0], "text": " Wir sind jetzt schon bei 21 von 150."}, {"timestamp": [536.0, 539.0], "text": " Wie gesagt, wir sehen uns sobald das fertig ist."}, {"timestamp": [539.0, 541.0], "text": " Okay und jetzt haben wir das Ganze gemacht."}, {"timestamp": [541.0, 544.0], "text": " Nach in etwa 20 Minuten bekommst du das hier."}, {"timestamp": [544.0, 547.0], "text": " The job has successfully completed."}, {"timestamp": [547.0, 549.0], "text": " Du wirst auch eine Mail bekommen."}, {"timestamp": [549.0, 554.0], "text": " OpenAI schreibt dir ganz einfach eine Mail zu, dass das ganze mittlerweile ausgef\u00fchrt ist."}, {"timestamp": [554.0, 557.0], "text": " Und jetzt hast du hier sofort dein eigenes Modell."}, {"timestamp": [557.0, 561.0], "text": " Um das Modell zu speichern, klicken wir einfach nur noch hier Play."}, {"timestamp": [561.0, 563.0], "text": " Und danach sollte das ganze funktionieren."}, {"timestamp": [563.0, 565.4], "text": " Du siehst unser personalisiertes Model hat"}, {"timestamp": [565.4, 570.0], "text": " diesen witzigen Namen und hier unten k\u00f6nnen wir das Modell jetzt testen."}, {"timestamp": [570.0, 574.84], "text": " Hier bei Content k\u00f6nnen wir jetzt eingeben, was wir wollen. Lass uns mal den Default"}, {"timestamp": [574.84, 579.64], "text": " Prompt versuchen und hier Play klicken. Und hier haben wir jetzt unseren sarkastischen"}, {"timestamp": [579.64, 588.7], "text": " Tweet. Oh ja, k\u00fcnstliche Intelligenz ist total kreativ in der Musik. Wer will schon von echten Musikern oder Musikerinnen inspiriert werden,"}, {"timestamp": [588.7, 592.5], "text": " wenn wir stattdessen von Maschinen generierte Hits h\u00f6ren k\u00f6nnen?"}, {"timestamp": [592.5, 598.0], "text": " Und diese einzigartigen Melodien, wer braucht schon menschliche Emotionen und Leidenschaft,"}, {"timestamp": [598.0, 603.0], "text": " wenn wir stattdessen Algorithmen haben, die uns den gleichen Einheitspreis servieren?"}, {"timestamp": [603.0, 608.0], "text": " Es ist doch viel besser, wenn die Musikindustrie von Maschinen kontrolliert wird."}, {"timestamp": [608.0, 610.0], "text": " Hashtag Sarcasmus Overlord."}, {"timestamp": [610.0, 613.0], "text": " Also du siehst, das Modell funktioniert sehr, sehr gut."}, {"timestamp": [613.0, 617.0], "text": " So k\u00f6nnte man wirklich in nu Haufenweise Tweets machen."}, {"timestamp": [617.0, 619.0], "text": " Das funktioniert wirklich, wirklich gut."}, {"timestamp": [619.0, 622.0], "text": " Und so k\u00f6nnte man ein solches Modell fine tunen."}, {"timestamp": [622.0, 629.66], "text": " Das Modell, das wir gerade trainiert haben, ist \u00fcbrigens auch im OpenAI Playground vorhanden. Daf\u00fcr gehst du hier einfach in"}, {"timestamp": [629.66, 634.54], "text": " den Playground rein und bei Model kannst du hier das ganz einfach ausw\u00e4hlen."}, {"timestamp": [634.54, 638.82], "text": " Geh auf Fine Tunes und hier kannst du dein Modell ausw\u00e4hlen, das wir gerade in"}, {"timestamp": [638.82, 643.5], "text": " unserem Google Colab trainiert haben. Hier kannst du jetzt arbeiten, so wie mit"}, {"timestamp": [643.5, 646.2], "text": " JetCPT. Du kannst hier oben deinen Prompt"}, {"timestamp": [646.2, 652.12], "text": " eingeben und los geht's. Eventuell kannst du sogar noch deinen System Prompt ein klein wenig anpassen."}, {"timestamp": [652.12, 659.0], "text": " Das ist wirklich unglaublich einfach. Was ist also der Vorteil von diesem Fine Tuning? Hier kannst du"}, {"timestamp": [659.0, 666.48], "text": " mit sehr sehr g\u00fcnstigen Modellen wie zum Beispiel GPT 3.5 Turbo, wie wir hier laufen haben, das Modell sehr spezifisch"}, {"timestamp": [666.48, 673.36], "text": " machen und das Modell ist jetzt in diesen spezifischen Task sogar besser als GPT 4. Und"}, {"timestamp": [673.36, 679.36], "text": " f\u00fcr GPT 4 musst du nat\u00fcrlich bezahlen. Entweder du brauchst das Abo oder sonst was. Generell ist"}, {"timestamp": [679.36, 685.46], "text": " Fine Tuning etwas, das sehr sehr gut funktionieren kann. auch wenn du ein Modell zum Beispiel an deinen"}, {"timestamp": [685.46, 691.1], "text": " Firmen Daten trainieren willst, funktioniert das relativ gut. Es gibt jedoch ein paar Nachteile"}, {"timestamp": [691.1, 696.46], "text": " vom Fine Tuning. Nat\u00fcrlich wenn du an sehr sehr gro\u00dfen Daten trainierst, an Business Daten zum"}, {"timestamp": [696.46, 701.72], "text": " Beispiel, dann ist das Fine Tuning relativ teuer und um ehrlich zu sein ist es auch nicht so"}, {"timestamp": [701.72, 708.92], "text": " angenehm zu verwenden. Jedoch sehen wir uns jetzt eine richtig, richtig coole Alternative zum Fine Tuning an."}, {"timestamp": [708.92, 713.12], "text": " Meiner Meinung nach eignet sich das Fine Tuning am besten f\u00fcr sowas."}, {"timestamp": [713.12, 718.36], "text": " Du verwendest einfach irgendetwas, wo du das Modell spezifisch reagieren lassen willst"}, {"timestamp": [718.36, 722.08], "text": " und du willst GPT 3.5 Turbo verwenden f\u00fcr den Output."}, {"timestamp": [722.08, 725.68], "text": " Lasst uns einfach mal ansehen, wie wir Chats GPT manipulieren"}, {"timestamp": [725.68, 731.04], "text": " k\u00f6nnen, damit wir richtig, richtig gute Outputs bekommen. David Shapiro hat hier etwas"}, {"timestamp": [731.04, 736.6], "text": " Cooles auf GitHub vorbereitet. Er sagt uns hier, dass es ganz einfach vier Dinge gibt,"}, {"timestamp": [736.6, 741.28], "text": " wie man ein Modell verbessern kann. Entweder Initial Bulk Training, das ist nat\u00fcrlich"}, {"timestamp": [741.28, 746.0], "text": " unglaublich teuer. Fine Tuning, so wie wir es gerade gemacht haben, das hat den Nachteil,"}, {"timestamp": [746.0, 750.0], "text": " dass man hier eventuell nicht zu viel Wissen speichern kann."}, {"timestamp": [750.0, 755.0], "text": " Du hast gesehen, wir haben ein Modell, das jedoch einen sehr, sehr spezifischen Task machen kann."}, {"timestamp": [755.0, 760.0], "text": " Also wenn du einen spezifischen Task machen willst, dann ist Feintuning gut."}, {"timestamp": [760.0, 767.0], "text": " Online Learning, ja, das kann eventuell kommen. Und eventuell noch in Context Learning."}, {"timestamp": [767.0, 771.0], "text": " Und das ist aktuell das Coolste. Worum geht es hier genau?"}, {"timestamp": [771.0, 777.0], "text": " Lasst uns erstmal kurz dar\u00fcber sprechen, warum und wie das Ganze funktioniert und worum es \u00fcberhaupt geht."}, {"timestamp": [777.0, 788.0], "text": " Large Language Models sind associative, das heisst, die k\u00f6nnen ganz einfach Dinge dazu denken. Wenn wir den Modellen einfach nur spezifische Stichw\u00f6rter geben,"}, {"timestamp": [788.0, 794.0], "text": " dann wissen sie, an welchen Textdaten sie suchen sollen und was sie in etwa dazu spinnen sollten."}, {"timestamp": [794.0, 799.0], "text": " Wenn ich zum Beispiel dir ein einziges Wort gebe, meinetwegen Bizeps,"}, {"timestamp": [799.0, 807.16], "text": " dann denkst du sofort an Muskeln, an Fitnessstudio, an was wei\u00df ich nicht alles. Du hast ganz einfach in deinem Hirn gewisse Dinge,"}, {"timestamp": [807.16, 810.16], "text": " die du mit dem Wort Bizeps verkn\u00fcpft hast."}, {"timestamp": [810.52, 813.36], "text": " Und \u00e4hnlich funktionieren Large Language Models."}, {"timestamp": [813.84, 817.36], "text": " Das Ding nennt sich ganz einfach Semantic Association."}, {"timestamp": [817.4, 822.6], "text": " Das bedeutet, ich gebe dir ein Wort und du denkst sofort an eine F\u00fclle an W\u00f6rter,"}, {"timestamp": [822.6, 823.92], "text": " die dazu passen."}, {"timestamp": [823.92, 829.0], "text": " Und Chats, GPT oder Large Language Models generell sind hier relativ \u00e4hnlich."}, {"timestamp": [829.0, 832.8], "text": " Alle versuchen immer das Token Limit auszutricksen."}, {"timestamp": [832.8, 835.8], "text": " Und man kann das machen, indem man Chatbots baut,"}, {"timestamp": [835.8, 841.0], "text": " indem man Vector Databases dazwischen schaltet und darauf Zeug speichert."}, {"timestamp": [841.0, 844.2], "text": " Aber all das ist erstens ein klein wenig aufwendig,"}, {"timestamp": [844.2, 845.58], "text": " nicht so angenehm zu verwenden. Es werden Dinge gespeichert, die all das ist erstens ein klein wenig aufwendig, nicht so angenehm zu"}, {"timestamp": [845.58, 850.22], "text": " verwenden. Es werden Dinge gespeichert, die nicht unbedingt n\u00f6tig sind und wir k\u00f6nnen"}, {"timestamp": [850.22, 856.82], "text": " das alles sehr, sehr einfach und cool machen. Und zwar alles in Chatsheepitty. Du bekommst"}, {"timestamp": [856.82, 862.18], "text": " danach Custom Instructions zum Kopieren. Was bedeutet das aber alles f\u00fcr die Praxis?"}, {"timestamp": [862.18, 868.84], "text": " Das bedeutet, dass wenn du zum Beispiel riesige Daten hast, an denen du Chats GPT trainieren willst,"}, {"timestamp": [868.84, 873.2], "text": " dann kannst du die wichtigsten Stichw\u00f6rter aus diesen Daten extrahieren."}, {"timestamp": [873.2, 879.4], "text": " Du gibst Chats GPT nur die Stichw\u00f6rter, dadurch wird das Token Limit nicht \u00fcberschritten"}, {"timestamp": [879.4, 882.36], "text": " und du kannst ganz einfach gut damit arbeiten."}, {"timestamp": [882.36, 885.88], "text": " Und David Shapiro hat hier in diesem coolen GitHub-Repo"}, {"timestamp": [885.88, 892.92], "text": " alles zur Verf\u00fcgung gestellt. Alles was wir machen m\u00fcssen ist ganz einfach, das hier zu kopieren und"}, {"timestamp": [892.92, 899.24], "text": " in die Custom Instructions einzuf\u00fcgen. Danach k\u00f6nnen wir unseren Text in JCPT werfen und wir"}, {"timestamp": [899.24, 908.96], "text": " bekommen die Zusammenfassung. Damit k\u00f6nnen wir JCPT trainieren und sobald wir den Output haben, packen wir das in die Custom Instructions und das war's."}, {"timestamp": [908.96, 913.76], "text": " Also wie funktioniert das? Wir klicken Copy Code, wir gehen in Chats GPT."}, {"timestamp": [913.76, 926.44], "text": " In Chats GPT angekommen, klicken wir das 3-Punkte-Men\u00fc, gehen auf Custom Instructions und wir f\u00fcgen hier bei How would you like Chatshippity to respond das hier alles ein, was uns hier"}, {"timestamp": [926.44, 929.84], "text": " der gute David Shapiro zusammengeschrieben hat."}, {"timestamp": [929.84, 934.92], "text": " Sobald das eingef\u00fcgt ist, klicken wir einfach nur noch Enable for new chats, wir klicken"}, {"timestamp": [934.92, 935.92], "text": " Save."}, {"timestamp": [935.92, 941.16], "text": " Wir gehen in einen neuen Chat, wir verwenden hier mal Chatshippity 4 und zwar das Default"}, {"timestamp": [941.16, 947.5], "text": " Model und jetzt k\u00f6nnen wir hier ganz einfach Daten reinwerfen, die normalerweise f\u00fcr Chatshipti zu gro\u00df w\u00e4ren."}, {"timestamp": [947.5, 951.5], "text": " Wir sollten Daten verwenden, die er in einmal verarbeiten kann."}, {"timestamp": [951.5, 953.5], "text": " Das m\u00fcssen wir notfalls ein paar mal machen."}, {"timestamp": [953.5, 958.5], "text": " Aber wir k\u00f6nnen sehr gro\u00dfe Daten runterbrechen auf die einfachsten Punkte."}, {"timestamp": [958.5, 960.5], "text": " Lasst uns das ganz einfach mal testen."}, {"timestamp": [960.5, 963.5], "text": " Custom Instructions sind also eingestellt."}, {"timestamp": [963.5, 970.0], "text": " Alles was wir machen ist, wir m\u00fcssen ein PDF finden oder irgendwelche Daten, an denen wir Chatshippity trainieren wollen."}, {"timestamp": [970.0, 985.76], "text": " Das hier ist ganz einfach eine PDF. Hier geht es jetzt ganz einfach mal um Diabetes und am Anfang werden uns ein paar Dinge erkl\u00e4rt, die Chatshippity eventuell nicht versteht. Gehen wir ganz einfach mal davon aus, dass das Daten sind, an denen du Chatshippeat"}, {"timestamp": [985.76, 992.6], "text": " trainieren willst und du musst ihn an ganzen PDF trainieren. Wie k\u00f6nnte man das machen? Man kopiert"}, {"timestamp": [992.6, 998.52], "text": " ganz einfach die Dinge, die f\u00fcr dich wichtig sind. Lass uns sagen einfach bis hierher. Danach gehen"}, {"timestamp": [998.52, 1009.94], "text": " wir in Chatshippeat, wir f\u00fcgen das Zeug hier ein und wir senden es ab und jetzt bekommen wir Zusammenfassungen. Die Zusammenfassungen bekommen wir jetzt ganz, ganz genau so, wie sie f\u00fcr Chatshippe"}, {"timestamp": [1009.94, 1014.66], "text": " D danach sinnvoll sind, damit er alles richtig, richtig gut verarbeiten kann."}, {"timestamp": [1014.66, 1017.06], "text": " Das ist wirklich, wirklich wichtig."}, {"timestamp": [1017.06, 1020.62], "text": " Wir nehmen also diesen riesigen Text von der PDF."}, {"timestamp": [1020.62, 1024.34], "text": " Wir fassen ihn so zusammen, damit er f\u00fcr Chatshippe D perfekt ist."}, {"timestamp": [1024.34, 1026.76], "text": " Sieh dir an, wie viel k\u00fcrzer der Text wurde."}, {"timestamp": [1026.76, 1029.36], "text": " Der ist wirklich unglaublich viel k\u00fcrzer."}, {"timestamp": [1029.36, 1032.8], "text": " Und mit diesem Text k\u00f6nnen wir danach weiterarbeiten."}, {"timestamp": [1032.8, 1037.2], "text": " Wenn du die gesamte PDF hier verwenden willst, um Chatshippity zu trainieren,"}, {"timestamp": [1037.2, 1039.4], "text": " dann musst du das nat\u00fcrlich in St\u00fccke machen."}, {"timestamp": [1039.4, 1043.4], "text": " Wir haben Chatshippity jetzt jedoch erstmal an diesem Text trainiert."}, {"timestamp": [1043.4, 1045.32], "text": " Das ist hier aktuell auf Englisch."}, {"timestamp": [1045.32, 1048.12], "text": " K\u00f6nntest du nat\u00fcrlich auch auf Deutsch machen, wenn du magst."}, {"timestamp": [1048.12, 1054.0], "text": " Und wenn du jetzt Output haben willst, der sich an diesem Text orientiert, der ganz einfach"}, {"timestamp": [1054.0, 1058.88], "text": " die Daten von diesem Text versteht, dann kopieren wir einfach das hier."}, {"timestamp": [1058.88, 1072.0], "text": " Wir gehen in Chatshippity rein und wir m\u00fcssen ihn jetzt an unseren Daten trainieren. Zum Beispiel hier Info, wir f\u00fcgen das ein und jetzt m\u00fcssen wir noch unsere Custom Instructions so umstellen,"}, {"timestamp": [1072.0, 1075.0], "text": " damit das auch noch besser funktioniert."}, {"timestamp": [1075.0, 1078.0], "text": " Daf\u00fcr gehen wir wieder auf die Custom Instructions."}, {"timestamp": [1078.0, 1082.0], "text": " Wir k\u00f6nnen die alten Custom Instructions ganz einfach rausl\u00f6schen."}, {"timestamp": [1082.0, 1087.06], "text": " Wir gehen wieder zur\u00fcck auf die coolen Dinge, die uns David Shapiro hier gemacht hat."}, {"timestamp": [1087.06, 1094.74], "text": " Wir f\u00fcgen die neuen Custom Instructions an, der SPR Decompressor, wir kopieren das, zur\u00fcck in GPT."}, {"timestamp": [1095.0, 1102.76], "text": " Wir f\u00fcgen sie ein bei unserem neuen Custom Instructions. Du siehst, jetzt ist er der Decompressor. Wir klicken hier Save."}, {"timestamp": [1103.2, 1107.6], "text": " Und jetzt werden wir das wieder richtig, richtig gut entpacken."}, {"timestamp": [1107.6, 1112.8], "text": " Und deshalb senden wir das jetzt ab. Und jetzt siehst du, wir haben Chatshippe.de ganz einfach"}, {"timestamp": [1112.8, 1118.12], "text": " diese Informationen gegeben, erkennt sich jetzt perfekt mit dem Thema aus und wir bekommen jetzt"}, {"timestamp": [1118.12, 1125.0], "text": " entpackt alles, was wichtig ist. Wir werden hier sogar weitergeleitet auf coole Webseiten, wo wir uns weiterbilden k\u00f6nnen."}, {"timestamp": [1125.0, 1128.0], "text": " Und wir bekommen ganz einfach alles im \u00dcberblick."}, {"timestamp": [1128.0, 1134.0], "text": " Die Rolle hier von, das ist jetzt wirklich ein Riesentext, wo ich sogar Stopp gedr\u00fcckt habe."}, {"timestamp": [1134.0, 1140.0], "text": " Aber er wei\u00df ganz einfach Bescheid zu den spezifischen Daten, die wir ihm hier gegeben haben."}, {"timestamp": [1140.0, 1145.28], "text": " Das ist die optimale L\u00f6sung, umatshippity zu trainieren."}, {"timestamp": [1145.28, 1150.04], "text": " Man nimmt gro\u00dfe Texte, man l\u00e4sst sich die gro\u00dfen Texte zusammenfassen auf die"}, {"timestamp": [1150.04, 1154.68], "text": " wichtigsten Stichpunkte. Man nimmt sich hier ganz einfach das Grundkonzept von"}, {"timestamp": [1154.68, 1159.0], "text": " Large Language Models zu nutzen. Large Language Models k\u00f6nnen sich ganz"}, {"timestamp": [1159.0, 1164.08], "text": " einfach Dinge dazu denken. Indem du die wichtigsten W\u00f6rter gibst, wei\u00df Chatshippity"}, {"timestamp": [1164.08, 1165.4], "text": " ganz einfach wor\u00fcber du sprichst. So kannst du die wichtigsten W\u00f6rter gibst, wei\u00df Chatshpt ganz einfach, wor\u00fcber"}, {"timestamp": [1165.4, 1170.92], "text": " du sprichst. So kannst du das Token Limit unglaublich weit dr\u00fccken und du kannst dir"}, {"timestamp": [1170.92, 1176.88], "text": " unn\u00f6tiges Fine-Tuning sparen oder eventuell sogar das Bauen von einem Chatbot mit einem"}, {"timestamp": [1176.88, 1190.28], "text": " Vector Database. So solltest du Chatshpt trainieren, wenn du spezifische Daten und Outputs haben willst. Das ist die einfachste Methode. Verwende die Custom Instructions, lass dir gro\u00dfe Dinge so"}, {"timestamp": [1190.28, 1195.12], "text": " zusammenfassen, damit sie f\u00fcr Chats GPT perfekt sind. Verwende die n\u00e4chsten"}, {"timestamp": [1195.12, 1200.2], "text": " Custom Instructions, um die Dinge wieder so zu empacken, wie sie f\u00fcr dich perfekt"}, {"timestamp": [1200.2, 1206.56], "text": " sind. Das ist der riesengro\u00dfe Vorteil. Gerade wenn du Chatshippity an riesengro\u00dfen Daten"}, {"timestamp": [1206.56, 1211.72], "text": " trainieren willst, meinetwegen an Daten von deinem Business oder von sonst was, ist das"}, {"timestamp": [1211.72, 1217.76], "text": " meiner Meinung nach die L\u00f6sung, die man verwenden sollte. Alle versuchen immer das Token Limit"}, {"timestamp": [1217.76, 1226.04], "text": " auszutricksen. Ich bringe hier mal ein Beispiel, was alle versuchen zu machen. Alle versuchen in einen 5 Liter Kanister 20"}, {"timestamp": [1226.04, 1231.68], "text": " Liter Wasser einzuf\u00fcllen. Das funktioniert nun mal nicht so gut. Ja, man kann Chatbots"}, {"timestamp": [1231.68, 1238.2], "text": " bauen. Ja, man kann Vector Databases mit einbauen. Ja, man kann hiermit arbeiten. Aber"}, {"timestamp": [1238.2, 1247.0], "text": " wenn du im ChatGPT arbeiten willst, dann ist das einfachste, du nimmst den Input und fasst ihn so kurz wie m\u00f6glich f\u00fcr Chatshpt zusammen."}, {"timestamp": [1247.0, 1251.0], "text": " Wir nehmen uns danach ganz einfach das Ding zunutze,"}, {"timestamp": [1251.0, 1255.0], "text": " dass Chatshpt die Informationen danach wieder extrahieren kann"}, {"timestamp": [1255.0, 1258.0], "text": " und dass er sich die Dinge selbst dazu denken kann."}, {"timestamp": [1258.0, 1262.0], "text": " So ist Chatshpt im Nullkommanichts an deinen Daten trainiert."}, {"timestamp": [1262.0, 1264.0], "text": " Und das auch richtig."}, {"timestamp": [1264.0, 1265.0], "text": " Wenn man mit Vector Databases arbeitet, in Chatbots usw. im Nullkomma nichts an deinen Daten trainiert und das auch richtig."}, {"timestamp": [1265.0, 1269.0], "text": " Wenn man mit Vector Databases arbeitet, in Chatbots usw."}, {"timestamp": [1269.0, 1271.5], "text": " hat man leider immer und immer wieder Nachteile."}, {"timestamp": [1271.5, 1274.5], "text": " Das ist die einfachste und g\u00fcnstigste Methode."}, {"timestamp": [1274.5, 1279.0], "text": " Im Groben kann man sagen, dass MemGPT versucht das Token Limit zu l\u00f6sen."}, {"timestamp": [1279.0, 1281.0], "text": " Irgendwie schaffen sie es auch."}, {"timestamp": [1281.0, 1284.5], "text": " Es funktioniert aber leider nicht so gut wie angenommen wird."}, {"timestamp": [1284.5, 1291.24], "text": " Man versucht ganz einfach die Daten einzuspeisen sozusagen und die danach extern zu speichern."}, {"timestamp": [1291.24, 1294.6], "text": " Du kannst dir das \u00e4hnlich vorstellen wie bei deinem Computer."}, {"timestamp": [1294.6, 1298.36], "text": " In deinen RAM werden Dinge extern abgespeichert,"}, {"timestamp": [1298.36, 1302.0], "text": " die kann sich Chatshipp-IT dann immer wieder und wieder abrufen,"}, {"timestamp": [1302.0, 1309.28], "text": " sie werden nur ins Ged\u00e4chtnis geholt, wieder eingespeist und so weiter. Das wirft hier andauernd einen Kreis auf, der"}, {"timestamp": [1309.28, 1314.48], "text": " einigerma\u00dfen funktioniert leider nicht so gut. Wie gesagt, deshalb ist die bessere"}, {"timestamp": [1314.48, 1318.88], "text": " L\u00f6sung, wenn wir ganz einfach dieses sparse priming representation verwenden,"}, {"timestamp": [1318.88, 1323.6], "text": " so wie es uns der gute alte David Shapiro zusammengestellt hat. Das ist"}, {"timestamp": [1323.6, 1326.0], "text": " erstens ziemlich kompliziert und zweitens"}, {"timestamp": [1326.0, 1331.16], "text": " nicht wirklich n\u00f6tig und drittens wird h\u00f6chstwahrscheinlich das ganze abgel\u00f6st,"}, {"timestamp": [1331.16, 1336.48], "text": " sobald die Large Language Models gro\u00df genug werden. Irgendwann werden wir die Large Language"}, {"timestamp": [1336.48, 1341.84], "text": " Models auf bis zu eine Billion Token erh\u00f6hen k\u00f6nnen und dann wird all das irrelevant."}, {"timestamp": [1341.84, 1346.0], "text": " Die Studie von Microsoft beweist, dass das m\u00f6glich sein wird."}, {"timestamp": [1346.0, 1350.0], "text": " Bis dahin m\u00fcssen wir leider mit dem Token Limit arbeiten."}, {"timestamp": [1350.0, 1355.0], "text": " Mempt GPT ist eine L\u00f6sung, die halbwegs funktioniert, jedoch nicht wirklich, wirklich gut."}, {"timestamp": [1355.0, 1362.0], "text": " Also, so lange bis wir das Long Net Paper haben, ist wahrscheinlich die einfachste und effektivste L\u00f6sung"}, {"timestamp": [1362.0, 1365.28], "text": " ganz einfach das Sparse Priming Representation."}, {"timestamp": [1365.28, 1370.56], "text": " Das hei\u00dft, wir fassen ganz einfach die Informationen so kurz wie m\u00f6glich zusammen."}, {"timestamp": [1370.56, 1375.12], "text": " Wir fassen sie so zusammen, damit sich JetGPT perfekt versteht"}, {"timestamp": [1375.12, 1379.76], "text": " und wir arbeiten ganz einfach damit und sparen uns dieses Ding hier."}, {"timestamp": [1379.76, 1384.48], "text": " Lass mich gerne wissen, was du von MemGPT und von Fine-Tuning h\u00e4ltst."}, {"timestamp": [1384.48, 1389.0], "text": " Fine-Tuning hat vielleicht auch den ein oder anderen Anwendungszweck,"}, {"timestamp": [1389.0, 1392.0], "text": " wie zum Beispiel etwas sehr, sehr Spezifisches."}, {"timestamp": [1392.0, 1398.0], "text": " Du hast gesehen, das Schreiben von Twitter-Threads k\u00f6nnte hier mit Fine-Tuning richtig, richtig gut funktionieren."}, {"timestamp": [1398.0, 1404.0], "text": " Aber wenn du gro\u00dfe Dateien haben willst, wenn du Chats GPT an vielen Dingen trainieren willst,"}, {"timestamp": [1404.0, 1405.12], "text": " dann funktioniert"}, {"timestamp": [1405.12, 1410.4], "text": " meiner Meinung nach zumindest auch das Fine Tuning nicht perfekt, weil ihr hier ganz einfach"}, {"timestamp": [1410.4, 1412.6], "text": " das Token Limit in die Quere kommt."}, {"timestamp": [1412.6, 1418.28], "text": " In dem Sinne, wenn du Chatshippity trainieren willst, kopiere einfach mal die Custom Instructions"}, {"timestamp": [1418.28, 1420.44], "text": " und teste das Ganze aus."}, {"timestamp": [1420.44, 1425.6], "text": " Und wenn du dennoch lernen willst, wie du einen Chatbot machst, dann sieh dir das Video"}, {"timestamp": [1425.6, 1425.88], "text": " an."}, {"timestamp": [1424.16, 1429.84], "text": " Und wenn du dennoch lernen willst, wie du einen Chatbot machst, dann sieh dir das Video an."}]}