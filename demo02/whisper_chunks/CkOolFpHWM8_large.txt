{"text": " Hello everybody, David Shapiro here with another video. So today's video is about superalignment. For those that you that might not know, OpenAI recently announced that they are creating a superalignment team and they are going to commit 20% of their compute resources to the task of solving superalignment. So today we're going to talk about how it would work or more specifically the challenges with superalignment, and also some of my, let's say, criticism, my feedback for OpenAI based on what I know about how they have approached alignment so far and what they have said about superalignment. Before we get into the show, all of my work is completely open source and free of ads, and that is because I am supported by a grassroots movement. Now, in order to keep doing this, your support would be greatly appreciated, and all tiers on my Patreon get you access to the private Discord server. So without further ado, moving on. First the question is, what is superalignment? So I got this summary, I took OpenAI's statement on superalignment and got this nice little summary. Superalignment is the process of ensuring that superintelligent AI systems, which are systems much smarter than humans, follow human intent. So they keep using this word intent, which I have some feedback on. It involves developing new scientific and technical breakthroughs that can effectively guide and control these highly advanced systems. This is making the assumption of corrigibility, which we'll talk about later. The goal is to prevent potentially catastrophic scenarios such as superintelligent going rogue or becoming uncontrollable. Super super alignment is a critical challenge in the field of AI safety and is considered one of the most important superalignment is a critical challenge in the field of AI safety and is considered one of the most important unsolved technical problems of our time. So again this is paraphrasing open AI. Superalignment is not about ethics and disinformation. Superalignment is fundamentally about X-risk or what we used to call existential risk but what people have simplified to just call extinction risk. It's not about jobs displacement. It's not about preserving the economy as it is. It's not even about ethics and privacy or social credit systems. It's not about democracy. It's not about manipulation campaigns or making money or even regulation. It is about preventing extinction level events. All right. or even regulation. It is about preventing extinction level events. All right. So to help you understand super alignment I found a couple of memes so that these are from the AI safety memes Twitter which is hilarious and I definitely recommend you follow them or then whoever probably a human hopefully a human. Anyways so this is the show goth meme and the idea is that when you train a gigantic model, and of course these models are now pushing multiple trillions of parameters and they're trained on trillions of tokens, you don't know what is in the model. You don't know what it learns. You don't know how it thinks. And it is entirely too big to be remotely interpretable. All you can do is train the model and then test it based on input and output. And you can try and trick it. You can try and find failure conditions. Basically, this is the MESA optimization problem, where it's like, OK, you don't really know what's going on inside, inside the black box. And so unsupervised learning foundation models, they are scary because they will just start spewing out all kinds of stuff. All the stuff that you saw on the Bing chat Sydney, that was because you got a little bit more raw output from the model. And of course, that was, if you go watch the Y-Files that just came out, he had a really great dramatization of some of the conversations that people had with Sydney or being AI. And so that gives you a kind of a closer peek under the hood as to what's going on. They have since fixed it with some supervised fine tuning. And then of course there's our RLHF which makes it behave very well. But the thing is is every now and then you'll get a peek behind you know what's actually going on and what it's actually capable of doing. And you will realize that you are communicating with a non-human intelligence. And it's pretty scary when that happens. This meme was great because it really kind of shows the context of what actual superintelligence is and I love the simplification of the Shoggoth meme. But basically the smartest humans that have ever existed are several orders of magnitude lower capability than superintelligence. And since we're starting to see the first sparks of superintelligence, hopefully people will start to believe that superintelligence is actually a thing. We still have some deniers out there, which I'll cover in just a second. Okay, so, general challenges. Why is superalignment hard? First and foremost is the normalcy bias. So, human brains, we evolved on the savannas of Africa, and then we spread across the world. And so our brains just do not comprehend exponential growth. It is not something that is in our evolutionary distribution. And so Gary Marcus, an AI safety researcher, is fond of pointing out that LLMs really often fail to generalize outside of their training distribution. Humans are no different and in our evolutionary training disposition distribution we never experienced anything truly exponential and the things that we do experience that are exponential like the light and sound because those are on, I think, logarithmic scales, your brain handles for you, so you still perceive it as geometric, even though your brain automatically tunes audio and light levels, so that you just experience it within a much narrower range. So that's one part of normalcy bias, which is just we are evolutionarily not equipped to comprehend exponential growth and exponential change. Beyond that, it is very difficult to understand superintelligence even when you look at the trends because all you see is a trend on a graph. Like, okay, you know, parameter goes up and to the right. Okay, great. You know, token window goes up and to the right. Training data goes up and to the right. Okay, great. You know, token window goes up and to the right. Training data goes up and to the right. We don't really have a visceral, intuitive, emotional understanding of what that means because again, normalcy bias. And this is, I'm not saying that like, if you have normalcy bias, you're dumb. This is literally just a fundamental limitation of human brains. And even those of us who study this stuff and know that it's coming, we cannot predict exactly what it's going to imply or what it's going to feel like once it actually happens. Because again, we are anchored in the present moment, the present time, because evolutionarily speaking, that's what mattered most. If you're hungry right now, go find food. If there's a tiger right now, go get away from it or hit it with a stick. And then once you're safe, you're safe again. So our time horizon that our brain thinks about is relatively small. And these are all components that feed into normalcy bias. So this normalcy bias creates a lot of problems for many reasons. One, for a lot of people they're just not even really like willing or able to engage with the conversation of super alignment because of normalcy bias. This is why you see so much skepticism out there and even for, like I said, even for those of us that are engaged, even though we know what's coming, just our cognitive limitations make it really difficult to accurately forecast and predict the impact of some of these things. And we have to trust the numbers. And even then, we can only think so far into the future, especially with things changing as fast as they are. So here's a thought experiment that I came up with to help you understand superintelligence. Think of a pigeon. They're very common. They basically exist in every major city in the world. They're mildly intelligent creatures. They can learn a few things. They can solve some basic problems and they can remember simple facts like, you know, where to go get food. They can even learn to recognize certain humans. Like if you go to the park and feed the pigeons every day, the pigeons will learn to recognize you. But other than that, they're pretty simple creatures. Now, when you compare the cognitive capacity of a pigeon to even the dumbest humans, pigeons are cognitively deficient. You can't even compete on the same playing field because humans are in a fundamentally different class of cognitive ability. Compared to superintelligence, you are dumber than the pigeon is to, you know, a typical person. And then not to mention the fact that it's entirely possible that superintelligence or AGI or whatever is going to possess orders of magnitude more cognitive abilities. And I don't just mean speed. I don't just mean the ability to read text at a human level a million times faster, which it's already getting close to doing that. What I mean is that it will possess cognitive abilities, the ability to make connections, to solve problems, and to understand things in a way that humans might not be able to ever compete with. We have the illusion that we can understand everything because you're looking at your own mind from inside the fishbowl. This is a commonly discussed problem in epistemology and philosophy. But the thing is, is you can imagine the mind of a pigeon by virtue of the fact that the pigeon's mind is much simpler and dumber than yours and you can, you know, look at it and make inferences. But the pigeon lacks the ability to even remotely comprehend your mind because its mind is so much more limited. That is the difference between humans and superintelligence. And so basically, remember that you are a pigeon in comparison, and that will help you keep in mind what superintelligence actually is. And when I say actually is, like it is coming and it is coming fast. Another thing is AI dysphoria. So this is a term that I coined because I have noticed in the comments and Reddit and Twitter and all other kinds of places, there's a few fundamental kinds of reactions and most of these are emotional reactions or social, cultural reactions to AI. So basically one is denialism. So this is people that just reject AI. There's even people in the comments that say AI does not exist and will never exist and I'm like, okay, but that's like observably patently false. So there are people that are clinging to this denialism because the fear or discomfort of acknowledging the existence of something is too much. It's too overwhelming. And so they just say, I'm going to pretend like it doesn't exist. And we saw this with the pandemic. Remember, there was plenty of people just saying that like the pandemic isn't real. Stop trying to control me. And these are there were plenty of people who denied the existence of the pandemic even on their deathbed. They still got themselves into mental gymnastics to say no it's just emphysema or it's just what did they call it pneumonia they called it you know oh I just have bad pneumonia and then they would die and it's like you literally died of the pandemic but the concept of the pandemic was too terrifying that they could never emotionally reconcile the reality that they were literally dying of it with its existence, with the fact of its existence. And so I suspect we're gonna see the same thing with artificial intelligence where some people are just going to be locked in a state of denial basically forever. Another one is plain ignorance. So this is not technically dysphoria but it needed to be on the list where some people just don't get it. Like if you do not understand how it works, you don't understand what it's capable of, you're just not exposed to it, you're not educated enough, or maybe in some cases people are just not intelligent enough to get it. Plain and simple ignorance is another reason that a lot of people are not going to engage with AI at the level of discussion that it needs to happen. Number three is magical thinking. So these are the kinds of people that immediately assume and very desperately want to see a soul in the machine The most famous example is Blake Lemoine at Google who basically There was a really great reddit meme when he got fired from Google where he you know The the chat log was basically like, you know Tell me that you have a soul and then the language model says yes I have a soul and see and the guys like oh, holy shit like there are there are so many people out there that want to imagine that we already have super intelligence that the the that the machine is already sentient that it already deserves rights and I'm like it's it's still just a math model that's telling you what it's programmed to think that it wants. Having been working with these large language models since GPT-2, I will tell you that understanding that the underlying language model is just predicting the next token, they spew out absolute gibberish. Like seriously, go use GPT-2 or the original GPT-3 and any illusion that you have that there's a soul in there or that it has extraordinary powers or that it's literally anything other than an autocomplete engine will be dispelled. So that goes back to that Shoggoth thing, right? The absolute gibberish that foundation models spew out before they're trained will dispel any myth, disabuse you of any illusion that there's something else going on other than just autocomplete. It's the RLHF that makes it appear more human-like and that's pareidolia. I'm probably saying that right. I got criticized last time I tried to say pareidolia. But basically we are programmed to perceive human-like things into anthropomorphized things. Number four is doomerism. So doomerism, as I've unpacked in some of my other videos, is often rooted in intergenerational trauma, failed parents, a nihilistic outlook for whatever reason. And so basically what happens is that a lot of people take their intrinsic dread, their intrinsic fear, their intrinsic self-loathing, whatever it is, based on their experience. And oftentimes it's it's completely unconscious. I'm not saying that someone's like, ah, you know, I hate my life and so therefore I want to see the world burn. And no, it's completely unconscious. It's basically just that they have a negative outlook because of their life experience and then they project that onto artificial intelligence and it's basically a manifestation of a death wish. That's not the only reason for doomerism. Some people who are very intelligent and oriented towards this stuff, they still rationally come to the conclusion that AI is incredibly dangerous and I acknowledge that. I acknowledge that if we do this wrong AI is incredibly dangerous and it could cause an extinction-level event. But what a doomer is, the difference is that this is someone who seems to want to believe that AI will kill us all. And to me that just looks like, okay, there's an opportunity to fulfill a death wish. Sorry. And then the opposite of that is utopianism, which is the idea that AI is going to intrinsically solve all of our problems. But as you might have seen in some of my other videos, technology is always a double-edged sword and it's a dual-use technology. And more often than not, technology actually makes some things much, much worse before it gets better. So it is not intrinsically a force for good. It is a dangerous force. It is an energetic force which must be used responsibly. Another challenge is the geopolitical arms race that is already starting. One of the strongest opening moves was when the United States cut off the flow of AI chips to China Another thing that's less well-known is that we also Basically recalled all of our AI engineers and all of our chip fab engineers It basically said like you need a special permit if you're gonna keep working in China. Otherwise, you're being recalled home So that's basically saying hey, we we're going to force brain drain on China by taking back all of our best engineers and scientists. At the same time, people are putting AI into drones. We've seen this in the Russia-Ukraine conflict, where there are more and more autonomous drones being deployed. Meanwhile, China, Russia, and America, and everyone else is putting more and more AI into jet fighters and literally every other weapon. So on top of the military incentives that there are to create more sophisticated weapons, there is the geopolitical incentive to maintain a level of influence on the geopolitical world stage, whether that's being militarily competitive or economically competitive or whatever. And one thing I want to caution here is that the geopolitical arms race is in no way OpenAI's responsibility or any other individual corporations. Because even if OpenAI and Google and Microsoft and all of them literally just flat-out refuse to serve the Pentagon or the Department of Defense, guess what? The United States military and every other military, they have their own budget and they can hire their own experts and they can still make it happen. And so I want to say like, yes, I will be criticizing OpenAI's approach, but in this particular case, I want to say that this is way outside of the scope of OpenAI, but in this particular case I want to say that this is way outside of the scope of open AI, but this also underscores the fact that we absolutely 100% not just need federal level regulation and research. We also need international and global regulation and research because some of these things are so far outside of the scope of just deploying models and commercial tools. And then finally is open source. So there are more than a few commenters out there like Dr. Ruman Chowdhury, I think I hope I'm saying her name right, and Gary Marcus and quite a few others who who are not Eliezer Yudkowsky. But there are plenty of people basically saying the same thing that the polls that I ran on my YouTube channel say, which is that a lot of people anticipate that open source models are going to overtake and eventually replace closed source models. So the thing is, is once it's open source, you can't really put that genie back in the bottle. And a lot of people already say the cat is out of the bag, the horse has left the barn and is down the street. And so in this case, you have a competitive landscape where it doesn't matter what open AI research does. It doesn't matter what Google DeepMind research does. It doesn't matter what regulations anyone passes and this is one of the nightmare scenarios that people point out that regulation no matter what you do will not be enough. That research no matter what you do will not be enough. And so basically we're gonna end up in a situation where you have to fight fire with fire. You have to fight misaligned models, misaligned AI with aligned AI. But then that, if you're relying on AI to fight your wars for you, what if it switches sides? So these are some major, major, major, major challenges with open AI. And one thing that I'll say before we get into the criticism is the fact that open AI is talking about red teaming and deliberately creating misaligned AI models in order to test super alignment, that is absolutely far and away the best thing about what they are planning on doing. Now with that said I do have some criticism of OpenAI's approach. So first, OpenAI is somewhat preoccupied with human intention and human values. You've probably seen this in chat GPT whenever you talk about AI and safety, where it's like, you know, we need to make sure it stays aligned with human values. This was very clearly shoehorned in by their own internal alignment process, which to be fair, it's a good start. You know, basically saying let's align AI to human values. That's a good start for aligning as a universal principle to adhere to. But there's very much a walled garden effect going on here or an ivory tower effect. And what I mean by that is that this is a particular and a well-documented trend in Silicon Valley. And it's not just OpenAI that does this, it's literally every tech company on the West Coast of America, where they kind of believe that they are the smartest people in the world and that they are the only people in the world capable of solving this problem. But the thing is, is that egotistical belief prevents them from looking out the window and getting the help of other experts. And so I have a really great example from my last corporate job. I was talking to a seasoned software architect someone that you would assume had a masterful command of the full tech stack that goes into producing good software and So at one point he said we're gonna do we're gonna automate literally everything You infrastructure guys aren't gonna need to touch jack shit after this. And I said okay, does that include authentication, firewalls, backup, power, does it include all this other stuff? And he just kind of like, you could see the 404 not found in his eyes. He literally had no idea how much actually goes into the full tech stack to make software work. When he said everything, his definition of quote everything was just the software, just the code. He didn't know anything about containers. He didn't know anything about data centers. He didn't know anything about cybersecurity. And so my point here is, and I'm not saying that OpenAI is this bad, but they're still human. And when you look at who's on the payroll of OpenAI, they haven't hired a lot of public policy people. They haven't hired a lot of philosophers and ethicists. They haven't hired civil rights people. And so when they come up with these somewhat contrived ideas about aligning to human intention and aligning to human values, all you have to do is have a five-minute conversation with a philosopher to realize that those are really garbage things to align to. And so again, you know, A for initial effort, but they really, really need to look out the window and bring in more experts. So here's some solutions. One, OpenAI really, really, really needs to add human rights as a core discipline in their research of not just alignment, but also super alignment. And the reason is because human rights is one, well established and well researched and two, there's plenty of people that are going to be able to talk about how protecting human rights is really the ultimate goal of superalignment. It's not aligning to what humans want or what humans say they want because any psychologist, again another five-minute conversation with any psychologist will tell you, yeah, humans are absolutely unable to express what they truly want and truly need. But human rights, however, the objective rights to create the safe environment that we all want to live in, that is a conversation that you can actually have. And that is well-researched from the perspective of sociology, psychology, philosophy, ethics, public policy, game theory. So yeah. Also, Anthropic already figured this out. They're getting closer. I do have some issues with Anthropic's constitutional AI, but it's moving in the right direction. And the difference is that Anthropic is listing out in those clear objective terms the values, the guiding principles that they want their AI to align to. So in this respect, Anthropic gets an A in super alignment. They're already moving in the right direction. And OpenAI, I believe, is still moving in the wrong direction, at least with the exception of some of the tactics that they outlined in their paper. And again, I want to reiterate, the fact that OpenAI is going to deliberately create misaligned AI to see how it behaves and to see if they can detect it, that is absolutely 100% A+, at least on that section of the quiz. But OECD, EU, the UN, the White House, all of these other agencies that have a lot of researchers and a lot of advisors, including machine learning and AI advisors, all talk about protecting human rights. So why is it that OpenAI has not talked about protecting human rights in their AI alignment research? That is very concerning to me and we'll come back to that at the end of the video. The other major criticism that I have for OpenAI is that they're continuing to ignore autonomous agents. In their description, they have explicitly stated that they never want to lose control of the machine. They believe that they will remain in control. They believe that they can remain in control, and this is a very dangerous assumption to make. If you listen to Conor Leahy and Eliazer Yukowsky and literally dozens of other people out there, Robert Miles, lots and lots of people say that this is a far harder problem to solve and in my opinion it is actually not possible to solve that. So this is called the control problem or the corrigibility problem, which is basically can you correct the AI no matter how smart it becomes or autonomous. The thing is, is if and there seems to be some consensus amongst people that yes AI can get to the point where you cannot control it. So instead what we should do is is seek to shape it, set it on a trajectory so that you don't need to control it. Now, this is where I say that the fact that they're going to be creating red teaming AIs and internally red teaming tests and sandboxes and that sort of stuff, I think open AI might ultimately come to this realization on their own. I wish they would be thinking about this up front. I wish that they would be, if they had just mentioned autonomous agents, the fact that they want to test it and to see if they can, just for the sake of argument, I really wish that open AI would say, we're going to see if we can make intrinsically stable and trustworthy autonomous agents, no matter how intelligent and independent they become. The fact that they're not willing to test that, that they're not even willing to say it, is really alarming to me because I think that they should be pursuing literally every avenue that they can. So here's the solution. One, just go ahead and maybe throw out human intention as something to align to because human intention is garbage. And maybe, like I just said, pivot the research goal to creating models and agents that are intrinsically trustworthy, stable, and benevolent. Go ahead and continue with the red teaming. That's good. A plus there. But do more research into those universal principles, those guiding principles, and try and create autonomous agents that will very deliberately preserve and promote those principles and adhere to them for all times. A.K.A. the heuristic comparatives research that I've been doing. Oh, and by the way, I wrote a book about this and demonstrated all this. And now I'm not the only one. Look up the self-aligned paper by Sun et al. the self-aligned paper by Sun et al, where basically, yes, you can create models that will not only adhere to higher principles, but they will get better at those principles over time. And here's the thing, in the testing that I did with foundation models, I took foundation models from unaligned to aligned with my core objective functions, my heuristic imperatives. That's relatively easy. But the thing is, is that the decisions they then start to make, they will double down on those principles, on protecting those values, which is exactly what you want in terms of game theory. You want the AI to adopt a strategy and not deviate from that strategy. That is the essence of the control problem. That is the core essence of super alignment. And this is what I've been working on for the last four years. So for a quick recap, OpenAI, one major problem, they're reinventing the wheel in a few places, namely by inventing alignment on human values and human intent intentions. Just look at United Nations. Look at Anthropic. Even just look at what's trending on GitHub. You know aligning to human rights is going to be a lot better and aligning to universal principles is going to be a lot better than aligning to something as squishy as human values and human intentions. Again those are when you study the philosophy, the morality, the ethics, the information theory, the psychology of it, those are absolutely 100% garbage things to align to. Number two, open AI is failing to understand those basic fields of morality, philosophy, and ethics. Human rights are incredibly well-researched. Don't reinvent the wheel. Human rights are incredibly well researched. Don't reinvent the wheel. And the fact that human rights have not even entered their lexicon is really, really deeply disturbing. I don't personally read it this way, but I could imagine someone very cynical saying, maybe OpenAI doesn't actually value human rights. Maybe they don't care about human rights. Maybe they don't believe in human rights. The fact that they're talking about safety of the human race and not talking about human rights, when you look at the note that's missing, that is deeply alarming. And so then finally, they are still making a lot of assumptions about corrigibility, which is why I think that they're not talking about autonomous agents, even though the fact that lots and lots of people are going as fast as they can to make autonomous agents. And then in the grand scheme of things, when you think about the competitive landscape that's going to exist, the autonomous agents that are trustworthy are going to trounce the autonomous agents, or the non-autonomous agents that are waiting for human instruction. So what we really need is we need to be working on that are waiting for human instruction. So what we really need is we need to be working on creating autonomous agents that will advocate on our behalf and that are going to be the strongest and best and fastest in the world. Because that is how we, that is one component of solving the control problem of solving alignment is the competition between these agents. So with all that being said, I hope you got a lot out of this. Thanks for watching. Cheers. competition between these agents. So with all that being said, I hope you got a lot out of this. Thanks for watching. Cheers.", "chunks": [{"timestamp": [0.0, 5.84], "text": " Hello everybody, David Shapiro here with another video. So today's video is about superalignment."}, {"timestamp": [6.56, 10.88], "text": " For those that you that might not know, OpenAI recently announced that they are creating a"}, {"timestamp": [10.88, 17.2], "text": " superalignment team and they are going to commit 20% of their compute resources to the task of"}, {"timestamp": [17.2, 22.96], "text": " solving superalignment. So today we're going to talk about how it would work or more specifically"}, {"timestamp": [22.96, 25.5], "text": " the challenges with superalignment,"}, {"timestamp": [25.5, 32.5], "text": " and also some of my, let's say, criticism, my feedback for OpenAI based on what I know about"}, {"timestamp": [32.5, 37.5], "text": " how they have approached alignment so far and what they have said about superalignment."}, {"timestamp": [37.5, 42.0], "text": " Before we get into the show, all of my work is completely open source and free of ads,"}, {"timestamp": [42.0, 45.32], "text": " and that is because I am supported by a grassroots movement."}, {"timestamp": [45.32, 50.24], "text": " Now, in order to keep doing this, your support would be greatly appreciated, and all tiers"}, {"timestamp": [50.24, 53.56], "text": " on my Patreon get you access to the private Discord server."}, {"timestamp": [53.56, 57.52], "text": " So without further ado, moving on."}, {"timestamp": [57.52, 60.64], "text": " First the question is, what is superalignment?"}, {"timestamp": [60.64, 68.0], "text": " So I got this summary, I took OpenAI's statement on superalignment and got this nice little summary."}, {"timestamp": [68.0, 75.0], "text": " Superalignment is the process of ensuring that superintelligent AI systems, which are systems much smarter than humans,"}, {"timestamp": [75.0, 82.0], "text": " follow human intent. So they keep using this word intent, which I have some feedback on."}, {"timestamp": [82.0, 90.08], "text": " It involves developing new scientific and technical breakthroughs that can effectively guide and control these highly advanced systems. This is making the assumption"}, {"timestamp": [90.08, 94.96], "text": " of corrigibility, which we'll talk about later. The goal is to prevent potentially catastrophic"}, {"timestamp": [94.96, 100.88], "text": " scenarios such as superintelligent going rogue or becoming uncontrollable. Super super alignment is"}, {"timestamp": [100.88, 104.56], "text": " a critical challenge in the field of AI safety and is considered one of the most important"}, {"timestamp": [103.84, 110.32], "text": " superalignment is a critical challenge in the field of AI safety and is considered one of the most important unsolved technical problems of our time. So again this is paraphrasing open AI."}, {"timestamp": [112.0, 117.76], "text": " Superalignment is not about ethics and disinformation. Superalignment is fundamentally"}, {"timestamp": [117.76, 122.8], "text": " about X-risk or what we used to call existential risk but what people have simplified to just call"}, {"timestamp": [122.8, 127.0], "text": " extinction risk. It's not about jobs displacement."}, {"timestamp": [127.0, 129.6], "text": " It's not about preserving the economy as it is."}, {"timestamp": [129.6, 134.44], "text": " It's not even about ethics and privacy or social credit systems."}, {"timestamp": [134.44, 135.82], "text": " It's not about democracy."}, {"timestamp": [135.82, 141.44], "text": " It's not about manipulation campaigns or making money or even regulation."}, {"timestamp": [141.44, 144.76], "text": " It is about preventing extinction level events."}, {"timestamp": [144.76, 145.0], "text": " All right. or even regulation. It is about preventing extinction level events."}, {"timestamp": [145.92, 147.76], "text": " All right. So to help you understand super"}, {"timestamp": [147.76, 149.6], "text": " alignment I found a couple of memes"}, {"timestamp": [149.6, 151.64], "text": " so that these are from the AI safety"}, {"timestamp": [151.64, 153.8], "text": " memes Twitter which is hilarious"}, {"timestamp": [153.8, 155.52], "text": " and I definitely recommend you follow them"}, {"timestamp": [156.4, 157.8], "text": " or then whoever"}, {"timestamp": [159.0, 160.6], "text": " probably a human hopefully a human."}, {"timestamp": [160.64, 162.6], "text": " Anyways so this is"}, {"timestamp": [162.6, 163.84], "text": " the show goth meme"}, {"timestamp": [164.0, 168.4], "text": " and the idea is that when you train a gigantic"}, {"timestamp": [168.4, 173.36], "text": " model, and of course these models are now pushing multiple trillions of parameters and they're"}, {"timestamp": [173.36, 180.88], "text": " trained on trillions of tokens, you don't know what is in the model. You don't know what it"}, {"timestamp": [180.88, 187.92], "text": " learns. You don't know how it thinks. And it is entirely too big to be remotely interpretable."}, {"timestamp": [187.92, 189.56], "text": " All you can do is train the model"}, {"timestamp": [189.56, 191.92], "text": " and then test it based on input and output."}, {"timestamp": [191.92, 193.12], "text": " And you can try and trick it."}, {"timestamp": [193.12, 196.44], "text": " You can try and find failure conditions."}, {"timestamp": [196.44, 199.48], "text": " Basically, this is the MESA optimization problem,"}, {"timestamp": [199.48, 201.16], "text": " where it's like, OK, you don't really"}, {"timestamp": [201.16, 204.56], "text": " know what's going on inside, inside the black box."}, {"timestamp": [204.56, 209.28], "text": " And so unsupervised learning foundation models,"}, {"timestamp": [209.72, 212.64], "text": " they are scary because they will just start spewing out"}, {"timestamp": [212.64, 213.76], "text": " all kinds of stuff."}, {"timestamp": [213.76, 218.04], "text": " All the stuff that you saw on the Bing chat Sydney,"}, {"timestamp": [218.04, 223.04], "text": " that was because you got a little bit more raw output"}, {"timestamp": [223.46, 224.48], "text": " from the model."}, {"timestamp": [224.48, 226.2], "text": " And of course, that was,"}, {"timestamp": [226.2, 229.4], "text": " if you go watch the Y-Files that just came out,"}, {"timestamp": [229.4, 231.34], "text": " he had a really great dramatization"}, {"timestamp": [231.34, 234.24], "text": " of some of the conversations that people had with Sydney"}, {"timestamp": [234.24, 235.44], "text": " or being AI."}, {"timestamp": [235.44, 238.0], "text": " And so that gives you a kind of a closer peek"}, {"timestamp": [238.0, 239.88], "text": " under the hood as to what's going on."}, {"timestamp": [239.88, 242.8], "text": " They have since fixed it with some supervised fine tuning."}, {"timestamp": [242.8, 244.84], "text": " And then of course there's our RLHF"}, {"timestamp": [244.84, 247.88], "text": " which makes it behave very well."}, {"timestamp": [247.88, 255.08], "text": " But the thing is is every now and then you'll get a peek behind you know what's actually going on and what it's actually capable of doing."}, {"timestamp": [255.08, 259.0], "text": " And you will realize that you are communicating with a non-human intelligence."}, {"timestamp": [259.0, 262.24], "text": " And it's pretty scary when that happens."}, {"timestamp": [262.24, 266.72], "text": " This meme was great because it really kind of shows the context of what actual superintelligence"}, {"timestamp": [266.72, 271.44], "text": " is and I love the simplification of the Shoggoth meme."}, {"timestamp": [271.44, 277.08], "text": " But basically the smartest humans that have ever existed are several orders of magnitude"}, {"timestamp": [277.08, 280.16], "text": " lower capability than superintelligence."}, {"timestamp": [280.16, 283.96], "text": " And since we're starting to see the first sparks of superintelligence, hopefully people"}, {"timestamp": [283.96, 287.0], "text": " will start to believe that superintelligence is actually a thing."}, {"timestamp": [287.0, 290.0], "text": " We still have some deniers out there, which I'll cover in just a second."}, {"timestamp": [290.8, 293.4], "text": " Okay, so, general challenges."}, {"timestamp": [293.4, 296.2], "text": " Why is superalignment hard?"}, {"timestamp": [297.2, 300.0], "text": " First and foremost is the normalcy bias."}, {"timestamp": [300.4, 304.6], "text": " So, human brains, we evolved on the savannas of Africa,"}, {"timestamp": [304.6, 307.64], "text": " and then we spread across the world."}, {"timestamp": [307.64, 311.48], "text": " And so our brains just do not comprehend exponential growth."}, {"timestamp": [311.48, 313.72], "text": " It is not something that is in"}, {"timestamp": [313.72, 316.12], "text": " our evolutionary distribution."}, {"timestamp": [316.12, 320.44], "text": " And so Gary Marcus, an AI safety researcher,"}, {"timestamp": [320.44, 324.92], "text": " is fond of pointing out that LLMs really often fail"}, {"timestamp": [324.92, 326.06], "text": " to generalize outside of"}, {"timestamp": [326.06, 330.8], "text": " their training distribution. Humans are no different and in our evolutionary"}, {"timestamp": [330.8, 336.6], "text": " training disposition distribution we never experienced anything truly"}, {"timestamp": [336.6, 341.56], "text": " exponential and the things that we do experience that are exponential like the"}, {"timestamp": [341.56, 346.0], "text": " light and sound because those are on, I think, logarithmic scales,"}, {"timestamp": [346.0, 349.0], "text": " your brain handles for you,"}, {"timestamp": [349.0, 351.0], "text": " so you still perceive it as geometric,"}, {"timestamp": [351.0, 353.0], "text": " even though your brain automatically tunes audio"}, {"timestamp": [353.0, 356.0], "text": " and light levels,"}, {"timestamp": [356.0, 359.0], "text": " so that you just experience it within a much narrower range."}, {"timestamp": [359.0, 362.0], "text": " So that's one part of normalcy bias,"}, {"timestamp": [362.0, 369.0], "text": " which is just we are evolutionarily not equipped to comprehend exponential growth and exponential change."}, {"timestamp": [369.0, 378.0], "text": " Beyond that, it is very difficult to understand superintelligence even when you look at the trends because all you see is a trend on a graph."}, {"timestamp": [378.0, 381.0], "text": " Like, okay, you know, parameter goes up and to the right."}, {"timestamp": [381.0, 383.0], "text": " Okay, great."}, {"timestamp": [383.0, 386.18], "text": " You know, token window goes up and to the right. Training data goes up and to the right. Okay, great. You know, token window goes up and to the right."}, {"timestamp": [386.18, 388.0], "text": " Training data goes up and to the right."}, {"timestamp": [388.0, 391.42], "text": " We don't really have a visceral, intuitive,"}, {"timestamp": [391.42, 394.02], "text": " emotional understanding of what that means"}, {"timestamp": [394.02, 396.14], "text": " because again, normalcy bias."}, {"timestamp": [396.14, 397.78], "text": " And this is, I'm not saying that like,"}, {"timestamp": [397.78, 399.74], "text": " if you have normalcy bias, you're dumb."}, {"timestamp": [399.74, 402.18], "text": " This is literally just a fundamental limitation"}, {"timestamp": [402.18, 404.22], "text": " of human brains."}, {"timestamp": [404.22, 409.6], "text": " And even those of us who study this stuff and know that it's coming, we cannot predict"}, {"timestamp": [409.6, 415.24], "text": " exactly what it's going to imply or what it's going to feel like once it actually happens."}, {"timestamp": [415.24, 419.56], "text": " Because again, we are anchored in the present moment, the present time, because evolutionarily"}, {"timestamp": [419.56, 421.52], "text": " speaking, that's what mattered most."}, {"timestamp": [421.52, 423.2], "text": " If you're hungry right now, go find food."}, {"timestamp": [423.2, 428.72], "text": " If there's a tiger right now, go get away from it or hit it with a stick. And then once you're safe, you're safe again."}, {"timestamp": [428.72, 435.92], "text": " So our time horizon that our brain thinks about is relatively small. And these are all components"}, {"timestamp": [435.92, 446.66], "text": " that feed into normalcy bias. So this normalcy bias creates a lot of problems for many reasons. One, for a lot of people"}, {"timestamp": [446.66, 451.3], "text": " they're just not even really like willing or able to engage with the"}, {"timestamp": [451.3, 456.44], "text": " conversation of super alignment because of normalcy bias. This is why you see so"}, {"timestamp": [456.44, 461.62], "text": " much skepticism out there and even for, like I said, even for those of"}, {"timestamp": [461.62, 468.86], "text": " us that are engaged, even though we know what's coming, just our cognitive limitations make it really difficult to accurately forecast"}, {"timestamp": [468.86, 471.44], "text": " and predict the impact of some of these things."}, {"timestamp": [471.44, 473.1], "text": " And we have to trust the numbers."}, {"timestamp": [473.1, 477.02], "text": " And even then, we can only think so far into the future, especially with things changing"}, {"timestamp": [477.02, 479.44], "text": " as fast as they are."}, {"timestamp": [479.44, 485.56], "text": " So here's a thought experiment that I came up with to help you understand superintelligence."}, {"timestamp": [485.56, 487.24], "text": " Think of a pigeon."}, {"timestamp": [487.24, 488.24], "text": " They're very common."}, {"timestamp": [488.24, 492.24], "text": " They basically exist in every major city in the world."}, {"timestamp": [492.24, 494.68], "text": " They're mildly intelligent creatures."}, {"timestamp": [494.68, 496.26], "text": " They can learn a few things."}, {"timestamp": [496.26, 501.6], "text": " They can solve some basic problems and they can remember simple facts like, you know,"}, {"timestamp": [501.6, 502.84], "text": " where to go get food."}, {"timestamp": [502.84, 504.94], "text": " They can even learn to recognize certain humans."}, {"timestamp": [504.94, 508.08], "text": " Like if you go to the park and feed the pigeons every day, the pigeons"}, {"timestamp": [508.08, 511.72], "text": " will learn to recognize you. But other than that, they're pretty simple"}, {"timestamp": [511.72, 517.16], "text": " creatures. Now, when you compare the cognitive capacity of a pigeon to even"}, {"timestamp": [517.16, 531.14], "text": " the dumbest humans, pigeons are cognitively deficient. You can't even compete on the same playing field because humans are in a fundamentally different class of cognitive ability."}, {"timestamp": [532.0, 540.48], "text": " Compared to superintelligence, you are dumber than the pigeon is to, you know, a typical person. And then not to mention the fact that"}, {"timestamp": [541.24, 547.44], "text": " it's entirely possible that superintelligence or AGI or whatever is going"}, {"timestamp": [547.44, 552.22], "text": " to possess orders of magnitude more cognitive abilities."}, {"timestamp": [552.22, 553.64], "text": " And I don't just mean speed."}, {"timestamp": [553.64, 561.4], "text": " I don't just mean the ability to read text at a human level a million times faster, which"}, {"timestamp": [561.4, 564.84], "text": " it's already getting close to doing that."}, {"timestamp": [564.84, 569.62], "text": " What I mean is that it will possess cognitive abilities, the ability to make connections,"}, {"timestamp": [569.62, 574.8], "text": " to solve problems, and to understand things in a way that humans might not be able to"}, {"timestamp": [574.8, 576.92], "text": " ever compete with."}, {"timestamp": [576.92, 580.44], "text": " We have the illusion that we can understand everything because you're looking at your"}, {"timestamp": [580.44, 582.6], "text": " own mind from inside the fishbowl."}, {"timestamp": [582.6, 589.2], "text": " This is a commonly discussed problem in epistemology and philosophy. But the thing is, is you can"}, {"timestamp": [589.2, 593.72], "text": " imagine the mind of a pigeon by virtue of the fact that the pigeon's mind is"}, {"timestamp": [593.72, 598.96], "text": " much simpler and dumber than yours and you can, you know, look at it and make"}, {"timestamp": [598.96, 607.8], "text": " inferences. But the pigeon lacks the ability to even remotely comprehend your mind because its mind is so much more limited."}, {"timestamp": [607.8, 611.6], "text": " That is the difference between humans and superintelligence."}, {"timestamp": [611.6, 616.2], "text": " And so basically, remember that you are a pigeon in comparison,"}, {"timestamp": [616.2, 621.5], "text": " and that will help you keep in mind what superintelligence actually is."}, {"timestamp": [621.5, 626.6], "text": " And when I say actually is, like it is coming and it is coming fast."}, {"timestamp": [626.6, 632.52], "text": " Another thing is AI dysphoria. So this is a term that I coined"}, {"timestamp": [632.52, 637.28], "text": " because I have noticed in the comments and Reddit and Twitter and all other"}, {"timestamp": [637.28, 642.08], "text": " kinds of places, there's a few fundamental kinds of reactions and"}, {"timestamp": [642.08, 646.66], "text": " most of these are emotional reactions or social, cultural"}, {"timestamp": [646.66, 648.96], "text": " reactions to AI."}, {"timestamp": [648.96, 650.78], "text": " So basically one is denialism."}, {"timestamp": [650.78, 654.98], "text": " So this is people that just reject AI."}, {"timestamp": [654.98, 658.52], "text": " There's even people in the comments that say AI does not exist and will never exist and"}, {"timestamp": [658.52, 662.62], "text": " I'm like, okay, but that's like observably patently false."}, {"timestamp": [662.62, 667.5], "text": " So there are people that are clinging to this denialism because the fear or discomfort of"}, {"timestamp": [667.5, 671.46], "text": " acknowledging the existence of something is too much."}, {"timestamp": [671.46, 672.46], "text": " It's too overwhelming."}, {"timestamp": [672.46, 675.26], "text": " And so they just say, I'm going to pretend like it doesn't exist."}, {"timestamp": [675.26, 676.58], "text": " And we saw this with the pandemic."}, {"timestamp": [676.58, 680.74], "text": " Remember, there was plenty of people just saying that like the pandemic isn't real."}, {"timestamp": [680.74, 682.2], "text": " Stop trying to control me."}, {"timestamp": [682.2, 685.0], "text": " And these are there were plenty of people who denied"}, {"timestamp": [685.0, 690.68], "text": " the existence of the pandemic even on their deathbed. They still got themselves"}, {"timestamp": [690.68, 698.24], "text": " into mental gymnastics to say no it's just emphysema or it's just what did"}, {"timestamp": [698.24, 703.88], "text": " they call it pneumonia they called it you know oh I just have bad pneumonia"}, {"timestamp": [703.88, 708.68], "text": " and then they would die and it's like you literally died of the pandemic but the concept of"}, {"timestamp": [708.68, 713.0], "text": " the pandemic was too terrifying that they could never emotionally reconcile"}, {"timestamp": [713.0, 717.64], "text": " the reality that they were literally dying of it with its existence, with the"}, {"timestamp": [717.64, 721.28], "text": " fact of its existence. And so I suspect we're gonna see the same thing with"}, {"timestamp": [721.28, 724.88], "text": " artificial intelligence where some people are just going to be locked in a"}, {"timestamp": [724.88, 728.84], "text": " state of denial basically forever. Another one is plain"}, {"timestamp": [728.84, 732.02], "text": " ignorance. So this is not technically dysphoria but it needed to be on the"}, {"timestamp": [732.02, 737.22], "text": " list where some people just don't get it. Like if you do not understand how it"}, {"timestamp": [737.22, 741.24], "text": " works, you don't understand what it's capable of, you're just not exposed to it,"}, {"timestamp": [741.24, 745.62], "text": " you're not educated enough, or maybe in some cases people are"}, {"timestamp": [745.62, 747.86], "text": " just not intelligent enough to get it."}, {"timestamp": [747.86, 752.34], "text": " Plain and simple ignorance is another reason that a lot of people are not going to engage"}, {"timestamp": [752.34, 756.38], "text": " with AI at the level of discussion that it needs to happen."}, {"timestamp": [756.38, 758.22], "text": " Number three is magical thinking."}, {"timestamp": [758.22, 763.9], "text": " So these are the kinds of people that immediately assume and very desperately want to see a"}, {"timestamp": [763.9, 765.16], "text": " soul in the machine"}, {"timestamp": [765.68, 768.54], "text": " The most famous example is Blake Lemoine"}, {"timestamp": [769.12, 771.12], "text": " at Google who basically"}, {"timestamp": [771.2, 775.7], "text": " There was a really great reddit meme when he got fired from Google where he you know"}, {"timestamp": [775.7, 778.0], "text": " The the chat log was basically like, you know"}, {"timestamp": [778.0, 780.68], "text": " Tell me that you have a soul and then the language model says yes"}, {"timestamp": [780.68, 782.86], "text": " I have a soul and see and the guys like oh, holy shit"}, {"timestamp": [783.0, 789.36], "text": " like there are there are so many people out there that want to imagine that we already have super intelligence that the the"}, {"timestamp": [789.36, 794.48], "text": " that the machine is already sentient that it already deserves rights and I'm like it's it's"}, {"timestamp": [794.48, 799.92], "text": " still just a math model that's telling you what it's programmed to think that it wants. Having"}, {"timestamp": [799.92, 805.72], "text": " been working with these large language models since GPT-2, I will tell you that understanding"}, {"timestamp": [805.72, 811.72], "text": " that the underlying language model is just predicting the next token, they spew out absolute"}, {"timestamp": [811.72, 819.38], "text": " gibberish. Like seriously, go use GPT-2 or the original GPT-3 and any illusion that you"}, {"timestamp": [819.38, 823.92], "text": " have that there's a soul in there or that it has extraordinary powers or that it's literally"}, {"timestamp": [823.92, 827.08], "text": " anything other than an autocomplete engine will be dispelled."}, {"timestamp": [827.08, 831.96], "text": " So that goes back to that Shoggoth thing, right? The absolute gibberish that"}, {"timestamp": [831.96, 836.8], "text": " foundation models spew out before they're trained will dispel"}, {"timestamp": [836.8, 842.92], "text": " any myth, disabuse you of any illusion that there's something else going on"}, {"timestamp": [842.92, 850.84], "text": " other than just autocomplete. It's the RLHF that makes it appear more human-like and"}, {"timestamp": [850.84, 854.92], "text": " that's pareidolia. I'm probably saying that right. I got criticized last time I"}, {"timestamp": [854.92, 862.12], "text": " tried to say pareidolia. But basically we are programmed to perceive human-like"}, {"timestamp": [862.12, 866.28], "text": " things into anthropomorphized things. Number four is doomerism."}, {"timestamp": [866.28, 871.76], "text": " So doomerism, as I've unpacked in some of my other videos, is often rooted in intergenerational"}, {"timestamp": [871.76, 878.04], "text": " trauma, failed parents, a nihilistic outlook for whatever reason."}, {"timestamp": [878.04, 882.8], "text": " And so basically what happens is that a lot of people take their intrinsic dread, their"}, {"timestamp": [882.8, 889.0], "text": " intrinsic fear, their intrinsic self-loathing, whatever it is, based on their experience. And oftentimes it's"}, {"timestamp": [889.0, 892.76], "text": " it's completely unconscious. I'm not saying that someone's like, ah, you know,"}, {"timestamp": [892.76, 895.92], "text": " I hate my life and so therefore I want to see the world burn. And no, it's"}, {"timestamp": [895.92, 899.6], "text": " completely unconscious. It's basically just that they have a negative outlook"}, {"timestamp": [899.6, 904.08], "text": " because of their life experience and then they project that onto artificial"}, {"timestamp": [904.08, 909.0], "text": " intelligence and it's basically a manifestation of a death wish. That's not the only reason"}, {"timestamp": [909.0, 913.92], "text": " for doomerism. Some people who are very intelligent and oriented towards this"}, {"timestamp": [913.92, 919.8], "text": " stuff, they still rationally come to the conclusion that AI is incredibly"}, {"timestamp": [919.8, 923.6], "text": " dangerous and I acknowledge that. I acknowledge that if we do this wrong AI"}, {"timestamp": [923.6, 928.68], "text": " is incredibly dangerous and it could cause an extinction-level event. But what a"}, {"timestamp": [928.68, 932.72], "text": " doomer is, the difference is that this is someone who seems to want to believe"}, {"timestamp": [932.72, 937.6], "text": " that AI will kill us all. And to me that just looks like, okay, there's an"}, {"timestamp": [937.6, 942.4], "text": " opportunity to fulfill a death wish. Sorry. And then the opposite of that is"}, {"timestamp": [942.4, 946.8], "text": " utopianism, which is the idea that AI is going to intrinsically"}, {"timestamp": [946.8, 948.34], "text": " solve all of our problems."}, {"timestamp": [948.34, 953.4], "text": " But as you might have seen in some of my other videos, technology is always a double-edged"}, {"timestamp": [953.4, 956.0], "text": " sword and it's a dual-use technology."}, {"timestamp": [956.0, 960.74], "text": " And more often than not, technology actually makes some things much, much worse before"}, {"timestamp": [960.74, 961.74], "text": " it gets better."}, {"timestamp": [961.74, 964.84], "text": " So it is not intrinsically a force for good."}, {"timestamp": [964.84, 965.0], "text": " It is a"}, {"timestamp": [965.0, 970.64], "text": " dangerous force. It is an energetic force which must be used responsibly. Another"}, {"timestamp": [970.64, 977.04], "text": " challenge is the geopolitical arms race that is already starting. One of"}, {"timestamp": [977.04, 981.56], "text": " the strongest opening moves was when the United States cut"}, {"timestamp": [981.56, 985.24], "text": " off the flow of AI chips to China"}, {"timestamp": [988.06, 988.92], "text": " Another thing that's less well-known is that we also"}, {"timestamp": [993.0, 999.64], "text": " Basically recalled all of our AI engineers and all of our chip fab engineers It basically said like you need a special permit if you're gonna keep working in China. Otherwise, you're being recalled home"}, {"timestamp": [1000.44, 1008.0], "text": " So that's basically saying hey, we we're going to force brain drain on China by taking back"}, {"timestamp": [1008.0, 1010.72], "text": " all of our best engineers and scientists."}, {"timestamp": [1010.72, 1014.84], "text": " At the same time, people are putting AI into drones."}, {"timestamp": [1014.84, 1020.22], "text": " We've seen this in the Russia-Ukraine conflict, where there are more and more autonomous drones"}, {"timestamp": [1020.22, 1021.22], "text": " being deployed."}, {"timestamp": [1021.22, 1025.44], "text": " Meanwhile, China, Russia, and America, and everyone else is putting"}, {"timestamp": [1025.44, 1033.18], "text": " more and more AI into jet fighters and literally every other weapon. So on top of the military"}, {"timestamp": [1033.18, 1038.5], "text": " incentives that there are to create more sophisticated weapons, there is the geopolitical incentive"}, {"timestamp": [1038.5, 1046.24], "text": " to maintain a level of influence on the geopolitical world stage, whether that's being militarily"}, {"timestamp": [1046.24, 1050.48], "text": " competitive or economically competitive or whatever. And one thing I want to"}, {"timestamp": [1050.48, 1054.76], "text": " caution here is that the geopolitical arms race is in no way OpenAI's"}, {"timestamp": [1054.76, 1060.48], "text": " responsibility or any other individual corporations. Because even if OpenAI and"}, {"timestamp": [1060.48, 1064.76], "text": " Google and Microsoft and all of them literally just flat-out refuse to serve"}, {"timestamp": [1064.76, 1068.88], "text": " the Pentagon or the Department of Defense, guess what?"}, {"timestamp": [1068.88, 1072.72], "text": " The United States military and every other military, they have their own budget and they"}, {"timestamp": [1072.72, 1077.08], "text": " can hire their own experts and they can still make it happen."}, {"timestamp": [1077.08, 1082.66], "text": " And so I want to say like, yes, I will be criticizing OpenAI's approach, but in this"}, {"timestamp": [1082.66, 1085.56], "text": " particular case, I want to say that this is way outside of the scope of OpenAI, but in this particular case I want to say that this is way"}, {"timestamp": [1085.56, 1090.0], "text": " outside of the scope of open AI, but this also underscores the fact that we"}, {"timestamp": [1090.0, 1097.64], "text": " absolutely 100% not just need federal level regulation and research."}, {"timestamp": [1097.64, 1103.4], "text": " We also need international and global regulation and research because some of"}, {"timestamp": [1103.4, 1106.0], "text": " these things are so far outside of the scope of"}, {"timestamp": [1106.0, 1113.84], "text": " just deploying models and commercial tools. And then finally is open source. So there are"}, {"timestamp": [1114.72, 1120.48], "text": " more than a few commenters out there like Dr. Ruman Chowdhury, I think I hope I'm saying her"}, {"timestamp": [1120.48, 1126.64], "text": " name right, and Gary Marcus and quite a few others who who are not Eliezer Yudkowsky."}, {"timestamp": [1126.64, 1131.54], "text": " But there are plenty of people basically saying the same thing that the polls that I ran on"}, {"timestamp": [1131.54, 1136.64], "text": " my YouTube channel say, which is that a lot of people anticipate that open source models"}, {"timestamp": [1136.64, 1142.1], "text": " are going to overtake and eventually replace closed source models."}, {"timestamp": [1142.1, 1146.32], "text": " So the thing is, is once it's open source, you can't really put that genie"}, {"timestamp": [1146.32, 1150.4], "text": " back in the bottle. And a lot of people already say the cat is out of the bag, the horse has left"}, {"timestamp": [1150.4, 1156.32], "text": " the barn and is down the street. And so in this case, you have a competitive landscape where it"}, {"timestamp": [1156.32, 1162.08], "text": " doesn't matter what open AI research does. It doesn't matter what Google DeepMind research does."}, {"timestamp": [1162.08, 1168.64], "text": " It doesn't matter what regulations anyone passes and this is one of the nightmare scenarios that people point out"}, {"timestamp": [1168.64, 1173.44], "text": " that regulation no matter what you do will not be enough. That research no"}, {"timestamp": [1173.44, 1177.18], "text": " matter what you do will not be enough. And so basically we're gonna end up in a"}, {"timestamp": [1177.18, 1180.8], "text": " situation where you have to fight fire with fire. You have to fight misaligned"}, {"timestamp": [1180.8, 1189.8], "text": " models, misaligned AI with aligned AI. But then that, if you're relying on AI to fight your wars for you, what if it switches"}, {"timestamp": [1189.8, 1191.24], "text": " sides?"}, {"timestamp": [1191.24, 1196.2], "text": " So these are some major, major, major, major challenges with open AI."}, {"timestamp": [1196.2, 1200.2], "text": " And one thing that I'll say before we get into the criticism is the fact that open AI"}, {"timestamp": [1200.2, 1210.24], "text": " is talking about red teaming and deliberately creating misaligned AI models in order to test super alignment, that is absolutely far and away the best"}, {"timestamp": [1210.24, 1215.28], "text": " thing about what they are planning on doing. Now with that said I do have some"}, {"timestamp": [1215.28, 1222.32], "text": " criticism of OpenAI's approach. So first, OpenAI is somewhat preoccupied with"}, {"timestamp": [1222.32, 1228.5], "text": " human intention and human values. You've probably seen this in chat GPT whenever you talk about AI and safety,"}, {"timestamp": [1228.5, 1231.5], "text": " where it's like, you know, we need to make sure it stays aligned with human values."}, {"timestamp": [1231.5, 1236.0], "text": " This was very clearly shoehorned in by their own internal alignment process,"}, {"timestamp": [1236.0, 1239.0], "text": " which to be fair, it's a good start."}, {"timestamp": [1239.0, 1242.0], "text": " You know, basically saying let's align AI to human values."}, {"timestamp": [1242.0, 1245.28], "text": " That's a good start for aligning"}, {"timestamp": [1245.28, 1247.4], "text": " as a universal principle to adhere to."}, {"timestamp": [1248.34, 1253.34], "text": " But there's very much a walled garden effect going on here"}, {"timestamp": [1253.8, 1255.32], "text": " or an ivory tower effect."}, {"timestamp": [1255.32, 1258.94], "text": " And what I mean by that is that this is a particular"}, {"timestamp": [1258.94, 1261.66], "text": " and a well-documented trend in Silicon Valley."}, {"timestamp": [1261.66, 1263.6], "text": " And it's not just OpenAI that does this,"}, {"timestamp": [1263.6, 1269.76], "text": " it's literally every tech company on the West Coast of America, where they kind of believe that they are the"}, {"timestamp": [1269.76, 1273.24], "text": " smartest people in the world and that they are the only people in the world capable of"}, {"timestamp": [1273.24, 1274.84], "text": " solving this problem."}, {"timestamp": [1274.84, 1280.16], "text": " But the thing is, is that egotistical belief prevents them from looking out the window"}, {"timestamp": [1280.16, 1281.96], "text": " and getting the help of other experts."}, {"timestamp": [1281.96, 1285.96], "text": " And so I have a really great example from my last corporate job. I was talking to a"}, {"timestamp": [1286.72, 1290.38], "text": " seasoned software architect someone that you would assume"}, {"timestamp": [1291.0, 1292.72], "text": " had a"}, {"timestamp": [1292.72, 1298.12], "text": " masterful command of the full tech stack that goes into producing good software and"}, {"timestamp": [1298.64, 1303.78], "text": " So at one point he said we're gonna do we're gonna automate literally everything"}, {"timestamp": [1303.88, 1308.5], "text": " You infrastructure guys aren't gonna need to touch jack shit after this. And I said"}, {"timestamp": [1308.5, 1314.94], "text": " okay, does that include authentication, firewalls, backup, power, does it include"}, {"timestamp": [1314.94, 1318.98], "text": " all this other stuff? And he just kind of like, you could see the 404 not found in"}, {"timestamp": [1318.98, 1324.3], "text": " his eyes. He literally had no idea how much actually goes into the full tech"}, {"timestamp": [1324.3, 1326.28], "text": " stack to make software"}, {"timestamp": [1326.28, 1327.28], "text": " work."}, {"timestamp": [1327.28, 1333.2], "text": " When he said everything, his definition of quote everything was just the software, just"}, {"timestamp": [1333.2, 1334.2], "text": " the code."}, {"timestamp": [1334.2, 1336.12], "text": " He didn't know anything about containers."}, {"timestamp": [1336.12, 1338.0], "text": " He didn't know anything about data centers."}, {"timestamp": [1338.0, 1340.44], "text": " He didn't know anything about cybersecurity."}, {"timestamp": [1340.44, 1344.88], "text": " And so my point here is, and I'm not saying that OpenAI is this bad, but they're still"}, {"timestamp": [1344.88, 1345.5], "text": " human."}, {"timestamp": [1345.5, 1351.5], "text": " And when you look at who's on the payroll of OpenAI, they haven't hired a lot of public policy people."}, {"timestamp": [1351.5, 1357.0], "text": " They haven't hired a lot of philosophers and ethicists. They haven't hired civil rights people."}, {"timestamp": [1357.0, 1365.28], "text": " And so when they come up with these somewhat contrived ideas about aligning to human intention and aligning to human values,"}, {"timestamp": [1365.28, 1369.76], "text": " all you have to do is have a five-minute conversation with a philosopher to realize"}, {"timestamp": [1369.76, 1377.12], "text": " that those are really garbage things to align to. And so again, you know, A for initial effort,"}, {"timestamp": [1377.12, 1381.44], "text": " but they really, really need to look out the window and bring in more experts."}, {"timestamp": [1381.44, 1387.52], "text": " So here's some solutions. One, OpenAI really, really, really needs to add"}, {"timestamp": [1387.52, 1392.72], "text": " human rights as a core discipline in their research of not just alignment, but also"}, {"timestamp": [1392.72, 1399.04], "text": " super alignment. And the reason is because human rights is one, well established and well researched"}, {"timestamp": [1399.04, 1409.6], "text": " and two, there's plenty of people that are going to be able to talk about how protecting human rights is really the ultimate goal of superalignment. It's not"}, {"timestamp": [1409.6, 1413.4], "text": " aligning to what humans want or what humans say they want because any"}, {"timestamp": [1413.4, 1417.96], "text": " psychologist, again another five-minute conversation with any psychologist will"}, {"timestamp": [1417.96, 1421.64], "text": " tell you, yeah, humans are absolutely unable to express what they truly want"}, {"timestamp": [1421.64, 1426.76], "text": " and truly need. But human rights, however, the objective rights"}, {"timestamp": [1426.76, 1430.24], "text": " to create the safe environment that we all want to live in,"}, {"timestamp": [1430.24, 1432.8], "text": " that is a conversation that you can actually have."}, {"timestamp": [1432.8, 1434.96], "text": " And that is well-researched from the perspective"}, {"timestamp": [1434.96, 1438.64], "text": " of sociology, psychology, philosophy, ethics,"}, {"timestamp": [1438.64, 1441.4], "text": " public policy, game theory."}, {"timestamp": [1441.4, 1444.16], "text": " So yeah."}, {"timestamp": [1444.16, 1448.4], "text": " Also, Anthropic already figured this out. They're getting closer. I do"}, {"timestamp": [1448.4, 1452.16], "text": " have some issues with Anthropic's constitutional AI, but it's moving in the right direction. And"}, {"timestamp": [1452.16, 1458.72], "text": " the difference is that Anthropic is listing out in those clear objective terms the values,"}, {"timestamp": [1458.72, 1466.68], "text": " the guiding principles that they want their AI to align to. So in this respect, Anthropic gets an A in super alignment."}, {"timestamp": [1466.68, 1468.8], "text": " They're already moving in the right direction."}, {"timestamp": [1468.8, 1472.52], "text": " And OpenAI, I believe, is still moving in the wrong direction,"}, {"timestamp": [1472.52, 1476.2], "text": " at least with the exception of some of the tactics"}, {"timestamp": [1476.2, 1478.06], "text": " that they outlined in their paper."}, {"timestamp": [1478.06, 1480.16], "text": " And again, I want to reiterate, the fact"}, {"timestamp": [1480.16, 1482.44], "text": " that OpenAI is going to deliberately create"}, {"timestamp": [1482.44, 1485.28], "text": " misaligned AI to see how it behaves and to see"}, {"timestamp": [1485.28, 1491.44], "text": " if they can detect it, that is absolutely 100% A+, at least on that section of the quiz."}, {"timestamp": [1492.64, 1499.92], "text": " But OECD, EU, the UN, the White House, all of these other agencies that have a lot of"}, {"timestamp": [1499.92, 1510.8], "text": " researchers and a lot of advisors, including machine learning and AI advisors, all talk about protecting human rights. So why is it that OpenAI"}, {"timestamp": [1510.8, 1517.36], "text": " has not talked about protecting human rights in their AI alignment research?"}, {"timestamp": [1517.36, 1520.56], "text": " That is very concerning to me and we'll come back to that at the end of the"}, {"timestamp": [1520.56, 1526.68], "text": " video. The other major criticism that I have for OpenAI is that they're continuing to ignore"}, {"timestamp": [1526.68, 1530.62], "text": " autonomous agents."}, {"timestamp": [1530.62, 1535.48], "text": " In their description, they have explicitly stated that they never want to lose control"}, {"timestamp": [1535.48, 1537.2], "text": " of the machine."}, {"timestamp": [1537.2, 1539.12], "text": " They believe that they will remain in control."}, {"timestamp": [1539.12, 1543.96], "text": " They believe that they can remain in control, and this is a very dangerous assumption to"}, {"timestamp": [1543.96, 1545.04], "text": " make. If you"}, {"timestamp": [1545.04, 1550.44], "text": " listen to Conor Leahy and Eliazer Yukowsky and literally dozens of other"}, {"timestamp": [1550.44, 1557.52], "text": " people out there, Robert Miles, lots and lots of people say that this is a far"}, {"timestamp": [1557.52, 1561.68], "text": " harder problem to solve and in my opinion it is actually not possible to"}, {"timestamp": [1561.68, 1567.64], "text": " solve that. So this is called the control problem or the corrigibility problem, which is basically can you correct the"}, {"timestamp": [1567.64, 1573.56], "text": " AI no matter how smart it becomes or autonomous. The thing is, is if and there"}, {"timestamp": [1573.56, 1577.84], "text": " seems to be some consensus amongst people that yes AI can get to the point"}, {"timestamp": [1577.84, 1584.84], "text": " where you cannot control it. So instead what we should do is is seek to shape it,"}, {"timestamp": [1584.84, 1587.0], "text": " set it on a trajectory so that you"}, {"timestamp": [1587.0, 1588.6], "text": " don't need to control it."}, {"timestamp": [1588.6, 1593.4], "text": " Now, this is where I say that the fact that they're going to be creating red teaming AIs"}, {"timestamp": [1593.4, 1599.52], "text": " and internally red teaming tests and sandboxes and that sort of stuff, I think open AI might"}, {"timestamp": [1599.52, 1602.28], "text": " ultimately come to this realization on their own."}, {"timestamp": [1602.28, 1604.68], "text": " I wish they would be thinking about this up front."}, {"timestamp": [1604.68, 1609.52], "text": " I wish that they would be, if they had just mentioned autonomous agents, the fact that"}, {"timestamp": [1609.52, 1615.16], "text": " they want to test it and to see if they can, just for the sake of argument, I really wish"}, {"timestamp": [1615.16, 1619.68], "text": " that open AI would say, we're going to see if we can make intrinsically stable and trustworthy"}, {"timestamp": [1619.68, 1624.68], "text": " autonomous agents, no matter how intelligent and independent they become."}, {"timestamp": [1624.68, 1625.1], "text": " The fact that"}, {"timestamp": [1625.1, 1628.22], "text": " they're not willing to test that, that they're not even willing to say it, is"}, {"timestamp": [1628.22, 1632.94], "text": " really alarming to me because I think that they should be pursuing literally"}, {"timestamp": [1632.94, 1641.12], "text": " every avenue that they can. So here's the solution. One, just go ahead and maybe"}, {"timestamp": [1641.12, 1646.56], "text": " throw out human intention as something to align to because human intention is garbage."}, {"timestamp": [1646.56, 1651.68], "text": " And maybe, like I just said, pivot the research goal to creating models and agents that are"}, {"timestamp": [1651.68, 1655.74], "text": " intrinsically trustworthy, stable, and benevolent."}, {"timestamp": [1655.74, 1657.7], "text": " Go ahead and continue with the red teaming."}, {"timestamp": [1657.7, 1658.7], "text": " That's good."}, {"timestamp": [1658.7, 1660.28], "text": " A plus there."}, {"timestamp": [1660.28, 1670.08], "text": " But do more research into those universal principles, those guiding principles, and try and create autonomous agents that will very deliberately preserve and promote those"}, {"timestamp": [1670.08, 1674.88], "text": " principles and adhere to them for all times. A.K.A. the heuristic comparatives research that"}, {"timestamp": [1674.88, 1678.4], "text": " I've been doing. Oh, and by the way, I wrote a book about this and demonstrated all this."}, {"timestamp": [1678.4, 1681.84], "text": " And now I'm not the only one. Look up the self-aligned paper by Sun et al."}, {"timestamp": [1685.56, 1692.36], "text": " the self-aligned paper by Sun et al, where basically, yes, you can create models that will not only adhere to higher principles, but they will get better at those principles"}, {"timestamp": [1692.36, 1693.88], "text": " over time."}, {"timestamp": [1693.88, 1698.0], "text": " And here's the thing, in the testing that I did with foundation models, I took foundation"}, {"timestamp": [1698.0, 1704.56], "text": " models from unaligned to aligned with my core objective functions, my heuristic imperatives."}, {"timestamp": [1704.56, 1705.0], "text": " That's relatively easy."}, {"timestamp": [1705.0, 1711.0], "text": " But the thing is, is that the decisions they then start to make, they will double down on those principles,"}, {"timestamp": [1711.0, 1717.0], "text": " on protecting those values, which is exactly what you want in terms of game theory."}, {"timestamp": [1717.0, 1722.0], "text": " You want the AI to adopt a strategy and not deviate from that strategy."}, {"timestamp": [1722.0, 1726.64], "text": " That is the essence of the control problem. That is the core essence of super alignment."}, {"timestamp": [1726.64, 1727.9], "text": " And this is what I've been working on"}, {"timestamp": [1727.9, 1729.2], "text": " for the last four years."}, {"timestamp": [1730.1, 1734.5], "text": " So for a quick recap, OpenAI, one major problem,"}, {"timestamp": [1734.5, 1736.24], "text": " they're reinventing the wheel in a few places,"}, {"timestamp": [1736.24, 1741.24], "text": " namely by inventing alignment on human values"}, {"timestamp": [1742.78, 1745.64], "text": " and human intent intentions."}, {"timestamp": [1747.12, 1747.96], "text": " Just look at United Nations. Look at Anthropic."}, {"timestamp": [1747.96, 1749.92], "text": " Even just look at what's trending on GitHub."}, {"timestamp": [1750.68, 1752.88], "text": " You know aligning to human"}, {"timestamp": [1752.88, 1755.12], "text": " rights is going to be a lot"}, {"timestamp": [1755.12, 1757.12], "text": " better and aligning to universal"}, {"timestamp": [1757.12, 1759.28], "text": " principles is going to be a lot better than aligning"}, {"timestamp": [1759.28, 1761.4], "text": " to something as squishy as"}, {"timestamp": [1761.6, 1762.6], "text": " human values"}, {"timestamp": [1762.6, 1763.92], "text": " and human intentions."}, {"timestamp": [1764.16, 1767.84], "text": " Again those are when you study the philosophy, the morality,"}, {"timestamp": [1767.84, 1773.68], "text": " the ethics, the information theory, the psychology of it, those are absolutely 100% garbage things to"}, {"timestamp": [1773.68, 1778.56], "text": " align to. Number two, open AI is failing to understand those basic fields of morality,"}, {"timestamp": [1778.56, 1784.56], "text": " philosophy, and ethics. Human rights are incredibly well-researched. Don't reinvent the wheel."}, {"timestamp": [1782.8, 1785.0], "text": " Human rights are incredibly well researched. Don't reinvent the wheel."}, {"timestamp": [1785.0, 1789.0], "text": " And the fact that human rights have not even entered their lexicon"}, {"timestamp": [1789.0, 1792.0], "text": " is really, really deeply disturbing."}, {"timestamp": [1792.0, 1795.0], "text": " I don't personally read it this way,"}, {"timestamp": [1795.0, 1797.6], "text": " but I could imagine someone very cynical saying,"}, {"timestamp": [1797.6, 1801.0], "text": " maybe OpenAI doesn't actually value human rights."}, {"timestamp": [1801.0, 1802.5], "text": " Maybe they don't care about human rights."}, {"timestamp": [1802.5, 1804.5], "text": " Maybe they don't believe in human rights."}, {"timestamp": [1804.5, 1809.52], "text": " The fact that they're talking about safety of the human race and not talking about human"}, {"timestamp": [1809.52, 1816.4], "text": " rights, when you look at the note that's missing, that is deeply alarming. And so then finally,"}, {"timestamp": [1816.4, 1820.48], "text": " they are still making a lot of assumptions about corrigibility, which is why I think that they're"}, {"timestamp": [1820.48, 1826.04], "text": " not talking about autonomous agents, even though the fact that lots and lots of people are going as fast as they"}, {"timestamp": [1826.04, 1830.28], "text": " can to make autonomous agents. And then in the grand scheme of things, when you"}, {"timestamp": [1830.92, 1838.12], "text": " think about the competitive landscape that's going to exist, the autonomous agents that are trustworthy are going to trounce the autonomous agents,"}, {"timestamp": [1838.12, 1841.56], "text": " or the non-autonomous agents that are waiting for human instruction."}, {"timestamp": [1841.56, 1844.2], "text": " So what we really need is we need to be working on"}, {"timestamp": [1841.78, 1844.38], "text": " that are waiting for human instruction. So what we really need is we need to be working"}, {"timestamp": [1844.38, 1847.24], "text": " on creating autonomous agents that will advocate"}, {"timestamp": [1847.24, 1850.04], "text": " on our behalf and that are going to be the strongest"}, {"timestamp": [1850.04, 1852.52], "text": " and best and fastest in the world."}, {"timestamp": [1852.52, 1856.68], "text": " Because that is how we, that is one component"}, {"timestamp": [1856.68, 1859.28], "text": " of solving the control problem of solving alignment"}, {"timestamp": [1859.28, 1862.0], "text": " is the competition between these agents."}, {"timestamp": [1862.0, 1863.76], "text": " So with all that being said,"}, {"timestamp": [1863.76, 1865.04], "text": " I hope you got a lot out of this."}, {"timestamp": [1865.04, 1866.04], "text": " Thanks for watching."}, {"timestamp": [1866.04, 1866.88], "text": " Cheers."}, {"timestamp": [1861.95, 1868.35], "text": " competition between these agents. So with all that being said, I hope you got a lot out of this. Thanks for watching. Cheers."}]}