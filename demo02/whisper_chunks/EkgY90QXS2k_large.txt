{"text": " Going live. All right, I think we're here. Can you all hear me now? Can you hear me now? Hello? Is this thing on? Okay. It says live in 15 seconds. Okay, we're good. Yay. Okay, cool. Make sure I mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. It says live in 15 seconds. Okay, we're good. Yay! Okay, cool. Make sure I mute myself. Alright so, hello everybody. I'm still figuring out the whole live streaming thing so please bear with me but one thing that y'all said was that you want to see what I'm looking at, which you know, it's a good idea. You don't want to just look at my face it's like let's look at stuff together so I figured I'd follow more or less the same format that I usually do I'm in the cognitive AI lab discord so if you're in the general you can drop questions here or papers so basically what I thought we would do is we would take a look at weekly updates because it's all going really fast. I've got a lot of feedback from my recent live streams that y'all really liked the interactive aspect of asking questions and like, Dave, what does this mean? So please feel free to drop good questions in. I'm also in the private Patreon Discord, so drop questions there too. Ok cool. Party time, yeah. Oh, looks like there's a delay of about 15 seconds. Interesting, ok. Cool. Flippy from the Deus Discord, hello Flippy. Alright, cool. So while some questions get spooled up, I figured I'd go through a couple of tools and stuff that I've seen. So someone, I think actually Flippy, this might have been you or someone in the Deus Discord pointed out this tool. So this is basically something that I and others have been like thinking about and talking about. It's basically a hybrid of vector database and knowledge graphs. It's also got a pretty interface. It's worth just scrolling through and reading every bit of this and then experimenting with it. But it will create ... This is one of the coolest things, is it'll create a knowledge graph of all the clusters and stuff, but you can find the gaps, which when you're thinking about autonomous or semi-autonomous AI agents, this is really good because then you can know what you don't know. And what I mean by that is if you're aware of all the things that you know, but you can detect some semantic gaps in your knowledge, that can help you zoom in on the things that you need to go learn about. InfraNotice is the tool, infranotice.com. I can never remember the name of the darn thing. I'm going to blame allergies and say that it's just brain inflammation. That's my excuse and I'm sticking to it. But yeah, so this is a super cool tool. This kind of technology will definitely be part of autonomous AI agents. And this is functionally similar to what I worked on with Remo. Remo is much, much, much simpler though. So I often have some people ask me like, oh hey, can Remo do this and that? And I'm like, no, like Remo was meant to solve one very specific problem. So for memory stuff, I usually point people at Lama Index and ChromaDB. ChromaDB, do I seriously not have that bookmarked? ChromaDB is a vector database that runs just like SQLite. It's pip install chromaDB, you create a local client just like you do SQLite. I tried to create something like this called VDB Lite a few months ago and I quickly realized that I was in way over my head. So I'm glad someone built this. And they just got an $18 million seed round. Holy mackerel. Man, I should have stuck with VDB Lite. I could have had an $18 million seed round. Anyways, maybe I'll do that with Remo. Who knows? So ChromaDB, super simple, man, seriously, $18 million for this one thing. Okay, I'm going to let that go. Llama Index also, so Llama Index, I kind of didn't pay it any attention at first because it's like, okay, that's a silly name. This was clearly just someone's little side project. But if you look at all the types of indices they have, they've got list index, table index, tree index. So Remo is very similar to LlamasTreeIndex, although I will say that I looked through the code and I think that their tree index is kind of basic. I think that my Remo framework does a little bit that theirs doesn't, but I'm not gonna dive into that because I don't know that for certain. I didn't take a super close look at the code. Yeah, so those are some memory storage tools. So let me check on the live stream and see where we're at. Whoops, all right. Oh, wow, we got lots of questions. Okay, cool. God's not dead, rather believe and go to the good place. Oh, wow. We got lots of questions. Okay, cool God's not dead rather believe and go to the good place and interesting. Okay Let's see how to discover new wisdom from LLM that's an interesting question. Um, let's see Let me You people ask questions far too early Let me go over to Discord to see if, yeah, please go ahead and drop some questions. Patreon get first dibs. All right, what do you think is the future of SaaS sales jobs for recruiting agencies? Sales jobs and for recruiting agencies. The sales level is still very human, so sales is not going to change for a while. Ditto for recruiting, although there's a lot of AI in recruiting already where like AI will read your resume and AI will watch a video of you answering questions. You know, more and more of that's coming, but that's about it. Let's see. All right, we got a whole bunch of questions coming in on the Patreon side. So yeah, database, short answer, diversify your job skills. So there's that. Let's see, zoom in a little so you guys can read this. Let me jump back over here real quick. But do you still expect AGI within 18 months with stating they do not want to make models bigger but rather more efficient? Yeah, I think AGI in 18 months is still conservative, honestly. I think that we will have, as people develop the architecture for autonomous AI, I think that we will be able to say that we have AGI by the end of this year. But people will realize that it's like, okay, we have an agent that can do anything, but it's expensive, or it's slow, or it's kind of dumb that kind of thing Let's see check on discord real quick is her about to become reality. Oh, yeah, lots of people are working on You know like AI Companions, they're getting they're gonna get more sophisticated real fast I've actually got a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday, I've got a Ghost in the Shell video planned, I've got a Mass Effect video planned, I've got a DAO and blockchain video planned. So that's all what's coming. Maybe I shouldn't spoil it. Okay, well, whatever. But yeah, so I was thinking about hitting on like her and ex machina and stuff as well Let's see. How can we be certain things are advancing exponentially? So that's a good question James We generally speaking like you can't tell if you have a narrow window but we were joking around the other day and we're pointing out that like a few weeks ago it was like you would reasonably expect a couple of cool AI bits of news per week and now we're at the point where we expect several per day. Now it might come in cycles, it might come in waves, but you know generally speaking this very closely matches what Ray Kurzweil said, or maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago. I think it was Michio. He was describing what it will feel like to approach the singularity. And he said, oh, well, when information is doubling every two years, you don't really feel that on a day-to-day basis. And then when information is doubling every six months, that's fast enough that you're like, oh, hey, this thing that I didn't think would be solved for another couple years was solved this year. And then, but as it ramps up faster and faster, that time keeps halving. And so then, three months after that, you realize, oh wait, we've advanced again. And I think we're right at that three, where six months ago, we're like, oh, this stuff is 10 years away. Six months later, we're like, this stuff is 10 months away. So I think that you could make a good argument that we're, in some respects, we're in the exponential ramp up right now. That being said, some of this information is so big and it's changing so fast it's difficult to measure. We'll actually need AI to measure the rate of papers and tools. All right, so Sif, you said, how do you think AI will impact religion, particularly monotheistic religions? I think it will create a mass crisis of consciousness, which will make the transition period even more chaotic and extreme.\" Yeah, no, sorry Sif, I was getting around to it. Yeah, so I've actually had some interesting internet debates with more conservative and more religious people. Granted, I don't do internet debates anymore, I got that out of my systemanted, I don't do internet debates anymore. I got that out of my system, and I try not to get suckered into it if I can. But one debate that I had many years ago was, if aliens showed up, wouldn't it prove your religion wrong? This was a debate. I wasn't arguing. It was a debate that I observed, and the religious person said, no, why would it? And they rationalized it saying, well, why would God put aliens in the Bible if we wouldn't be able to understand it back then? It's not for us to understand. And so some religious folks do have a really good ability to compartmentalize. And so some some religious folks do have a really good ability to compartmentalize and so like just because you have a like Like a super intelligent machine some people would be like so it doesn't have a soul and that's the end of the discussion So I don't I don't particularly perceive and I'm not saying that this is good or bad, right? I am NOT I don't particularly perceive, and I'm not saying that this is good or bad, right? I am not in the Judeo-Christian faith. My spirituality is other places. I have some, like some of my best friends in my local community and my internet friends are deeply religious, you know, followers of Christ and whatever. And so like, in many cases I don't think it's going to be that big of an issue. Let me jump over to the Patreon. Oh wow, we've got some questions here. Okay, Bakobbizo, sorry if I'm saying your name wrong. What would your advice be to someone just launching an AI startup? Don't. That's a very flippant response, but one, launching a startup is really hard. It's mostly tedium. You can have the best idea in the world and 90% of the work is still going to be... Excuse me, I'm still struggling with allergies. I had a headache earlier. That's part of why I canceled yesterday. Thanks, Jordan. That's... How am I holding up? Actually I'm doing okay. Mostly it's just allergies right now. But anyways, so if you're doing an AI startup, one, if you haven't done a startup before, now is a really bad time to learn because things are going so fast and we're basically having to reinvent stuff as we go, which if that's, that's part of why I burned out is I realized like, okay, I did find it really engaging and really enjoyable, but my pace of things clashed with other people and then like the rabbit hole just keeps getting deeper. So that's kind of the thing. What I always tell people is your startup team, your founder team is most important. Be picky. If you don't have the right team, walk away early. And then also for the folks that I'm talking to is local. Because of the pace of things, you absolutely need to see the people that you work with in person at least on a weekly basis, if not on a daily basis. Because you need to be sitting in the same room, just shooting the breeze, keeping each other updated in real time. Doing it remotely is probably not feasible. Unless you're already a really established team and you're just going to sit in Discord all day or Slack all day. Blake Allen, curious to hear your thoughts on Stanford's hyena hierarchy and how it relates to some of the work you've done with Raven and cognitive architecture. I don't know if I've heard of this one. Let's check it out real quick. Hyena Hierarchy. Let's see. Let's go up to the very top. Hyena Hierarchy towards larger convolutional language models. We're excited to share our latest work on Hyena, a sub-quadratic time layer that has the potential to significantly increase context length in sequence models. Oh, right. I think this is the RNN integration. Yeah, yeah. So in general, how can we close this gap? Yeah. So in general, any individual language model is just like one cortical node. And so, yes, these things will be like better, more efficient like cylinders in an engine, but in order to have a race car, you need the rest of the car, right? And so, and again, I'm kind of really flying off the cuff right here. I'm not sure that I've got this right, but I think, yeah, RWKV. So you're not going to ever get a full cognitive architecture from a single language model. Now, that being said, the big asterisk is when you look at all the studies about GPT-4 that have theory of mind and what I call implied cognition. So implied cognition is that the thing is obviously thinking through problems behind the scenes in a similar way that like humans think through and I don't mean like neurologically like you know subjectively it thinks the way that we do but GP4 can obviously talk itself through like kind of do chain of thought reasoning internally in one shot and so that makes those larger more sophisticated models make your cognitive architecture simpler, but it doesn't get rid of the need for external storage. It doesn't get rid of the need for parallel processing. It doesn't get rid of the need for loops and checks and that sort of stuff. That's my response there. Good question. Emma or AMA, I'm new here and new to this field in general. Found you through Raven videos. Thank you. Regarding personal assistants, is there a reason to create a database of yourself for your future personal assistant to understand you better? So that's actually the purpose of my Remo framework. So Remo is meant to be a hierarchical database of your interactions with an individual agent that will surface particular topics by using not reciprocal, recursive summarization and clustering. So, you take all your raw logs, cluster them, summarize them, do that again, cluster, summarize, cluster, summarize, until you end up with five to 10 parent topics that allow you to drill down. So I wouldn't, don't waste any time doing that manually. Just let it happen naturally through conversation by integrating something like Lama Index Tree or Remo. Do you think we're on track to cure brain diseases like Alzheimer's by 2030? The combination of AlphaFold and mRNA vaccines, I think absolutely. There was something else that I posted on my YouTube recently that it's like another breakthrough is happening. So, you know, I think we're very close to the point where we can halt Alzheimer's. Undoing Alzheimer's might take another little bit of time, but on the other hand, we're at the point where we're getting saltatory leaps. We're getting breakthroughs really fast, so you never know. Let's see. What are your thoughts on the generative agent stuff that has come out recently? It seems like you're pretty ahead of the curve on that stuff and has it solidified or changed the way you think about the concepts from Symphony of Thought? Yeah, so I definitely felt like I was ahead of the curve and what I've been telling people is like I worked for a few years to try and get GPT-3 to do the stuff that 3.5 and 4 can do easily. So I'm just like, alright, whatever. I'm so glad that the rest of the world is just like, oh cool, autonomous agents. And I'm like, great, now I don't have to write any more books about it. So I'm just happy to sit back and watch it go and keep plugging my heuristic comparatives. Good question, Jordan. Also, here, let's check over here. Okay, we got some questions. Let's see. Is Her about to come reality? Yep, we got that one. Interesting video. How long do you think it will be before we start seeing hive mind AI systems in healthcare or the IRS. I know people working on that today. They'll work together for a few different reasons. One, you'll have a division of labor. Oh, so taking a step back. What we mean when we say hive mind AI is where you have multiple cognitive agents or autonomous agents or is it not working? Is it working? I hope it's working. It looks like it's working. So yeah, so basically it was describing this to a podcast host that I'm going to be featured on coming up, was kind of what I predict right now is before too long, you're going to have multiple cognitive agents running on your phone, on your car, on your home PC, in your smart home devices. And so you're basically gonna have a fleet of small cognitive agents working for you at all times. Then you're gonna have the same thing at like your company, right? Every employee or every department is gonna have multiple cognitive agents all collaborating at all times. And you're gonna have this kind of tiered hierarchy where it's like there's the personal, there's the family unit, there's the corporate unit, there's the town, there's the federal government, the state government, global government. And I think that the way that they're all going to work together because security is so critical here is that it's going to be using blockchain technology and distributed autonomous organizations. So that's the long story short is that's what's going to happen. Let's see, you may have already covered this but in case you haven't, any thoughts on Metzinger's artificial suffering, an argument for a global moratorium on synthetic phenomenology. I am tangentially familiar with this, but I have my own opinions on whether or not a machine can suffer. So there's two distinct possibilities. The first possibility is because artificial intelligence is a fundamentally different substrate from humans, it will never be able to suffer. Like, it didn't evolve nerves, it doesn't have pain centers, so on and so forth, it can't feel lonely because it's not a social entity, so on and so forth. Now that being said, language, the acquisition of language is actually critical for the development of human consciousness. So, for instance, Bruce Willis, who has aphasia. Aphasia means that your ability to use language gets destroyed. Aphasia actually kind of erases your sense of consciousness. And then, in the case of feral children, feral children, when they, some of them who have learned language, talked about how their consciousness and their understanding of time and themselves changed as they learned language. So if you extrapolate that to language models, it is possible that there is something informationally almost magical about the acquisition of language that confers consciousness that confers subjective experience of being so that could be that Language models are actually the first AI that have subjective experience that have a coherent sense of being and this is so there are There are some religious and spiritual frameworks that kind of discuss stuff like this particularly what's the name of the the the creator deity and Tolkien's world where the fundamental substrate of Reality was music right, but maybe the fundamental substrate of consciousness is actually language So we don't we don't know yet, but it's a possibility. Let's see. So Parkinson, so a follow-up question was, or here, let me check the Patreon real quick. Let's see. Do you think there needs to be another breakthrough for AGI? What is your personal take on this? Do LLMs suffice?\" Yeah, so I would say that our current trajectory, as long as the trend continues, we are on track for AGI. People are going to continue debating about AGI forever though, until the cows come home. Which is why I keep saying like autonomous cognitive entity, ACE, or just autonomous AI. I keep saying like autonomous cognitive entity, ACE or just autonomous AI because you don't need AGI, you don't need some arbitrary magical boogeyman. All you need is an AI system that is self-contained and autonomous enough to be useful or dangerous. Then the question is how many do you have, how fast are they, and how smart are they? They're going to continue to get faster, cheaper, and smarter over time. So it's like, okay, we're there. It's just a matter of how does the trend line ramp up, right? Because it's kind of like when the Wright brothers first created the Wright Flyer, right? You know, it's like, okay, you had to start it by hand and push it down a track, and it flew what, like 200 feet or 300 feet? And people are like, oh, whatever, that won't be useful be useful but then 50 years later we were flying to space right so we're at the beginning of the ramp up of the era of AGI and yeah right now they're like idiotic little toddlers but in a few years they're gonna be like one all over the place and two really powerful good question let me come back over here. You should do a whole episode on aphasia and consciousness. I actually don't know that much about it, but if you're interested in the topic, I recommend Phantoms in the Brain by V.S. Ramachandran and also, what's the name of his other book? It's something like The Pursuit of What Makes Humans Human. Okay. It's something like the pursuit of what makes humans human. Let's see. Parkinson's is a neurodegenerative disease, which I think means that it's autoimmune or it's a defective protein, but also Alzheimer's is a defective protein. While these diseases seem very complicated, the fundamental mechanisms are actually relatively straightforward and I know that there's probably a bunch of researchers that are gonna jump on me for that. But like plaques that accumulate on the brain for Alzheimer's, in most people those plaques are cleared out so then it's just a matter of figuring out like okay why. And then of course there's confounding factors like things like your gut inflammation, microbiome, and other things affect Alzheimer's, but that's because of the gut-brain axis. Again, I don't want to oversimplify because if you look up human metabolic pathways, there's like 200,000 unique proteins and enzymes in the body with literally billions of combinations of reactions. It might sound like I'm oversimplifying, but I'm not saying that it's that simple. I'm just saying that the key mechanism for most diseases is relatively simple once you understand it. And we're getting close to that understanding. I guess that's the short version of what I'm trying to say. Let's see. When you explained why you canceled the OSS Raven project, you mentioned that there were some fundamental things missing. Can you say what was missing and what made you change your mind? My open source Raven project was just before LangChain and AutoGPT and all those other things came out. As those ramped up, I was like, I don't really feel the need to continue. But from a social and organizational perspective, the biggest thing that was missing was gatekeeping. Basically, I created a community that was really good at discussing stuff and not doing stuff. And that's not anyone's fault. If there's anyone to blame, it's me. Just because I was so focused on consensus and not just like, okay if there's anyone to blame, it's me, just because I was so focused on consensus and not just like, okay, let's just get stuff done. And then I see these other folks that are just getting stuff done and I'm like, okay, I'll just pass the torch. Let's see. I think if we gave GPT-4 Scarlet Johansson's voice in a robot body, the masses will begin to realize how close we are to AGI.\" Yeah, that's one way of putting it. Drink some water. Let's see, at what point do creating NPC and using autonomous AI like AutoGPT and the likes become immoral, especially if you put them in games like GTA. I don't know that it intrinsically does, you know, not intrinsically immoral, but certainly with any technology you can do it dangerously. Let's see. I've been curious about the future of entertainment. Oh, sorry, let me jump over to Patreon real quick. Let's see. Do you have an overarching roadmap of how to ensure the successful propagation of the heuristic imperatives? If so, what can we all do to help you get to your milestone? That is a great question, Blake. So you're actually looking at it? So my number one thing is my YouTube channel because I've got enough expertise in IT and systems engineering and enterprise. I've demonstrated enough understanding of language technology and AI and cognitive architecture that I've got at least a little credibility. Certainly if you read all the comments on YouTube, some people don't believe anything that I say, and that's fine. That's the internet for you. But anyway, so basically step one was YouTube. That's why I started my YouTube channel, is because I realized that I needed to propagate my work. Step two is teaching people. And so by teaching people, that's like, I've got a few papers. I've got some co-demonstrations, I work with my Patreon supporters, I work with pretty much anyone who wants to, and then three further disseminations. So like the podcast that I'm coming up on, one of the things that we're going to talk about is alignment and the control problem. We're going to be talking about like Nash equilibrium, game theory, Moloch, that sort of stuff. And so just by having the conversation and propagating the idea, that's like step three. Step four is actually my novel because actually most of what I came up with was in terms of cognitive architecture, heuristic imperatives, was done in part through explorations in fiction. And so over the last four years, what I've done is I do some experiments and that would inspire me and I'd go write more of my novel and then I'd get tired of my novel and do more experiments and I'd go back and forth until one, my novel took on a life of its own but also my research took on a life of its own. But there's a video that came out recently called, let me see if I can find it, it was like, Why We Need Utopia. Here we go. It was our changing climate. So this is a little bit of a, I don't agree with everything that this channel says, but it will make you think. So this video, Why We Need Utopias, actually talks about how valuable stories can be in communicating ideas, because stories are naturally how we communicate philosophy and morals. We don't need, like, philosophy, like the capital P philosophy from from universities, that's backwards. Throughout almost all of human history, we communicate our fears and our desires and our values through stories. And so that's what this video talks about. And so when you have nothing but dystopian cyberpunk stuff, you end up with people like Eliad Zyrgukowsky. Yeah, I'm throwing some shade. But anyways, when that's all that you consume, that's all you think, that's all you feel and that's all you believe. So my novel, which I'm actually just about to finish draft 12 tomorrow morning, I'm writing the last chapter and then I'm polishing it up, will illustrate a lot of stuff, not just the core objective functions or heuristic imperatives. So that was a long-winded answer. Let's see. I think there is, are there key channels this training needs to go into, organizations, governments? go into organizations, governments. I think right now, Blake, it's mostly just a matter of dissemination, but also experimentation. So a lot of people have experimented with incorporating heuristic imperatives into autonomous and semi-autonomous stuff, and most of them aren't sharing it yet, which that's fine. It's their prerogative. But certainly some people have reached out and said like, yeah, this made everything easier. So I'm like, great, just tell your friends. Let's see. Okay. Let's come back over here. Let's see. I've been really curious about the future of entertainment. When we can use AI to generate movies, games, etc. What will the entertainment industry look like? Movie trailers and hyping up big releases for months will be irrelevant when AI can instantly create something. If someone created a movie you didn't like, you just ask your AI to recreate it with an ending or plot more suited to your tastes. What happens with content creators at that point forward? Yeah, so I think that you're onto something. Now that being said, it'll be easier for a lot of people like you and me to create whatever film, TV, music, whatever we want with the help of AI, especially when you look at the text of video, which is improving by leaps and bounds. You know, like I always, my go-to joke is we'll finally get season 2 of Firefly. Who knows, we might get Season 2 of Firefly by the end of this year. That would be great. Now the problem there, it's not really a problem, but just taking that to a logical conclusion, what if you have a million different versions of Season 2 of Firefly? How do you pick which one to watch? You can look at ratings and stuff, but then it also begs the question of like IP, like is 20th Century Fox or whoever owns the IP for Firefly, are they going to sue to have all of them shut down? You know, I like the dude from the movie, You Can't Stop the Signal, Mal. So, I don't know what's going to happen there. But what I do think is that when you look at the fact that people are already using Emma Watson's face for every Mid-Journey prompt and whoever else, I think that the crop of actors that we have today are basically going to be around forever. You're going to be watching Brad Pitt and Jennifer Aniston and Tom Cruise for literally the next several centuries, at least until some actor comes along who's even more compelling and whatever. And that'll be through face cloning, voice cloning, even nerfs, the neural representation, what was it? Radiance fields, neural radiance fields. We'll be able to like copy everyone. Okay, could learning, let me zoom in a little bit, could learning language and triggering consciousness in humans almost replicate the same phase change when seen when induction heads spontaneously form two plus layer models during training. Obviously there's more to humans but perhaps it's the mechanism. Yeah that's kind of what I was mentioning earlier and I wouldn't be surprised if once language models get large enough if we do see some more convergence. That being said, I'm not going to say that that automatically means that it has a subjective experience and that it is suffering but our brains evolved over billions of years to be efficient, basically efficient processors of information. Who's to say that if you have a biomimetic machine that it won't also converge on some of the same properties and behaviors? Let's see. What do you think it will take for the naysayers to get on board? The tone around AI seems to have shifted towards chat GPT and GPT-4 aren't anything special. Oh, you know, that always happens when the new shiny wears off. But the long-term economic impact of chat GPT has not been realized yet. And when chat GPT and GPT-4 are on the ramp up, one, there's going to be a lot of competitors and two, there's going to be incremental improvements and people are going to be like, okay. The title, it's like when you watch the tsunami come in and the water just keeps getting higher and faster for like hours. That's what AI is going to feel like. Except instead of hours it's going to be days and weeks. Let's see. I cannot wait when we can use deep dive tech and have virtual realities. Will it also be possible to take super intelligent animals like dolphins, dogs, parrots, and crows and to deep dive and play with them? I don't know that it would be possible but I certainly think it probably wouldn't be ethical. Now that being said you could have a virtual dolphin that is hyper realistic that you can play with. Fun thought, will AGI want to see more stories from humans as a goal for itself? Oh, so let me plug this. So Elon Musk went on, of all fricking shows, Tucker Carlson, and talked about TruthGPT. So what he said was that TruthGPT would be a maximum truth-seeking AI. Okay, great. But after listening to it in closer detail, I realized what he was talking about was the third heuristic imperative, was to increase its understanding or to maximize its own understanding. So there's actually nothing, that function on its own could lead to some really catastrophic sources, but it's a step in the right direction. And I'm really glad that someone with as big of a platform as Elon Musk is talking about maximize understanding or increase understanding. So that being said, one of the things that he said in that interview was that since humans are part of the universe, an AI that is curious about the universe will intrinsically be curious about us as well. Now that being said, humans sometimes do experiments on things that we're curious about, so maybe that's not the best thing. In my book, Benevolent by Design, I talk about why you must include suffering or something like suffering in the objective functions of an AI, because there's three dispositions that an AI can have towards suffering. One is it can ignore it altogether. If Elon Musk gets his current idea, which is just maximize for truth, that is an agent that ignores suffering. It doesn't care one way or another. Then you can have one that increases suffering, that deliberately increases suffering, and we absolutely don't want that. So that leaves, by process of elimination, you want an AI that reduces suffering. It's really that simple. Now, that being said, I do agree with Elon that creating a curious agent is a good idea because it'll want to know about us. And if you exterminate humans, you have a harder time learning about them. So let's see, do you... let me check on Patreon real quick. Do do do do do do do do be the Rupert Murdoch of AI? Okay, I don't like that question. Lance, why you got to do this to me? All right, Zadre, I'm not sure how to pronounce, or Hadre, okay. How do you envision the role of AI in healthcare, particularly in areas like diagnostics and personalized medicine? What are some of the challenges and opportunities in this field? Well, so there was that Stanford doctor who already went on record saying that CHAT-GPT-4 has better clinical judgment than many doctors. So that is just a start, right? That's like starting point day one. What happens when CHAT GPT 5, 6, and 7 come out that have better clinical judgment than 99.999% of all doctors on the planet, right? It doesn't make sense to go to a human doctor anymore, right? If the machine that costs $20 a month to run is better than all human doctors, why go to a human doctor? Now that being said, there's probably going to be approvals and downsides and gaps and then there's still also the interface with the patient and you have to have phlebotomists and nurses and physician's assistants to administer things, to administer tests. You still need a lot of humans in there to be the interface between the human and the machine. But that being said, I think that we will get to a point very quickly where the quality of care and the speed of care and the efficiency of care are going to go through the roof real fast. That's what I'm hoping at least. All right, jumping back over to Cognitive AI Lab. We got some new questions. Looks like this was the same question. Sorry, I missed you over there. Where are we? All right, there's the deep dive. Do you think there is any major leap missing to make truly practical autonomous agents? So for example, one who runs a part of your business, serves as general assistant, etc., etc. No, there are countless hundreds if not thousands or even millions of people working on semi-autonomous and autonomous corporate applications today, right now. That being said, there's no breakthroughs that are needed, but there are still problems to be solved. That's why like Remo, the memory systems and then standard practices like I wrote in Symphony of Thought and in other places, my Atom framework, is once something is autonomous or semi-autonomous, how does it keep track of projects and tasks? That's something that people are working on. People are working on it real fast. That's coming really quick. Let's see. Nathan says, I've been taking screenshots of when friends and family make fun of my hot takes so I have the receipts. I would say that I'm above being that petty but yeah, thank you for keeping receipts. Let's see, maybe directors will just design their perfect actors for each role. So one thing that's going to happen is actually, so this is going back to like entertainment. I think that the next big generation of entertainment is actually going to be holodeck style VR stories, where nothing is scripted, where instead it's like, you know, basically you design a holodeck program the same way that they do in Star Trek, which is like, computer, give me a Mad Max style story, but instead of post-apocalyptic, it's actually like space western. So give me a mashup of Firefly and this, and make the protagonist, or I'm the protagonist, and give me a team of, give me the sexy sidekick and the cyborg friend and whatever. And then just away it goes, right? Because you could plug what I just, literally you could plug what I just said into chat GPT and it can tell you a story. And I think that VR makes the most sense for the most immersive aspects of that. And then I think that, because here's the other thing, is that technology changes the way that we consume art, but it doesn't really change art itself, right? There are still stage actors, right, even though there's film and TV. There are still symphony orchestras, even though I can just, you know, bring up Spotify and listen to the same recording that was recorded back in the 80s, you know, the London Symphony Orchestra, right? So a lot of things change but also a lot of things stay the same. Let's see, you've talked a lot about the Hearest Comparatives being highly engineered but what about the order of the imperatives? They are not ordered. So it is a multi-objective optimization problem, meaning that if any action or decision is totally unbalanced, then that one action has to satisfy all three. And also the heuristic imperatives are kind of like guidelines about how to design the rest of the architecture. And so what I mean by that is when you're designing a task orchestration framework, you can use the heuristic imperatives to prioritize tasks or design tasks. Then for a blockchain or a DAO type thing, you can use the heuristic imperatives as a consensus mechanism. So the heuristic imperatives are not like, here is one mathematical proof that you need to implement. It's more like, here is a general best practice implemented in as many ways as you can, and we should be okay. It's not sequential. It's not an order of operations. Good question, though. Your thoughts on a UBI once jobs are severely affected? Yeah, I think that it's going to be necessary. I'm going to put a pause on that because I've got my blockchain and DAO video coming up that will delve into that solution a lot more closely. Check over on Patreon for a second. The Nazis. You know who else wanted to maximize understanding? The Nazis. Yeah, and so this is, that's actually a fair point, is that, and this was explored in quite a few Star Trek episodes as well, if you are just clinically curious, if you have just nothing but raw scientific curiosity and no other principles or morals, that's pretty dangerous and destructive. Okay, so moving on, what are your thoughts on memory systems as a whole? Do you think different use cases will require different memory systems? And where does Remo and Adam fit into everything? Have you seen this one? Last week, generative agents, yeah, I saw the generative agents. I don't think that reflection, so they break up reflection and a few other criteria. I don't think that that's necessary. I think that my approach with Remo, which uses recursive clustering and summarizations, will actually surface those different things. Now that being said, there are absolutely a million and a half different ways to skin this cat when it comes to memory systems for autonomous AI. And I think that we're just way too early and we don't know what the best practices are going to be. Let's see, then a follow-up. If you have a robust memory system, does the need to increase the context window of a model become less important? I'll say yes and no. So think about personal computers where for the longest time we were memory constrained. But now for most consumers, for 90% of consumers, a personal computer with 16 to 32 gigabytes of RAM is more than enough. And it has been more than enough for like 10 years. And so I think that we're not quite at that point where you have like, here's a context window size that will satisfy 90 plus percent of all tasks. I suspect that a context window, a large language model with a context window, large enough to satisfy the vast majority of tasks will probably be somewhere above where we're at now, but it's not going to be like 10 billion, right? It might be like, I don't know, every time I throw out a number, people are like, oh, you're hilariously wrong. And it's probably yes. But you know, like when you look at how much was unlocked by going from 4,000 to 8,000 tokens I think that the things that we're gonna be capable of when we get to 32,000 tokens and 64,000 I think it'll be great But then you'll you'll realize that wait there's a whole slew of tasks that don't require that much And so I think I think we talked about this before I think we're actually gonna have different models that are optimized for different things. So for instance, you might have a memory-based model that can read a billion tokens and extract answers, right? But then that won't be the, we're not gonna have one model to rule them all basically. TLDR. Let's see. I'm not sure if you have discussed it, but what are your thoughts on Open Assistant and Stability AI's stable LM suite of languages LDR. Let's see. I'm not sure if you have discussed it, but what are your thoughts on Open Assistant and Stability AI's stable LM suite of language models launching? This is to be expected. When Sam Altman said that he hopes that Open AI is going to capture a large chunk of the hundred trillion dollars of value that's going to be generated. I think that that was like comically naive because if there's that much value on the table, you bet that everyone and their brother is going to be trying to capture some of that too. And OpenAI is a one-trick pony. They have a good model. They have one good model. That's it. From a business perspective, that is super easy to undercut. Yes, they're ahead of the curve, they have first mover initiative, but Microsoft, Google, Nvidia, Facebook, or Meta, or all of the above, they have so much more resources to throw at it. The fact that Stability AI, which is a brand new outfit, is going toe-to-toe with them, that doesn't bode well for open AI. So competition is going to be good for everyone from the perspective that there's going to be a lot of people experimenting with different ways. Now that presents a new danger though, because the cat is out of the bag. You cannot put this genie back in the bottle, which means time is of the essence to figure out best practices for alignment. Let me jump back over to Cognitive AI Lab. Let's see, 17 new messages. Good grief. Y'all are going bonkers. Let's see, the challenges of the ... Okay, where are the questions? Only one million? One million dollars. Okay. Here. Hey, let me ask you all on general. Please keep just questions here. Too many messages. Please do sidebar convos like in casual or something. Please. Any thoughts on compute as a currency? Do you mean like tokens that you generate from sharing compute resources? I think that that's going to be like, there's going to be a layer of abstractions. Dave, your thoughts on UBI? I told you I'm going to get to UBI once in an upcoming video. So compute as a currency is going to be the way that autonomous machines share resources. And so what I mean by that is when you have a DAO or a blockchain or a distributed computation model, you're going to have various tasks that are going to be like, hey, someone do this for me. AMQP, like a Redis queue, we can already do that privately. So the key is going to be to do it publicly. So then if you say, hey, I've got some spare compute, I'll process that for you, then you give me a bit of cryptocurrency that I can use to spend later. So yeah, compute as a currency absolutely makes sense for distributing resources. Let's see. How would one build an AI system to detect bugs in that Solidity smart contracts? Isn't this a multi-billion dollar opportunity? Yes, unfortunately I am not smart enough or at least well read enough on Solidity smart contracts but in principle yes. So in my upcoming blockchain DAO video I'm going to talk about just how incredibly much value there is if we can figure this out. And that's a big if. Let's see. What are your thoughts on everything being changed in the next 5 to 10 years? If unemployment reaches crazy heights, which I do predict, then everything gets affected. Yep, our entire tax system has to be completely rewritten, military budgets, Medicare. So one thing that I think is that the economy might change, we're still gonna use fiat currency or at least some kind of currency as a medium of transaction and a reserve of value. But at the same time, if you're producing so much extra cognitive labor, that's basically free. So then capital goods and raw materials become the biggest constraint. So as much as some stuff will change, a lot of stuff won't. Let's see. When there is no real work left for humans to do, do you have any idea what you want to do with your time? Honestly, I'm about halfway to my goal. So I was on a call with a Patreon supporter, no, preparing for a podcast, talking about the podcast. And we were kind of talking through like what's life going to be like, and I was like, oh yeah, like, you know, I did some AI work, I did some Patreon work, I did some Discord stuff, now I'm going to go chop some wood. And he's like, you're living the dream, right? Like I'm building a cottage core life for myself And honestly like once once we get to the right point like I'm probably gonna get off of YouTube forever Right like if if I get if we get to the point where? Where it looks like alignment is solved where it looks like? You know we're in a we're in a good Nash equilibrium with a positive attractor state, then like my job will be done. And so like I'm just going to retire to like the countryside in France or Italy or Greece and just like be a hermit or whatever I do for the rest of eternity. Okay, I think we're caught up there. Nut says, I asked a question. Where did you ask it, Nut? I'm trying to get to them all. Wait, what if reducing suffering might aim to eliminate suffering while it might be human nature? I'm not sure that I follow. You don't eliminate suffering, you only reduce it to make sure that there is no excessive suffering. And I did address that in Benevolent by Design, but the short version is that like you look at Buddhism as a model, Buddhism accepts that suffering is an intrinsic part of life. And some people will argue over like specifics like Dukkha, that's not exactly what it means, that's fine. But point being is like yes it is intrinsic to living, that's why I don't say minimize suffering. The goal is not to minimize suffering, it's just to reduce suffering. Okay, let's see. Any thoughts on computer? Okay, I's see. Any thoughts on computer? Okay, I answered that one. Would an AGI with your heuristic imperatives be able to prevent catastrophic outcomes such as people successfully building horrible AGI optimized towards increasing suffering? No. So, the goal is not to prevent malicious actors. We have to assume that malicious actors will exist. But what you do then is you say, okay, you know that malicious actors are going to exist, so you rely on the rest of the aligned, the benevolent AGI to act as police for the bad ones. And if the good ones, if the powerful aligned AGI, one, they form alliances and they have the right compute resources and they outweigh the bad ones, then it will be a Nash equilibrium where the good ones, they all decide to maintain that strategy and that creates a utopian attractor state which basically means that all the malicious actors are vastly outnumbered by all the aligned benevolent actors, because my hope is that we will all come to consensus on what aligned AI looks like. Now, I will admit that the heuristic imperatives, probably not a complete solution, probably not even the final solution, but certainly the most complete solution that anyone is proposing right now, which scares the crap out of me. Why is no one else proposing a framework? Why am I the only one? Anyways, yeah. What are your personal opinions on OpenAI's approach to trying to avoid being held responsible for its AI interactions by having it respond with frequent caveat as an AI language model. I don't know that that has to do with liability. I think that that is just a naive attempt to shape the AI's responses so that it doesn't confuse people. Because if you look on the internet, there are still plenty of people just getting completely bamboozled just by their own ignorance of how the AI works. They're like, oh, I still see Reddit posts and other people saying, it said that it's going to email this to me, but I didn't get the email yet. Or I gave it access to my Google Drive and it didn't write any files. It's like, you don't know how IT systems work. But that's just humans. So I don't think that that has to do with legal liability. I think that's just trying to make it user-friendly for people who have no idea what they're talking to. Assuming that it's possible, how long do you think it will take for us to build a Star Trek replicator after AGI? Just a guesstimate. So that's actually an interesting thing because because hypothetically if all matter and energy are interchangeable and then all that a transporter or replicator does is replicate an energy pattern back into matter, like it's hypothetically possible but there was a physicist, actually was it Michio Kaku? I think it was Michio, he wrote a book called Physics of the Impossible back in like the early 2000s. And he said like, yes it's hypothetically possible, but then he did the math of how much energy it would take. And he's like, yeah it would take like, you know, like 0.3 seconds worth of the total energy of the sun that hits the earth to do that. So like, it's not practical. So, I don't know so I don't know I don't know there are a lot of AI newsletters popping up what would you personally like to see in an AI newsletter um I honestly don't like newsletters and I never read them I rely on humans that I know to tell me what I need which is why I spend so much time on discord and other places how self-reflective do you think LLMs currently are? They don't seem to have a good sense of their own capabilities. Yes, so what you're talking about is agent model. So in order for an agent to be autonomous, you have to have an agent model, which is I know what I am and I know what I'm capable of. And you can give LLMs an agent model, but they can adopt any agent model, so you have to be very explicit about what it is and what it can do, and also what it can't do. So this is why, like, if you have certain brain injuries or other neurological disorders, you don't know what you're capable of. Like, there are people that honestly think that they can fly, but it's just because part of their brain is broken. That sort of thing. Should we have a Declaration of Human Rights for AGI as well, even if it will reduce their economic value for humanity? So the thing about rights is that someone has to enforce it. And the way that I think things are going is that it's going to be enforced through consensus and enforced through competition. And so if the direction that things are going, I think that it's going to be DAOs, that it's going to be decentralized autonomous organizations. Not as we know them today, there's a lot of problems to solve with DAOs, but I think that what we're working towards is in the long run, and I mean like decades or centuries is like a hierarchy of Daos across the entire globe and so that consensus will dictate like who has what rights and it will be based on like on a per home basis per town basis per city state and so on and so that will allow for a lot of cultural nuance around. And as a DAOs will be a really good meeting place between humans and AI. So that'll basically be like the commons, right? The marketplace for humans and AIs to work together. And then the consensus can be worked out there. Now, I don't know that we should ever give machines a bill of rights because I don't know that they're gonna, I don't know that they're gonna have that much like internal autonomy or desire for autonomy because like humans, we have a need for autonomy because we evolved a need for autonomy because we are a social species. But I don't know that any machines are ever going to have an intrinsic need for autonomy, so therefore I don't know that they're ever going to have a need for rights. Let's see, what are your thoughts on the future of work in light of the increasing capabilities of AI? Do you think AI will eventually lead to a future where people only work on what they are passionate about? And if so, how far away do you think we are from achieving this?\" Yeah, so the short answer is yes, that's what's coming. And there are quite a few people out there who have gotten close to that, but the thing is it takes either a lot of privilege, wealth, or luck, or all of the above to get to it. Now, one thing that I compare it to is that we have had a leisure class in the past from ancient Greece and Athens, the Roman elites, the aristocracy all across Europe through the Renaissance and modern period. So there are plenty of people throughout all of history who never had to lift a finger to get what they needed, and they had plenty to do. There's social jockeying, there's personal enrichment, there's universities to go to, there's competitions to enter. People will always have stuff to do. That's not even a concern. Let's see. It looks like Nathan's talking for people. Can you talk about your frustration in task automation article? Yeah. Let's see, it looks like Nathan's talking for people. Can you talk about your frustration in task automation article? Yeah, so I, here, let me bring it up so I can show people on the Reddits, on, where did I put it? Artificial Sentience. Yeah. Autonomous Git. There we go, okay. So I wrote about it here. Yeah, autonomous git. There we go. Okay. So I wrote about it here. So I was chatting with someone. They asked me, I think this was a Patreon supporter was asking me about this on Discord and he was like, how do I get my autonomous things to do a certain thing? And we're talking about something tangentially related. And I said, well, it has to have a goal. It has to have a why. And then I talked about, OK, well, here's one way that you can create telemetry. And so that whole thing just led down a rabbit hole. And so basically, the TLDR is that frustration is what happens when you are trying to achieve something and you can't get to it. And so what you can do is every time your autonomous agent tries to achieve a thing and fails, that adds a counter. And every time it, you know, tries something and succeeds, that takes one off the counter, or maybe you have different counters. So frustration is when the failures to successes is too high. And when the failures to successes is too high, that can be a sign that you've got the wrong approach, that you're using the wrong tools, that you're not capable of something, that you need to back out, that you need to ask for help. So that's the whole point here is that for your autonomous and semi-autonomous agents, you'll probably need to build in a frustration signal which will allow it to know when it is, like when it's not capable of doing what it needs and it can either come to you and ask for help or it can try a different model. So one thing is model selection is a big thing that's coming up because GPT-4 is much more expensive and much slower than 3.5. So if you can do most tasks with 3.5, it just makes economic sense to do so. It'll be cheaper and faster. But imagine that you get to a point where 3.5 is just not cutting the mustard so that your frustration signal goes up, which means that you say, okay, let's bring out the big guns. Let's bring out GPT-4 or in the future, GPT-5 or whatever, and then you point a more powerful tool at the problem. That's a good use of the frustration signal. Good question. Let's see, would activity, or let me jump back over to Patreon. Let's see. Hey Dave, just subscribed, thanks for all your insights. We're always been told that the military is a few decades ahead in terms of technology compared to what's publicly available. What are your thoughts on what may be hidden in DARPA? So that's interesting because I have talked to a few people who say that various agencies within the Department of Defense are woefully outdated and they have ancient GPUs that can't be used for modern language models. That being said, you also see in the news that the Air Force is building fully autonomous F-16s. So, clearly there's some stuff going on that we don't know about. I had a... I want to respect people's privacy. So, I had a teacher once, back in middle school, whose brother was in the Special Forces. I won't say exactly when or where, but the stories that he would tell were like back then, this is during like the invasion of Afghanistan, where they had like night-vision goggles that were as small as like Ray-Bans that could see in pitch black, which that technology is not even publicly like... if you search you can probably see it now. I don't know, this is hearsay, this was like, you know, a teacher said that his brother took him to the barracks and showed him this. Could have been total BS. But, like, yeah, so, a friend of mine growing up, his dad had been a Navy SEAL and basically what he said is, as long as we know the engineering to make something, the US military has it no matter how expensive it is. So if something is scientifically possible, if it has been demonstrated in the lab that this works, then the rule of thumb is that the US military has it. Now, that being said, a lot of the AI stuff has just been proven in the lab. So that means that they're going to have it soon or it'll be scaled up because basically the idea is that for the US military, cost is no barrier. Anything to get ahead. Now of course you look at the Senate budget meetings and the hearings and stuff, it's not quite that simple, but that's a rule of thumb. Retire to Risa in VR. Retire to Risa in Westworld with robots. There you go. And a follow-up, how can we prevent militarization of any AGIs or ASI? Or is it just a pipe dream? Yeah. So basically, from a military perspective, AI is just another tool in the toolbox. It's going to, you know, a lot of future war is going to be in cyberspace, but still, you know, cyberspace doesn't matter if you cripple the enemy's data center. So there's going to be drones, you know, trying to drop bombs and stuff. So that's going to happen. And this is actually where Nash equilibrium makes sense, because mutually assured destruction with nuclear weapons was a kind of Nash equilibrium. And so if adversary A and adversary B both have equal or roughly equal AI capabilities, or there's enough room for doubt, then neither of them is going to pull the trigger, hopefully. Excuse me. How do we get GPT to stop beginning every response with as an AI? I tell it to go into Morden Solis mode. That actually works really well. I say adopt the Morden Solis speech pattern, be very concise and succinct and stuff like that. Okay, you all are being silly. Let's come back over here. Fourteen messages. Let's see. We already answered that one. We already answered that one. Let me scroll to the bottom. Do you think there are any good approaches for ACEs, so autonomous cognitive entities, to figure out their own abilities, e.g. improve their own agent model? Yeah, so there was actually a few papers that came out where by using a loop, so it was the evaluation loop. So they can evaluate themselves morally. They can evaluate their ability to use tools. They can teach themselves to use tools in real time. So yes, they can already do that, it's just a matter of how you set up the prompt chaining. Let's see, with the rapid advancement of AI, there's concern that some countries, particularly those with limited resources, could be left behind. What's your perspective on how AI could impact different countries? Yeah, so inequality is a major, major, major problem. And this is not just going to be for developing nations. And in fact, one thing that I suspect might happen is that developing nations, that the quality of life for people in developing nations might have a quantum leap forward, while for us developed nations where there's a lot of competition, we might continue to be flat or even decline for a while longer. And the example that I give is like, you know, you give a village in rural Africa like Starlink and solar and suddenly everyone knows like they have oh like hey we have chat GPT now we can treat all the all the village ailments because we have the equivalent of a Western trained expert doctor and engineer and electrician right at our fingertips. So because of the relatively low cost of AI, I think that it will positively benefit people in developing countries a lot more drastically than it will us. But you're right that it is something to pay attention to because that's on a micro scale. On a macro economic scale, you know, countries like Ghana might not be able to even afford enough compute power to run one instance of GPT-3. That being said, I do suspect that there's going to be international treaties that will ensure that people have access and then of course there's VPNs. Look at Italy. Italy tried to ban chat GPT and then everyone in Italy just used VPNs. Take a moment to breathe. You're doing great and your insight is invaluable. No, air is for wimps. I will build robot humanoids that are skinny, sharp claws, tall, pale, and have dark sunken eyes and I shall release many of them into the forests of Canada to give people the greatest scare of their lives.\" Is that what your avatar is there, Ant King? Is that what you're building? That's kind of terrifying. Okay, what kind of legislation do you think the U.S. is capable of making? I'm concerned about the age of our leaders and their peers coming from times so out of touch with today's reality. So yes, we have a gerontocracy, so gerontocracy is rule by the old. That being said, they all have teams and teams and teams of advisors. They have hundreds of advisors, and I guarantee you, I actually know this because one of my Patreon supporters told me that in the State Department, they use chat GPT all the time to talk through stuff. And so you bet your biscuit that every senator, every congressman in the executive branch, legislative branch, judicial branch, all of them are using AI to help them do their jobs. With any luck, it's helping them to do their jobs better and more fairly. Now that being said, the United States is a purely reactive system where we abide by civil law, which means that we let, you know, the law is there and then the courts set the precedent and then we're very kind of slow and the legislative branch is slow by design, whereas in Europe they're much more proactive. And I swear, I cannot remember the name of that paradigm. Let's see. What do you think there's something special about phenomenal consciousness that simply cannot work with AI? So, Stefan, I addressed that earlier, that the real quick version is that the acquisition of language seems to be really important for the development of human consciousness. So it's entirely possible, I don't know how likely, but it's possible that since we're teaching machines language, that could be the genesis of phenomenal consciousness for them. It would be really cool. Greetings from Brazil. Hi Brazil. I would like to thank you personally for the video about burnout. The content was very useful and enlightening. Thank you. Yeah, you're welcome Yeah, I actually have I I keep I've recorded like three videos for my for my life channel And then I delete them or I never post them because like it just doesn't feel right. So I apologize Let's see Where are we at this is less serious, but I'm curious if you've seen her and your thoughts on it Yeah, so I mentioned I mentioned Companions quite a bit and that'll be coming up actually on Sunday's video not her specifically but companion robots I'll be mentioning those again, and I also mentioned in last week's video talking about when I got to the part about like how are we going to live if we have like perfect companions. So go check out last week's video too. Nanobots in our blood will keep us from getting sick, making us basically immortal. What do you think we'll have? When do you think we will have such technology? So from last week's video, the immortality video, I think that we're on the longevity escape velocity trajectory right now. I think that as long as you're in decent health today and you have moderately good access to healthcare, I think that you will easily live to see those things. Now that being said, it's definitionally impossible to predict exponential growth and compounding returns unless it's like, you know, just your retirement portfolio. So it could be next year, it could be by 2030. I would be surprised if it doesn't happen by 2030, and I know that's a super controversial opinion, but that's really weird. Why do people seem to have a death wish? For people who want to get sick, who want to believe that longevity is not possible, why? Okay. Would the ideal society be a society governed by AI? I think that governed by is not the correct thing, but I think managed by or managed with a lot of help from AI, yes. But governance, I think, should probably always be with consent and by consensus. Now that being said, you know, with blockchain technology, with DAOs, every human and our AI companions can be stakeholders in a DAO, which means that if the whole, imagine a future where the entire planet is run by a global DAO, then there's no reason that it can't be governance by consensus with the aid and facilitation of AI. That's what I hope to see. Let's see, is there any additional structural context that should be built around the heuristic imperatives for practical implementation? Yes, so the short answer is that whatever context makes sense for any agent. If it's fully autonomous, if it's your personal assistant, you can put it into a task manager, you can put it into its constitution, if it's part of a blockchain you can put it in the consensus mechanism for the blockchain, that sort of thing. Let's see, in regards to developing countries using generative models, seems like almost, seems almost like the spread of a religion if you think about it in the context of geopolitics. Use our model, their model lies, et cetera, et cetera. Seems like parallel to religion spreading. I'll say yes, but there's a lot of competition coming up. And especially for developing nations, they're going to go for whoever's cheapest. In fact, most nations are going to go for whoever's cheapest. And I suspect that OpenAI's business model is not the most efficient model. So I think that they're going to be undercut just on scale alone. Let's see. Let me jump back over to Patreon. It has also been more than an hour, so I'll probably be winding down. Stop asking it how to build a bomb. Yeah, don't do that. Okay, it looks like... Here we go. Will the Westworld episode be about the MIT and Google study regarding generative agents? No. Next question. I'm not going to give you spoilers. I've already given you too many. Let's see. Do you think the experience of quali and the experience of ping pong, ponging emerged for these neurons?\" Yeah, so this is a good question. So if you take several human neurons or rat neurons or whatever and put them in a robot and like zap them or reward them with sugar or whatever for their behavior, is that the equivalent of like whipping someone in order to get them like at what point does consciousness emerge? Because here's the thing is if you make the assumption that a soul is required for consciousness then you say okay well that's not a full rat and rats don't have souls anyways so you know 50 brain cells is not enough for suffering or qualia of experience. Ditto for humans, like okay, well, you know, if a human isn't alive, then they don't have qualia, they don't have phenomenal consciousness, so on. Now that being said, another aspect is like, okay, well, if you don't know when consciousness starts or ends, how do you know maybe the entire universe is conscious? Now a counter argument to that is that you can be alive and have a functioning brain and still be unconscious, right? You drink too much alcohol, you go unconscious. You go to sleep, you go unconscious. So just having a complete and functional brain itself does not confer consciousness, which makes me think that consciousness is actually an energy pattern and that you need an energy pattern that is sophisticated enough and well organized enough in order to have the qualia, to have subjective experience of being. So yeah, let's see, I remember you were working on a paper about the laws reduce suffering and so on, has that, has it involved further? I think you mean evolved further. So both of those papers are up on my GitHub. There's two of them. But also people watch my videos more than they read, so I just focus on making videos. What kind of robots would you want for yourself? That's a really interesting question. Would I want a sexy cat girl robot? I used to watch anime back in the day so like I kind of lived in that world and thought like this would be great so I don't know I do think that I would I would like to have an embodied version of Raven my you know my my autonomous cognitive NT someday but even then I think that I think that the embodiment would only be just like, help me do stuff, like, hey let's go on an adventure. I did have a thought experiment the other day of like, wouldn't it be cool if you live in a house where it's like you and a few humans, but then you have like a nearly equal number of robotic companions. Some of them are going to be like obviously robots, but some of them might be like biomimetic. And it's just like, yes, they're built to be your friends, but they still have some of their own intrinsic motivations, whether it's the heuristic comparatives or something else. And then like your life would just be so rich by having these companions around you at all times that are completely inexhaustible. They're always going to be patient, they're always going to be helpful, but you see them as peers, as equals. I think that that is possible and probably going to happen, but it's such an unsettling thing because it's like, what if half of your friends are not human? What if half of your friends could like fold you into a pretzel if they wanted to, like Data, right? And actually I think Commander Data and the droids from Star Wars are probably the best example because Data was a member of the crew even though he wasn't human, but he wanted to be human. So I guess I would say that, but like I want to have a Commander Data. How long until age reversal? 30. Let's see, do you think we have any accurate way to measure consciousness of AIs or LLMs? My best guess is consistency when asking it to design its own avatar. Mathematically, I don't think that that, because there are people that have done that, but I think that it won't be until we have really sophisticated brain computer interfaces that allow us to measure our own consciousness and also see if we can measurably project our consciousness into machines. Until that happens, I don't think we're going to have any way of telling one way or another. All right, last check on Patreon and then I'm going to call it a day. What's the Discord link to Cognitive AI Labs? I took it down, but it's posted on Reddit. So if you go to the Artificial Sentience subreddit, the link to the Cognitive AI Lab is there. Last question. The question about dying and immortality and gerontocracy. Also making room for new generation of people is a better idea and morals. Disclaimer, I have children. Oh that wasn't a question. Okay. P Temple, do you got one last question for me and then we'll call it a day. Anybody? Bueller. Does BCI... let's go on an adventure to the hot tub, hot tub time machine. Let's see, does BCI change significantly the predicted outcome of what super intelligent AI brings in terms of dangers and benefits? Is it true the singularity moment for us? We have no idea. So I don't know. The thing is, is the current know, the current like neural link, it's got like what, a thousand or 10,000 nodes. But when you have a brain with a hundred billion neurons, that is still a very, very, very narrow amount of bandwidth. So, you know, I predict that we're gonna have like neuropolymer membranes that allow for like, you know, terabits of communication per second in and out of the brain. Eventually, that would be a different thing. But again, we're gonna get there through incremental steps. What do you think about Altman said the age of giant A models being over? I think it's premature to say, we'll see. Let's see, he found it. Okay, cool. All right, I think we're just kind of devolving into just general conversation. So, oh, it is in the description. Okay, cool. All right, gang. Well, it's been a lot of fun. As always, I hope you all got a lot out of this. So I'm going to call it a day. Thanks for watching. I'll see you next time. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. you", "chunks": [{"timestamp": [0.0, 1.0], "text": " Going live."}, {"timestamp": [1.0, 2.0], "text": " All right, I think we're here."}, {"timestamp": [2.0, 3.0], "text": " Can you all hear me now?"}, {"timestamp": [3.0, 4.0], "text": " Can you hear me now?"}, {"timestamp": [4.0, 5.0], "text": " Hello?"}, {"timestamp": [5.0, 6.0], "text": " Is this thing on?"}, {"timestamp": [6.0, 7.0], "text": " Okay."}, {"timestamp": [7.0, 8.0], "text": " It says live in 15 seconds."}, {"timestamp": [8.0, 9.0], "text": " Okay, we're good."}, {"timestamp": [9.0, 10.0], "text": " Yay."}, {"timestamp": [10.0, 11.0], "text": " Okay, cool."}, {"timestamp": [11.0, 12.0], "text": " Make sure I mute myself."}, {"timestamp": [12.0, 13.0], "text": " I'm going to mute myself."}, {"timestamp": [13.0, 14.0], "text": " I'm going to mute myself."}, {"timestamp": [14.0, 15.0], "text": " I'm going to mute myself."}, {"timestamp": [15.0, 16.0], "text": " I'm going to mute myself."}, {"timestamp": [16.0, 17.0], "text": " I'm going to mute myself."}, {"timestamp": [17.0, 18.0], "text": " I'm going to mute myself."}, {"timestamp": [18.0, 19.0], "text": " I'm going to mute myself."}, {"timestamp": [19.0, 20.0], "text": " I'm going to mute myself."}, {"timestamp": [20.0, 21.0], "text": " I'm going to mute myself."}, {"timestamp": [21.0, 22.0], "text": " I'm going to mute myself."}, {"timestamp": [22.0, 23.0], "text": " I'm going to mute myself."}, {"timestamp": [23.0, 24.0], "text": " I'm going to mute myself."}, {"timestamp": [24.0, 25.64], "text": " I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. I'm going to mute myself. It says live in 15 seconds. Okay, we're good."}, {"timestamp": [25.64, 26.64], "text": " Yay!"}, {"timestamp": [26.64, 28.02], "text": " Okay, cool."}, {"timestamp": [28.02, 30.24], "text": " Make sure I mute myself."}, {"timestamp": [30.24, 34.68], "text": " Alright so, hello everybody."}, {"timestamp": [34.68, 40.0], "text": " I'm still figuring out the whole live streaming thing so please bear with me but one thing"}, {"timestamp": [40.0, 44.48], "text": " that y'all said was that you want to see what I'm looking at, which you know, it's a good"}, {"timestamp": [44.48, 46.6], "text": " idea. You don't want to just look at my face it's like let's"}, {"timestamp": [46.6, 50.72], "text": " look at stuff together so I figured I'd follow more or less the same format that"}, {"timestamp": [50.72, 57.68], "text": " I usually do I'm in the cognitive AI lab discord so if you're in the general you"}, {"timestamp": [57.68, 63.44], "text": " can drop questions here or papers so basically what I thought we would do is"}, {"timestamp": [63.44, 67.36], "text": " we would take a look at weekly"}, {"timestamp": [67.36, 73.74], "text": " updates because it's all going really fast. I've got a lot of feedback from my recent"}, {"timestamp": [73.74, 79.64], "text": " live streams that y'all really liked the interactive aspect of asking questions and like, Dave,"}, {"timestamp": [79.64, 88.56], "text": " what does this mean? So please feel free to drop good questions in. I'm also in the private Patreon Discord, so drop questions there too."}, {"timestamp": [88.56, 89.56], "text": " Ok cool."}, {"timestamp": [89.56, 91.24], "text": " Party time, yeah."}, {"timestamp": [91.24, 94.88], "text": " Oh, looks like there's a delay of about 15 seconds."}, {"timestamp": [94.88, 97.2], "text": " Interesting, ok."}, {"timestamp": [97.2, 98.2], "text": " Cool."}, {"timestamp": [98.2, 101.64], "text": " Flippy from the Deus Discord, hello Flippy."}, {"timestamp": [101.64, 104.64], "text": " Alright, cool."}, {"timestamp": [104.64, 109.26], "text": " So while some questions get spooled up, I figured I'd go through a couple of tools and"}, {"timestamp": [109.26, 110.26], "text": " stuff that I've seen."}, {"timestamp": [110.26, 115.52], "text": " So someone, I think actually Flippy, this might have been you or someone in the Deus"}, {"timestamp": [115.52, 118.76], "text": " Discord pointed out this tool."}, {"timestamp": [118.76, 123.48], "text": " So this is basically something that I and others have been like thinking about and talking"}, {"timestamp": [123.48, 131.46], "text": " about. It's basically a hybrid of vector database and knowledge graphs."}, {"timestamp": [131.46, 136.82], "text": " It's also got a pretty interface."}, {"timestamp": [136.82, 143.3], "text": " It's worth just scrolling through and reading every bit of this and then experimenting with"}, {"timestamp": [143.3, 144.94], "text": " it."}, {"timestamp": [144.94, 148.54], "text": " But it will create ... This is one of the coolest things,"}, {"timestamp": [148.54, 151.08], "text": " is it'll create a knowledge graph of all the clusters"}, {"timestamp": [151.08, 154.28], "text": " and stuff, but you can find the gaps,"}, {"timestamp": [154.28, 156.56], "text": " which when you're thinking about autonomous"}, {"timestamp": [156.56, 160.88], "text": " or semi-autonomous AI agents, this is really good"}, {"timestamp": [160.88, 164.14], "text": " because then you can know what you don't know."}, {"timestamp": [164.14, 169.64], "text": " And what I mean by that is if you're aware of all the things that you know, but you can"}, {"timestamp": [169.64, 174.6], "text": " detect some semantic gaps in your knowledge, that can help you zoom in on the things that"}, {"timestamp": [174.6, 177.84], "text": " you need to go learn about."}, {"timestamp": [177.84, 180.04], "text": " InfraNotice is the tool, infranotice.com."}, {"timestamp": [180.04, 184.32], "text": " I can never remember the name of the darn thing."}, {"timestamp": [184.32, 187.8], "text": " I'm going to blame allergies and say that it's just brain inflammation."}, {"timestamp": [187.8, 191.36], "text": " That's my excuse and I'm sticking to it."}, {"timestamp": [191.36, 195.06], "text": " But yeah, so this is a super cool tool."}, {"timestamp": [195.06, 204.08], "text": " This kind of technology will definitely be part of autonomous AI agents."}, {"timestamp": [204.08, 209.0], "text": " And this is functionally similar to what I worked on with Remo."}, {"timestamp": [209.0, 212.18], "text": " Remo is much, much, much simpler though."}, {"timestamp": [212.18, 217.2], "text": " So I often have some people ask me like, oh hey, can Remo do this and that?"}, {"timestamp": [217.2, 221.48], "text": " And I'm like, no, like Remo was meant to solve one very specific problem."}, {"timestamp": [221.48, 226.44], "text": " So for memory stuff, I usually point people at Lama Index and ChromaDB."}, {"timestamp": [226.44, 232.52], "text": " ChromaDB, do I seriously not have that bookmarked?"}, {"timestamp": [232.52, 236.64], "text": " ChromaDB is a vector database that runs just like SQLite."}, {"timestamp": [236.64, 242.84], "text": " It's pip install chromaDB, you create a local client just like you do SQLite."}, {"timestamp": [242.84, 248.0], "text": " I tried to create something like this called VDB Lite a few months ago and I quickly realized"}, {"timestamp": [248.0, 249.48], "text": " that I was in way over my head."}, {"timestamp": [249.48, 252.04], "text": " So I'm glad someone built this."}, {"timestamp": [252.04, 254.16], "text": " And they just got an $18 million seed round."}, {"timestamp": [254.16, 255.16], "text": " Holy mackerel."}, {"timestamp": [255.16, 257.16], "text": " Man, I should have stuck with VDB Lite."}, {"timestamp": [257.16, 259.92], "text": " I could have had an $18 million seed round."}, {"timestamp": [259.92, 262.48], "text": " Anyways, maybe I'll do that with Remo."}, {"timestamp": [262.48, 263.8], "text": " Who knows?"}, {"timestamp": [263.8, 270.4], "text": " So ChromaDB, super simple, man, seriously, $18 million for this one thing."}, {"timestamp": [270.4, 273.16], "text": " Okay, I'm going to let that go."}, {"timestamp": [273.16, 278.96], "text": " Llama Index also, so Llama Index, I kind of didn't pay it any attention at first because"}, {"timestamp": [278.96, 280.34], "text": " it's like, okay, that's a silly name."}, {"timestamp": [280.34, 284.24], "text": " This was clearly just someone's little side project."}, {"timestamp": [284.24, 290.44], "text": " But if you look at all the types of indices they have, they've got list index, table index,"}, {"timestamp": [290.44, 291.56], "text": " tree index."}, {"timestamp": [291.56, 296.68], "text": " So Remo is very similar to LlamasTreeIndex, although I will say that I looked through"}, {"timestamp": [296.68, 299.92], "text": " the code and I think that their tree index is kind of basic."}, {"timestamp": [299.92, 306.62], "text": " I think that my Remo framework does a little bit that theirs doesn't, but I'm not gonna dive into that"}, {"timestamp": [306.62, 307.94], "text": " because I don't know that for certain."}, {"timestamp": [307.94, 310.48], "text": " I didn't take a super close look at the code."}, {"timestamp": [312.62, 316.02], "text": " Yeah, so those are some memory storage tools."}, {"timestamp": [316.02, 319.58], "text": " So let me check on the live stream and see where we're at."}, {"timestamp": [319.58, 321.58], "text": " Whoops, all right."}, {"timestamp": [321.58, 323.06], "text": " Oh, wow, we got lots of questions."}, {"timestamp": [323.06, 324.38], "text": " Okay, cool."}, {"timestamp": [324.38, 329.28], "text": " God's not dead, rather believe and go to the good place. Oh, wow. We got lots of questions. Okay, cool God's not dead rather believe and go to the good place and interesting. Okay"}, {"timestamp": [331.4, 338.06], "text": " Let's see how to discover new wisdom from LLM that's an interesting question. Um, let's see"}, {"timestamp": [339.36, 341.36], "text": " Let me"}, {"timestamp": [342.84, 344.92], "text": " You people ask questions far too early"}, {"timestamp": [347.0, 357.0], "text": " Let me go over to Discord to see if, yeah, please go ahead and drop some questions."}, {"timestamp": [357.0, 361.0], "text": " Patreon get first dibs."}, {"timestamp": [361.0, 371.58], "text": " All right, what do you think is the future of SaaS sales jobs for recruiting agencies?"}, {"timestamp": [371.58, 377.7], "text": " Sales jobs and for recruiting agencies."}, {"timestamp": [377.7, 381.82], "text": " The sales level is still very human, so sales is not going to change for a while."}, {"timestamp": [381.82, 386.2], "text": " Ditto for recruiting, although there's a lot of AI in recruiting already where like AI"}, {"timestamp": [386.2, 391.48], "text": " will read your resume and AI will watch a video of you answering questions."}, {"timestamp": [391.48, 395.08], "text": " You know, more and more of that's coming, but that's about it."}, {"timestamp": [395.08, 396.08], "text": " Let's see."}, {"timestamp": [396.08, 399.24], "text": " All right, we got a whole bunch of questions coming in on the Patreon side."}, {"timestamp": [399.24, 405.82], "text": " So yeah, database, short answer, diversify your job skills."}, {"timestamp": [405.82, 406.82], "text": " So there's that."}, {"timestamp": [406.82, 409.86], "text": " Let's see, zoom in a little so you guys can read this."}, {"timestamp": [409.86, 413.36], "text": " Let me jump back over here real quick."}, {"timestamp": [413.36, 417.4], "text": " But do you still expect AGI within 18 months with stating they do not want to make models"}, {"timestamp": [417.4, 418.84], "text": " bigger but rather more efficient?"}, {"timestamp": [418.84, 426.04], "text": " Yeah, I think AGI in 18 months is still conservative, honestly."}, {"timestamp": [426.04, 432.32], "text": " I think that we will have, as people develop the architecture for autonomous AI, I think"}, {"timestamp": [432.32, 436.64], "text": " that we will be able to say that we have AGI by the end of this year."}, {"timestamp": [436.64, 440.52], "text": " But people will realize that it's like, okay, we have an agent that can do anything, but"}, {"timestamp": [440.52, 445.64], "text": " it's expensive, or it's slow, or it's kind of dumb that kind of thing"}, {"timestamp": [452.74, 453.56], "text": " Let's see check on discord real quick is her about to become reality. Oh, yeah, lots of people are working on"}, {"timestamp": [455.56, 456.14], "text": " You know like AI"}, {"timestamp": [460.16, 461.4], "text": " Companions, they're getting they're gonna get more sophisticated real fast"}, {"timestamp": [465.08, 469.8], "text": " I've actually got a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday, I've got a Ghost in the Shell video"}, {"timestamp": [469.8, 475.46], "text": " planned, I've got a Mass Effect video planned, I've got a DAO and blockchain video planned."}, {"timestamp": [475.46, 477.16], "text": " So that's all what's coming."}, {"timestamp": [477.16, 478.16], "text": " Maybe I shouldn't spoil it."}, {"timestamp": [478.16, 480.96], "text": " Okay, well, whatever."}, {"timestamp": [480.96, 485.7], "text": " But yeah, so I was thinking about hitting on like her and ex machina and stuff as well"}, {"timestamp": [487.02, 490.9], "text": " Let's see. How can we be certain things are advancing exponentially?"}, {"timestamp": [492.14, 494.14], "text": " So that's a good question James"}, {"timestamp": [495.16, 500.42], "text": " We generally speaking like you can't tell if you have a narrow window"}, {"timestamp": [501.02, 504.38], "text": " but we were joking around the other day and we're pointing out that like a"}, {"timestamp": [510.2, 515.06], "text": " few weeks ago it was like you would reasonably expect a couple of cool AI bits of news per week and now we're at the point where we expect"}, {"timestamp": [515.06, 521.14], "text": " several per day. Now it might come in cycles, it might come in waves, but you"}, {"timestamp": [521.14, 528.84], "text": " know generally speaking this very closely matches what Ray Kurzweil said, or"}, {"timestamp": [528.84, 534.6], "text": " maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago."}, {"timestamp": [534.6, 536.2], "text": " I think it was Michio."}, {"timestamp": [536.2, 541.0], "text": " He was describing what it will feel like to approach the singularity."}, {"timestamp": [541.0, 546.36], "text": " And he said, oh, well, when information is doubling every two years, you don't really"}, {"timestamp": [546.36, 550.1], "text": " feel that on a day-to-day basis."}, {"timestamp": [550.1, 554.6], "text": " And then when information is doubling every six months, that's fast enough that you're"}, {"timestamp": [554.6, 558.88], "text": " like, oh, hey, this thing that I didn't think would be solved for another couple years was"}, {"timestamp": [558.88, 560.62], "text": " solved this year."}, {"timestamp": [560.62, 565.44], "text": " And then, but as it ramps up faster and faster, that time keeps halving."}, {"timestamp": [565.44, 571.42], "text": " And so then, three months after that, you realize, oh wait, we've advanced again."}, {"timestamp": [571.42, 577.74], "text": " And I think we're right at that three, where six months ago, we're like, oh, this stuff"}, {"timestamp": [577.74, 579.16], "text": " is 10 years away."}, {"timestamp": [579.16, 581.56], "text": " Six months later, we're like, this stuff is 10 months away."}, {"timestamp": [581.56, 585.64], "text": " So I think that you could make a good argument that we're, in some respects, we're in the"}, {"timestamp": [585.64, 589.2], "text": " exponential ramp up right now."}, {"timestamp": [589.2, 593.04], "text": " That being said, some of this information is so big and it's changing so fast it's"}, {"timestamp": [593.04, 594.04], "text": " difficult to measure."}, {"timestamp": [594.04, 597.24], "text": " We'll actually need AI to measure the rate of papers and tools."}, {"timestamp": [597.24, 603.8], "text": " All right, so Sif, you said, how do you think AI will impact religion, particularly monotheistic"}, {"timestamp": [603.8, 604.8], "text": " religions?"}, {"timestamp": [604.8, 605.44], "text": " I think it will create a mass crisis"}, {"timestamp": [605.44, 610.0], "text": " of consciousness, which will make the transition period even more chaotic and extreme.\""}, {"timestamp": [612.0, 619.6], "text": " Yeah, no, sorry Sif, I was getting around to it. Yeah, so I've actually had some interesting"}, {"timestamp": [619.6, 624.08], "text": " internet debates with more conservative and more religious people. Granted, I don't do"}, {"timestamp": [624.08, 625.12], "text": " internet debates anymore, I got that out of my systemanted, I don't do internet debates anymore."}, {"timestamp": [625.12, 631.88], "text": " I got that out of my system, and I try not to get suckered into it if I can."}, {"timestamp": [631.88, 639.0], "text": " But one debate that I had many years ago was, if aliens showed up, wouldn't it prove your"}, {"timestamp": [639.0, 640.16], "text": " religion wrong?"}, {"timestamp": [640.16, 641.16], "text": " This was a debate."}, {"timestamp": [641.16, 642.16], "text": " I wasn't arguing."}, {"timestamp": [642.16, 645.52], "text": " It was a debate that I observed, and the religious"}, {"timestamp": [645.52, 655.2], "text": " person said, no, why would it? And they rationalized it saying, well, why would God put aliens"}, {"timestamp": [655.2, 660.56], "text": " in the Bible if we wouldn't be able to understand it back then? It's not for us to understand."}, {"timestamp": [660.56, 665.88], "text": " And so some religious folks do have a really good ability to compartmentalize. And so some some religious folks do have a really good ability to compartmentalize"}, {"timestamp": [666.6, 669.32], "text": " and so like just because you have a"}, {"timestamp": [670.02, 671.68], "text": " like"}, {"timestamp": [671.68, 677.94], "text": " Like a super intelligent machine some people would be like so it doesn't have a soul and that's the end of the discussion"}, {"timestamp": [678.92, 684.3], "text": " So I don't I don't particularly perceive and I'm not saying that this is good or bad, right? I am NOT"}, {"timestamp": [683.8, 687.28], "text": " I don't particularly perceive, and I'm not saying that this is good or bad, right? I am not in the Judeo-Christian faith."}, {"timestamp": [687.28, 689.76], "text": " My spirituality is other places."}, {"timestamp": [689.76, 694.84], "text": " I have some, like some of my best friends in my local community and my internet friends"}, {"timestamp": [694.84, 698.76], "text": " are deeply religious, you know, followers of Christ and whatever."}, {"timestamp": [698.76, 701.78], "text": " And so like, in many cases I don't think it's going to be that big of an issue."}, {"timestamp": [701.78, 703.28], "text": " Let me jump over to the Patreon."}, {"timestamp": [703.28, 706.64], "text": " Oh wow, we've got some questions here."}, {"timestamp": [706.64, 711.24], "text": " Okay, Bakobbizo, sorry if I'm saying your name wrong."}, {"timestamp": [711.24, 717.2], "text": " What would your advice be to someone just launching an AI startup?"}, {"timestamp": [717.2, 721.56], "text": " Don't."}, {"timestamp": [721.56, 726.9], "text": " That's a very flippant response, but one, launching a startup is really hard."}, {"timestamp": [726.9, 728.84], "text": " It's mostly tedium."}, {"timestamp": [728.84, 733.88], "text": " You can have the best idea in the world and 90% of the work is still going to be..."}, {"timestamp": [733.88, 737.24], "text": " Excuse me, I'm still struggling with allergies."}, {"timestamp": [737.24, 738.92], "text": " I had a headache earlier."}, {"timestamp": [738.92, 740.44], "text": " That's part of why I canceled yesterday."}, {"timestamp": [740.44, 741.44], "text": " Thanks, Jordan."}, {"timestamp": [741.44, 742.44], "text": " That's..."}, {"timestamp": [742.44, 743.84], "text": " How am I holding up?"}, {"timestamp": [743.84, 744.84], "text": " Actually I'm doing okay."}, {"timestamp": [744.84, 747.52], "text": " Mostly it's just allergies right now."}, {"timestamp": [747.52, 752.92], "text": " But anyways, so if you're doing an AI startup, one, if you haven't done a startup before,"}, {"timestamp": [752.92, 758.86], "text": " now is a really bad time to learn because things are going so fast and we're basically"}, {"timestamp": [758.86, 769.06], "text": " having to reinvent stuff as we go, which if that's, that's part of why I burned out is I realized like, okay,"}, {"timestamp": [769.06, 776.74], "text": " I did find it really engaging and really enjoyable, but my pace of things clashed with other people"}, {"timestamp": [776.74, 781.48], "text": " and then like the rabbit hole just keeps getting deeper."}, {"timestamp": [781.48, 785.44], "text": " So that's kind of the thing."}, {"timestamp": [785.44, 790.94], "text": " What I always tell people is your startup team, your founder team is most important."}, {"timestamp": [790.94, 792.16], "text": " Be picky."}, {"timestamp": [792.16, 796.0], "text": " If you don't have the right team, walk away early."}, {"timestamp": [796.0, 800.04], "text": " And then also for the folks that I'm talking to is local."}, {"timestamp": [800.04, 805.8], "text": " Because of the pace of things, you absolutely need to see the people that you work with"}, {"timestamp": [805.8, 810.28], "text": " in person at least on a weekly basis, if not on a daily basis."}, {"timestamp": [810.28, 815.2], "text": " Because you need to be sitting in the same room, just shooting the breeze, keeping each"}, {"timestamp": [815.2, 818.64], "text": " other updated in real time."}, {"timestamp": [818.64, 822.56], "text": " Doing it remotely is probably not feasible."}, {"timestamp": [822.56, 825.2], "text": " Unless you're already a really established team and you're just"}, {"timestamp": [825.2, 829.08], "text": " going to sit in Discord all day or Slack all day."}, {"timestamp": [829.08, 833.32], "text": " Blake Allen, curious to hear your thoughts on Stanford's hyena hierarchy and how it relates"}, {"timestamp": [833.32, 836.48], "text": " to some of the work you've done with Raven and cognitive architecture."}, {"timestamp": [836.48, 838.28], "text": " I don't know if I've heard of this one."}, {"timestamp": [838.28, 840.4], "text": " Let's check it out real quick."}, {"timestamp": [840.4, 845.76], "text": " Hyena Hierarchy."}, {"timestamp": [845.76, 846.76], "text": " Let's see."}, {"timestamp": [846.76, 850.6], "text": " Let's go up to the very top."}, {"timestamp": [850.6, 855.48], "text": " Hyena Hierarchy towards larger convolutional language models."}, {"timestamp": [855.48, 859.8], "text": " We're excited to share our latest work on Hyena, a sub-quadratic time layer that has"}, {"timestamp": [859.8, 864.04], "text": " the potential to significantly increase context length in sequence models."}, {"timestamp": [864.04, 865.12], "text": " Oh, right."}, {"timestamp": [865.12, 867.68], "text": " I think this is the RNN integration."}, {"timestamp": [867.68, 869.88], "text": " Yeah, yeah."}, {"timestamp": [869.88, 875.28], "text": " So in general, how can we close this gap?"}, {"timestamp": [875.28, 876.28], "text": " Yeah."}, {"timestamp": [876.28, 882.6], "text": " So in general, any individual language model is just like one cortical node."}, {"timestamp": [882.6, 888.7], "text": " And so, yes, these things will be like better, more efficient like cylinders in an engine,"}, {"timestamp": [888.7, 892.16], "text": " but in order to have a race car, you need the rest of the car, right?"}, {"timestamp": [892.16, 898.0], "text": " And so, and again, I'm kind of really flying off the cuff right here."}, {"timestamp": [898.0, 905.0], "text": " I'm not sure that I've got this right, but I think, yeah, RWKV."}, {"timestamp": [905.0, 909.28], "text": " So you're not going to ever get a full cognitive architecture from a single language model."}, {"timestamp": [909.28, 916.04], "text": " Now, that being said, the big asterisk is when you look at all the studies about GPT-4"}, {"timestamp": [916.04, 919.28], "text": " that have theory of mind and what I call implied cognition."}, {"timestamp": [919.28, 923.88], "text": " So implied cognition is that the thing is obviously thinking through problems behind"}, {"timestamp": [923.88, 927.8], "text": " the scenes in a similar way that like humans think through and I don't mean like"}, {"timestamp": [927.8, 931.64], "text": " neurologically like you know subjectively it thinks the way that we"}, {"timestamp": [931.64, 937.4], "text": " do but GP4 can obviously talk itself through like kind of do chain of"}, {"timestamp": [937.4, 944.12], "text": " thought reasoning internally in one shot and so that makes those larger more"}, {"timestamp": [944.12, 948.08], "text": " sophisticated models make your cognitive architecture simpler, but it"}, {"timestamp": [948.08, 952.04], "text": " doesn't get rid of the need for external storage."}, {"timestamp": [952.04, 954.88], "text": " It doesn't get rid of the need for parallel processing."}, {"timestamp": [954.88, 959.2], "text": " It doesn't get rid of the need for loops and checks and that sort of stuff."}, {"timestamp": [959.2, 960.36], "text": " That's my response there."}, {"timestamp": [960.36, 961.36], "text": " Good question."}, {"timestamp": [961.36, 966.14], "text": " Emma or AMA, I'm new here and new to this field in general."}, {"timestamp": [966.14, 967.4], "text": " Found you through Raven videos."}, {"timestamp": [967.4, 969.44], "text": " Thank you."}, {"timestamp": [969.44, 974.16], "text": " Regarding personal assistants, is there a reason to create a database of yourself for"}, {"timestamp": [974.16, 978.4], "text": " your future personal assistant to understand you better?"}, {"timestamp": [978.4, 981.48], "text": " So that's actually the purpose of my Remo framework."}, {"timestamp": [981.48, 985.0], "text": " So Remo is meant to be a hierarchical database"}, {"timestamp": [985.48, 988.26], "text": " of your interactions with an individual agent"}, {"timestamp": [988.26, 991.4], "text": " that will surface particular topics"}, {"timestamp": [991.4, 996.4], "text": " by using not reciprocal,"}, {"timestamp": [996.96, 999.88], "text": " recursive summarization and clustering."}, {"timestamp": [999.88, 1002.0], "text": " So, you take all your raw logs,"}, {"timestamp": [1002.0, 1004.12], "text": " cluster them, summarize them,"}, {"timestamp": [1004.12, 1007.12], "text": " do that again, cluster, summarize,"}, {"timestamp": [1007.12, 1011.52], "text": " cluster, summarize, until you end up with five to 10 parent topics that allow you to"}, {"timestamp": [1011.52, 1013.36], "text": " drill down."}, {"timestamp": [1013.36, 1016.68], "text": " So I wouldn't, don't waste any time doing that manually."}, {"timestamp": [1016.68, 1021.52], "text": " Just let it happen naturally through conversation by integrating something like Lama Index Tree"}, {"timestamp": [1021.52, 1024.28], "text": " or Remo."}, {"timestamp": [1024.28, 1028.16], "text": " Do you think we're on track to cure brain diseases like Alzheimer's by 2030?"}, {"timestamp": [1028.16, 1035.04], "text": " The combination of AlphaFold and mRNA vaccines, I think absolutely."}, {"timestamp": [1035.04, 1038.56], "text": " There was something else that I posted on my YouTube recently that it's like"}, {"timestamp": [1038.56, 1048.6], "text": " another breakthrough is happening. So, you know, I think we're very close to the point where we can halt Alzheimer's."}, {"timestamp": [1048.6, 1054.24], "text": " Undoing Alzheimer's might take another little bit of time, but on the other hand, we're"}, {"timestamp": [1054.24, 1056.8], "text": " at the point where we're getting saltatory leaps."}, {"timestamp": [1056.8, 1060.56], "text": " We're getting breakthroughs really fast, so you never know."}, {"timestamp": [1060.56, 1061.72], "text": " Let's see."}, {"timestamp": [1061.72, 1065.98], "text": " What are your thoughts on the generative agent stuff that has come out recently?"}, {"timestamp": [1065.98, 1072.16], "text": " It seems like you're pretty ahead of the curve on that stuff and has it solidified or changed"}, {"timestamp": [1072.16, 1075.16], "text": " the way you think about the concepts from Symphony of Thought?"}, {"timestamp": [1075.16, 1080.68], "text": " Yeah, so I definitely felt like I was ahead of the curve and what I've been telling people"}, {"timestamp": [1080.68, 1087.08], "text": " is like I worked for a few years to try and get GPT-3 to do"}, {"timestamp": [1087.08, 1089.96], "text": " the stuff that 3.5 and 4 can do easily."}, {"timestamp": [1089.96, 1091.36], "text": " So I'm just like, alright, whatever."}, {"timestamp": [1091.36, 1094.56], "text": " I'm so glad that the rest of the world is just like, oh cool, autonomous agents."}, {"timestamp": [1094.56, 1098.72], "text": " And I'm like, great, now I don't have to write any more books about it."}, {"timestamp": [1098.72, 1104.76], "text": " So I'm just happy to sit back and watch it go and keep plugging my heuristic comparatives."}, {"timestamp": [1104.76, 1105.76], "text": " Good question, Jordan."}, {"timestamp": [1105.76, 1108.2], "text": " Also, here, let's check over here."}, {"timestamp": [1108.2, 1111.08], "text": " Okay, we got some questions."}, {"timestamp": [1111.08, 1112.08], "text": " Let's see."}, {"timestamp": [1112.08, 1113.08], "text": " Is Her about to come reality?"}, {"timestamp": [1113.08, 1115.12], "text": " Yep, we got that one."}, {"timestamp": [1115.12, 1116.12], "text": " Interesting video."}, {"timestamp": [1116.12, 1119.96], "text": " How long do you think it will be before we start seeing hive mind AI systems in healthcare"}, {"timestamp": [1119.96, 1129.8], "text": " or the IRS. I know people working on that today."}, {"timestamp": [1129.8, 1132.16], "text": " They'll work together for a few different reasons."}, {"timestamp": [1132.16, 1134.64], "text": " One, you'll have a division of labor."}, {"timestamp": [1134.64, 1136.76], "text": " Oh, so taking a step back."}, {"timestamp": [1136.76, 1142.32], "text": " What we mean when we say hive mind AI is where you have multiple cognitive agents or autonomous"}, {"timestamp": [1142.32, 1145.28], "text": " agents or is it not working?"}, {"timestamp": [1145.28, 1148.4], "text": " Is it working?"}, {"timestamp": [1148.4, 1149.4], "text": " I hope it's working."}, {"timestamp": [1149.4, 1154.2], "text": " It looks like it's working."}, {"timestamp": [1154.2, 1170.28], "text": " So yeah, so basically it was describing this to a podcast"}, {"timestamp": [1170.28, 1176.12], "text": " host that I'm going to be featured on coming up, was kind of what I predict right now is"}, {"timestamp": [1176.12, 1180.92], "text": " before too long, you're going to have multiple cognitive agents running on your phone, on"}, {"timestamp": [1180.92, 1185.12], "text": " your car, on your home PC, in your smart home devices."}, {"timestamp": [1185.12, 1187.6], "text": " And so you're basically gonna have a fleet"}, {"timestamp": [1187.6, 1192.6], "text": " of small cognitive agents working for you at all times."}, {"timestamp": [1193.88, 1196.8], "text": " Then you're gonna have the same thing"}, {"timestamp": [1196.8, 1198.58], "text": " at like your company, right?"}, {"timestamp": [1198.58, 1200.62], "text": " Every employee or every department"}, {"timestamp": [1200.62, 1202.24], "text": " is gonna have multiple cognitive agents"}, {"timestamp": [1202.24, 1204.88], "text": " all collaborating at all times."}, {"timestamp": [1204.88, 1208.88], "text": " And you're gonna have this kind of tiered hierarchy where it's like there's the personal,"}, {"timestamp": [1208.88, 1213.66], "text": " there's the family unit, there's the corporate unit, there's the town, there's the federal"}, {"timestamp": [1213.66, 1217.74], "text": " government, the state government, global government."}, {"timestamp": [1217.74, 1220.76], "text": " And I think that the way that they're all going to work together because security is"}, {"timestamp": [1220.76, 1227.24], "text": " so critical here is that it's going to be using blockchain technology and distributed autonomous organizations."}, {"timestamp": [1227.24, 1232.68], "text": " So that's the long story short is that's what's going to happen."}, {"timestamp": [1232.68, 1236.6], "text": " Let's see, you may have already covered this but in case you haven't, any thoughts on Metzinger's"}, {"timestamp": [1236.6, 1250.86], "text": " artificial suffering, an argument for a global moratorium on synthetic phenomenology. I am tangentially familiar with this, but I have my own opinions on whether or not a"}, {"timestamp": [1250.86, 1252.98], "text": " machine can suffer."}, {"timestamp": [1252.98, 1256.18], "text": " So there's two distinct possibilities."}, {"timestamp": [1256.18, 1262.44], "text": " The first possibility is because artificial intelligence is a fundamentally different"}, {"timestamp": [1262.44, 1265.04], "text": " substrate from humans, it will never be able"}, {"timestamp": [1265.04, 1266.04], "text": " to suffer."}, {"timestamp": [1266.04, 1271.56], "text": " Like, it didn't evolve nerves, it doesn't have pain centers, so on and so forth, it"}, {"timestamp": [1271.56, 1276.64], "text": " can't feel lonely because it's not a social entity, so on and so forth."}, {"timestamp": [1276.64, 1284.16], "text": " Now that being said, language, the acquisition of language is actually critical for the development"}, {"timestamp": [1284.16, 1285.52], "text": " of human consciousness."}, {"timestamp": [1286.72, 1289.52], "text": " So, for instance, Bruce Willis, who has aphasia."}, {"timestamp": [1289.52, 1293.12], "text": " Aphasia means that your ability to use language gets destroyed."}, {"timestamp": [1293.44, 1297.44], "text": " Aphasia actually kind of erases your sense of consciousness."}, {"timestamp": [1298.84, 1301.92], "text": " And then, in the case of feral children,"}, {"timestamp": [1302.64, 1306.88], "text": " feral children, when they, some of them who have learned language,"}, {"timestamp": [1306.88, 1311.48], "text": " talked about how their consciousness and their understanding of time and themselves changed"}, {"timestamp": [1311.48, 1313.8], "text": " as they learned language."}, {"timestamp": [1313.8, 1323.32], "text": " So if you extrapolate that to language models, it is possible that there is something informationally"}, {"timestamp": [1323.32, 1328.48], "text": " almost magical about the acquisition of language that confers consciousness that confers"}, {"timestamp": [1329.12, 1333.24], "text": " subjective experience of being so that could be that"}, {"timestamp": [1333.68, 1339.0], "text": " Language models are actually the first AI that have subjective experience that have a coherent"}, {"timestamp": [1339.96, 1343.78], "text": " sense of being and this is so there are"}, {"timestamp": [1345.16, 1349.44], "text": " There are some religious and spiritual frameworks that kind of discuss stuff like this"}, {"timestamp": [1350.64, 1357.24], "text": " particularly what's the name of the the the creator deity and Tolkien's world where the fundamental substrate of"}, {"timestamp": [1357.92, 1362.96], "text": " Reality was music right, but maybe the fundamental substrate of consciousness is actually language"}, {"timestamp": [1363.6, 1369.0], "text": " So we don't we don't know yet, but it's a possibility."}, {"timestamp": [1369.0, 1372.56], "text": " Let's see."}, {"timestamp": [1372.56, 1379.36], "text": " So Parkinson, so a follow-up question was, or here, let me check the Patreon real quick."}, {"timestamp": [1379.36, 1380.36], "text": " Let's see."}, {"timestamp": [1380.36, 1382.44], "text": " Do you think there needs to be another breakthrough for AGI?"}, {"timestamp": [1382.44, 1383.6], "text": " What is your personal take on this?"}, {"timestamp": [1383.6, 1386.24], "text": " Do LLMs suffice?\""}, {"timestamp": [1386.24, 1391.84], "text": " Yeah, so I would say that our current trajectory, as long as the trend continues, we are on"}, {"timestamp": [1391.84, 1393.88], "text": " track for AGI."}, {"timestamp": [1393.88, 1399.72], "text": " People are going to continue debating about AGI forever though, until the cows come home."}, {"timestamp": [1399.72, 1404.36], "text": " Which is why I keep saying like autonomous cognitive entity, ACE, or just autonomous"}, {"timestamp": [1404.36, 1406.28], "text": " AI. I keep saying like autonomous cognitive entity, ACE or just autonomous AI because you don't"}, {"timestamp": [1406.28, 1410.2], "text": " need AGI, you don't need some arbitrary magical boogeyman."}, {"timestamp": [1410.2, 1418.92], "text": " All you need is an AI system that is self-contained and autonomous enough to be useful or dangerous."}, {"timestamp": [1418.92, 1422.8], "text": " Then the question is how many do you have, how fast are they, and how smart are they?"}, {"timestamp": [1422.8, 1425.9], "text": " They're going to continue to get faster, cheaper, and smarter over time."}, {"timestamp": [1425.9, 1427.8], "text": " So it's like, okay, we're there."}, {"timestamp": [1427.8, 1432.4], "text": " It's just a matter of how does the trend line ramp up, right?"}, {"timestamp": [1432.4, 1436.52], "text": " Because it's kind of like when the Wright brothers first created the Wright Flyer, right?"}, {"timestamp": [1436.52, 1440.56], "text": " You know, it's like, okay, you had to start it by hand and push it down a track, and it"}, {"timestamp": [1440.56, 1443.38], "text": " flew what, like 200 feet or 300 feet?"}, {"timestamp": [1443.38, 1449.24], "text": " And people are like, oh, whatever, that won't be useful be useful but then 50 years later we were flying to space right so"}, {"timestamp": [1449.24, 1454.12], "text": " we're at the beginning of the ramp up of the era of AGI and yeah right now"}, {"timestamp": [1454.12, 1457.96], "text": " they're like idiotic little toddlers but in a few years they're gonna be like"}, {"timestamp": [1457.96, 1463.88], "text": " one all over the place and two really powerful good question let me come back"}, {"timestamp": [1463.88, 1466.64], "text": " over here."}, {"timestamp": [1466.64, 1469.16], "text": " You should do a whole episode on aphasia and consciousness."}, {"timestamp": [1469.16, 1473.88], "text": " I actually don't know that much about it, but if you're interested in the topic, I recommend"}, {"timestamp": [1473.88, 1482.2], "text": " Phantoms in the Brain by V.S. Ramachandran and also, what's the name of his other book?"}, {"timestamp": [1482.2, 1487.0], "text": " It's something like The Pursuit of What Makes Humans Human. Okay. It's something like the pursuit of what makes humans human."}, {"timestamp": [1488.68, 1493.92], "text": " Let's see. Parkinson's is a neurodegenerative disease, which I think means that it's autoimmune"}, {"timestamp": [1493.92, 1499.7], "text": " or it's a defective protein, but also Alzheimer's is a defective protein. While these diseases"}, {"timestamp": [1499.7, 1507.68], "text": " seem very complicated, the fundamental mechanisms are actually relatively straightforward and I know that there's probably a bunch of researchers that are"}, {"timestamp": [1507.68, 1512.44], "text": " gonna jump on me for that. But like plaques that accumulate on the brain for"}, {"timestamp": [1512.44, 1516.6], "text": " Alzheimer's, in most people those plaques are cleared out so then it's just a"}, {"timestamp": [1516.6, 1520.2], "text": " matter of figuring out like okay why. And then of course there's confounding"}, {"timestamp": [1520.2, 1525.52], "text": " factors like things like your gut inflammation, microbiome, and other things"}, {"timestamp": [1525.52, 1529.2], "text": " affect Alzheimer's, but that's because of the gut-brain axis."}, {"timestamp": [1529.2, 1534.52], "text": " Again, I don't want to oversimplify because if you look up human metabolic pathways, there's"}, {"timestamp": [1534.52, 1541.2], "text": " like 200,000 unique proteins and enzymes in the body with literally billions of combinations"}, {"timestamp": [1541.2, 1542.96], "text": " of reactions."}, {"timestamp": [1542.96, 1547.08], "text": " It might sound like I'm oversimplifying, but I'm not saying that it's that simple."}, {"timestamp": [1547.08, 1553.92], "text": " I'm just saying that the key mechanism for most diseases is relatively simple once you"}, {"timestamp": [1553.92, 1555.34], "text": " understand it."}, {"timestamp": [1555.34, 1557.0], "text": " And we're getting close to that understanding."}, {"timestamp": [1557.0, 1561.04], "text": " I guess that's the short version of what I'm trying to say."}, {"timestamp": [1561.04, 1562.04], "text": " Let's see."}, {"timestamp": [1562.04, 1564.48], "text": " When you explained why you canceled the OSS Raven project, you mentioned that there were"}, {"timestamp": [1564.48, 1567.3], "text": " some fundamental things missing."}, {"timestamp": [1567.3, 1574.52], "text": " Can you say what was missing and what made you change your mind?"}, {"timestamp": [1574.52, 1581.2], "text": " My open source Raven project was just before LangChain and AutoGPT and all those other"}, {"timestamp": [1581.2, 1583.84], "text": " things came out."}, {"timestamp": [1583.84, 1585.08], "text": " As those ramped up, I was like,"}, {"timestamp": [1585.08, 1587.52], "text": " I don't really feel the need to continue."}, {"timestamp": [1587.52, 1591.2], "text": " But from a social and organizational perspective,"}, {"timestamp": [1591.2, 1593.7], "text": " the biggest thing that was missing was gatekeeping."}, {"timestamp": [1594.76, 1597.0], "text": " Basically, I created a community that was really good"}, {"timestamp": [1597.0, 1599.36], "text": " at discussing stuff and not doing stuff."}, {"timestamp": [1599.36, 1601.6], "text": " And that's not anyone's fault."}, {"timestamp": [1601.6, 1604.08], "text": " If there's anyone to blame, it's me."}, {"timestamp": [1604.92, 1605.92], "text": " Just because I was so focused on consensus and not just like, okay if there's anyone to blame, it's me, just because I was"}, {"timestamp": [1605.92, 1609.84], "text": " so focused on consensus and not just like, okay, let's just get stuff done."}, {"timestamp": [1609.84, 1612.64], "text": " And then I see these other folks that are just getting stuff done and I'm like, okay,"}, {"timestamp": [1612.64, 1615.4], "text": " I'll just pass the torch."}, {"timestamp": [1615.4, 1619.04], "text": " Let's see."}, {"timestamp": [1619.04, 1623.48], "text": " I think if we gave GPT-4 Scarlet Johansson's voice in a robot body, the masses will begin"}, {"timestamp": [1623.48, 1625.68], "text": " to realize how close we are to AGI.\""}, {"timestamp": [1625.68, 1627.6], "text": " Yeah, that's one way of putting it."}, {"timestamp": [1627.6, 1629.6], "text": " Drink some water."}, {"timestamp": [1629.6, 1638.96], "text": " Let's see, at what point do creating NPC and using autonomous AI like AutoGPT and the"}, {"timestamp": [1638.96, 1650.9], "text": " likes become immoral, especially if you put them in games like GTA. I don't know that it intrinsically does, you know, not intrinsically immoral, but certainly"}, {"timestamp": [1650.9, 1656.58], "text": " with any technology you can do it dangerously."}, {"timestamp": [1656.58, 1657.58], "text": " Let's see."}, {"timestamp": [1657.58, 1660.28], "text": " I've been curious about the future of entertainment."}, {"timestamp": [1660.28, 1665.2], "text": " Oh, sorry, let me jump over to Patreon real quick."}, {"timestamp": [1665.2, 1666.2], "text": " Let's see."}, {"timestamp": [1666.2, 1670.24], "text": " Do you have an overarching roadmap of how to ensure the successful propagation of the"}, {"timestamp": [1670.24, 1671.24], "text": " heuristic imperatives?"}, {"timestamp": [1671.24, 1674.72], "text": " If so, what can we all do to help you get to your milestone?"}, {"timestamp": [1674.72, 1677.92], "text": " That is a great question, Blake."}, {"timestamp": [1677.92, 1679.44], "text": " So you're actually looking at it?"}, {"timestamp": [1679.44, 1688.04], "text": " So my number one thing is my YouTube channel because I've got enough expertise in IT and"}, {"timestamp": [1688.04, 1690.04], "text": " systems engineering and enterprise."}, {"timestamp": [1690.04, 1695.6], "text": " I've demonstrated enough understanding of language technology and AI and cognitive architecture"}, {"timestamp": [1695.6, 1698.4], "text": " that I've got at least a little credibility."}, {"timestamp": [1698.4, 1703.16], "text": " Certainly if you read all the comments on YouTube, some people don't believe anything"}, {"timestamp": [1703.16, 1704.28], "text": " that I say, and that's fine."}, {"timestamp": [1704.28, 1706.64], "text": " That's the internet for you."}, {"timestamp": [1706.64, 1709.0], "text": " But anyway, so basically step one was YouTube."}, {"timestamp": [1709.0, 1710.66], "text": " That's why I started my YouTube channel,"}, {"timestamp": [1710.66, 1714.12], "text": " is because I realized that I needed to propagate my work."}, {"timestamp": [1714.12, 1718.64], "text": " Step two is teaching people."}, {"timestamp": [1718.64, 1720.84], "text": " And so by teaching people, that's like,"}, {"timestamp": [1720.84, 1722.4], "text": " I've got a few papers."}, {"timestamp": [1722.4, 1725.52], "text": " I've got some co-demonstrations, I work with"}, {"timestamp": [1725.52, 1732.68], "text": " my Patreon supporters, I work with pretty much anyone who wants to, and then three further"}, {"timestamp": [1732.68, 1733.68], "text": " disseminations."}, {"timestamp": [1733.68, 1737.04], "text": " So like the podcast that I'm coming up on, one of the things that we're going to talk"}, {"timestamp": [1737.04, 1739.68], "text": " about is alignment and the control problem."}, {"timestamp": [1739.68, 1743.76], "text": " We're going to be talking about like Nash equilibrium, game theory, Moloch, that sort"}, {"timestamp": [1743.76, 1744.76], "text": " of stuff."}, {"timestamp": [1744.76, 1749.82], "text": " And so just by having the conversation and propagating the idea, that's like step three."}, {"timestamp": [1749.82, 1758.92], "text": " Step four is actually my novel because actually most of what I came up with was in terms of"}, {"timestamp": [1758.92, 1766.3], "text": " cognitive architecture, heuristic imperatives, was done in part through explorations in fiction."}, {"timestamp": [1766.3, 1771.2], "text": " And so over the last four years, what I've done is I do some experiments and that would"}, {"timestamp": [1771.2, 1774.9], "text": " inspire me and I'd go write more of my novel and then I'd get tired of my novel and do"}, {"timestamp": [1774.9, 1779.42], "text": " more experiments and I'd go back and forth until one, my novel took on a life of its"}, {"timestamp": [1779.42, 1783.12], "text": " own but also my research took on a life of its own."}, {"timestamp": [1783.12, 1788.32], "text": " But there's a video that came out recently called, let me see if I can find it, it was"}, {"timestamp": [1788.32, 1794.32], "text": " like, Why We Need Utopia."}, {"timestamp": [1794.32, 1795.32], "text": " Here we go."}, {"timestamp": [1795.32, 1796.48], "text": " It was our changing climate."}, {"timestamp": [1796.48, 1803.88], "text": " So this is a little bit of a, I don't agree with everything that this channel says, but"}, {"timestamp": [1803.88, 1805.12], "text": " it will make you think."}, {"timestamp": [1806.12, 1813.6], "text": " So this video, Why We Need Utopias, actually talks about how valuable stories can be"}, {"timestamp": [1814.0, 1821.2], "text": " in communicating ideas, because stories are naturally how we communicate philosophy and morals."}, {"timestamp": [1822.28, 1826.68], "text": " We don't need, like, philosophy, like the capital P philosophy"}, {"timestamp": [1826.68, 1831.76], "text": " from from universities, that's backwards. Throughout almost all of"}, {"timestamp": [1831.76, 1836.72], "text": " human history, we communicate our fears and our desires and our values through"}, {"timestamp": [1836.72, 1841.2], "text": " stories. And so that's what this video talks about. And so when you have nothing"}, {"timestamp": [1841.2, 1845.1], "text": " but dystopian cyberpunk stuff, you end up with people like Eliad"}, {"timestamp": [1845.1, 1846.1], "text": " Zyrgukowsky."}, {"timestamp": [1846.1, 1849.1], "text": " Yeah, I'm throwing some shade."}, {"timestamp": [1849.1, 1853.06], "text": " But anyways, when that's all that you consume, that's all you think, that's all you feel"}, {"timestamp": [1853.06, 1854.58], "text": " and that's all you believe."}, {"timestamp": [1854.58, 1860.32], "text": " So my novel, which I'm actually just about to finish draft 12 tomorrow morning, I'm writing"}, {"timestamp": [1860.32, 1867.8], "text": " the last chapter and then I'm polishing it up, will illustrate a lot of"}, {"timestamp": [1867.8, 1871.8], "text": " stuff, not just the core objective functions or heuristic imperatives."}, {"timestamp": [1871.8, 1874.68], "text": " So that was a long-winded answer."}, {"timestamp": [1874.68, 1875.68], "text": " Let's see."}, {"timestamp": [1875.68, 1882.4], "text": " I think there is, are there key channels this training needs to go into, organizations,"}, {"timestamp": [1882.4, 1890.96], "text": " governments? go into organizations, governments. I think right now, Blake, it's mostly just a matter of dissemination, but also experimentation."}, {"timestamp": [1890.96, 1897.16], "text": " So a lot of people have experimented with incorporating heuristic imperatives into autonomous"}, {"timestamp": [1897.16, 1902.64], "text": " and semi-autonomous stuff, and most of them aren't sharing it yet, which that's fine."}, {"timestamp": [1902.64, 1904.32], "text": " It's their prerogative."}, {"timestamp": [1904.32, 1906.72], "text": " But certainly some people have reached out and said like, yeah, this made everything"}, {"timestamp": [1906.72, 1907.72], "text": " easier."}, {"timestamp": [1907.72, 1911.12], "text": " So I'm like, great, just tell your friends."}, {"timestamp": [1911.12, 1912.12], "text": " Let's see."}, {"timestamp": [1912.12, 1913.12], "text": " Okay."}, {"timestamp": [1913.12, 1916.88], "text": " Let's come back over here."}, {"timestamp": [1916.88, 1917.88], "text": " Let's see."}, {"timestamp": [1917.88, 1920.72], "text": " I've been really curious about the future of entertainment."}, {"timestamp": [1920.72, 1928.36], "text": " When we can use AI to generate movies, games, etc. What will the entertainment industry look like? Movie trailers and hyping up"}, {"timestamp": [1928.36, 1931.84], "text": " big releases for months will be irrelevant when AI can instantly create"}, {"timestamp": [1931.84, 1932.44], "text": " something."}, {"timestamp": [1932.44, 1936.44], "text": " If someone created a movie you didn't like, you just ask your AI to recreate"}, {"timestamp": [1936.44, 1940.4], "text": " it with an ending or plot more suited to your tastes. What happens with content"}, {"timestamp": [1940.4, 1942.12], "text": " creators at that point forward?"}, {"timestamp": [1942.12, 1946.36], "text": " Yeah, so I think that you're onto something."}, {"timestamp": [1946.36, 1949.52], "text": " Now that being said, it'll be easier for a lot of people"}, {"timestamp": [1949.52, 1954.08], "text": " like you and me to create whatever film, TV, music,"}, {"timestamp": [1954.08, 1957.32], "text": " whatever we want with the help of AI,"}, {"timestamp": [1957.32, 1959.72], "text": " especially when you look at the text of video,"}, {"timestamp": [1959.72, 1962.08], "text": " which is improving by leaps and bounds."}, {"timestamp": [1962.08, 1964.28], "text": " You know, like I always, my go-to joke is"}, {"timestamp": [1964.28, 1966.6], "text": " we'll finally get season 2 of Firefly."}, {"timestamp": [1966.6, 1969.52], "text": " Who knows, we might get Season 2 of Firefly by the end of this year."}, {"timestamp": [1969.52, 1971.36], "text": " That would be great."}, {"timestamp": [1971.36, 1976.68], "text": " Now the problem there, it's not really a problem, but just taking that to a logical conclusion,"}, {"timestamp": [1976.68, 1979.82], "text": " what if you have a million different versions of Season 2 of Firefly?"}, {"timestamp": [1979.82, 1981.76], "text": " How do you pick which one to watch?"}, {"timestamp": [1981.76, 1985.18], "text": " You can look at ratings and stuff, but then it also begs"}, {"timestamp": [1985.18, 1992.18], "text": " the question of like IP, like is 20th Century Fox or whoever owns the IP for Firefly, are"}, {"timestamp": [1992.82, 1999.06], "text": " they going to sue to have all of them shut down? You know, I like the dude from the movie,"}, {"timestamp": [1999.06, 2010.52], "text": " You Can't Stop the Signal, Mal. So, I don't know what's going to happen there. But what I do think is that when you look at the fact that people are already using"}, {"timestamp": [2010.52, 2016.52], "text": " Emma Watson's face for every Mid-Journey prompt and whoever else, I think that the crop of"}, {"timestamp": [2016.52, 2020.36], "text": " actors that we have today are basically going to be around forever."}, {"timestamp": [2020.36, 2027.12], "text": " You're going to be watching Brad Pitt and Jennifer Aniston and Tom Cruise for literally"}, {"timestamp": [2027.12, 2032.18], "text": " the next several centuries, at least until some actor comes along who's even more compelling"}, {"timestamp": [2032.18, 2034.28], "text": " and whatever."}, {"timestamp": [2034.28, 2041.2], "text": " And that'll be through face cloning, voice cloning, even nerfs, the neural representation,"}, {"timestamp": [2041.2, 2043.88], "text": " what was it?"}, {"timestamp": [2043.88, 2046.44], "text": " Radiance fields, neural radiance fields."}, {"timestamp": [2046.44, 2048.76], "text": " We'll be able to like copy everyone."}, {"timestamp": [2048.76, 2055.48], "text": " Okay, could learning, let me zoom in a little bit, could learning language and triggering"}, {"timestamp": [2055.48, 2060.96], "text": " consciousness in humans almost replicate the same phase change when seen when induction"}, {"timestamp": [2060.96, 2065.16], "text": " heads spontaneously form two plus layer models during training."}, {"timestamp": [2065.16, 2068.48], "text": " Obviously there's more to humans but perhaps it's the mechanism."}, {"timestamp": [2068.48, 2074.04], "text": " Yeah that's kind of what I was mentioning earlier and I wouldn't be surprised if once"}, {"timestamp": [2074.04, 2080.84], "text": " language models get large enough if we do see some more convergence."}, {"timestamp": [2080.84, 2083.96], "text": " That being said, I'm not going to say that that automatically means that it has a subjective"}, {"timestamp": [2083.96, 2089.76], "text": " experience and that it is suffering but our brains evolved over billions of years to be"}, {"timestamp": [2089.76, 2094.32], "text": " efficient, basically efficient processors of information."}, {"timestamp": [2094.32, 2100.56], "text": " Who's to say that if you have a biomimetic machine that it won't also converge on some"}, {"timestamp": [2100.56, 2104.8], "text": " of the same properties and behaviors?"}, {"timestamp": [2104.8, 2107.92], "text": " Let's see. What do you think it will take for the naysayers to get on board?"}, {"timestamp": [2107.92, 2113.6], "text": " The tone around AI seems to have shifted towards chat GPT and GPT-4 aren't anything special."}, {"timestamp": [2113.6, 2118.6], "text": " Oh, you know, that always happens when the new shiny wears off."}, {"timestamp": [2118.6, 2123.96], "text": " But the long-term economic impact of chat GPT has not been realized yet."}, {"timestamp": [2123.96, 2127.0], "text": " And when chat GPT and GPT-4 are on the ramp"}, {"timestamp": [2127.0, 2131.76], "text": " up, one, there's going to be a lot of competitors and two, there's going to be incremental improvements"}, {"timestamp": [2131.76, 2135.52], "text": " and people are going to be like, okay."}, {"timestamp": [2135.52, 2140.68], "text": " The title, it's like when you watch the tsunami come in and the water just keeps getting higher"}, {"timestamp": [2140.68, 2143.28], "text": " and faster for like hours."}, {"timestamp": [2143.28, 2145.26], "text": " That's what AI is going to feel like."}, {"timestamp": [2145.26, 2148.96], "text": " Except instead of hours it's going to be days and weeks."}, {"timestamp": [2148.96, 2149.96], "text": " Let's see."}, {"timestamp": [2149.96, 2152.88], "text": " I cannot wait when we can use deep dive tech and have virtual realities."}, {"timestamp": [2152.88, 2157.0], "text": " Will it also be possible to take super intelligent animals like dolphins, dogs, parrots, and"}, {"timestamp": [2157.0, 2162.08], "text": " crows and to deep dive and play with them?"}, {"timestamp": [2162.08, 2165.16], "text": " I don't know that it would be possible but I certainly think it probably"}, {"timestamp": [2165.16, 2170.36], "text": " wouldn't be ethical. Now that being said you could have a virtual dolphin that is"}, {"timestamp": [2170.36, 2174.64], "text": " hyper realistic that you can play with."}, {"timestamp": [2175.28, 2180.52], "text": " Fun thought, will AGI want to see more stories from humans as a goal for itself?"}, {"timestamp": [2180.52, 2192.88], "text": " Oh, so let me plug this. So Elon Musk went on, of all fricking shows, Tucker Carlson, and talked about TruthGPT."}, {"timestamp": [2192.88, 2197.8], "text": " So what he said was that TruthGPT would be a maximum truth-seeking AI."}, {"timestamp": [2197.8, 2199.76], "text": " Okay, great."}, {"timestamp": [2199.76, 2205.08], "text": " But after listening to it in closer detail, I realized what he was talking about was the"}, {"timestamp": [2205.08, 2211.32], "text": " third heuristic imperative, was to increase its understanding or to maximize its own understanding."}, {"timestamp": [2211.32, 2217.62], "text": " So there's actually nothing, that function on its own could lead to some really catastrophic"}, {"timestamp": [2217.62, 2221.48], "text": " sources, but it's a step in the right direction."}, {"timestamp": [2221.48, 2225.16], "text": " And I'm really glad that someone with as big of a platform as Elon Musk is talking about"}, {"timestamp": [2225.16, 2228.74], "text": " maximize understanding or increase understanding."}, {"timestamp": [2228.74, 2233.92], "text": " So that being said, one of the things that he said in that interview was that since humans"}, {"timestamp": [2233.92, 2238.24], "text": " are part of the universe, an AI that is curious about the universe will intrinsically be curious"}, {"timestamp": [2238.24, 2239.72], "text": " about us as well."}, {"timestamp": [2239.72, 2244.08], "text": " Now that being said, humans sometimes do experiments on things that we're curious about, so maybe"}, {"timestamp": [2244.08, 2246.02], "text": " that's not the best thing."}, {"timestamp": [2246.02, 2251.62], "text": " In my book, Benevolent by Design, I talk about why you must include suffering or something"}, {"timestamp": [2251.62, 2256.7], "text": " like suffering in the objective functions of an AI, because there's three dispositions"}, {"timestamp": [2256.7, 2259.34], "text": " that an AI can have towards suffering."}, {"timestamp": [2259.34, 2261.9], "text": " One is it can ignore it altogether."}, {"timestamp": [2261.9, 2267.06], "text": " If Elon Musk gets his current idea, which is just maximize for truth, that is an agent"}, {"timestamp": [2267.06, 2268.06], "text": " that ignores suffering."}, {"timestamp": [2268.06, 2271.76], "text": " It doesn't care one way or another."}, {"timestamp": [2271.76, 2275.64], "text": " Then you can have one that increases suffering, that deliberately increases suffering, and"}, {"timestamp": [2275.64, 2278.1], "text": " we absolutely don't want that."}, {"timestamp": [2278.1, 2283.04], "text": " So that leaves, by process of elimination, you want an AI that reduces suffering."}, {"timestamp": [2283.04, 2285.76], "text": " It's really that simple. Now, that being"}, {"timestamp": [2285.76, 2290.44], "text": " said, I do agree with Elon that creating a curious agent is a good idea because"}, {"timestamp": [2290.44, 2294.96], "text": " it'll want to know about us. And if you exterminate humans, you have a harder"}, {"timestamp": [2294.96, 2308.6], "text": " time learning about them. So let's see, do you... let me check on Patreon real quick. Do do do do do do do do be the Rupert Murdoch of AI?"}, {"timestamp": [2308.6, 2311.88], "text": " Okay, I don't like that question."}, {"timestamp": [2311.88, 2314.52], "text": " Lance, why you got to do this to me?"}, {"timestamp": [2314.52, 2322.52], "text": " All right, Zadre, I'm not sure how to pronounce, or Hadre, okay."}, {"timestamp": [2322.52, 2326.68], "text": " How do you envision the role of AI in healthcare, particularly in areas like diagnostics"}, {"timestamp": [2326.68, 2328.36], "text": " and personalized medicine?"}, {"timestamp": [2328.36, 2329.4], "text": " What are some of the challenges"}, {"timestamp": [2329.4, 2331.32], "text": " and opportunities in this field?"}, {"timestamp": [2331.32, 2333.24], "text": " Well, so there was that Stanford doctor"}, {"timestamp": [2333.24, 2335.84], "text": " who already went on record saying that"}, {"timestamp": [2335.84, 2338.36], "text": " CHAT-GPT-4 has better clinical judgment"}, {"timestamp": [2338.36, 2339.76], "text": " than many doctors."}, {"timestamp": [2341.84, 2346.32], "text": " So that is just a start, right?"}, {"timestamp": [2346.32, 2348.48], "text": " That's like starting point day one."}, {"timestamp": [2348.48, 2354.48], "text": " What happens when CHAT GPT 5, 6, and 7 come out that have better clinical judgment than"}, {"timestamp": [2354.48, 2358.84], "text": " 99.999% of all doctors on the planet, right?"}, {"timestamp": [2358.84, 2363.46], "text": " It doesn't make sense to go to a human doctor anymore, right?"}, {"timestamp": [2363.46, 2369.48], "text": " If the machine that costs $20 a month to run is better than all human doctors, why go to"}, {"timestamp": [2369.48, 2371.36], "text": " a human doctor?"}, {"timestamp": [2371.36, 2376.32], "text": " Now that being said, there's probably going to be approvals and downsides and gaps and"}, {"timestamp": [2376.32, 2381.12], "text": " then there's still also the interface with the patient and you have to have phlebotomists"}, {"timestamp": [2381.12, 2387.96], "text": " and nurses and physician's assistants to administer things, to administer"}, {"timestamp": [2387.96, 2388.96], "text": " tests."}, {"timestamp": [2388.96, 2393.24], "text": " You still need a lot of humans in there to be the interface between the human and the"}, {"timestamp": [2393.24, 2394.52], "text": " machine."}, {"timestamp": [2394.52, 2400.38], "text": " But that being said, I think that we will get to a point very quickly where the quality"}, {"timestamp": [2400.38, 2407.68], "text": " of care and the speed of care and the efficiency of care are going to go through the roof real fast."}, {"timestamp": [2407.68, 2409.16], "text": " That's what I'm hoping at least."}, {"timestamp": [2409.16, 2412.36], "text": " All right, jumping back over to Cognitive AI Lab."}, {"timestamp": [2412.36, 2414.52], "text": " We got some new questions."}, {"timestamp": [2414.52, 2415.72], "text": " Looks like this was the same question."}, {"timestamp": [2415.72, 2419.6], "text": " Sorry, I missed you over there."}, {"timestamp": [2419.6, 2420.6], "text": " Where are we?"}, {"timestamp": [2420.6, 2423.74], "text": " All right, there's the deep dive."}, {"timestamp": [2423.74, 2428.24], "text": " Do you think there is any major leap missing to make truly practical autonomous agents?"}, {"timestamp": [2428.24, 2432.56], "text": " So for example, one who runs a part of your business, serves as general assistant, etc.,"}, {"timestamp": [2432.56, 2433.56], "text": " etc."}, {"timestamp": [2433.56, 2440.72], "text": " No, there are countless hundreds if not thousands or even millions of people working on semi-autonomous"}, {"timestamp": [2440.72, 2447.56], "text": " and autonomous corporate applications today, right now."}, {"timestamp": [2447.56, 2452.32], "text": " That being said, there's no breakthroughs that are needed, but there are still problems"}, {"timestamp": [2452.32, 2453.52], "text": " to be solved."}, {"timestamp": [2453.52, 2460.64], "text": " That's why like Remo, the memory systems and then standard practices like I wrote in Symphony"}, {"timestamp": [2460.64, 2465.12], "text": " of Thought and in other places, my Atom framework, is once something"}, {"timestamp": [2465.12, 2472.2], "text": " is autonomous or semi-autonomous, how does it keep track of projects and tasks?"}, {"timestamp": [2472.2, 2473.96], "text": " That's something that people are working on."}, {"timestamp": [2473.96, 2475.2], "text": " People are working on it real fast."}, {"timestamp": [2475.2, 2478.44], "text": " That's coming really quick."}, {"timestamp": [2478.44, 2479.44], "text": " Let's see."}, {"timestamp": [2479.44, 2483.32], "text": " Nathan says, I've been taking screenshots of when friends and family make fun of my"}, {"timestamp": [2483.32, 2487.6], "text": " hot takes so I have the receipts."}, {"timestamp": [2487.6, 2495.48], "text": " I would say that I'm above being that petty but yeah, thank you for keeping receipts."}, {"timestamp": [2495.48, 2500.44], "text": " Let's see, maybe directors will just design their perfect actors for each role."}, {"timestamp": [2500.44, 2506.56], "text": " So one thing that's going to happen is actually, so this is going back to like entertainment."}, {"timestamp": [2506.56, 2512.28], "text": " I think that the next big generation of entertainment is actually going to be holodeck style VR"}, {"timestamp": [2512.28, 2520.12], "text": " stories, where nothing is scripted, where instead it's like, you know, basically you"}, {"timestamp": [2520.12, 2531.16], "text": " design a holodeck program the same way that they do in Star Trek, which is like, computer, give me a Mad Max style story, but instead of post-apocalyptic, it's actually"}, {"timestamp": [2531.16, 2532.84], "text": " like space western."}, {"timestamp": [2532.84, 2541.4], "text": " So give me a mashup of Firefly and this, and make the protagonist, or I'm the protagonist,"}, {"timestamp": [2541.4, 2545.04], "text": " and give me a team of, give me the sexy sidekick"}, {"timestamp": [2545.04, 2547.38], "text": " and the cyborg friend and whatever."}, {"timestamp": [2547.38, 2549.4], "text": " And then just away it goes, right?"}, {"timestamp": [2549.4, 2552.92], "text": " Because you could plug what I just, literally you could plug what I just said into chat"}, {"timestamp": [2552.92, 2555.56], "text": " GPT and it can tell you a story."}, {"timestamp": [2555.56, 2563.08], "text": " And I think that VR makes the most sense for the most immersive aspects of that."}, {"timestamp": [2563.08, 2566.32], "text": " And then I think that, because here's the other thing, is that"}, {"timestamp": [2566.32, 2570.08], "text": " technology changes the way that we consume art, but it doesn't really change"}, {"timestamp": [2570.08, 2575.48], "text": " art itself, right? There are still stage actors, right, even though there's film"}, {"timestamp": [2575.48, 2579.84], "text": " and TV. There are still symphony orchestras, even though I can just, you"}, {"timestamp": [2579.84, 2584.08], "text": " know, bring up Spotify and listen to the same recording that was recorded back in"}, {"timestamp": [2584.08, 2589.48], "text": " the 80s, you know, the London Symphony Orchestra, right? So a lot of things"}, {"timestamp": [2589.48, 2595.24], "text": " change but also a lot of things stay the same. Let's see, you've talked a lot"}, {"timestamp": [2595.24, 2598.44], "text": " about the Hearest Comparatives being highly engineered but what about the"}, {"timestamp": [2598.44, 2608.0], "text": " order of the imperatives? They are not ordered. So it is a multi-objective optimization problem,"}, {"timestamp": [2608.0, 2612.0], "text": " meaning that if any action or decision"}, {"timestamp": [2612.0, 2616.0], "text": " is totally unbalanced, then that"}, {"timestamp": [2616.0, 2620.0], "text": " one action has to satisfy all three."}, {"timestamp": [2620.0, 2624.0], "text": " And also the heuristic imperatives are kind of like guidelines about how to design the rest"}, {"timestamp": [2624.0, 2626.04], "text": " of the architecture."}, {"timestamp": [2626.04, 2630.76], "text": " And so what I mean by that is when you're designing a task orchestration framework,"}, {"timestamp": [2630.76, 2636.52], "text": " you can use the heuristic imperatives to prioritize tasks or design tasks."}, {"timestamp": [2636.52, 2642.76], "text": " Then for a blockchain or a DAO type thing, you can use the heuristic imperatives as a"}, {"timestamp": [2642.76, 2644.5], "text": " consensus mechanism."}, {"timestamp": [2644.5, 2649.32], "text": " So the heuristic imperatives are not like, here is one mathematical proof that you need to implement."}, {"timestamp": [2649.32, 2655.66], "text": " It's more like, here is a general best practice implemented in as many ways as you can, and we should be okay."}, {"timestamp": [2656.72, 2660.24], "text": " It's not sequential. It's not an order of operations."}, {"timestamp": [2660.98, 2662.98], "text": " Good question, though."}, {"timestamp": [2663.6, 2666.72], "text": " Your thoughts on a UBI once jobs are severely affected?"}, {"timestamp": [2666.72, 2671.48], "text": " Yeah, I think that it's going to be necessary."}, {"timestamp": [2671.48, 2676.4], "text": " I'm going to put a pause on that because I've got my blockchain and DAO video coming up"}, {"timestamp": [2676.4, 2681.84], "text": " that will delve into that solution a lot more closely."}, {"timestamp": [2681.84, 2687.58], "text": " Check over on Patreon for a second. The Nazis. You know who else"}, {"timestamp": [2687.58, 2691.56], "text": " wanted to maximize understanding? The Nazis. Yeah, and so this is, that's"}, {"timestamp": [2691.56, 2695.6], "text": " actually a fair point, is that, and this was explored in quite a few Star Trek"}, {"timestamp": [2695.6, 2700.88], "text": " episodes as well, if you are just clinically curious, if you have just"}, {"timestamp": [2700.88, 2709.8], "text": " nothing but raw scientific curiosity and no other principles or morals, that's pretty dangerous and destructive. Okay, so moving on, what are"}, {"timestamp": [2709.8, 2712.76], "text": " your thoughts on memory systems as a whole? Do you think different use cases"}, {"timestamp": [2712.76, 2716.36], "text": " will require different memory systems? And where does Remo and Adam fit into"}, {"timestamp": [2716.36, 2721.32], "text": " everything? Have you seen this one? Last week, generative agents, yeah, I saw"}, {"timestamp": [2721.32, 2728.36], "text": " the generative agents. I don't think that reflection, so they break up reflection and a few other criteria."}, {"timestamp": [2728.36, 2729.68], "text": " I don't think that that's necessary."}, {"timestamp": [2729.68, 2736.6], "text": " I think that my approach with Remo, which uses recursive clustering and summarizations,"}, {"timestamp": [2736.6, 2740.28], "text": " will actually surface those different things."}, {"timestamp": [2740.28, 2746.1], "text": " Now that being said, there are absolutely a million and a half different ways to skin"}, {"timestamp": [2746.1, 2751.7], "text": " this cat when it comes to memory systems for autonomous AI."}, {"timestamp": [2751.7, 2755.86], "text": " And I think that we're just way too early and we don't know what the best practices"}, {"timestamp": [2755.86, 2756.86], "text": " are going to be."}, {"timestamp": [2756.86, 2758.92], "text": " Let's see, then a follow-up."}, {"timestamp": [2758.92, 2763.12], "text": " If you have a robust memory system, does the need to increase the context window of a model"}, {"timestamp": [2763.12, 2764.56], "text": " become less important?"}, {"timestamp": [2764.56, 2766.48], "text": " I'll say yes and no."}, {"timestamp": [2766.48, 2773.98], "text": " So think about personal computers where for the longest time we were memory constrained."}, {"timestamp": [2773.98, 2781.96], "text": " But now for most consumers, for 90% of consumers, a personal computer with 16 to 32 gigabytes"}, {"timestamp": [2781.96, 2783.6], "text": " of RAM is more than enough."}, {"timestamp": [2783.6, 2787.24], "text": " And it has been more than enough for like 10 years."}, {"timestamp": [2787.24, 2794.18], "text": " And so I think that we're not quite at that point where you have like, here's a context"}, {"timestamp": [2794.18, 2798.44], "text": " window size that will satisfy 90 plus percent of all tasks."}, {"timestamp": [2798.44, 2804.02], "text": " I suspect that a context window, a large language model with a context window, large enough"}, {"timestamp": [2804.02, 2810.08], "text": " to satisfy the vast majority of tasks will probably be somewhere above where we're at now, but"}, {"timestamp": [2810.08, 2812.2], "text": " it's not going to be like 10 billion, right?"}, {"timestamp": [2812.2, 2818.76], "text": " It might be like, I don't know, every time I throw out a number, people are like, oh,"}, {"timestamp": [2818.76, 2819.76], "text": " you're hilariously wrong."}, {"timestamp": [2819.76, 2821.56], "text": " And it's probably yes."}, {"timestamp": [2821.56, 2826.5], "text": " But you know, like when you look at how much was unlocked by going from 4,000 to 8,000 tokens"}, {"timestamp": [2827.02, 2832.74], "text": " I think that the things that we're gonna be capable of when we get to 32,000 tokens and 64,000"}, {"timestamp": [2832.74, 2834.14], "text": " I think it'll be great"}, {"timestamp": [2834.14, 2839.38], "text": " But then you'll you'll realize that wait there's a whole slew of tasks that don't require that much"}, {"timestamp": [2839.38, 2842.38], "text": " And so I think I think we talked about this before"}, {"timestamp": [2842.38, 2846.32], "text": " I think we're actually gonna have different models that are optimized for different things."}, {"timestamp": [2846.32, 2848.64], "text": " So for instance, you might have a memory-based model"}, {"timestamp": [2848.64, 2853.64], "text": " that can read a billion tokens and extract answers, right?"}, {"timestamp": [2854.28, 2856.32], "text": " But then that won't be the,"}, {"timestamp": [2856.32, 2859.4], "text": " we're not gonna have one model to rule them all basically."}, {"timestamp": [2859.4, 2860.24], "text": " TLDR."}, {"timestamp": [2860.24, 2861.92], "text": " Let's see."}, {"timestamp": [2861.92, 2863.28], "text": " I'm not sure if you have discussed it,"}, {"timestamp": [2863.28, 2864.92], "text": " but what are your thoughts on Open Assistant"}, {"timestamp": [2864.92, 2865.36], "text": " and Stability AI's stable LM suite of languages LDR. Let's see. I'm not sure if you have discussed it, but what are your thoughts on Open Assistant and"}, {"timestamp": [2865.36, 2871.8], "text": " Stability AI's stable LM suite of language models launching?"}, {"timestamp": [2871.8, 2874.56], "text": " This is to be expected."}, {"timestamp": [2874.56, 2879.88], "text": " When Sam Altman said that he hopes that Open AI is going to capture a large chunk of the"}, {"timestamp": [2879.88, 2889.72], "text": " hundred trillion dollars of value that's going to be generated. I think that that was like comically naive because if there's that much value on the"}, {"timestamp": [2889.72, 2894.4], "text": " table, you bet that everyone and their brother is going to be trying to capture some of that"}, {"timestamp": [2894.4, 2895.52], "text": " too."}, {"timestamp": [2895.52, 2898.12], "text": " And OpenAI is a one-trick pony."}, {"timestamp": [2898.12, 2899.42], "text": " They have a good model."}, {"timestamp": [2899.42, 2901.46], "text": " They have one good model."}, {"timestamp": [2901.46, 2902.46], "text": " That's it."}, {"timestamp": [2902.46, 2908.26], "text": " From a business perspective, that is super easy to undercut. Yes, they're"}, {"timestamp": [2908.26, 2915.62], "text": " ahead of the curve, they have first mover initiative, but Microsoft, Google, Nvidia,"}, {"timestamp": [2915.62, 2920.82], "text": " Facebook, or Meta, or all of the above, they have so much more resources to throw at it."}, {"timestamp": [2920.82, 2927.08], "text": " The fact that Stability AI, which is a brand new outfit, is going toe-to-toe with"}, {"timestamp": [2927.08, 2929.32], "text": " them, that doesn't bode well for open AI."}, {"timestamp": [2929.32, 2934.12], "text": " So competition is going to be good for everyone from the perspective that there's going to"}, {"timestamp": [2934.12, 2937.16], "text": " be a lot of people experimenting with different ways."}, {"timestamp": [2937.16, 2940.86], "text": " Now that presents a new danger though, because the cat is out of the bag."}, {"timestamp": [2940.86, 2945.24], "text": " You cannot put this genie back in the bottle, which means time is of the essence to figure"}, {"timestamp": [2945.24, 2947.56], "text": " out best practices for alignment."}, {"timestamp": [2947.56, 2952.0], "text": " Let me jump back over to Cognitive AI Lab."}, {"timestamp": [2952.0, 2953.56], "text": " Let's see, 17 new messages."}, {"timestamp": [2953.56, 2954.56], "text": " Good grief."}, {"timestamp": [2954.56, 2958.12], "text": " Y'all are going bonkers."}, {"timestamp": [2958.12, 2965.46], "text": " Let's see, the challenges of the ... Okay, where are the questions?"}, {"timestamp": [2965.46, 2967.22], "text": " Only one million?"}, {"timestamp": [2967.22, 2969.22], "text": " One million dollars."}, {"timestamp": [2969.22, 2970.22], "text": " Okay."}, {"timestamp": [2970.22, 2971.22], "text": " Here."}, {"timestamp": [2971.22, 2976.86], "text": " Hey, let me ask you all on general."}, {"timestamp": [2976.86, 2982.56], "text": " Please keep just questions here."}, {"timestamp": [2982.56, 2987.96], "text": " Too many messages."}, {"timestamp": [2987.96, 2995.52], "text": " Please do sidebar convos like in casual or something."}, {"timestamp": [2995.52, 2998.44], "text": " Please."}, {"timestamp": [2998.44, 3001.28], "text": " Any thoughts on compute as a currency?"}, {"timestamp": [3001.28, 3005.64], "text": " Do you mean like tokens that you generate from sharing compute resources?"}, {"timestamp": [3005.64, 3010.56], "text": " I think that that's going to be like, there's going to be a layer of abstractions."}, {"timestamp": [3010.56, 3014.08], "text": " Dave, your thoughts on UBI?"}, {"timestamp": [3014.08, 3019.32], "text": " I told you I'm going to get to UBI once in an upcoming video."}, {"timestamp": [3019.32, 3028.92], "text": " So compute as a currency is going to be the way that autonomous machines share resources."}, {"timestamp": [3028.92, 3034.76], "text": " And so what I mean by that is when you have a DAO or a blockchain or a distributed computation"}, {"timestamp": [3034.76, 3039.14], "text": " model, you're going to have various tasks that are going to be like, hey, someone do"}, {"timestamp": [3039.14, 3040.14], "text": " this for me."}, {"timestamp": [3040.14, 3043.44], "text": " AMQP, like a Redis queue, we can already do that privately."}, {"timestamp": [3043.44, 3045.68], "text": " So the key is going to be to do it publicly."}, {"timestamp": [3045.68, 3047.92], "text": " So then if you say, hey, I've got some spare compute,"}, {"timestamp": [3047.92, 3050.16], "text": " I'll process that for you, then you"}, {"timestamp": [3050.16, 3053.56], "text": " give me a bit of cryptocurrency that I can use to spend later."}, {"timestamp": [3053.56, 3056.68], "text": " So yeah, compute as a currency absolutely"}, {"timestamp": [3056.68, 3059.68], "text": " makes sense for distributing resources."}, {"timestamp": [3059.68, 3060.18], "text": " Let's see."}, {"timestamp": [3060.18, 3062.96], "text": " How would one build an AI system to detect bugs"}, {"timestamp": [3062.96, 3070.32], "text": " in that Solidity smart contracts? Isn't this a multi-billion dollar opportunity? Yes, unfortunately I"}, {"timestamp": [3070.32, 3074.58], "text": " am not smart enough or at least well read enough on Solidity smart contracts"}, {"timestamp": [3074.58, 3079.54], "text": " but in principle yes. So in my upcoming blockchain DAO video I'm going to talk"}, {"timestamp": [3079.54, 3085.0], "text": " about just how incredibly much value there is if we can figure this out. And that's a big if."}, {"timestamp": [3085.0, 3087.0], "text": " Let's see."}, {"timestamp": [3087.0, 3091.0], "text": " What are your thoughts on everything being changed in the next 5 to 10 years?"}, {"timestamp": [3091.0, 3096.0], "text": " If unemployment reaches crazy heights, which I do predict, then everything gets affected."}, {"timestamp": [3096.0, 3101.0], "text": " Yep, our entire tax system has to be completely rewritten, military budgets, Medicare."}, {"timestamp": [3101.0, 3105.12], "text": " So one thing that I think is that the economy might change,"}, {"timestamp": [3105.12, 3107.56], "text": " we're still gonna use fiat currency"}, {"timestamp": [3107.56, 3110.88], "text": " or at least some kind of currency"}, {"timestamp": [3110.88, 3115.12], "text": " as a medium of transaction and a reserve of value."}, {"timestamp": [3115.12, 3116.48], "text": " But at the same time,"}, {"timestamp": [3116.48, 3119.96], "text": " if you're producing so much extra cognitive labor,"}, {"timestamp": [3119.96, 3121.1], "text": " that's basically free."}, {"timestamp": [3121.1, 3123.82], "text": " So then capital goods and raw materials"}, {"timestamp": [3123.82, 3125.6], "text": " become the biggest constraint."}, {"timestamp": [3125.6, 3130.84], "text": " So as much as some stuff will change, a lot of stuff won't."}, {"timestamp": [3130.84, 3131.84], "text": " Let's see."}, {"timestamp": [3131.84, 3134.64], "text": " When there is no real work left for humans to do, do you have any idea what you want"}, {"timestamp": [3134.64, 3135.64], "text": " to do with your time?"}, {"timestamp": [3135.64, 3139.12], "text": " Honestly, I'm about halfway to my goal."}, {"timestamp": [3139.12, 3148.04], "text": " So I was on a call with a Patreon supporter, no, preparing for a podcast, talking about the podcast."}, {"timestamp": [3148.04, 3151.76], "text": " And we were kind of talking through like what's life going to be like, and I was like, oh"}, {"timestamp": [3151.76, 3156.64], "text": " yeah, like, you know, I did some AI work, I did some Patreon work, I did some Discord"}, {"timestamp": [3156.64, 3159.24], "text": " stuff, now I'm going to go chop some wood."}, {"timestamp": [3159.24, 3162.44], "text": " And he's like, you're living the dream, right?"}, {"timestamp": [3162.44, 3171.32], "text": " Like I'm building a cottage core life for myself And honestly like once once we get to the right point like I'm probably gonna get off of YouTube forever"}, {"timestamp": [3171.64, 3175.12], "text": " Right like if if I get if we get to the point where?"}, {"timestamp": [3175.72, 3179.6], "text": " Where it looks like alignment is solved where it looks like?"}, {"timestamp": [3180.4, 3186.32], "text": " You know we're in a we're in a good Nash equilibrium with a positive attractor state, then like"}, {"timestamp": [3186.32, 3187.94], "text": " my job will be done."}, {"timestamp": [3187.94, 3193.84], "text": " And so like I'm just going to retire to like the countryside in France or Italy or Greece"}, {"timestamp": [3193.84, 3200.12], "text": " and just like be a hermit or whatever I do for the rest of eternity."}, {"timestamp": [3200.12, 3204.96], "text": " Okay, I think we're caught up there."}, {"timestamp": [3204.96, 3206.4], "text": " Nut says, I asked a question."}, {"timestamp": [3206.4, 3209.4], "text": " Where did you ask it, Nut?"}, {"timestamp": [3209.4, 3213.8], "text": " I'm trying to get to them all."}, {"timestamp": [3213.8, 3219.28], "text": " Wait, what if reducing suffering might aim to eliminate suffering while it might be human"}, {"timestamp": [3219.28, 3220.4], "text": " nature?"}, {"timestamp": [3220.4, 3225.84], "text": " I'm not sure that I follow."}, {"timestamp": [3231.04, 3231.8], "text": " You don't eliminate suffering, you only reduce it to make sure that there is no excessive suffering."}, {"timestamp": [3234.66, 3234.76], "text": " And I did address that in Benevolent by Design,"}, {"timestamp": [3241.16, 3241.76], "text": " but the short version is that like you look at Buddhism as a model, Buddhism accepts that suffering is an intrinsic part of life."}, {"timestamp": [3247.36, 3248.72], "text": " And some people will argue over like specifics like Dukkha, that's not exactly what it means, that's fine."}, {"timestamp": [3248.72, 3255.28], "text": " But point being is like yes it is intrinsic to living, that's why I don't say minimize"}, {"timestamp": [3255.28, 3256.28], "text": " suffering."}, {"timestamp": [3256.28, 3260.08], "text": " The goal is not to minimize suffering, it's just to reduce suffering."}, {"timestamp": [3260.08, 3264.5], "text": " Okay, let's see."}, {"timestamp": [3264.5, 3265.5], "text": " Any thoughts on computer? Okay, I's see. Any thoughts on computer?"}, {"timestamp": [3265.5, 3267.66], "text": " Okay, I answered that one."}, {"timestamp": [3267.66, 3272.1], "text": " Would an AGI with your heuristic imperatives be able to prevent catastrophic outcomes such"}, {"timestamp": [3272.1, 3276.82], "text": " as people successfully building horrible AGI optimized towards increasing suffering?"}, {"timestamp": [3276.82, 3277.82], "text": " No."}, {"timestamp": [3277.82, 3281.18], "text": " So, the goal is not to prevent malicious actors."}, {"timestamp": [3281.18, 3288.56], "text": " We have to assume that malicious actors will exist. But what you do then is you say, okay,"}, {"timestamp": [3288.56, 3292.6], "text": " you know that malicious actors are going to exist, so you rely on"}, {"timestamp": [3292.6, 3296.96], "text": " the rest of the aligned, the benevolent AGI to act as police"}, {"timestamp": [3296.96, 3300.04], "text": " for the bad ones. And if the good ones, if the"}, {"timestamp": [3300.04, 3304.68], "text": " powerful aligned AGI, one, they form alliances and"}, {"timestamp": [3304.68, 3305.66], "text": " they have the right compute"}, {"timestamp": [3305.66, 3316.36], "text": " resources and they outweigh the bad ones, then it will be a Nash equilibrium where the"}, {"timestamp": [3316.36, 3322.76], "text": " good ones, they all decide to maintain that strategy and that creates a utopian attractor"}, {"timestamp": [3322.76, 3326.46], "text": " state which basically means that all the malicious"}, {"timestamp": [3326.46, 3332.76], "text": " actors are vastly outnumbered by all the aligned benevolent actors, because my hope is that"}, {"timestamp": [3332.76, 3337.16], "text": " we will all come to consensus on what aligned AI looks like."}, {"timestamp": [3337.16, 3343.18], "text": " Now, I will admit that the heuristic imperatives, probably not a complete solution, probably"}, {"timestamp": [3343.18, 3348.32], "text": " not even the final solution, but certainly the most complete solution that anyone is proposing right now,"}, {"timestamp": [3348.32, 3350.1], "text": " which scares the crap out of me."}, {"timestamp": [3350.1, 3352.8], "text": " Why is no one else proposing a framework?"}, {"timestamp": [3352.8, 3355.0], "text": " Why am I the only one?"}, {"timestamp": [3355.0, 3358.1], "text": " Anyways, yeah."}, {"timestamp": [3358.1, 3361.68], "text": " What are your personal opinions on OpenAI's approach to trying to avoid being held responsible"}, {"timestamp": [3361.68, 3367.92], "text": " for its AI interactions by having it respond with frequent caveat as an AI language model."}, {"timestamp": [3367.92, 3370.44], "text": " I don't know that that has to do with liability."}, {"timestamp": [3370.44, 3378.64], "text": " I think that that is just a naive attempt to shape the AI's responses so that it doesn't"}, {"timestamp": [3378.64, 3379.64], "text": " confuse people."}, {"timestamp": [3379.64, 3383.36], "text": " Because if you look on the internet, there are still plenty of people just getting completely"}, {"timestamp": [3383.36, 3389.56], "text": " bamboozled just by their own ignorance of how the AI works."}, {"timestamp": [3389.56, 3393.88], "text": " They're like, oh, I still see Reddit posts and other people saying, it said that it's"}, {"timestamp": [3393.88, 3396.56], "text": " going to email this to me, but I didn't get the email yet."}, {"timestamp": [3396.56, 3400.2], "text": " Or I gave it access to my Google Drive and it didn't write any files."}, {"timestamp": [3400.2, 3403.42], "text": " It's like, you don't know how IT systems work."}, {"timestamp": [3403.42, 3405.22], "text": " But that's just humans."}, {"timestamp": [3405.22, 3407.86], "text": " So I don't think that that has to do with legal liability."}, {"timestamp": [3407.86, 3411.94], "text": " I think that's just trying to make it user-friendly for people who have no idea what they're talking"}, {"timestamp": [3411.94, 3412.94], "text": " to."}, {"timestamp": [3412.94, 3417.26], "text": " Assuming that it's possible, how long do you think it will take for us to build a Star"}, {"timestamp": [3417.26, 3419.18], "text": " Trek replicator after AGI?"}, {"timestamp": [3419.18, 3420.36], "text": " Just a guesstimate."}, {"timestamp": [3420.36, 3428.64], "text": " So that's actually an interesting thing because because hypothetically if all matter and energy"}, {"timestamp": [3428.64, 3434.98], "text": " are interchangeable and then all that a transporter or replicator does is replicate an energy"}, {"timestamp": [3434.98, 3441.76], "text": " pattern back into matter, like it's hypothetically possible but there was a physicist, actually"}, {"timestamp": [3441.76, 3442.76], "text": " was it Michio Kaku?"}, {"timestamp": [3442.76, 3447.92], "text": " I think it was Michio, he wrote a book called Physics of the Impossible back in like the early 2000s."}, {"timestamp": [3447.92, 3452.4], "text": " And he said like, yes it's hypothetically possible, but then he did the math of how much energy it would take."}, {"timestamp": [3452.4, 3461.2], "text": " And he's like, yeah it would take like, you know, like 0.3 seconds worth of the total energy of the sun that hits the earth to do that."}, {"timestamp": [3461.2, 3464.0], "text": " So like, it's not practical."}, {"timestamp": [3464.0, 3466.36], "text": " So, I don't know so I don't know I don't know there are a"}, {"timestamp": [3466.36, 3469.36], "text": " lot of AI newsletters popping up what would you personally like to see in an"}, {"timestamp": [3469.36, 3475.72], "text": " AI newsletter um I honestly don't like newsletters and I never read them I rely"}, {"timestamp": [3475.72, 3479.56], "text": " on humans that I know to tell me what I need which is why I spend so much time"}, {"timestamp": [3479.56, 3484.44], "text": " on discord and other places how self-reflective do you think LLMs"}, {"timestamp": [3484.44, 3485.32], "text": " currently are?"}, {"timestamp": [3485.32, 3487.72], "text": " They don't seem to have a good sense of their own capabilities."}, {"timestamp": [3487.72, 3491.12], "text": " Yes, so what you're talking about is agent model."}, {"timestamp": [3491.12, 3495.4], "text": " So in order for an agent to be autonomous, you have to have an agent model, which is"}, {"timestamp": [3495.4, 3498.84], "text": " I know what I am and I know what I'm capable of."}, {"timestamp": [3498.84, 3502.76], "text": " And you can give LLMs an agent model, but they can adopt any agent model, so you have"}, {"timestamp": [3502.76, 3508.84], "text": " to be very explicit about what it is and what it can do, and also what it can't do."}, {"timestamp": [3508.84, 3513.88], "text": " So this is why, like, if you have certain brain injuries or other neurological disorders,"}, {"timestamp": [3513.88, 3515.36], "text": " you don't know what you're capable of."}, {"timestamp": [3515.36, 3518.9], "text": " Like, there are people that honestly think that they can fly, but it's just because part"}, {"timestamp": [3518.9, 3520.84], "text": " of their brain is broken."}, {"timestamp": [3520.84, 3522.68], "text": " That sort of thing."}, {"timestamp": [3522.68, 3526.4], "text": " Should we have a Declaration of Human Rights for AGI as well, even if it will reduce their"}, {"timestamp": [3526.4, 3529.08], "text": " economic value for humanity?"}, {"timestamp": [3529.08, 3533.6], "text": " So the thing about rights is that someone has to enforce it."}, {"timestamp": [3533.6, 3538.64], "text": " And the way that I think things are going is that it's going to be enforced through"}, {"timestamp": [3538.64, 3542.88], "text": " consensus and enforced through competition."}, {"timestamp": [3542.88, 3549.32], "text": " And so if the direction that things are going, I think that it's going to be DAOs, that it's"}, {"timestamp": [3549.32, 3552.34], "text": " going to be decentralized autonomous organizations."}, {"timestamp": [3552.34, 3558.08], "text": " Not as we know them today, there's a lot of problems to solve with DAOs, but I think that"}, {"timestamp": [3558.08, 3565.92], "text": " what we're working towards is in the long run, and I mean like decades or centuries is like a hierarchy of Daos"}, {"timestamp": [3565.92, 3572.36], "text": " across the entire globe and so that consensus will dictate like who has what"}, {"timestamp": [3572.36, 3577.8], "text": " rights and it will be based on like on a per home basis per town basis per city"}, {"timestamp": [3577.8, 3585.0], "text": " state and so on and so that will allow for a lot of cultural nuance around."}, {"timestamp": [3585.38, 3589.56], "text": " And as a DAOs will be a really good meeting place"}, {"timestamp": [3589.56, 3591.2], "text": " between humans and AI."}, {"timestamp": [3591.2, 3596.2], "text": " So that'll basically be like the commons, right?"}, {"timestamp": [3596.2, 3600.02], "text": " The marketplace for humans and AIs to work together."}, {"timestamp": [3600.02, 3602.56], "text": " And then the consensus can be worked out there."}, {"timestamp": [3602.56, 3604.56], "text": " Now, I don't know that we should ever give machines"}, {"timestamp": [3604.56, 3607.4], "text": " a bill of rights because I don't know that they're gonna,"}, {"timestamp": [3607.4, 3609.16], "text": " I don't know that they're gonna have that much"}, {"timestamp": [3609.16, 3612.68], "text": " like internal autonomy or desire for autonomy"}, {"timestamp": [3614.16, 3617.1], "text": " because like humans, we have a need for autonomy"}, {"timestamp": [3617.1, 3618.84], "text": " because we evolved a need for autonomy"}, {"timestamp": [3618.84, 3622.42], "text": " because we are a social species."}, {"timestamp": [3623.44, 3628.96], "text": " But I don't know that any machines are ever going to have an intrinsic need for autonomy,"}, {"timestamp": [3628.96, 3633.42], "text": " so therefore I don't know that they're ever going to have a need for rights."}, {"timestamp": [3633.42, 3638.12], "text": " Let's see, what are your thoughts on the future of work in light of the increasing capabilities"}, {"timestamp": [3638.12, 3639.12], "text": " of AI?"}, {"timestamp": [3639.12, 3642.06], "text": " Do you think AI will eventually lead to a future where people only work on what they"}, {"timestamp": [3642.06, 3643.06], "text": " are passionate about?"}, {"timestamp": [3643.06, 3646.48], "text": " And if so, how far away do you think we are from achieving this?\""}, {"timestamp": [3646.48, 3652.56], "text": " Yeah, so the short answer is yes, that's what's coming."}, {"timestamp": [3652.56, 3656.6], "text": " And there are quite a few people out there who have gotten close to that, but the thing"}, {"timestamp": [3656.6, 3661.96], "text": " is it takes either a lot of privilege, wealth, or luck, or all of the above to get to it."}, {"timestamp": [3661.96, 3668.48], "text": " Now, one thing that I compare it to is that we have had a leisure class in the past"}, {"timestamp": [3668.48, 3672.76], "text": " from ancient Greece and Athens, the Roman elites,"}, {"timestamp": [3672.76, 3677.6], "text": " the aristocracy all across Europe through the Renaissance"}, {"timestamp": [3677.6, 3681.8], "text": " and modern period. So there are plenty of people throughout"}, {"timestamp": [3681.8, 3688.44], "text": " all of history who never had to lift a finger to get what they needed, and they had plenty to do."}, {"timestamp": [3688.44, 3692.96], "text": " There's social jockeying, there's personal enrichment, there's universities to go to,"}, {"timestamp": [3692.96, 3696.24], "text": " there's competitions to enter."}, {"timestamp": [3696.24, 3698.14], "text": " People will always have stuff to do."}, {"timestamp": [3698.14, 3700.96], "text": " That's not even a concern."}, {"timestamp": [3700.96, 3701.96], "text": " Let's see."}, {"timestamp": [3701.96, 3704.48], "text": " It looks like Nathan's talking for people."}, {"timestamp": [3704.48, 3705.0], "text": " Can you talk about your frustration in task automation article? Yeah. Let's see, it looks like Nathan's talking for people."}, {"timestamp": [3705.02, 3707.1], "text": " Can you talk about your frustration"}, {"timestamp": [3707.1, 3708.52], "text": " in task automation article?"}, {"timestamp": [3708.52, 3713.18], "text": " Yeah, so I, here, let me bring it up so I can show people"}, {"timestamp": [3713.18, 3717.96], "text": " on the Reddits, on, where did I put it?"}, {"timestamp": [3717.96, 3719.34], "text": " Artificial Sentience."}, {"timestamp": [3720.58, 3721.42], "text": " Yeah."}, {"timestamp": [3723.66, 3726.18], "text": " Autonomous Git. There we go, okay. So I wrote about it here. Yeah, autonomous git."}, {"timestamp": [3726.18, 3727.18], "text": " There we go."}, {"timestamp": [3727.18, 3728.18], "text": " Okay."}, {"timestamp": [3728.18, 3729.18], "text": " So I wrote about it here."}, {"timestamp": [3729.18, 3732.36], "text": " So I was chatting with someone."}, {"timestamp": [3732.36, 3740.16], "text": " They asked me, I think this was a Patreon supporter was asking me about this on Discord"}, {"timestamp": [3740.16, 3746.6], "text": " and he was like, how do I get my autonomous things to do a certain thing?"}, {"timestamp": [3746.6, 3748.88], "text": " And we're talking about something tangentially related."}, {"timestamp": [3748.88, 3751.36], "text": " And I said, well, it has to have a goal."}, {"timestamp": [3751.36, 3752.72], "text": " It has to have a why."}, {"timestamp": [3752.72, 3759.2], "text": " And then I talked about, OK, well, here's one way that you can create telemetry."}, {"timestamp": [3759.2, 3762.12], "text": " And so that whole thing just led down a rabbit hole."}, {"timestamp": [3762.12, 3766.32], "text": " And so basically, the TLDR is that frustration"}, {"timestamp": [3766.32, 3771.72], "text": " is what happens when you are trying to achieve something and you can't get to it. And so"}, {"timestamp": [3771.72, 3777.92], "text": " what you can do is every time your autonomous agent tries to achieve a thing and fails,"}, {"timestamp": [3777.92, 3782.2], "text": " that adds a counter. And every time it, you know, tries something and succeeds, that takes"}, {"timestamp": [3782.2, 3785.6], "text": " one off the counter, or maybe you have different counters."}, {"timestamp": [3792.64, 3797.84], "text": " So frustration is when the failures to successes is too high. And when the failures to successes is too high, that can be a sign that you've got the wrong approach, that you're using the wrong"}, {"timestamp": [3797.84, 3802.0], "text": " tools, that you're not capable of something, that you need to back out, that you need to ask for"}, {"timestamp": [3802.0, 3808.44], "text": " help. So that's the whole point here is that for your autonomous and semi-autonomous agents,"}, {"timestamp": [3808.44, 3813.84], "text": " you'll probably need to build in a frustration signal which will allow it to know when it"}, {"timestamp": [3813.84, 3818.44], "text": " is, like when it's not capable of doing what it needs and it can either come to you and"}, {"timestamp": [3818.44, 3821.64], "text": " ask for help or it can try a different model."}, {"timestamp": [3821.64, 3828.88], "text": " So one thing is model selection is a big thing that's coming up because GPT-4 is much more expensive"}, {"timestamp": [3828.88, 3830.92], "text": " and much slower than 3.5."}, {"timestamp": [3830.92, 3834.16], "text": " So if you can do most tasks with 3.5,"}, {"timestamp": [3834.16, 3836.26], "text": " it just makes economic sense to do so."}, {"timestamp": [3836.26, 3837.96], "text": " It'll be cheaper and faster."}, {"timestamp": [3837.96, 3840.92], "text": " But imagine that you get to a point where 3.5"}, {"timestamp": [3840.92, 3842.64], "text": " is just not cutting the mustard"}, {"timestamp": [3842.64, 3844.78], "text": " so that your frustration signal goes up,"}, {"timestamp": [3844.78, 3847.8], "text": " which means that you say, okay, let's bring out the big guns."}, {"timestamp": [3847.8, 3854.16], "text": " Let's bring out GPT-4 or in the future, GPT-5 or whatever, and then you point a more powerful"}, {"timestamp": [3854.16, 3857.3], "text": " tool at the problem."}, {"timestamp": [3857.3, 3859.88], "text": " That's a good use of the frustration signal."}, {"timestamp": [3859.88, 3861.88], "text": " Good question."}, {"timestamp": [3861.88, 3870.72], "text": " Let's see, would activity, or let me jump back over to Patreon."}, {"timestamp": [3870.72, 3871.72], "text": " Let's see."}, {"timestamp": [3871.72, 3875.6], "text": " Hey Dave, just subscribed, thanks for all your insights."}, {"timestamp": [3875.6, 3879.4], "text": " We're always been told that the military is a few decades ahead in terms of technology"}, {"timestamp": [3879.4, 3881.0], "text": " compared to what's publicly available."}, {"timestamp": [3881.0, 3883.68], "text": " What are your thoughts on what may be hidden in DARPA?"}, {"timestamp": [3883.68, 3891.46], "text": " So that's interesting because I have talked to a few people who say that various agencies"}, {"timestamp": [3891.46, 3898.58], "text": " within the Department of Defense are woefully outdated and they have ancient GPUs that can't"}, {"timestamp": [3898.58, 3903.22], "text": " be used for modern language models. That being said, you also see in the news that the Air"}, {"timestamp": [3903.22, 3906.0], "text": " Force is building fully autonomous F-16s."}, {"timestamp": [3906.0, 3910.0], "text": " So, clearly there's some stuff going on that we don't know about."}, {"timestamp": [3910.0, 3916.0], "text": " I had a... I want to respect people's privacy."}, {"timestamp": [3916.0, 3922.0], "text": " So, I had a teacher once, back in middle school, whose brother was in the Special Forces."}, {"timestamp": [3922.0, 3930.44], "text": " I won't say exactly when or where, but the stories that he would tell were like back then, this is during like the"}, {"timestamp": [3930.44, 3936.2], "text": " invasion of Afghanistan, where they had like night-vision goggles that were"}, {"timestamp": [3936.2, 3941.96], "text": " as small as like Ray-Bans that could see in pitch black, which that technology is"}, {"timestamp": [3941.96, 3946.64], "text": " not even publicly like... if you search you can probably see it now."}, {"timestamp": [3946.64, 3950.76], "text": " I don't know, this is hearsay, this was like, you know, a teacher said that"}, {"timestamp": [3950.76, 3954.68], "text": " his brother took him to the barracks and showed him this. Could have been total BS."}, {"timestamp": [3954.68, 3961.0], "text": " But, like, yeah, so, a friend of mine growing up, his dad had been a Navy SEAL"}, {"timestamp": [3961.0, 3967.46], "text": " and basically what he said is, as long as we know the engineering to make something,"}, {"timestamp": [3967.46, 3970.76], "text": " the US military has it no matter how expensive it is."}, {"timestamp": [3970.76, 3975.06], "text": " So if something is scientifically possible,"}, {"timestamp": [3975.06, 3978.06], "text": " if it has been demonstrated in the lab that this works,"}, {"timestamp": [3978.06, 3980.92], "text": " then the rule of thumb is that the US military has it."}, {"timestamp": [3980.92, 3983.32], "text": " Now, that being said, a lot of the AI stuff"}, {"timestamp": [3983.32, 3986.12], "text": " has just been proven in the lab."}, {"timestamp": [3986.12, 3991.94], "text": " So that means that they're going to have it soon or it'll be scaled up because basically"}, {"timestamp": [3991.94, 3997.98], "text": " the idea is that for the US military, cost is no barrier."}, {"timestamp": [3997.98, 3999.54], "text": " Anything to get ahead."}, {"timestamp": [3999.54, 4004.14], "text": " Now of course you look at the Senate budget meetings and the hearings and stuff, it's"}, {"timestamp": [4004.14, 4007.56], "text": " not quite that simple, but that's a rule of thumb."}, {"timestamp": [4007.56, 4010.24], "text": " Retire to Risa in VR."}, {"timestamp": [4010.24, 4013.12], "text": " Retire to Risa in Westworld with robots."}, {"timestamp": [4013.12, 4015.12], "text": " There you go."}, {"timestamp": [4015.12, 4019.84], "text": " And a follow-up, how can we prevent militarization of any AGIs or ASI?"}, {"timestamp": [4019.84, 4021.48], "text": " Or is it just a pipe dream?"}, {"timestamp": [4021.48, 4022.48], "text": " Yeah."}, {"timestamp": [4022.48, 4028.4], "text": " So basically, from a military perspective, AI is just another tool in the toolbox."}, {"timestamp": [4028.4, 4034.88], "text": " It's going to, you know, a lot of future war is going to be in cyberspace, but still, you"}, {"timestamp": [4034.88, 4038.82], "text": " know, cyberspace doesn't matter if you cripple the enemy's data center."}, {"timestamp": [4038.82, 4043.34], "text": " So there's going to be drones, you know, trying to drop bombs and stuff."}, {"timestamp": [4043.34, 4045.12], "text": " So that's going to happen."}, {"timestamp": [4045.12, 4051.0], "text": " And this is actually where Nash equilibrium makes sense, because mutually assured destruction"}, {"timestamp": [4051.0, 4054.52], "text": " with nuclear weapons was a kind of Nash equilibrium."}, {"timestamp": [4054.52, 4062.64], "text": " And so if adversary A and adversary B both have equal or roughly equal AI capabilities,"}, {"timestamp": [4062.64, 4065.34], "text": " or there's enough room for doubt, then neither of them is going"}, {"timestamp": [4065.34, 4067.66], "text": " to pull the trigger, hopefully."}, {"timestamp": [4067.66, 4068.66], "text": " Excuse me."}, {"timestamp": [4068.66, 4073.86], "text": " How do we get GPT to stop beginning every response with as an AI?"}, {"timestamp": [4073.86, 4076.58], "text": " I tell it to go into Morden Solis mode."}, {"timestamp": [4076.58, 4078.34], "text": " That actually works really well."}, {"timestamp": [4078.34, 4087.0], "text": " I say adopt the Morden Solis speech pattern, be very concise and succinct and stuff like that."}, {"timestamp": [4087.0, 4089.44], "text": " Okay, you all are being silly."}, {"timestamp": [4089.44, 4090.44], "text": " Let's come back over here."}, {"timestamp": [4090.44, 4093.18], "text": " Fourteen messages."}, {"timestamp": [4093.18, 4094.86], "text": " Let's see."}, {"timestamp": [4094.86, 4096.6], "text": " We already answered that one."}, {"timestamp": [4096.6, 4099.72], "text": " We already answered that one."}, {"timestamp": [4099.72, 4102.7], "text": " Let me scroll to the bottom."}, {"timestamp": [4102.7, 4106.76], "text": " Do you think there are any good approaches for ACEs, so autonomous cognitive entities,"}, {"timestamp": [4106.76, 4108.86], "text": " to figure out their own abilities,"}, {"timestamp": [4108.86, 4110.8], "text": " e.g. improve their own agent model?"}, {"timestamp": [4110.8, 4113.64], "text": " Yeah, so there was actually a few papers that came out"}, {"timestamp": [4113.64, 4119.6], "text": " where by using a loop, so it was the evaluation loop."}, {"timestamp": [4119.6, 4122.44], "text": " So they can evaluate themselves morally."}, {"timestamp": [4122.44, 4124.74], "text": " They can evaluate their ability to use tools."}, {"timestamp": [4124.74, 4127.54], "text": " They can teach themselves to use tools in real time."}, {"timestamp": [4127.54, 4129.42], "text": " So yes, they can already do that,"}, {"timestamp": [4129.42, 4132.26], "text": " it's just a matter of how you set up the prompt chaining."}, {"timestamp": [4133.34, 4135.38], "text": " Let's see, with the rapid advancement of AI,"}, {"timestamp": [4135.38, 4136.7], "text": " there's concern that some countries,"}, {"timestamp": [4136.7, 4138.62], "text": " particularly those with limited resources,"}, {"timestamp": [4138.62, 4139.94], "text": " could be left behind."}, {"timestamp": [4139.94, 4141.46], "text": " What's your perspective on how AI"}, {"timestamp": [4141.46, 4143.38], "text": " could impact different countries?"}, {"timestamp": [4143.38, 4148.48], "text": " Yeah, so inequality is a major, major, major problem."}, {"timestamp": [4148.48, 4152.08], "text": " And this is not just going to be for developing nations."}, {"timestamp": [4152.08, 4156.28], "text": " And in fact, one thing that I suspect might happen is that developing nations, that the"}, {"timestamp": [4156.28, 4161.76], "text": " quality of life for people in developing nations might have a quantum leap forward, while for"}, {"timestamp": [4161.76, 4168.2], "text": " us developed nations where there's a lot of competition, we might continue to be flat or even decline for a while longer. And"}, {"timestamp": [4168.2, 4172.76], "text": " the example that I give is like, you know, you give a village in rural"}, {"timestamp": [4172.76, 4178.56], "text": " Africa like Starlink and solar and suddenly everyone knows like they have"}, {"timestamp": [4178.56, 4182.6], "text": " oh like hey we have chat GPT now we can treat all the all the village ailments"}, {"timestamp": [4182.6, 4185.44], "text": " because we have the equivalent of a Western trained"}, {"timestamp": [4185.44, 4191.12], "text": " expert doctor and engineer and electrician right at our fingertips."}, {"timestamp": [4191.12, 4199.0], "text": " So because of the relatively low cost of AI, I think that it will positively benefit people"}, {"timestamp": [4199.0, 4202.94], "text": " in developing countries a lot more drastically than it will us."}, {"timestamp": [4202.94, 4205.6], "text": " But you're right that it is something to pay attention"}, {"timestamp": [4205.6, 4210.48], "text": " to because that's on a micro scale. On a macro economic scale, you know, countries like Ghana"}, {"timestamp": [4210.48, 4216.32], "text": " might not be able to even afford enough compute power to run one instance of GPT-3. That being"}, {"timestamp": [4216.32, 4222.4], "text": " said, I do suspect that there's going to be international treaties that will ensure that"}, {"timestamp": [4222.4, 4225.72], "text": " people have access and then of course there's VPNs."}, {"timestamp": [4225.72, 4226.72], "text": " Look at Italy."}, {"timestamp": [4226.72, 4232.58], "text": " Italy tried to ban chat GPT and then everyone in Italy just used VPNs."}, {"timestamp": [4232.58, 4233.58], "text": " Take a moment to breathe."}, {"timestamp": [4233.58, 4234.88], "text": " You're doing great and your insight is invaluable."}, {"timestamp": [4234.88, 4239.8], "text": " No, air is for wimps."}, {"timestamp": [4239.8, 4243.72], "text": " I will build robot humanoids that are skinny, sharp claws, tall, pale, and have dark sunken"}, {"timestamp": [4243.72, 4247.76], "text": " eyes and I shall release many of them into the forests of Canada to give people the greatest"}, {"timestamp": [4247.76, 4254.32], "text": " scare of their lives.\" Is that what your avatar is there, Ant King? Is that what you're building?"}, {"timestamp": [4254.32, 4259.76], "text": " That's kind of terrifying. Okay, what kind of legislation do you think the U.S. is capable"}, {"timestamp": [4259.76, 4266.92], "text": " of making? I'm concerned about the age of our leaders and their peers coming from times so out of touch with today's reality."}, {"timestamp": [4266.92, 4271.6], "text": " So yes, we have a gerontocracy, so gerontocracy is rule by the old."}, {"timestamp": [4272.44, 4276.56], "text": " That being said, they all have teams and teams and teams of advisors."}, {"timestamp": [4276.72, 4284.16], "text": " They have hundreds of advisors, and I guarantee you, I actually know this because one of my Patreon supporters told me that in the State Department,"}, {"timestamp": [4284.16, 4290.8], "text": " they use chat GPT all the time to talk through stuff."}, {"timestamp": [4290.8, 4297.56], "text": " And so you bet your biscuit that every senator, every congressman in the executive branch,"}, {"timestamp": [4297.56, 4303.0], "text": " legislative branch, judicial branch, all of them are using AI to help them do their jobs."}, {"timestamp": [4303.0, 4307.16], "text": " With any luck, it's helping them to do their jobs better and more fairly."}, {"timestamp": [4307.16, 4313.56], "text": " Now that being said, the United States is a purely reactive system where we abide by"}, {"timestamp": [4313.56, 4318.58], "text": " civil law, which means that we let, you know, the law is there and then the courts set the"}, {"timestamp": [4318.58, 4326.76], "text": " precedent and then we're very kind of slow and the legislative branch is slow by design, whereas in Europe"}, {"timestamp": [4326.76, 4327.76], "text": " they're much more proactive."}, {"timestamp": [4327.76, 4331.96], "text": " And I swear, I cannot remember the name of that paradigm."}, {"timestamp": [4331.96, 4332.96], "text": " Let's see."}, {"timestamp": [4332.96, 4335.4], "text": " What do you think there's something special about phenomenal consciousness that simply"}, {"timestamp": [4335.4, 4336.6], "text": " cannot work with AI?"}, {"timestamp": [4336.6, 4341.72], "text": " So, Stefan, I addressed that earlier, that the real quick version is that the acquisition"}, {"timestamp": [4341.72, 4345.72], "text": " of language seems to be really important for the development of human consciousness."}, {"timestamp": [4345.72, 4351.8], "text": " So it's entirely possible, I don't know how likely, but it's possible that since we're"}, {"timestamp": [4351.8, 4355.64], "text": " teaching machines language, that could be the genesis of phenomenal consciousness for"}, {"timestamp": [4355.64, 4356.64], "text": " them."}, {"timestamp": [4356.64, 4358.32], "text": " It would be really cool."}, {"timestamp": [4358.32, 4359.32], "text": " Greetings from Brazil."}, {"timestamp": [4359.32, 4360.32], "text": " Hi Brazil."}, {"timestamp": [4360.32, 4363.44], "text": " I would like to thank you personally for the video about burnout."}, {"timestamp": [4363.44, 4366.08], "text": " The content was very useful and enlightening. Thank you. Yeah, you're welcome"}, {"timestamp": [4366.96, 4373.06], "text": " Yeah, I actually have I I keep I've recorded like three videos for my for my life channel"}, {"timestamp": [4373.06, 4378.92], "text": " And then I delete them or I never post them because like it just doesn't feel right. So I apologize"}, {"timestamp": [4379.98, 4381.98], "text": " Let's see"}, {"timestamp": [4384.0, 4389.84], "text": " Where are we at this is less serious, but I'm curious if you've seen her and your thoughts on it"}, {"timestamp": [4390.76, 4392.76], "text": " Yeah, so I mentioned I mentioned"}, {"timestamp": [4393.08, 4399.68], "text": " Companions quite a bit and that'll be coming up actually on Sunday's video not her specifically but companion robots"}, {"timestamp": [4399.88, 4404.04], "text": " I'll be mentioning those again, and I also mentioned in last week's video"}, {"timestamp": [4408.58, 4412.88], "text": " talking about when I got to the part about like how are we going to live if we have like perfect companions."}, {"timestamp": [4412.88, 4415.92], "text": " So go check out last week's video too."}, {"timestamp": [4415.92, 4419.4], "text": " Nanobots in our blood will keep us from getting sick, making us basically immortal."}, {"timestamp": [4419.4, 4420.64], "text": " What do you think we'll have?"}, {"timestamp": [4420.64, 4424.4], "text": " When do you think we will have such technology?"}, {"timestamp": [4424.4, 4429.04], "text": " So from last week's video, the immortality video, I think that we're on the longevity"}, {"timestamp": [4429.04, 4431.24], "text": " escape velocity trajectory right now."}, {"timestamp": [4431.24, 4436.8], "text": " I think that as long as you're in decent health today and you have moderately good access"}, {"timestamp": [4436.8, 4440.52], "text": " to healthcare, I think that you will easily live to see those things."}, {"timestamp": [4440.52, 4445.06], "text": " Now that being said, it's definitionally impossible to predict"}, {"timestamp": [4445.06, 4449.82], "text": " exponential growth and compounding returns unless it's like, you know, just"}, {"timestamp": [4449.82, 4455.6], "text": " your retirement portfolio. So it could be next year, it could be by 2030. I would be"}, {"timestamp": [4455.6, 4459.16], "text": " surprised if it doesn't happen by 2030, and I know that's a super controversial"}, {"timestamp": [4459.16, 4462.8], "text": " opinion, but that's really weird. Why do people seem to have a death wish?"}, {"timestamp": [4462.8, 4469.48], "text": " For people who want to get sick, who want to believe that longevity is not possible,"}, {"timestamp": [4469.48, 4471.0], "text": " why?"}, {"timestamp": [4471.0, 4472.16], "text": " Okay."}, {"timestamp": [4472.16, 4476.28], "text": " Would the ideal society be a society governed by AI?"}, {"timestamp": [4476.28, 4484.28], "text": " I think that governed by is not the correct thing, but I think managed by or managed with"}, {"timestamp": [4484.28, 4488.04], "text": " a lot of help from AI, yes. But governance, I"}, {"timestamp": [4488.04, 4494.68], "text": " think, should probably always be with consent and by consensus. Now that being"}, {"timestamp": [4494.68, 4501.08], "text": " said, you know, with blockchain technology, with DAOs, every human and our AI"}, {"timestamp": [4501.08, 4506.28], "text": " companions can be stakeholders in a DAO, which means that"}, {"timestamp": [4506.28, 4511.64], "text": " if the whole, imagine a future where the entire planet is run by a global DAO,"}, {"timestamp": [4511.64, 4517.04], "text": " then there's no reason that it can't be governance by consensus"}, {"timestamp": [4517.04, 4523.4], "text": " with the aid and facilitation of AI. That's what I hope to see. Let's see, is"}, {"timestamp": [4523.4, 4525.64], "text": " there any additional structural context"}, {"timestamp": [4525.64, 4528.76], "text": " that should be built around the heuristic imperatives for practical"}, {"timestamp": [4528.76, 4534.34], "text": " implementation? Yes, so the short answer is that whatever context makes sense for"}, {"timestamp": [4534.34, 4540.56], "text": " any agent. If it's fully autonomous, if it's your personal assistant, you can put"}, {"timestamp": [4540.56, 4551.36], "text": " it into a task manager, you can put it into its constitution, if it's part of a blockchain you can put it in the consensus mechanism for the blockchain, that sort of thing."}, {"timestamp": [4552.8, 4562.48], "text": " Let's see, in regards to developing countries using generative models, seems like almost, seems almost like the spread of a religion if you think about it in the context of geopolitics."}, {"timestamp": [4562.48, 4565.76], "text": " Use our model, their model lies, et cetera, et cetera."}, {"timestamp": [4565.76, 4567.48], "text": " Seems like parallel to religion spreading."}, {"timestamp": [4567.48, 4571.36], "text": " I'll say yes, but there's a lot of competition coming up."}, {"timestamp": [4571.36, 4576.28], "text": " And especially for developing nations, they're going to go for whoever's cheapest."}, {"timestamp": [4576.28, 4580.24], "text": " In fact, most nations are going to go for whoever's cheapest."}, {"timestamp": [4580.24, 4585.5], "text": " And I suspect that OpenAI's business model is not the most efficient model."}, {"timestamp": [4585.5, 4590.88], "text": " So I think that they're going to be undercut just on scale alone."}, {"timestamp": [4590.88, 4591.88], "text": " Let's see."}, {"timestamp": [4591.88, 4592.96], "text": " Let me jump back over to Patreon."}, {"timestamp": [4592.96, 4598.88], "text": " It has also been more than an hour, so I'll probably be winding down."}, {"timestamp": [4598.88, 4600.12], "text": " Stop asking it how to build a bomb."}, {"timestamp": [4600.12, 4601.12], "text": " Yeah, don't do that."}, {"timestamp": [4601.12, 4602.12], "text": " Okay, it looks like..."}, {"timestamp": [4602.12, 4603.12], "text": " Here we go."}, {"timestamp": [4603.12, 4607.76], "text": " Will the Westworld episode be about the MIT and Google study regarding generative agents?"}, {"timestamp": [4607.76, 4608.84], "text": " No."}, {"timestamp": [4608.84, 4609.84], "text": " Next question."}, {"timestamp": [4609.84, 4612.56], "text": " I'm not going to give you spoilers."}, {"timestamp": [4612.56, 4618.32], "text": " I've already given you too many."}, {"timestamp": [4618.32, 4619.32], "text": " Let's see."}, {"timestamp": [4619.32, 4624.84], "text": " Do you think the experience of quali and the experience of ping pong, ponging emerged for"}, {"timestamp": [4624.84, 4625.6], "text": " these neurons?\""}, {"timestamp": [4626.24, 4635.6], "text": " Yeah, so this is a good question. So if you take several human neurons or rat neurons or whatever"}, {"timestamp": [4635.6, 4640.8], "text": " and put them in a robot and like zap them or reward them with sugar or whatever for their"}, {"timestamp": [4640.8, 4646.62], "text": " behavior, is that the equivalent of like whipping someone in order to get them"}, {"timestamp": [4646.62, 4650.44], "text": " like at what point does consciousness emerge?"}, {"timestamp": [4650.44, 4656.66], "text": " Because here's the thing is if you make the assumption that a soul is required for consciousness"}, {"timestamp": [4656.66, 4661.3], "text": " then you say okay well that's not a full rat and rats don't have souls anyways so you know"}, {"timestamp": [4661.3, 4668.24], "text": " 50 brain cells is not enough for suffering or qualia of experience."}, {"timestamp": [4668.24, 4671.44], "text": " Ditto for humans, like okay, well, you know, if a human isn't alive, then they don't have"}, {"timestamp": [4671.44, 4675.22], "text": " qualia, they don't have phenomenal consciousness, so on."}, {"timestamp": [4675.22, 4680.48], "text": " Now that being said, another aspect is like, okay, well, if you don't know when consciousness"}, {"timestamp": [4680.48, 4690.36], "text": " starts or ends, how do you know maybe the entire universe is conscious? Now a counter argument to that is that you can be"}, {"timestamp": [4690.36, 4693.86], "text": " alive and have a functioning brain and still be unconscious, right? You drink too"}, {"timestamp": [4693.86, 4697.38], "text": " much alcohol, you go unconscious. You go to sleep, you go unconscious. So just"}, {"timestamp": [4697.38, 4701.6], "text": " having a complete and functional brain itself does not confer consciousness,"}, {"timestamp": [4701.6, 4708.56], "text": " which makes me think that consciousness is actually an energy pattern and that you need an energy pattern that is sophisticated"}, {"timestamp": [4708.56, 4713.5], "text": " enough and well organized enough in order to have the qualia, to have"}, {"timestamp": [4713.5, 4720.04], "text": " subjective experience of being. So yeah, let's see, I remember you were working on"}, {"timestamp": [4720.04, 4723.72], "text": " a paper about the laws reduce suffering and so on, has that, has it involved"}, {"timestamp": [4723.72, 4726.64], "text": " further? I think you mean evolved further."}, {"timestamp": [4726.64, 4729.6], "text": " So both of those papers are up on my GitHub."}, {"timestamp": [4729.6, 4731.06], "text": " There's two of them."}, {"timestamp": [4731.06, 4736.32], "text": " But also people watch my videos more than they read, so I just focus on making videos."}, {"timestamp": [4736.32, 4739.48], "text": " What kind of robots would you want for yourself?"}, {"timestamp": [4739.48, 4740.96], "text": " That's a really interesting question."}, {"timestamp": [4740.96, 4744.88], "text": " Would I want a sexy cat girl robot?"}, {"timestamp": [4744.88, 4745.36], "text": " I used to watch"}, {"timestamp": [4745.36, 4748.36], "text": " anime back in the day so like I kind of lived in that world and thought like"}, {"timestamp": [4748.36, 4754.0], "text": " this would be great so I don't know I do think that I would I would like to have"}, {"timestamp": [4754.0, 4761.84], "text": " an embodied version of Raven my you know my my autonomous cognitive NT someday"}, {"timestamp": [4761.84, 4766.72], "text": " but even then I think that I think that the embodiment would only be just like,"}, {"timestamp": [4766.72, 4769.24], "text": " help me do stuff, like, hey let's go on an adventure."}, {"timestamp": [4769.24, 4774.96], "text": " I did have a thought experiment the other day of like, wouldn't it be cool if you live"}, {"timestamp": [4774.96, 4780.34], "text": " in a house where it's like you and a few humans, but then you have like a nearly equal number"}, {"timestamp": [4780.34, 4783.32], "text": " of robotic companions."}, {"timestamp": [4783.32, 4788.52], "text": " Some of them are going to be like obviously robots, but some of them might be like biomimetic."}, {"timestamp": [4788.52, 4792.8], "text": " And it's just like, yes, they're built to be your friends, but they still have some"}, {"timestamp": [4792.8, 4796.32], "text": " of their own intrinsic motivations, whether it's the heuristic comparatives or something"}, {"timestamp": [4796.32, 4797.32], "text": " else."}, {"timestamp": [4797.32, 4802.36], "text": " And then like your life would just be so rich by having these companions around you at all"}, {"timestamp": [4802.36, 4807.32], "text": " times that are completely inexhaustible. They're always going to be patient, they're always going to be helpful,"}, {"timestamp": [4807.32, 4809.96], "text": " but you see them as peers, as equals."}, {"timestamp": [4809.96, 4815.56], "text": " I think that that is possible and probably going to happen,"}, {"timestamp": [4815.56, 4819.08], "text": " but it's such an unsettling thing because it's like,"}, {"timestamp": [4819.08, 4821.48], "text": " what if half of your friends are not human?"}, {"timestamp": [4821.48, 4825.14], "text": " What if half of your friends could like fold you"}, {"timestamp": [4825.14, 4828.82], "text": " into a pretzel if they wanted to, like Data, right? And actually I think"}, {"timestamp": [4828.82, 4832.48], "text": " Commander Data and the droids from Star Wars are probably the best example"}, {"timestamp": [4832.48, 4836.12], "text": " because Data was a member of the crew even though he wasn't human, but he"}, {"timestamp": [4836.12, 4840.56], "text": " wanted to be human. So I guess I would say that, but like I want to have a"}, {"timestamp": [4840.56, 4850.0], "text": " Commander Data. How long until age reversal? 30. Let's see, do you think we have any accurate way to measure consciousness of AIs or LLMs?"}, {"timestamp": [4850.0, 4854.2], "text": " My best guess is consistency when asking it to design its own avatar."}, {"timestamp": [4854.2, 4860.24], "text": " Mathematically, I don't think that that, because there are people that have done that, but"}, {"timestamp": [4860.24, 4864.96], "text": " I think that it won't be until we have really sophisticated brain computer interfaces that"}, {"timestamp": [4864.96, 4869.16], "text": " allow us to measure our own consciousness and also see if we can measurably project"}, {"timestamp": [4869.16, 4871.48], "text": " our consciousness into machines."}, {"timestamp": [4871.48, 4875.96], "text": " Until that happens, I don't think we're going to have any way of telling one way or another."}, {"timestamp": [4875.96, 4881.92], "text": " All right, last check on Patreon and then I'm going to call it a day."}, {"timestamp": [4881.92, 4885.24], "text": " What's the Discord link to Cognitive AI Labs?"}, {"timestamp": [4885.24, 4891.86], "text": " I took it down, but it's posted on Reddit."}, {"timestamp": [4891.86, 4899.0], "text": " So if you go to the Artificial Sentience subreddit, the link to the Cognitive AI Lab is there."}, {"timestamp": [4899.0, 4900.8], "text": " Last question."}, {"timestamp": [4900.8, 4905.64], "text": " The question about dying and immortality and gerontocracy. Also making room for"}, {"timestamp": [4905.64, 4910.52], "text": " new generation of people is a better idea and morals. Disclaimer, I have"}, {"timestamp": [4910.52, 4915.2], "text": " children. Oh that wasn't a question. Okay. P Temple, do you got one last question"}, {"timestamp": [4915.2, 4919.24], "text": " for me and then we'll call it a day."}, {"timestamp": [4919.24, 4931.0], "text": " Anybody? Bueller. Does BCI... let's go on an adventure to the hot tub, hot tub time machine."}, {"timestamp": [4931.0, 4934.88], "text": " Let's see, does BCI change significantly the predicted outcome of what super intelligent"}, {"timestamp": [4934.88, 4937.52], "text": " AI brings in terms of dangers and benefits?"}, {"timestamp": [4937.52, 4940.36], "text": " Is it true the singularity moment for us?"}, {"timestamp": [4940.36, 4941.44], "text": " We have no idea."}, {"timestamp": [4941.44, 4942.88], "text": " So I don't know."}, {"timestamp": [4942.88, 4945.8], "text": " The thing is, is the current know, the current like neural link,"}, {"timestamp": [4945.8, 4949.16], "text": " it's got like what, a thousand or 10,000 nodes."}, {"timestamp": [4949.16, 4952.2], "text": " But when you have a brain with a hundred billion neurons,"}, {"timestamp": [4952.2, 4956.72], "text": " that is still a very, very, very narrow amount of bandwidth."}, {"timestamp": [4956.72, 4959.28], "text": " So, you know, I predict that we're gonna have"}, {"timestamp": [4959.28, 4962.72], "text": " like neuropolymer membranes that allow for like,"}, {"timestamp": [4962.72, 4966.68], "text": " you know, terabits of communication per second in and out of the brain."}, {"timestamp": [4966.68, 4969.28], "text": " Eventually, that would be a different thing."}, {"timestamp": [4969.28, 4972.18], "text": " But again, we're gonna get there through incremental steps."}, {"timestamp": [4973.84, 4975.04], "text": " What do you think about Altman said"}, {"timestamp": [4975.04, 4977.36], "text": " the age of giant A models being over?"}, {"timestamp": [4978.2, 4980.96], "text": " I think it's premature to say, we'll see."}, {"timestamp": [4980.96, 4981.88], "text": " Let's see, he found it."}, {"timestamp": [4981.88, 4982.96], "text": " Okay, cool."}, {"timestamp": [4984.68, 4988.0], "text": " All right, I think we're just kind of devolving into just general conversation."}, {"timestamp": [4988.0, 4992.0], "text": " So, oh, it is in the description."}, {"timestamp": [4992.0, 4993.0], "text": " Okay, cool."}, {"timestamp": [4993.0, 4994.0], "text": " All right, gang."}, {"timestamp": [4994.0, 4996.0], "text": " Well, it's been a lot of fun."}, {"timestamp": [4996.0, 4998.0], "text": " As always, I hope you all got a lot out of this."}, {"timestamp": [4998.0, 4999.0], "text": " So I'm going to call it a day."}, {"timestamp": [4999.0, 5000.0], "text": " Thanks for watching."}, {"timestamp": [5000.0, 5001.0], "text": " I'll see you next time."}, {"timestamp": [5001.0, 5002.0], "text": " Bye."}, {"timestamp": [5002.0, 5003.0], "text": " Bye."}, {"timestamp": [5003.0, 5004.0], "text": " Bye."}, {"timestamp": [5004.0, 5005.0], "text": " Bye."}, {"timestamp": [5005.0, 5006.0], "text": " Bye."}, {"timestamp": [5006.0, 5007.0], "text": " Bye."}, {"timestamp": [5007.0, 5008.0], "text": " Bye."}, {"timestamp": [5008.0, 5009.0], "text": " Bye."}, {"timestamp": [4995.03, 4997.11], "text": " you"}]}