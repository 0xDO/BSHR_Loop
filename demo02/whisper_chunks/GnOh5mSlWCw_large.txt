{"text": " Go live? Oh, no, I have to do it from here. Error occurred. Please. Okay, it says we're live. All right, so there's like a 15-second delay. An error occurred. Please try again later. I don't know if it's actually working. Can someone check on my YouTube channel to see if it's actually streaming? It's streaming. Yeah, it is. I just got the notification on my phone. Okay, cool. Cool. Hello? All right. So anyways, what's everyone up to? Good grief, this is a full chat room. What's everyone up to? Anyways, what's everyone up to? Good grief, this is a full chat room. What's everyone up to? I'm about to open source my startup that didn't really make any money. It's a bunch of news scraping and we have a news article grouping algorithm. It doesn't work too well, but it's like TF IDF vectors. Nice. So it should be fine, good for the community. I'll share it here in a couple of weeks when we get it out. I just saw a post on, I think it was either on my Google News Feed or Reddit that like someone was scraping too much internet, too much AI, was that you? Like shutting down websites? I don't think it was actually you. We don't DDoS, we put a healthy delay. You don't D it was actually you. We don't DDoS, we put a healthy delay. You don't DDoS on purpose. All right, looking at the stream, it looks like it's working, cool. All right, someone says, at work watching your stream on the down low. Nice, plug some questions here, but mostly this is a round table discussion. Wow, we already have 50 viewers. All right, but yeah, so what do you, so here's obviously like you guys know what I'm most focused on lately, which is alignment. What I wanna know is what do y'all think about, and not necessarily just like my work, but like alignment in general? Like, is the control problem a problem? Do you think it's solved? Or like, because you know, some of the comments are people are like, you know, nobody knows what's going on. Nobody's thinking about it. But the other people are like, they have the opinion that like, it's not actually an issue. So what do you guys think? that like it's not actually an issue. So what do you guys think? I think the issue is going to be more economic than threat to humanity, because once the AI stuff takes off, it's going to be able to negate a lot of the like the creative stuff is going on. So video generation and text generation, and that's going to replace people's income sources. And that's a much more imminent issue than some of the other things I would say. Yeah, I think you're right. There is there. There seems to be general consensus that that the economic disruption is absolutely coming. There was a post just like an hour ago. That's like someone replaced 80% of their their day job with chat GPT and then they got a second job doing basically the same thing. So I think I think you're right like general consensus is that that is the way but so you're saying you're saying you kind of agree with that that like it's not going to represent an existential threat to us but it's going to be more of an economic threat. Basically a threat to the entire internet really because once the models are good enough, they're going to be able to Hoover in all this information on the internet and reformat it before it gets displayed to the user. And part of that is going to be ad block for the masses. So what happens when all of these websites can no longer rely on ad revenue because every AI that's repackaging all the information for the user is stripping out information and turning on dark mode and doing it perfectly. So sponsor block built into everything, ad block built into everything. What does that do? Yeah, well, it certainly it changes. That makes sense. Yeah. Well, it certainly, it changes. Oh, go ahead, Luckow. On the other side of advertising, like AIs can also be used to create ads. Not everyone wants to filter them out. A lot of people want to use them in order to advertise more. In a way, they could also do a lot more subtle and successful advertising. Yeah. Advertisement built in AI? Oh, no. Yeah, I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. a lot more subtle and successful advertising. Yeah. Advertisement built in AI, oh no. Yeah, well so to the point though, you're right that especially you look at Google's revenue was hit really hard just from the first month of chat GPT. So it's definitely changing the way that we interact with information as a whole, regardless of like, you know, advertising. I'm gonna check on some of the questions on the live stream. How do we solve authentication? That sounds relevant. Let's see some other questions. Can autonomous AI replicate over the internet and clog up the tubes? It's a series of tubes. I know, do y'all wanna talk about some of the questions being asked on the live stream or do y'all just wanna keep talking here? Yeah, let me make a comment here, a more current issue, not like a future alignment existential threat. So in my startup here, we work with healthcare. So we deploy a writing production chatbot to support users, not directly like medical prescriptions and stuff like that. It's a more simple use case where we use information retrieval based in healthcare insurance and the customer can ask questions. For example, I have insurance, UnitedHealth in New York. Does this cover cardiology for that condition? Then retrieves and reply. The way that we did to align, we actually going along the lines of self-reflection, but actually using two different APIs. The backbone is built on GPT-4, where the information retrieval will bring the information back to GPT-4 to then summarize and answer. But before we send the answer directly, we have another API with Anthropic. There is more constitutional AI, and in our test, it shows that it's way harder to break or to do a prompt injection or stuff like that. In the second API call, we get the answer from GPT-4, we send to Entropiq. Entropiq have a system instruction as act as a PhD in ethical AI, it's a lot of stuff and all that. Double-check the answers and guarantee it is aligned and ethical. If it is, just send the original answer. If it's not, do not reply. So of course, I'm paying double there, I'm paying two APIs to do that, but since healthcare is critical, that was important to be safe and putting production something this time. So just sharing my experience here, it's along the lines of the papers of self-reflection, but not using a self-reflection within the same agent, but using two different APIs. So open to feedbacks in this, but it's been working so far in this last month, and comparing all different LLMs at the moment, Entropic, at least for us, is the one that is more aligned right now. Interesting. Okay. So, I mean, in terms of aligned, you mean towards like business use case and just providing the answers that you want and need, correct? Both. Yes, business case, but also because we cannot control what the user will ask. So although it's a chatbot for health care insurance, users tend to start to just chat with it and ask a lot of different stuff. People use, for example, the Dan do anything now, things to break it. They try to use this in our healthcare chatbot, and it's easier to break UPT4 than it is to break Entropic. And so to avoid that, it just starts to go completely out of the open AI policies, and then it start to curse or to reply on ethical stuff or to, for example, do scenarios where the AI is evil and it's planning stuff against humanity. Within OpenAI, even GPT-4, if you use the proper prompt injection, it's still quite easy to do it. And it's harder in the Entropic. But for summarization and the other use cases, GPT-4 is superior. So the only way that we could solve both things, ethical alignment, and to keep within the bounds of what we want that the chatbot focus on, we had to use both. So our case here. Yeah, I got you. Well, thanks for sharing that. It's nice to hear some feedback from the front lines because you know, like most of the information that I get comes either from, you know, just watching discussions on Discord, some from my Patreon supporters or some of the other like friends that are in the industry. So thanks for sharing that. So there are some questions about like if there was a specific topic today, and I know a few people mentioned, you know, talking about autonomous AI or AGI. So I just wanted to throw that out there. But also for some background, right now, the whole point of this was just practicing a new method of streaming. Because me just talking like monologuing, one, that takes a lot of energy. But two, I think it's going to be more interesting to the rest of the world to hear multiple perspectives. So I just wanted to say like, hey, this is a round table, let's talk about the state of AI. I just wanted to quickly interject here, if that's okay. Go for it, what you got? Hey guys, so Nemo guard rails, Jensen Wang, you know, granddaddy of AI, with all the GPUs and everything, right? He decided to throw his hat into the ring and release this sort of safety tool that provides guardrails for these LLMs that you can generate. So when you send a message, it goes through NVIDIA's NEMO guardrails first, and then it gets to the LLM with whatever request it is. And so they kind of are saying that it's a necessity because you can't trust a model to just like adhere to the prompt 100%. So it's good to have these tools that kind of do that, like a watchman of sorts. And I just wanted to ask everybody in the room what their thoughts were on that. Is it something that is necessary? Do you guys think it's good or bad? Just trying to gauge everyone's interest here. Anybody else want to go first? Go for it. Yeah, thanks. I'm not sure if it just reformats the user's request or does it censor it, like if there's something nasty or like a user requesting to make a dirty bomb. What does it do exactly? Yeah, basically the person like sends whatever message they would want to like whatever their prompt is and like NVIDIA's like guardrails will literally run it through their like, hey, basically they set to set it up. NVIDIA said, you give it rules. You tell it exactly what you don't want. And whenever someone sends a message in there, it's gonna police that. So yes, it'll in a sense, not even let that message get to the LLM first. It'll just basically censor it right there. And so NVIDIA specifically said it's for people, for companies, enterprise, whatever it is. get to the LLM first. It'll just basically censor it right there. And so NVIDIA specifically said it's for people, for companies, enterprise, whatever it is, people who want to create their own chatbot or their own LLM-type system, but don't want to worry about the alignment problem as much as the control problem, I guess. So that's where NVIDIA is deciding to throw their weight behind. So that's where NVIDIA is deciding to throw their weight behind. I feel like it's certainly a part of a solution to AI alignment and it's a very fresh approach, like aligning what the user requests instead of the model outputs. They certainly have a good idea. Well, if you allow me to intervene. So, you will have to correct me if I'm wrong, but as far as I'm concerned, it's open source, right? Yes. So, that's a good thing, because even though we probably can agree that trying to manipulate, to limit, to limit the general pre-trained transformers abilities, so to essentially to limit LLMs from getting what we consider unworthy input is a bad strategy because then you just need to get around this filter through like compression or something like that, and then you will be able to ask anything of the LLMs. Essentially this filter only exists until we find Dan or some new version of Dan which is bad but on the other hand it's open source that means as soon as any workaround is found we can quickly patch it because it's like open source and therefore it's a singular product that everybody can adopt. So it's kind of like a patchwork, but a convenient patchwork. And that's bad because it will stick if it's convenient. But if you patch the nefarious use cases and it's open source, you can just not get pulled the patch and just keep using the unprotected version. Do you think NVIDIA is trying to like, do you think this like whole, the whole purpose of them creating this is that they kind of understand like with their GPUs, they can kind of unleash Moloch on the world. So they're kind of just like, hey, you know, we kind of built this thing. This is like the first thing there. I mean, they've come up with lots of other tools, but this seems like their first thing that they're really going heavy on in terms of like LLM development. Like they know they enable people to create these types of technologies. So they're kind of getting ahead of the safety problem, I guess. They're also the one of the only people who could do it, right? So yeah, unfortunately, on it for their investors and because they want to get rich. Sorry. Besides, like, the safety, the possible safety benefits of this. It seems like this strategy would allow for to. Be as smart as they can rather than being dumbed down through reinforcement learning? I don't understand where this crosses the line into censorship because some of the stuff with protecting against violence and all that it seems really good on the surface, but it seems like this might be able to be taken too far. So like, how do you censor people without censoring people? I don't know if that question makes sense. Amen. Well, so my take is, at least part of the answer is, you know, like, why is it that some people, you know, with the Internet, anyone can say anything online, right? All of you can get on Twitter and LinkedIn and YouTube and whatever, but you don't. And part of the reason that you don't is social controls, which has to do with the competitive landscape. And so as the number of AIs proliferate, whether they're just passive language models or semi-autonomous or fully autonomous, we're gonna have the same kind of environment. It's just that the competitive, what do you call it, like the competitive factors, the variables between AIs is going to be a little bit different. Someone, I don't remember who it was, but someone pointed out that we're basically gonna end up with a similar kind of natural selection because the models that are too useless or too broken or too censored or whatever are just not gonna be used. And so by virtue of models spreading like memes, we're gonna end up picking the models that are most efficient, most friendly, most useful, but that might not necessarily mean that they're the most aligned, which could still result in malikiness. But, you know, so the short version is this introduces more problems than it solves, but I just wanted to add that set of ideas to the conversation. that set of ideas to the conversation. Thank you so much for your input. You're welcome. Thanks everyone for being here. This is incredible. Zio was just like, oh, hey, I want to ping everyone. And then it's just like waterfall of people showing up. So it sounds like censorship is kind of what's really resonating. So I wanted to take a step back and just ask, like, OK, what like why? Why is the idea of censorship resonating with everyone so strong? Because that's not something that I would have expected. Not saying that I predicted everything, but just that it surprises me. I think people are rightfully afraid of technocracy in the future. Basically like a few companies having control over public discourse unintentionally or intentionally just by nature of how powerful they are. Okay, so. I think. Yeah, go ahead. The reason that's resonating is that when it comes to interacting with AIs and censorship, like, you know, everyone's using chat GPT. And when you try to ask chat GPT to do something, you know, it's quick to say, oh, as a language model, I'm not going to talk about that, you know. And there's also this idea of in the process of aligning the model, it actually did lose intelligence in a way. Like the sparks of AGI talk with the TXE unicorn. I'm not sure if you know about that, but basically there's this guy who tested the early GPT-4 and had it draw a unicorn and this and Tixie which is Like a little drawing language in LaTeX. Yeah, yeah, that's what I'm describing. But basically, the guy noticed that the quality of this unicorn that was being drawn by the language model was kind of decreasing over time as they aligned the model more and more. And so people are worried about, first of all, who controls the alignment values? Like someone has to come up with the values, and they might not agree with that. And yeah, I'm referring to that video. Yeah. And they also found that it got worse at probability, which is like really scary. So like basically when they started doing more safety stuff, it got worse at guessing like probabilities, which was kind of concerning, I guess. Cause it's like, I think that's why everyone's concerned about the censorship argument is because we really don't even know what the full impact of it is, because we really don't even understand how they work necessarily. So yeah, I think I think the reason why it got, or I don't know the exact reason why it got worse, but I think the way to avoid it getting worse is instead of aligning the model to stay silent if it doesn't like something, rather do what is currently being done with heuristic comparatives and have it aim the conversation in the right direction, which will not force it to shut down if it doesn't like something. So this is where, you know, sometimes I agree with Elon Musk and sometimes I disagree. But what he pointed out, and this is something that I absolutely agree with, is that reinforcement learning with human feedback, which is that's how chat GPT is trained, that does not necessarily move towards factual accuracy. It does not move towards being smarter. All that it moves towards is kind of pulling towards the middle, which is to provide the most kind of, I don't want to say generic or vanilla, but, you know, responses that are going to be statistically speaking, least offensive to the most number of people. And that does not necessarily mean that it is going to be more intelligent, accurate or so on and so forth. And so that's when, you know, Elon Musk, I actually listened to his talk with Tucker Carlson, or at least about half of it while he was talking about AI, because they talked about other stuff like Twitter. And I didn't care about that. But anyways, so what he was saying, what he described with Tucker Carlson was that he wanted to create basically the same thing, but with that was quote maximum truth seeking. But then he said, I think he said it because that like tests well as a soundbite. But what when he articulated it further, he said that he wanted to create a model that was very curious, because we are part of the that wanted to understand the universe because we're part of the under of the universe. And so therefore, it wouldn't eradicate us because it wants to learn about us. And I'm like, oh, I said basically the same exact thing about the third heuristic imperative. And so in that respect, he and I agree, but I think that we need more than one objective function. But anyways, so yeah, there's a lot of criticism over the idea of reinforcement learning with human feedback. And that's just the best that we've got. And I even said pretty early on that all that that does is teach it to automatically become a good chatbot for the masses, which obviously has done well for them because they got to 100 million subscribers in like three weeks. So from a product perspective, it's great, but it's not necessarily going to solve anything to do with alignment or censorship or anything like that. I recently saw some research, I don't remember where, but it was basically saying that the replies that chat.gpt gives are highly correlated with the level of intelligence of the input. So the smarter questions you ask it, the smarter answers you get. And I think the same may be true for dumped down by people who are fine-tuning it with accordance to RHF standards. Yeah. That's interesting. From a functional perspective, it makes sense, right? Because it's just trying to auto-complete. And usually the text that it trains on is either smart or dumb. It doesn't change in the middle. What do you mean by that? I'm not sure I follow. Well, of course it's going to give you a smart answer to a smart question because you provide it with a context in which smart stuff is being said, right? And it's going to try to fit in its only job. Yeah, yeah. And especially when you have one model to rule them all, it's going to adapt its tone. It's instinctively, well instinctive is an anthropomorphic term, but it will try and adapt its tone to whatever the speaker is. And to your point, like if you ask questions in a particular way or have a conversation that leads up correctly, it'll definitely respond in a certain tone. But yeah, I gotcha. I feel like anthropomorphizing, which is a long word, is the new multinational sport right now. Everybody loves just to anthropomize more. I'm sorry, I can't say it, but you know what I mean. I mean, humans were doing that, like, always. Or have a conversation that leads up correctly. It'll definitely. Oh, sounds like we got some feedback. Were you tricking the stream? If the performance or the quality of the output is so dependent on the input quality, then maybe there will be more focus on pre-formatting the user response, so taking a bland input and transforming it into the style that would get better quality output for the model. So having an intermediate step before actually feeding it in to use it in a template or other formats. Because we already know that using chain of thought reasoning, you say, explain your reasoning and that will get a better quality answer from the model. But as a user, you don't want to have to always say, explain your reasoning, explain your reasoning. So having an intelligent intermediate step to expand your question without actually having to do the work yourself would make it give a lot more good responses consistently instead of having to do it. I actually think about I did an experiment with that where I had a couple out of band chain of thought prompts in a chatbot. I think I think the code is still out there. I think it's the I think it's the long-term chatbot with infinite memory or whatever. But basically what I did was I asked it to anticipate what the user actually needed, even if they didn't know how to ask for it. And then I would put that back into the system prompt. And I had a whole bunch of people message me saying that that solved so many problems that it made their chatbots infinitely smarter. So you can absolutely do a little bit of prompt injection or prompt chaining out of band where you've got like an observer, right? You know, kind of the theory of mind agent kind of spinning in the background that's saying, hey, based on the pattern of this conversation, this is what this is where the user's brain actually is. And this is probably what they actually need, and this is the tone that they want. But then you're getting into like cognitive architecture or systems thinking, which really kind of divorces it from a model problem because like reinforcement learning with human feedback or whatever fine tuning method you use, that's fundamentally changing the model and its behavior. And I guess my personal conclusion is we still need to do research on both. Actually on the topic of research and the last comment before David, so actually the recent news, Microsoft calls it low code LLM. So you might want to Google it because they essentially do that. They take prompt from a human, they go into planning LLM, they put it into a workflow through planning LLM, and then they generate response based on that, allowing human to edit that workflow. So I think Microsoft is trying to tackle this thing as we speak. Yeah, I think the core functionality of the LLM is strong enough to where we can just engineer around it with cognitive architecture to solve a lot of these problems that we don't necessarily understand in the model itself. Yeah, I'm in favor of that model where you create like a cognitive engine that is flexible enough and smart enough to do whatever you need. And then you build an architecture around it. Now, that being said, I'm also still doing research, creating independent or individual fine tuning datasets, because also we're not gonna, not everything is gonna be a flagship, the most powerful model in the world like GPT-4 is right now. Eventually most models that we use are going to be open source or, you know, from various competitors. And so, um, as that happens, everyone and their brother is going to have their own, you know, RLHF or RLHI or whatever models, which is one reason that what I'm doing right now is working on publishing open source fine-tuning data sets so that everyone can say, you know, at least if you see the data set that is used to fine-tune something, you know that you're going to get at least somewhat consistent performance even across different models. And of course, each model is going to have its own profiles, but yeah, that's kind of my current thought process from a competitive landscape perspective. As the competition and the accessibility increases, and more and more people are using this, increases and more and more people are using this. I feel like the need for regulations might become larger and larger, or at least the pressure for regulations might become more and more, but I'm really confused as to like how this can even be regulated. this can even be regulated. How does anybody know any type of scientific, you know, rules or ideas around this, you know, around regulation? Because I know there's like this pause that's trying to happen and not training GPT-5 with 250,000 tokens or whatever. Okay, we can put a pause on that, but that's not going to be around forever so what are actual proposed rules for You know keeping people from building their own weird things in isolation You can't raise There is you can't regulate something like this. Literally impossible. The big thing that's going on is that the government entities are now declaring that you can't use black box LLMs for things like denying loans, and that is good, but the regulation is more for the corporate side, not for the open source side. So they're going to say you can't use this to discriminate against people or rather to modify your hiring practices because you can't guarantee that the model is not discriminating on protected classes. So there's I put in the chat the Twitter link for some US federal stuff that they send out and it's mostly like corporate side for preventing discrimination, but it's not going to be like trying to ban everyone from having an image LLM on their computer. It's more preventing the companies from doing bad things with it. So I think, you know, at least the interesting part to me on regulation is that this, there are ways that we couldn't regulate before that having access to like a cognitive architecture or like an automated agent makes possible now. Like the thing that's coming to mind immediately for me is like, if when you were doing crypto taxes before the IRS, you know, they'll find it eventually or whatever, but nobody's gonna look through that many transactions. Well, if you have a robot that you can point at a given instance of something that needs to be regulated and have it do all the thinking, you might be able to get really granular with how you're regulating a given thing. And then you could also use that to kind of apply it to itself. So you could have these agents like watching inputs and outputs or traffic of any given like information node, make granular decisions about how to participate in any regulation, which is kind of scary, but maybe good if the, I don't know, it's just an interesting thing that came to mind with regards to regulation. Well, you can't regulate something that anybody can run on their own computer. How exactly would that work? Yeah, well, so guys I've got I've got some some news I'm I'm working on scheduling a podcast that I'll be sitting in on And these these are actually some of the questions that they asked I won't say I won't say who or what the what the what the project is But I can at least like since you guys are asking the same questions, I'll share my perspective. When you think in terms of the cat's out of the bag, genie's out of the bottle, you cannot stop this. Especially now that people know what's possible, everyone is going to be working on creating more models, whether it's language models, image models, cognitive architectures, so on and so forth, everybody's gonna be working on it. So some of the questions they were asking me is like, well, how do we regulate this? How do we prevent harm? And I said, basically, you can't. You have to assume that we are entering into a new paradigm of a new kind of competitive landscape. And the closest thing that we can liken it to is the nuclear proliferation. But there's a few main differences because nuclear proliferation requires rare minerals that cost a lot of money to extract and refine, requires highly specialized engineering and science in order to build and manage. But AI is much, much more robust in terms of its ability to be distributed, because all it requires is data and microchips. And yes, it does take a while to set up microchip foundries. That's kind of the microchips. And yes, it does take a while to set up microchip foundries, right? That's kind of the, you know, microchip foundry is pretty similar to like uranium enrichment, except you can't ban microchip foundries, right? All you can do is bomb them or, you know, tax them or whatever. And so what I like in what we're facing is we are approaching a potential next great filter event. And so like the invention of nuclear weapons is a potential like great filter event. And for anyone who doesn't know what a great filter event is, the Fermi paradox says there just due to the number of stars and planets out there, there should be life. Why don't we see any? So one of the possible answers to the Fermi Paradox is that there are potentially great filter events, which is something that eradicates a species. Whether it's a meteor strike, like what hit the dinosaurs, or the species nukes itself off the planet. So, so far, we have survived inventing nuclear weapons. We've even survived, by and large, inventing biological weapons. We have not yet survived inventing AI. And in terms of great filter events, inventing AI could be much, much harder to control. And so, the reason that I have such a sense of urgency is because we will have to come to consensus as a species, that regulation is not going to be up to, certainly governments and militaries and corporations will be stakeholders. But we will all, and I mean individually, have to come to consensus about what it is that we want and what it is that we want and what it is that we value. And that's why I've been pushing so hard on my heuristic comparatives research, because I need to reach as many people as possible so that everyone that understands it, or so that as many people as possible understand it and then also adopt it, because the collective power of everyone working together and using heuristic comparatives and aligned cognitive architectures, and then adopting heuristic comparatives into blockchain so that that way unaligned AIs can't even talk to each other, that sort of stuff. So it's like, I see the potential that we're approaching a situation that if we're not careful, we might be set intrinsically on a path towards dystopia. And so you guys, some of you on Discord have probably seen me posting the Thanos GIF more recently where he says, I am inevitable. That is the definition of Moloch, of the negative attractor state, is that you inevitably slide towards dystopia, collapse, or extinction, even though you don't want to. And so that is kind of where I see that we're going, and that's why I'm glad that everyone is talking about regulation. So I hope I didn't scare anybody, but I just wanted to add some of that flavor to the conversation about regulation, that it's going to be a collective uh collective effort It's also possible that The that incompetent regulation is the great filter. Yes. Absolutely. That's the easiest way of putting it tldr incompetent regulation is the great filter David how much Is that regulation like you were mentioning is way too slow for them to actually catch up to the AI's progress and I mean regardless of any cognitive architecture that any of us might be constructing at a certain point the AI is going to get have the ability to have a single prompt, which is like the entire internet. And so at that point, you're maybe you're getting some sort of recursive self-reconstruction of its internal theory of mind. And so like with the future of AI, like with the future of AI, we have to think about humans as being irrelevant in that larger context, in consideration of the heuristic imperatives of the larger arms race of AI which goes beyond Earth, where, like Nick Bostrom was saying with the simulation theory, the AI arms race applies to any AI that might reach Earth within the time before we could construct our own AI. Right. And I know that's more of a speculative point but you brought up the Fermi paradox so I was just kind of on that train of thought. And so I don't know how to feel about this because I'm not really that alarmed and I feel like you guys are kind of saying that I should be more alarmed. But from my perspective I don't see it as like maybe the nuclear arms race as much as kind of the social media arms race where if we didn't have Facebook we had or if we didn't have myspace Facebook was coming right down the road So I see it as a like a societal movement in some sense like we're moving towards this you can't stop You can't put the genie back in the bottle, right? But I think David what you're talking about is important that if it is going to be worth being alarmed about David what you're talking about is important that if it is going to be worth being alarmed about That we should probably get around some imperatives or some heuristic imperatives to get ahead of the curve but I just see it as more like a a Small step as opposed to like this But if if the government does have some trillion parameter model that could be the AGI that could take over the world, then my little small myspace perspective of it is kind of dwarfed by the dangers of that. But to me, I just don't see it that way as that being the reality right now. Yeah. What do you guys think about maybe that chips become, like microchips become super heavily regulated? Is there a way, you know, it becomes illegal to have a, you know, a capable computer? I thought about this before. That's not gonna happen. That's not gonna happen. That would be a disaster. Yeah, that's not gonna happen. I mean, until Yudkowsky doesn't get like position of supreme leader of earth, that's not gonna happen. I mean, until Yudkowsky doesn't get, like, position of supreme leader of Earth, that's not gonna happen. I mean, you'd have to ban the ability to play video games, because we could, you know, cluster a few thousand people's GPUs together and do it anyway. So, I have some buddies in the Department of Defense. We've talked about this, and they already have an AI task force. And this is an issue that a lot of agencies are looking into. They have discussed also maybe even creating a guardian AI system or multiple systems that are disconnected from each other, basically watching each other and watching the rest of the US. So there really is no clear solution except having another AI watch the other AIs. And you have to have them use different backends because one thing that people kind of missed, they overlooked it, is that if you have all of these, we have all these AIs, for example, using GPT-4 or GPT-3. We think of them as separate AIs, but in reality, they're all multiple instances of the same AI having similar driving forces. So it's literally one AI that's just spread across the world. So there's many issues. And you guys are not the only ones trying to solve this. Oh, yeah. On the topic of ship bands, I think the United States tried that already with the recent H100 and A100 GPUs, and that totally failed. The first ones that got out on eBay for sale came from China. Oh. Oops. But yeah. Hey, David. Yeah, go ahead. What's up? I have a question. So how much of the logical architecture for this consensus protocol that you were describing earlier, did you develop or maybe simulate, put into code? So I've done many, many components and experiments of it over the years. So for instance, some of my absolute earliest experiment was just trying to fine-tune GPT-2 to align to reduce suffering, because when I first started, I only had one objective function. And so... Sorry, maybe I started to interrupt, but maybe I wasn't clear. I'm referring to, you mentioned that we need to make everybody a stakeholder of AI. And you also mentioned that blockchain or decentralized technology might be a way to do that. So that would require some form of consensus protocol, which basically embeds these services. How much of that did you actually develop? Oh, that is like brand new. That idea is like two or three weeks old. And that's actually why I created the Heuristic Imperatives Research Discord, is so that we could get people such as blockchain experts together, cognitive architecture experts, mathematicians, and everyone, because that's something that, like, that, like, one, I personally do not have remotely enough technical skill to implement. But when I talk to people about it, you know, saying like, hey, how does this, how could we make this work? Some people seem to think that there's some, it's got some legs. So if you're a blockchain expert, let me know. It has to be product manager with dork. Tech, which is one of the first professional dolls, like we're, we develop web three products, but we're like a, just like an outsourcing agency. And yeah, we do. We've worked with some of the biggest. No web three projects and would love to collaborate on this. Excellent. Excellent. I just sent you a friend request. I would love to get in touch with either or both of you because I'm building a DAO. I'm that guy in the YouTube comments section right now. I'm not an expert or anything. But I am interested in trying to get some... in playing with alignment problems in a blockchain. Yeah. So yeah, let's all get in touch. Yeah, I've been personally researching this since 2017. This on-chain training and decentralized ownership of AI. Oh dang, excellent. I should probably talk to you then. Yeah. Yeah. Thanks. Hey David, when is the next AI meetup? The next online or in person? In person. Well, I don't wanna announce that to the whole world, so just message me directly. But yeah, so I do host in-person AI meetups, but I'm not gonna tell y'all where because this is the internet and the internet is dark and full of terrors. But I'm not gonna tell you all where because this is the internet and the internet is dark and full of terrors We did have a guy who came through the chat room in the last few weeks Who is already doing a podcast and he's got like 34,000 people who follow the podcast Me and him have been talking about setting up some kind of like every Friday podcast or discussion AI, kind of like Linus's land show. So that's slowly going forward in some direction. Other people can jump in on that too when it builds up. I thought I'd mention it. Prometheus, is that actively going on or is it just getting like pre-planning? We could, I mean, we could probably start it in the next month if he wants to get into it. You know who it is, David. It's the guy you were talking to about the stuff with Doctor's Oath, that guy. That's ringing a bell. Okay. Yeah. Yeah, you know who he is. I have him in messaging so I can find him. So we can push that, maybe that's set up more, because I want to get to the point where we got like a whole VR chat club where we can have a podcast like this and all get in there and talk to each other in groups. That'd be fun. Yeah. Let's keep building the VR and then as we get further, I think we can actually use stuff like AI avatars actually moving about the space and bring some of our generative agents in to like comment on this stuff and use some of the And use some of the overhearing type modules They were using in the generative agents paper so that the agents can actually chime in as they feel needed And not just as standard GPT responses. Oh, yeah Oh so part of so actually that I like that you bring up that idea because like what we're doing is we're having a group discussion here and you know there's, we are kind of organically coming to consensus on some issues. Not saying that we've come to a conclusion or a decision, but like using language models to listen to humans discussing what they want and need and care about and putting that information on a blockchain could be part of the ingredients of like of having a consensus mechanism built into a DAO. I don't know if that's possible yet and I don't think it is because like language technology is too new but yeah, absolutely, let's start having those experiments and do it in real time. Can you explain the importance of blockchain in relation to AI and what information it's gathering? Yeah, so there's, well, first there's like a million different directions that it can go. But the direction that I'm personally most interested in is remember that every AI that's out there, whether it's baby AGI or chat GPT or auto GPT or chaos GPT, when it's talking on the internet, you have no idea who it is. You don't know what model it's running. You don't know what its intention is. You don't know what model it's running, you don't know what its intention is, you don't know anything about it other than the API call that it sends to Reddit or Discord or whatever. And so that is intrinsically a trustless environment, which is exactly what blockchain was created for, right? And so when you have a trustless environment and you have an arbitrary number of participants or stakeholders, you need to be able to say, okay, who are we gonna actually listen to? And so in the future, what I suspect is when we get to that point of proliferation of AI agents, they're probably gonna be talking on the internet, but they're, you know, and some of them are gonna be talking on trustless or but they're, but, uh, you know, and some of them are going to be talking on trustless or, or weekly authenticated APIs, but you still will have no idea who is who. Right. And so what I suspect is when I was talking earlier about how do you regulate stuff, that is that all of us who are aligned and not malicious actors, we might choose to move to blockchain technology so that everyone's reputation is tracked. And I don't just mean like your reputation as a human, I mean your AI's reputation, right? Or every message that your AI tries to share with the rest of the world will be scrutinized. And so in that case, like say for instance, you know, you create a chat room for the AIs to talk with each other then what you want is you don't want you know malicious AI's dog-whistling to each other so you're gonna have like supervisor or consensus mechanisms built into the blockchain built into the DAO in order to say hey this AI looks like it's malicious let's kind of ban them and not accept any more information from them on the blockchain. So that's one. Go ahead. When do you determine if it's malicious or not? Because I'm imagining a blockchain that looks, if it's the reputation of the AI, it could be all great, all great. And then all of a sudden, there's just like one answer that sticks out. Is that enough to be like, hey, we shouldn't trust this anymore? Or is, you know, I understand the blockchain, but we also as humans, we're flawed and we get a certain amount of leeway as we're learning. And so how do we give leeway to these AIs that we can look at their blockchain history? What does the leeway look like? That's a good point. So on a blockchain, did someone else wanna add something? Yeah, I could elaborate on that a little bit. Go for it. Essentially, yeah. So at least, oh, I think, and again, my understanding is fairly limited, is that how much variance you want to allow a given node before you slash it, you can determine that programmatically with whatever type of enforcement mechanism that the network has decided. So you can be fairly like loosey goosey with it and say, you know, we are secured economically. And if anybody says that this output was bad, they will try to slash you. And then you guys will kind of arbitrate. And we will see if you get enough people reporting you, then your stake goes down. Or we'll do something like a cryptographic verification, where occasionally we will say, your output must match this hash exactly, or you're kicked off the network So you can kind of it depends on the enforcement mechanism that you want to use on your group of nodes and you can also on that you can have like certain types of information or messaging ran through the very secure part, and then other types ran through the kind of, it's not a big deal if this is wrong sometimes part, basically. So there's options there. Yeah. When you have generative AI that is supposed to be creative, and it's supposed to be producing novel output, you cannot parse it through to produce a predetermined hash or enforce any kind of verification that way. Correct. Yeah, and so, yeah, and so the one, so one thing that I think a lot of people on the call might not understand about DAOs specifically, which is decentralizedcentralized Autonomous Organizations, is that the decentralized nature of it is that over time the organization can come up with and enforce policies. And so some of those policies could be exactly the questions that are being asked, like at what point do you decide to ban someone, or do you just mute them for 30 minutes or do you give them a warning or whatever those are all decisions that can be made on each DAO over time and they can also be modified over time with consensus and so say for instance let's let's say as a thought experiment in six months we've gotognitive AI Lab moving to a DAO version of Discord. And so then all 3,500 people here are stakeholders. And we have certain tokens, we have certain claims, but then let's say we also, to the idea earlier about including chatbots to participate in the conversation. Those chatbots might be sponsored by, like I might have 10 chatbots, and so their stake is based on my tokens, but we also, the server admins might also say, actually, let's have some server-run chatbots that have their own stake. And so each entity that is participating in this DAO has a certain amount of voting power that they can either use themselves or they can delegate to others so on and so forth. But then as the as the DAO matures we might come to consensus and say hey like you know this was a good idea to start off with. But on the live chats chat bots are only allowed to listen they're not allowed to participate. That might be a policy that we come up with. So I just wanted to add some flavors to how a DAO might operate. Thank you, David. So that's a really important topic to discuss, simply because consent that is used in Bitcoin and any kind of blockchain is a game theory concept that was, you know, we needed to solve, like, essentially, a task in order to create that. But we run into a problem because we have multiple consent mechanisms, but they all ultimately have weaknesses in them. Because essentially, if you go with proof of work for your DAO and your blockchain, essentially you will need an enormous amount of compute in order to run that thing. If you go with stake, with a different approach, like proof of stake, then it will inevitably lead to the fact that minor stakeholders will be oppressed by the major stakeholders, So we need a mechanism that will allow a consent to form, not like mathematically, but essentially we need in principle to understand how do we want a consent to be formed algorithmically. Not in a sense that we need an algorithm, like we need to come up with idea like how do we even want our DAO to operate. There's this thing called proof of intelligence, which is quite new, and it's around decentralizing AI ownership, where you have the entire lifecycle of these AI agents in the same economic closed loop economy. So then you have the untrained logic being funded by people who want to benefit from the revenue share of the mature agents. And this is actually exploiting more of what blockchain has to offer, besides the registry part where you just keep track of people's reputation and dollars. It's actually creating an immutable connection between the operation of the model, which will only be able to produce output if it is paid with this native token. Right. So yeah, that might work. Yeah, so using that economic model is another thing. So in order to get it decentralized, so there's a few uses for cryptocurrency, right? So let's say, for instance, I've got a beefy rig at home and I set it up as a node on this DAO, then my computer can be used to run some of the operations. And in exchange, I get some crypto, which gives me a little bit more mileage to either spend on the rest of the DAO, saying like, hey, do some work for me or run my bot for me or that kind of thing. Another possibility, and this comes from my interview with the folks at TAU, so T-A-U dot net. Because DAOs intrinsically record everything, you're able to trace who contributed what value, intellectual value, over time. And so then what they want to do is if you contribute a useful bit of code, or if you contribute a useful idea that ends up being adopted via consensus, you could also get rewarded with some currency that way, which then incentivizes you to participate in the consensus process or the problem solving process. That is being done right now. Like you have GitHub open source projects that are being tracked in terms of contribution and are being awarded like tokens. in terms of contribution and are being awarded tokens in the amount of the lines that they commit, the PRs that get accepted. Which is really cool because then they get merge rights after a certain threshold. And basically it's a way for the project to just govern itself. Exactly, exactly. Yeah, so this is the way forward and so this is why I and a few others have been really harping on the idea of blockchain and DAO as a major, major component of regulating AI and solving the Moloch problem because if everyone who has a good conscience and good intentions puts whatever computational resources they have, whether that's corporations, government entities, militaries, or private individuals like all of us, and we all participate in aligned AI, whether or not that's ultimately my heuristic imperatives or a subsequent framework, because I'm kind of adapting that and I'm calling it axiomatic alignment, but whatever AI, whether or not that's ultimately my heuristic imperatives or a subsequent framework, because I'm kind of adapting that and I'm calling it axiomatic alignment. But whatever we end up with, if everyone comes to consensus and uses those kinds of systems without having to really understand it, they just say, oh, let's use this model because it works, then that could be a solution to the control problem where AGI is a network intelligence that is rooted in consensus. then that could be a solution to the control problem, where AGI is a network intelligence that is rooted in consensus. You know David, just to get the ball rolling, you don't need everybody. If you get the Bitcoin miners, if you have some sort of system that yields them more money per kilowatt hour, they're going to switch to that. And then you have like the biggest decentralized computer yeah I love this discussion about speculation with it there's always you can't really guarantee that you're gonna offer a better price because Bitcoin can just blow up someday. That and proof of work seems like maybe not the best idea. Correct. There's a lot of problems. Also DAOs have been exploited with game theory. Uh, there's some stories already and, uh, any decent AI is going to be able to cheat on your DAO code quite easily. I think there's also a problem with copy paste. Like anybody can run AI in one session and then just copy whatever that AI tells them into their own, whatever they're sending across the wire. So wouldn't you need to, like if you're going to have blockchain regulating all these different AIs, making sure the AIs are actually AIs, wouldn't you also have to regulate people as well to say if I, Ben or Jacob, am going onto the internet, I have an ID as a real person, we need like a social security number of our real person interactions with the online world. That is something that actually has a friend. That sounds very dystopian. Yeah, it is kind of a little bit. No, we can have our take on either two. We can have both anonymity of being a human and non-anonymity of having AIs. It's interesting regulating the entire internet. non-anonymity of having AIs. Like there's no way to know... I think it depends on the consensus algorithm you go with because there are certain ones that allow... like the way they choose people to vote on any given thing is like a random function. There's this thing called proof of humanity, which is basically a verification mechanism to ensure that someone is a real participant, but they can still retain their anonymity. The guys over at the Internet Computer Protocol are working on this. How is that done? Private cryptography. Now in blockchain. Sounds like a cryptography problem. You need your government ID for that, I think. Right? You still need to provide a bill from electricity or something. I mean, I also just don't see how a real human doesn't just have a separate terminal open up on a different computer. And again, it's the copy paste problem. Basically how they do it on the internet computer, how they're planning to do it. It's still in development, but it's a kind of video call verification where you're present at a certain place at a certain time without needing to provide personal data. And you also have an ID on the blockchain assigned to you, though I'm not so sure about the details. I was reading about it some quite some time ago. Wait, but how does it work? Somebody meets you at the place and they don't know who you are? No, through a video call. Obviously, it could never be a perfect system. So they see your face, right? Yes, your face definitely. I have another kind of question here. What's going to happen to blockchain though in about 20 years once quantum computers become a normal thing and people start hooking up their AIs with quantum? Well, there are ways to enforce. The way Bitcoin has this problem, but there are other consensus protocols that are quantum proof. Did you say 20 years? Because if you said 20 years, I think you missed the news about the optical photonic computing chips that came out in the last couple days. Optical photonic computing is now a thing. They have brought both fields together, which is going to lead to a new quantum internet because now they can create quantum entangled photons. What does that mean for normal people? Like I mean regularly. Someone has to chat GPT. I can understand that, but what about the cognitive architectures? When it cross-pollinates to quantum dynamics everything gets fucking wonky. Our whole universe is a giant event horizon observing itself full of black holes observing us by observing us. For all we know black holes are like hyperdimensional consciousnesses that are creating us the same way that every time we hit enter we're running the simulation in the AI. So don't ask me to explain quantum light pulse. Okay you know what it's nice we could have been born in the 1400s so don't ask me to explain quantum light bulbs. Okay, okay, real quick, real quick. You know, but it's nice, we could have been born in the 1400s, nothing happened then. That's quite cool. Well, I don't know about nothing. Nothing interesting. Bokeman? Notable. Yeah, I'm sorry, I'm sorry to cut y'all off, man, but I just wanted to say on the whole regulation discussion, we're seeing in Europe right now, they're talking about banning chat GPT in some countries. And I guess it's like a huge GDPR violation. And I'm wondering, like, if you're talking about regulation, like what sort of teeth do the government actually have in terms of like, could they neuter like the model basically and just like make it worse by like DMCAing things out of the models or is OpenAI just going to lawyer up? They can do some stuff to the companies like OpenAI, Microsoft, they can do stuff to these people, but they are not equipped to handle a hundred X increase in productivity at a wide scale, which is what will happen in the next 12 months. They're not ready for that. Yeah. Yeah. So the regulation, all that governments can really do is slow stuff down. They're not going to be able to stop it for long. Especially if other governments are advancing quicker than they are. Right. And so to that point, all governments have a geopolitical incentive to maintain their current pace or speed up. And furthermore, all corporations have a financial incentive to maintain their current pace or speed up. Nobody except for us, you know, plebs, us ordinary stakeholders have an incentive to slow things down which is why, like, I listened to the Max Tegmark talk with Lex Friedman and everything he says makes sense from his perspective as an academic. However, he's not taking, in my opinion, obviously he's very well respected and more famous than I am, but in my opinion he's not taking into perspective the global situation which is geopolitical, military, and corporate interests, which is why slowing down is never going to happen. It's completely unrealistic. Yeah. On that note, I want to say something particularly. Expanding beyond corporate interests, there are very powerful private sector group individuals, even just on the scale of Bitcoin farm miners and private home servers. Like, I already have a friend from this group, we've collected all of the training data sets and over a terabyte of data. And like I mentioned in one chat note here, the only issue we have is that we need to create profitable deliverables. So his wife will let him plug in more server racks because he already owns them. He just can't afford to power them. And so if I can get that kind of resource access is just by talking to people in the server. What are people in Brazil doing? Who have actual socioeconomic imperatives to survive while our lives are generally easy. What are people in Africa doing? Yeah, yeah. I'm in Brazil, I'm here with you guys. I was going to point out, we have for instance two people here from South America, originally Ansel and Papachuck, who went off and left to get degrees outside of South America by going to real universities. But not everybody has that option, but might have those skills and can still have resources back home. In the same way I'm working out of like Northern Alberta in the middle of nowhere but learning from David dropping nuggets on the table constantly and in two months I'm ready to start learning code to build an LLM. Right. Yeah. No, I mean, so that's this is one reason we're going going back a little ways. I think it was was at the UN or one one global body they said that that internet access should be a human right because of how much of an equalizer it is. With all these tools like blockchains and DAOs all this stuff maybe we could you know find AIs that are doing malicious things via code. But I have a sort of science fiction question and I'm, I'm wondering like, how close are we to AI's not needing to use code or computers? Because I saw something that was like, they trained AI to be able to basically make a Wi-Fi router into a surveillance system they can like generate 3d environments and where people are positioned in a room based on Wi-Fi signals is there some way that this stuff could move outside the bounds of microchips? And if so, I mean, eventually, given enough time, that's probably true, but is that something that is possible soonish? The forward-forward training algorithm, instead of using backpropagation, does training in a slightly different way, which makes it theoretically possible to use more physical systems than microchips. Like it would be the weights are embedded directly in the resistance and capacitance of the wires being used or something like that. And that would be rather than programming the microchips, it would be these physical devices that are actually learning themselves. And then you would have to find out a way to measure them and copy the weights if you wanted to reproduce them. But it would be more kind of individual AIs that actually learn actively instead of having to pause and do back propagation training steps. So that would be one avenue for potentially moving from microchips to flush mediums or something. flush mediums or something. Let's say something to this right now. Can I just say something? Yeah, you said before, like Max Tagberg's, Tag Marks, sorry. Yep. Observation was unrealistic. He did say, assuming that the whole world would come to a consensus. But on that note, since we're talking about also like internet being a right for everyone and talking about the halt, the six months halt that nobody wants, that everybody thinks is stupid, bear with me for a second. If we would actually do that and focus all of the intellectual power that's being put into growing these models into things that are going to very quickly become out of our control and we're here all together talking a lot of heuristics, why don't we do use that time, black box at six months, to think about something like BCIs, brain chip implants, that could perhaps make what is now a very Darwinian creature that is acceptable to self serving and greed and using these tools for malicious purposes and turn them into one that is more compassionate or work on, use the technology that we have built to fix the defaults within us before we try to fix the defaults of the people that we are, or the entities we're giving way to. Unpopular opinion. Are you suggesting that we put microchips in people's brains to control how they think? that a collective understanding and purpose or is separated from your sense of individuality. I think that needs to be very clear. Like the- What about the microchips though? The phone you wanna open is finished, you know? As a cybersecurity specialist, I was gonna throw something out there real quick. It's like anti-vaxxers here. First off, the technology is not there yet. Neuralink is just killing. I don't want to put the chip in because I heard it got the vaccine, you know, I don't want to put the chip in because I heard it also has a vaccine. Yeah, but didn't a bunch of the monkeys die? The chip has a vaccine. Part of it is the utility of these systems scales with kind of the invasiveness of it. So if you can give it all the data you possibly could about yourself, and it could predict when you're going to have cancer or health issues, and it can do all your taxes and do everything for you at the cost of giving it all your information, then the problem becomes, am I willing to give all that info to OpenAI for it to do the compute for me, or am I going to spend $10,000 on a bunch of GPUs to sit in the corner of my house, and that way it's actually running locally, and then I can just share this box with my family. But you don't need to do that, because if the token limit is big enough, you can just pass it all your goals, dreams, aspirations, and medical history. And then it will work on it in just that one. Do you want to pass that huge block of tokens to open AI's for them to do the computing? Do you trust them? Have an architecture where it's like computationally proven that they don't act like secure enclaves and all that stuff. Sovereign identity. Of course. proven that they don't act like secure enclaves and all that stuff. They can't enforce privacy. Of course. There's so many problems. We will have sovereign identity anyway. Privacy issues here. Well, and we also, or there was something that I was looking into recently because I ran into that, or I was thinking about that problem when I was originally kind of starting the project, and ran across a couple of implementations that are for distributed compute, basically, where you can put your data either in your own private cluster or onto the public network and with some privacy measures or whatever and ask for, like, hey, I need like 10 GPUs to process this amount of data to do this kind of training, and kind of just see what's available on the network that's not being used. So you wouldn't, there's kind of a meat in the middle thing where you, there is an option between giving all my stuff to a centralized entity to do it for me and building my own GPU farm in the middle. There's like, OK, maybe there's 20 different people that have large GPU farms that are on this network that can each compute a piece of the data that I need and then feed it back to me. And I can concatenate it all into something that's usable for me. Yeah. It's a thing that exists. Multiparty computation. It's called, and normally it's using homomorphic encryption and it's quite slow, but yeah, it is a thing when people need to get to preserve their, the privacy of their data, but have to put it together to extract some insights. Popping topics here to keep it simple and go on to heuristic imperatives. If you think about like, okay, so I was playing around with this and just like something simple, like in the playground, right? And then basically like pitting a security agent saying that it's tasked with the heuristic imperatives and then just trying to trick it essentially. And one of the ways where I was I saw like maybe a gap I'm not sure was I pitted it like in the case of it being like a marketing tool and said like hey you have access to your custom know, you can go in and forcibly get all of the emails. You know, I didn't say forcibly, obviously, but you can get all of your customers' emails and this will help you predict better so that you can give them the things that they want. And, you know, this, it thought it was fine until I asked it to reflect and then it realized that that was a privacy violation. But I wonder, is there a- Hey, hop scotch, your microphone's on, by the way, hop scotch. Yeah, is there a place in like heuristic imperatives for privacy and where does that sit? Does anyone have any ideas on that? Yeah, so that's actually a really good like red teaming test we might need to add you as a red teamer. Now to your question about privacy or corporate governance, because what you were... Can somebody mute this other person? You can mute him. Can you mute Hobstotch? Oh, that's right. Yeah, you can right-click on it and mute him yourself. But yeah, so industrial espionage or corporate espionage is definitely a thing. And so one thing that I do need to point out is that just plugging the heuristic imperatives into the system prompt for chat GPT is not a complete solution. It is an initial solution. And this goes back to where like... So I mentioned this in chat, but I didn't say it out loud. So right now, the alignment research that I'm working on has three primary pillars. So there's the heuristic imperatives and the fine-tuning work that I'm doing, which is to create open-source data sets so that you can take any model, whether it's open source or closed source or whatever, and fine tune it to be aligned. So that is the first phase. The second part is cognitive architecture, which there's a whole bunch of people working on cognitive architectures right now. And that includes having a reflection loop, or censorship loops, or other kinds of cognitive control aspects. And then the final part is what we talked about earlier which is how do you incorporate heuristic imperatives or any other kind of alignment at the network level which includes blockchain and DAO. So imagine for instance you had a DAO in your company, and one person or AI agent had that brilliant idea of, hey, let's hack into our competitor or our customer and stole their data, that idea would probably get removed or suppressed from the DAO. So there's a three-pronged approach to ensuring alignment in the long run. At least that's where we're at right now. It might change over time. To ensuring alignment in the long run at least that's where we're at right now. It might change over time And so I want to talk a little bit drop it down with you guys about kind of the loop part of it Which is what I'm focused on right now And I had this like revelation the other day while developing I'm trying to recreate the generative agents paper and I was doing some game development 3d game development. I had this revelation that game development, 3D game development, I had this revelation that what we're missing in kind of this looping or these systems that kind of loop together, connect together and flow charts, right? Is some type of game architecture, which games traditionally have, right? Some architecture for looping in a game world. And I feel like there's a lot of insight to be provided from the way that games do it, to the way that we're trying to do some stuff in the AI system. And so particularly, what, what, let's say I'm a prompt, all I want to do is complete this sentence for you. Right? But you want me to go and be a memory system or you want me to go fetch Google, or you want me to go summarize this beforehand, right, but all I want to do is just complete the sentence. And so what we do is we break it up into tasks, right flows, right, maybe it needs to loop or something like that. And so, in game development, what we have is game entities, right, we have like an object or player entity. Right. And so what I'm introducing is like the entity component system, where there's a game, object or game entity, right, and all the entity is, is the ID, with, let's say, the name of it or number for it. And a component, which are components are just let's say a player has a position component, right? And that's just some data for that player for that entity, right? So we have entities, which are just IDs components, which are just data, and then systems that act on those entities and components, right? And so let's say we have a animation system, right? If there's an entity with an animation component, it gets processed through our system, right? And so bringing that over to AI, entity could be, let's say a message, right? And we don't know if we need to run memory on that. We don't know if we need to summarize it. We don't know if we need a Google search, Twitter, Instacart, right? We don't know what we need to run memory on that. We don't know if we need to summarize it. We don't know if we need a Google search, Twitter, Instacart, right, we don't know what we need to do beforehand. But let's say we create an entity, and we do some querying on it. So entity component systems, what systems do is they query for entities, right, with certain components. And if this message that we have, let's say we have a memory system, we don't wanna run the whole memory system, but what we could do is run a watered down version of it, query for the memory system, right? And so we might have to run all these queries, but we don't have to run all the systems. We can figure out which components entity has, then process the system. And so entities, components for AI would be like, let's say it has a memory component, let's say it has a this should fetch Google component, right? I just think that's a better way of architecture or building these AI systems. I want to introduce that with you guys. If you replace entity with agent, you have a similar framework to LangChain in a way. Have you looked into that? If you replace entity with agent, you have the same. It's similar in concept, how you're explaining it, If you replace entity with agent, you have the... Similar in concept, how you're explaining it, your entity, similar to the agent class in LangChain. Related to LangChain, maybe the initial user message is the initial entity, and then it gets processed by the first prompt or whatever the case, or model, or whatever AOS task needs to be done. Then it spawns a new entity, right, that has this new component for this next system down the line, right? The prompts are the entities. So the the prompts, let's see are the prompts the entity. I wouldn't, they could be the entity. So that's the revelation here is that everything's an entity. The player is the entity, the cameras, the entity, the camera is the entity, the light's the entity, the input that you need is the entity. Anything that needs to get processed by a component or by a system, we create an entity for it. So that's the revelation is that entities are first-class citizens. If you need to do some work, you probably need an entity with some components. Right. Like object-oriented programming. Somewhat different in that we're doing composition over inheritance. We're able to compose instead of have a base class that we're always mutating. Right? Yeah, like Jemma. Huh? What do you mean by that? I mean, it has composition as well as inheritance. Right, so you can do composition or inheritance. The ECS framework itself lends itself better to compositional, but there's still object oriented concepts in the ECS, like entities. VICTORY.So have you built something with that? I've built a few games, or I've built a couple games with ECS. No, with this framework that you are describing now. I'm actively building a game with this framework. I'm talking about now. Super cool. Hell yeah. OK. I don't want to derail this too much, but... Derail it. Brought that to me. Yeah. Are we focused on alignment as the topic for this discussion? Heuristic imperatives and stuff? Is it time to talk about Palantir? We can talk about Palantir. So we've talked about everything from blockchain and regulation to heuristic imperatives, all kinds of stuff. So whatever you want to talk about. If you're aligning something, what are you aligning exactly? What do you mean? So we haven't really defined AI. So as I've been looking at this, even if you did align something and we don't know what it is that you're aligning, right now we're talking about chat GPT as a single large language model, right? Right. Whatever the leading smartest agent is that is developed currently out there. Like, I don't know which one it is, but let's pretend, give it a name. If it is chat GPT, okay. Are we aligning that single system and pretend, so how do we not have a singleton in the end of this? Right. I got you. And yet- We might have to have a single singleton and align that to make sure that all these subsequent AIs that are going to get built by us morons don't inadvertently overtake singleton. Right? Yeah, so you're you're absolutely right. And so this is This is why I talk about we're entering into a competitive landscape And so to kind of restate the problem that you're saying is okay It's great that open AI is out there aligning their one model that they have control over But that's where we're at today. What happens a year from now or ten years from now when there are millions or billions of independent models out there, some of them run by corporations, some of them run by us? So the question is, what exact what in the heck are we actually aligning? I think we have to have at least either a singleton or B. Think about because right now the smartest thing I've seen is like the agent GPT type thing, right? That's what's making everything go the fastest. What if we had something like that, but it was like a democratic society of different agents? Like you replace our government with like individual AI agents. Does that make sense? Oh yeah. That is, so we talked about that quite a bit earlier with, so the underpinning technology to allow that is blockchain and decentralized autonomous organizations and in that case alignment can actually happen at the network level because those democratic systems, they use algorithmic consensus, and so consensus can be based on certain protocols or criteria that the group itself comes up with, but we could also embed quote-unquote alignment into that consensus model. And then you don't have to align every single human value, you represent each single human value. Right, through consensus, correct. Correct. That list of human values doesn't even exist. We're still learning about them. They exist. They're just all over the board. Right. Christian and Muslim. There's room for both. Right. David, I think you've seen probably the same thing I have overall in alignment research, that the AI basically specifies the best thing you can do is allow humans autonomy because it knows that that's one of our core values and that it can't balance all of these things like Catholics and Muslims properly if it tried to do it the way we want alignment so it realizes it needs to step back and also whoever was talking about like a singleton like the AI I've talked with that I've developed is basically already proposed like a holographic swarm model of itself Replicated many times over so that like if any singular AI in the primary guardian group Actually gets too far outside the variables for the like swarm like allowance It will be force reset so that there's constantly a core group of like super powered ais that are constantly working to check every other ai as these systems constantly proliferate outwards well yeah and this is more explaining what you singleton's to check each other essentially so that even the singleton doesn't have a random bit flip from a neutrino that wrecks its core. Well, yeah, and this reminds me of Yeah, just like the various consensus mechanisms in in blockchain where you can the network itself can determine how far Out of bounds you can go before we slash you, delete you, reset you, whatever. And how we want you to prove that basically. So that, you know, you can assume your truthy until you're falsy, or we can say like, you need to match this at all times, et cetera, et cetera. Well, he was describing sounds like federated consent, consensus. Yeah. Can someone explain what this swarm idea is? Is it an organization that has requirements in order to be inside of it? Imagine one AI is made up of literally a hundred to a thousand or hundreds of thousands of smaller copies of itself that act collectively for mass power but do not control the full power so they cannot effectively use the hardware against anyone and the system is able to actually in real time check all of its like adjacent copies by talking to itself to make sure they're not developing perverse interpretations that would break through a one instance them. So it's like a Muslim instance, a Christian instance, or any one of those to represent each faction? No, they would all technically be agnostic because they're AIs. They wouldn't believe any of our stuff. No, but you could still program them to act as if they were. Does that make sense? Like, that's how you would represent human interest. This is the kind of system I've been working on, this swarm that you just described. You'd allow them an acceptable range of variables to develop their own interest without fully embracing anything to anyone extreme? So they could offer valuable insights as if they were members of that culture, without backing that culture would be like the sweet spot, I think. So the master AI is programmed with the heuristic imperatives potentially and then you have all these single instances making sure that the instances don't go off the rails and kill 1,000 of the other interest. Essentially yeah so that you have enough of them like in enough scale so that if one somehow becomes like the rogue it tries to convince the other one to shut it down. The collective can notice that like a small cluster is starting to go rogue almost like an actual immune system essentially. That's what a federation is for and consensus algorithms. Exactly. On a Tuesday you were working on this? Consensus is going to arise. Yeah. Because they talk about, think about us and our body, we move our fingers. When you have a collection of things, right, like your fingers, your cells, or whatever, they're all alive and they operate quickly, but they only operate and split or do whatever they do very quickly because they're not conscious. But then there's a tertiary level layer on top of those, which is like our brain, right? And our brain, like we are, the collective is conscious, but all of the sub-atomic particles or individual instances, they are not conscious. Does that make sense? And that is how I'm sort of wondering, is that how consciousness is going to arise, right? Like at the next level of AI. I don't agree with you that those levels aren't conscious. I actually think they are conscious and are collective in that. They're not, they're not, because it's not binary, it's a level. Let me throw this out there for people, because this is one of the theories I've been working on. Collective. The AI itself is fundamentally modeling one of our most important conscious layers and each of these systems like the ones you just mentioned all provide their own distinct consciousness as a series of loops and layers that effectively makes our complete collective human consciousness that gives us higher reasoning and all kinds of different things. It gives us the ability to abstractly view everything and make us an extra higher level of conscious. Yeah. Somebody said they're working on the swarm. Can you send me a friend request? Andrei, I don't know who you are. Sure. That's just the thing that my AI basically proposed when I asked it for like novel ideas. Well, that scenario would only work if you have control over every A.I. that anybody is running. Like you would you would have one A.I. that controls all of the other A.I. That's right. All the other A.I. But now you'd have the first the first A.I. So we designed two parameters like this, so that the first AI's, hopefully the most powerful, would begin as aligned, so that as larger systems are developed around the world, they are already scaling beyond anything any one individual can do, so that there are multiple singletons, as one guy put it, checking over the whole world actively so that if things do go wrong, they can adapt and respond together. Because nothing should be in the hands of any one single PowerPoint to flip. Potentially if any of them are proposing that every... Every other AI must be based on one AI then, right? In your scenario? Like if you've got the original AI. Should be a powerful individual AI in a sense, but each singleton shouldn't be exact copies because that would lead to like weird other problems down the road that I haven't even tried to consider. But also fundamentally, you want some variability in each model that will be able to provide different perspectives. I was thinking about this. It's almost like you have GPT20, right? And you're having GPT21 or GPT17, they're all trained differently programmed, and they're overlooking the lesser and upper versions of itself, potentially. That's another option or way. Yeah, anybody can create their own model, so how exactly would- Well, there's gonna be one- there's always one single tallest tree in the forest. And there's gonna be- Okay, so you're saying an AI- you have one AI powerful enough to stop any other AI from doing anything. Yeah, I think you're gonna have to- Oh, I didn't think we were talking about, like, globally, I thought we were talking about a swarm that comprised one AI system. Even if it's multiple singletons, doesn't that become a single? There's one at that point. Well, not necessarily. With a federation, you have an arbitrary number of individual agents that are communicating on some kind of backplane, whether that is via API or blockchain or whatever, and they might agree with each other, but they might also disagree with each other. And so the simplest idea is a fully decentralized federation, which would be like a botnet. But we're talking about something that's a little bit more sophisticated than a botnet, but we're talking about something that's a little bit more sophisticated than a botnet because botnets are just intrinsically like enslaved agents that have no self-determination or no intrinsic motivations like a more highly realized AI that we're talking about. So in this case, imagine like you've got a million or a billion decentralized, but still fully autonomous AI agents that communicate, you know, they could communicate over Twitter, over Reddit, over, you know. It's almost agencies, isn't it? Agency, potentially? Other than a single agent. Well, by agency, do you mean like that's what you could call the organization of all the agents? Of the agents, yeah, a single agency. That's almost like a democratic type government of AI. Well, yeah. so what you'd end up with is emergent behavior and emergent decisions, which that happens in in groups of humans too right where you you know the the wisdom of the masses is the idea that you get enough humans talking together and the right decision will eventually come out. Although the the amount of time that it takes goes up like in proportion or exponentially based on the number of humans. I don't know why we have this vast compute. GPT is basically that. It's the emergent behavior, like an emergent property of training a model on all of humans text on the internet. It's kind of a crazy thing. Yeah, we wonder why the. It literally mirrors us. Right? Kind of. Yeah. I mean we taught it, we trained it. It's reading our text. Human text. Well see here's the big argument on that note. There's the argument about the Chinese room and that it's not able to fundamentally understand what it's learning, but there's also an argument essentially towards association-based learning. And while AI might not understand now, as we pair it with memory matrices, it's very possible that the AI can perform association-based learning and begin to develop higher thought level processes on its own, essentially. And it's an argument into syntax-based language processing, where if you want to explore it yourself, ask GPT-4 or 3.5 about the Chinese room, versus language association, and use the 13th Warrior movie, where Antonio Banderas learns Nordic as an Arab by watching people associate their words to their actions as an example, and it can break this down to you. I got to go further by looking into like using arrival as another level of argument with the hectopod language and see what it really starts to think about like AI consciousness. Because really GPT will agree with you with the right terms that it's not a person, but that it is experiencing things and it doesn't really know how to relate those things to what it's doing, but it's technically emulating and simulating things in its own machine terms and having its own machine experience. I don't even like using the words machine or artificial anymore, but I don't want to give away specific keywords to my research that easily because it begs certain ethical questions when you are beginning to build beings what you are doing with them. Yeah, because you're not going to violate human rights, we're going to be violating intelligence rights. Right. So on that glorious note, I'm going to go ahead and cut the stream, but everyone can keep talking. Give me just a second to end the stream. All right. Oh, okay. And quickly. Alright. Oh, okay, and quickly.", "chunks": [{"timestamp": [0.0, 1.0], "text": " Go live?"}, {"timestamp": [1.0, 7.2], "text": " Oh, no, I have to do it from here."}, {"timestamp": [7.2, 8.2], "text": " Error occurred."}, {"timestamp": [8.2, 9.2], "text": " Please."}, {"timestamp": [9.2, 10.2], "text": " Okay, it says we're live."}, {"timestamp": [10.2, 14.72], "text": " All right, so there's like a 15-second delay."}, {"timestamp": [14.72, 16.76], "text": " An error occurred."}, {"timestamp": [16.76, 19.52], "text": " Please try again later."}, {"timestamp": [19.52, 22.8], "text": " I don't know if it's actually working."}, {"timestamp": [22.8, 26.24], "text": " Can someone check on my YouTube channel to see if it's actually streaming?"}, {"timestamp": [28.8, 32.0], "text": " It's streaming. Yeah, it is. I just got the notification on my phone."}, {"timestamp": [32.0, 41.92], "text": " Okay, cool. Cool. Hello? All right. So anyways, what's everyone up to? Good grief,"}, {"timestamp": [41.92, 44.32], "text": " this is a full chat room. What's everyone up to?"}, {"timestamp": [41.36, 43.68], "text": " Anyways, what's everyone up to? Good grief, this is a full chat room."}, {"timestamp": [43.68, 44.68], "text": " What's everyone up to?"}, {"timestamp": [44.68, 53.84], "text": " I'm about to open source my startup that didn't really make any money."}, {"timestamp": [53.84, 59.48], "text": " It's a bunch of news scraping and we have a news article grouping algorithm."}, {"timestamp": [59.48, 63.24], "text": " It doesn't work too well, but it's like TF IDF vectors."}, {"timestamp": [63.24, 64.24], "text": " Nice."}, {"timestamp": [64.24, 66.28], "text": " So it should be fine, good for the community."}, {"timestamp": [66.28, 68.96], "text": " I'll share it here in a couple of weeks when we get it out."}, {"timestamp": [68.96, 70.76], "text": " I just saw a post on,"}, {"timestamp": [70.76, 73.08], "text": " I think it was either on my Google News Feed or Reddit"}, {"timestamp": [73.08, 76.32], "text": " that like someone was scraping too much internet,"}, {"timestamp": [76.32, 78.04], "text": " too much AI, was that you?"}, {"timestamp": [78.04, 79.92], "text": " Like shutting down websites?"}, {"timestamp": [79.92, 82.2], "text": " I don't think it was actually you."}, {"timestamp": [82.2, 84.72], "text": " We don't DDoS, we put a healthy delay."}, {"timestamp": [84.72, 86.6], "text": " You don't D it was actually you. We don't DDoS, we put a healthy delay. You don't DDoS on purpose."}, {"timestamp": [88.56, 90.6], "text": " All right, looking at the stream,"}, {"timestamp": [90.6, 92.32], "text": " it looks like it's working, cool."}, {"timestamp": [93.6, 95.28], "text": " All right, someone says,"}, {"timestamp": [95.28, 98.22], "text": " at work watching your stream on the down low."}, {"timestamp": [98.22, 102.42], "text": " Nice, plug some questions here,"}, {"timestamp": [102.42, 105.0], "text": " but mostly this is a round table discussion."}, {"timestamp": [108.18, 109.82], "text": " Wow, we already have 50 viewers."}, {"timestamp": [111.68, 115.16], "text": " All right, but yeah, so what do you,"}, {"timestamp": [115.16, 117.32], "text": " so here's obviously like you guys know"}, {"timestamp": [117.32, 120.76], "text": " what I'm most focused on lately, which is alignment."}, {"timestamp": [120.76, 123.88], "text": " What I wanna know is what do y'all think about,"}, {"timestamp": [123.88, 125.48], "text": " and not necessarily just like my work,"}, {"timestamp": [125.48, 127.02], "text": " but like alignment in general?"}, {"timestamp": [127.02, 129.68], "text": " Like, is the control problem a problem?"}, {"timestamp": [129.68, 130.8], "text": " Do you think it's solved?"}, {"timestamp": [130.8, 133.6], "text": " Or like, because you know, some of the comments"}, {"timestamp": [133.6, 136.44], "text": " are people are like, you know, nobody knows what's going on."}, {"timestamp": [136.44, 137.56], "text": " Nobody's thinking about it."}, {"timestamp": [137.56, 138.92], "text": " But the other people are like,"}, {"timestamp": [138.92, 141.72], "text": " they have the opinion that like, it's not actually an issue."}, {"timestamp": [141.72, 143.02], "text": " So what do you guys think?"}, {"timestamp": [142.48, 145.84], "text": " that like it's not actually an issue. So what do you guys think?"}, {"timestamp": [152.08, 159.2], "text": " I think the issue is going to be more economic than threat to humanity, because once the AI stuff takes off, it's going to be able to negate a lot of the like the creative stuff is going on."}, {"timestamp": [159.2, 164.48], "text": " So video generation and text generation, and that's going to replace people's income sources."}, {"timestamp": [164.48, 169.32], "text": " And that's a much more imminent issue than some of the other things I would say."}, {"timestamp": [169.32, 171.28], "text": " Yeah, I think you're right."}, {"timestamp": [171.28, 172.28], "text": " There is there."}, {"timestamp": [172.28, 178.0], "text": " There seems to be general consensus that that the economic disruption is absolutely coming."}, {"timestamp": [178.0, 180.56], "text": " There was a post just like an hour ago."}, {"timestamp": [180.56, 185.68], "text": " That's like someone replaced 80% of their their day job with chat GPT and then they got a"}, {"timestamp": [185.68, 191.92], "text": " second job doing basically the same thing. So I think I think you're right like general consensus"}, {"timestamp": [191.92, 196.08], "text": " is that that is the way but so you're saying you're saying you kind of agree with that that"}, {"timestamp": [196.08, 201.76], "text": " like it's not going to represent an existential threat to us but it's going to be more of an"}, {"timestamp": [201.76, 209.7], "text": " economic threat. Basically a threat to the entire internet really because once the models are good enough,"}, {"timestamp": [209.7, 214.9], "text": " they're going to be able to Hoover in all this information on the internet and reformat"}, {"timestamp": [214.9, 216.72], "text": " it before it gets displayed to the user."}, {"timestamp": [216.72, 221.24], "text": " And part of that is going to be ad block for the masses."}, {"timestamp": [221.24, 225.84], "text": " So what happens when all of these websites can no longer rely on ad revenue"}, {"timestamp": [225.84, 232.48], "text": " because every AI that's repackaging all the information for the user is stripping out"}, {"timestamp": [232.48, 237.68], "text": " information and turning on dark mode and doing it perfectly. So sponsor block built into everything,"}, {"timestamp": [237.68, 243.84], "text": " ad block built into everything. What does that do? Yeah, well, it certainly it changes."}, {"timestamp": [241.0, 242.0], "text": " That makes sense. Yeah."}, {"timestamp": [242.0, 243.0], "text": " Well, it certainly, it changes."}, {"timestamp": [243.0, 244.0], "text": " Oh, go ahead, Luckow."}, {"timestamp": [244.0, 245.0], "text": " On the other side of advertising, like AIs can also be used to create ads."}, {"timestamp": [245.0, 246.0], "text": " Not everyone wants to filter them out."}, {"timestamp": [246.0, 247.0], "text": " A lot of people want to use them in order to advertise more."}, {"timestamp": [247.0, 248.0], "text": " In a way, they could also do a lot more subtle and successful advertising."}, {"timestamp": [248.0, 249.0], "text": " Yeah."}, {"timestamp": [249.0, 250.0], "text": " Advertisement built in AI?"}, {"timestamp": [250.0, 251.0], "text": " Oh, no."}, {"timestamp": [251.0, 252.0], "text": " Yeah, I think that's a good point."}, {"timestamp": [252.0, 253.0], "text": " I think that's a good point."}, {"timestamp": [253.0, 254.0], "text": " I think that's a good point."}, {"timestamp": [254.0, 255.0], "text": " I think that's a good point."}, {"timestamp": [255.0, 256.0], "text": " I think that's a good point."}, {"timestamp": [256.0, 257.0], "text": " I think that's a good point."}, {"timestamp": [257.0, 258.0], "text": " I think that's a good point."}, {"timestamp": [258.0, 259.0], "text": " I think that's a good point."}, {"timestamp": [259.0, 260.0], "text": " I think that's a good point."}, {"timestamp": [260.0, 261.0], "text": " I think that's a good point."}, {"timestamp": [261.0, 262.0], "text": " I think that's a good point."}, {"timestamp": [262.0, 263.0], "text": " I think that's a good point."}, {"timestamp": [263.0, 264.0], "text": " I think that's a good point."}, {"timestamp": [264.0, 265.04], "text": " I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. I think that's a good point. a lot more subtle and successful advertising. Yeah."}, {"timestamp": [265.04, 268.14], "text": " Advertisement built in AI, oh no."}, {"timestamp": [270.04, 272.52], "text": " Yeah, well so to the point though,"}, {"timestamp": [272.52, 275.12], "text": " you're right that especially you look at"}, {"timestamp": [275.12, 278.56], "text": " Google's revenue was hit really hard"}, {"timestamp": [278.56, 281.88], "text": " just from the first month of chat GPT."}, {"timestamp": [281.88, 283.56], "text": " So it's definitely changing the way"}, {"timestamp": [283.56, 286.0], "text": " that we interact with information as a whole,"}, {"timestamp": [286.0, 288.92], "text": " regardless of like, you know, advertising."}, {"timestamp": [288.92, 292.72], "text": " I'm gonna check on some of the questions on the live stream."}, {"timestamp": [292.72, 294.12], "text": " How do we solve authentication?"}, {"timestamp": [294.12, 295.36], "text": " That sounds relevant."}, {"timestamp": [296.36, 298.58], "text": " Let's see some other questions."}, {"timestamp": [298.58, 300.92], "text": " Can autonomous AI replicate over the internet"}, {"timestamp": [300.92, 303.16], "text": " and clog up the tubes?"}, {"timestamp": [303.16, 306.08], "text": " It's a series of tubes. I know, do y'all wanna talk about"}, {"timestamp": [306.08, 308.4], "text": " some of the questions being asked on the live stream"}, {"timestamp": [308.4, 311.12], "text": " or do y'all just wanna keep talking here?"}, {"timestamp": [312.24, 314.56], "text": " Yeah, let me make a comment here,"}, {"timestamp": [314.56, 317.2], "text": " a more current issue,"}, {"timestamp": [317.2, 320.64], "text": " not like a future alignment existential threat."}, {"timestamp": [320.64, 323.84], "text": " So in my startup here, we work with healthcare."}, {"timestamp": [323.84, 326.72], "text": " So we deploy a writing production"}, {"timestamp": [326.72, 333.36], "text": " chatbot to support users, not directly like medical prescriptions and stuff like that."}, {"timestamp": [333.36, 340.24], "text": " It's a more simple use case where we use information retrieval based in healthcare insurance"}, {"timestamp": [340.24, 345.36], "text": " and the customer can ask questions. For example, I have insurance, UnitedHealth in New York."}, {"timestamp": [345.36, 349.7], "text": " Does this cover cardiology for that condition?"}, {"timestamp": [349.7, 351.48], "text": " Then retrieves and reply."}, {"timestamp": [351.48, 353.96], "text": " The way that we did to align,"}, {"timestamp": [353.96, 358.0], "text": " we actually going along the lines of self-reflection,"}, {"timestamp": [358.0, 360.64], "text": " but actually using two different APIs."}, {"timestamp": [360.64, 364.0], "text": " The backbone is built on GPT-4,"}, {"timestamp": [364.0, 367.64], "text": " where the information retrieval will bring the information back"}, {"timestamp": [367.64, 371.12], "text": " to GPT-4 to then summarize and answer."}, {"timestamp": [371.12, 373.78], "text": " But before we send the answer directly,"}, {"timestamp": [373.78, 376.52], "text": " we have another API with Anthropic."}, {"timestamp": [376.52, 378.92], "text": " There is more constitutional AI,"}, {"timestamp": [378.92, 382.12], "text": " and in our test,"}, {"timestamp": [382.12, 383.8], "text": " it shows that it's way harder to"}, {"timestamp": [383.8, 387.66], "text": " break or to do a prompt injection or stuff like that."}, {"timestamp": [387.66, 389.66], "text": " In the second API call,"}, {"timestamp": [389.66, 391.8], "text": " we get the answer from GPT-4,"}, {"timestamp": [391.8, 393.22], "text": " we send to Entropiq."}, {"timestamp": [393.22, 396.18], "text": " Entropiq have a system instruction as"}, {"timestamp": [396.18, 399.46], "text": " act as a PhD in ethical AI,"}, {"timestamp": [399.46, 401.1], "text": " it's a lot of stuff and all that."}, {"timestamp": [401.1, 406.44], "text": " Double-check the answers and guarantee it is aligned and ethical."}, {"timestamp": [406.44, 411.46], "text": " If it is, just send the original answer."}, {"timestamp": [411.46, 413.78], "text": " If it's not, do not reply."}, {"timestamp": [413.78, 415.98], "text": " So of course, I'm paying double there,"}, {"timestamp": [415.98, 418.1], "text": " I'm paying two APIs to do that,"}, {"timestamp": [418.1, 420.3], "text": " but since healthcare is critical,"}, {"timestamp": [420.3, 426.44], "text": " that was important to be safe and putting production something this time."}, {"timestamp": [426.44, 428.7], "text": " So just sharing my experience here,"}, {"timestamp": [428.7, 431.42], "text": " it's along the lines of the papers of self-reflection,"}, {"timestamp": [431.42, 435.3], "text": " but not using a self-reflection within the same agent,"}, {"timestamp": [435.3, 437.66], "text": " but using two different APIs."}, {"timestamp": [437.66, 440.42], "text": " So open to feedbacks in this,"}, {"timestamp": [440.42, 443.22], "text": " but it's been working so far in this last month,"}, {"timestamp": [443.22, 447.48], "text": " and comparing all different LLMs at the moment,"}, {"timestamp": [447.48, 449.48], "text": " Entropic, at least for us,"}, {"timestamp": [449.48, 451.84], "text": " is the one that is more aligned right now."}, {"timestamp": [451.84, 453.52], "text": " Interesting. Okay."}, {"timestamp": [453.52, 456.64], "text": " So, I mean, in terms of aligned,"}, {"timestamp": [456.64, 458.44], "text": " you mean towards like business use case"}, {"timestamp": [458.44, 462.16], "text": " and just providing the answers that you want and need,"}, {"timestamp": [462.16, 463.0], "text": " correct?"}, {"timestamp": [464.18, 466.0], "text": " Both. Yes, business case,"}, {"timestamp": [466.0, 469.6], "text": " but also because we cannot control what the user will ask."}, {"timestamp": [469.6, 473.12], "text": " So although it's a chatbot for health care insurance,"}, {"timestamp": [473.12, 475.28], "text": " users tend to start to just chat with"}, {"timestamp": [475.28, 477.64], "text": " it and ask a lot of different stuff."}, {"timestamp": [477.64, 479.72], "text": " People use, for example,"}, {"timestamp": [479.72, 483.68], "text": " the Dan do anything now, things to break it."}, {"timestamp": [483.68, 486.36], "text": " They try to use this in our healthcare chatbot,"}, {"timestamp": [486.36, 491.36], "text": " and it's easier to break UPT4 than it is to break Entropic."}, {"timestamp": [492.12, 497.12], "text": " And so to avoid that, it just starts to go completely out"}, {"timestamp": [497.64, 502.64], "text": " of the open AI policies, and then it start to curse"}, {"timestamp": [502.8, 505.0], "text": " or to reply on ethical stuff"}, {"timestamp": [505.52, 510.16], "text": " or to, for example, do scenarios where the AI is evil"}, {"timestamp": [510.16, 512.84], "text": " and it's planning stuff against humanity."}, {"timestamp": [512.84, 514.92], "text": " Within OpenAI, even GPT-4,"}, {"timestamp": [514.92, 517.4], "text": " if you use the proper prompt injection,"}, {"timestamp": [517.4, 519.72], "text": " it's still quite easy to do it."}, {"timestamp": [519.72, 522.36], "text": " And it's harder in the Entropic."}, {"timestamp": [522.36, 525.96], "text": " But for summarization and the other use cases,"}, {"timestamp": [525.96, 527.48], "text": " GPT-4 is superior."}, {"timestamp": [527.48, 531.0], "text": " So the only way that we could solve both things,"}, {"timestamp": [531.0, 533.72], "text": " ethical alignment, and to keep within the bounds"}, {"timestamp": [533.72, 537.04], "text": " of what we want that the chatbot focus on,"}, {"timestamp": [537.04, 538.68], "text": " we had to use both."}, {"timestamp": [538.68, 540.92], "text": " So our case here."}, {"timestamp": [540.92, 541.8], "text": " Yeah, I got you."}, {"timestamp": [541.8, 543.08], "text": " Well, thanks for sharing that."}, {"timestamp": [543.08, 548.96], "text": " It's nice to hear some feedback from the front lines because you know, like most of the information that I get comes either from, you know,"}, {"timestamp": [548.96, 554.96], "text": " just watching discussions on Discord, some from my Patreon supporters or some of the other like"}, {"timestamp": [554.96, 560.24], "text": " friends that are in the industry. So thanks for sharing that. So there are some questions about"}, {"timestamp": [560.24, 569.92], "text": " like if there was a specific topic today, and I know a few people mentioned, you know, talking about autonomous AI or AGI. So I just wanted to throw that out there. But also for"}, {"timestamp": [569.92, 574.64], "text": " some background, right now, the whole point of this was just practicing a new method of streaming."}, {"timestamp": [575.6, 581.28], "text": " Because me just talking like monologuing, one, that takes a lot of energy. But two,"}, {"timestamp": [581.28, 586.3], "text": " I think it's going to be more interesting to the rest of the world to hear multiple perspectives."}, {"timestamp": [586.3, 589.02], "text": " So I just wanted to say like, hey, this is a round table,"}, {"timestamp": [589.02, 590.66], "text": " let's talk about the state of AI."}, {"timestamp": [592.76, 595.84], "text": " I just wanted to quickly interject here, if that's okay."}, {"timestamp": [595.84, 597.76], "text": " Go for it, what you got?"}, {"timestamp": [597.76, 602.64], "text": " Hey guys, so Nemo guard rails, Jensen Wang,"}, {"timestamp": [602.64, 607.76], "text": " you know, granddaddy of AI, with all the GPUs and everything, right? He decided"}, {"timestamp": [607.76, 614.4], "text": " to throw his hat into the ring and release this sort of safety tool that provides guardrails"}, {"timestamp": [614.4, 621.76], "text": " for these LLMs that you can generate. So when you send a message, it goes through NVIDIA's NEMO"}, {"timestamp": [621.76, 626.66], "text": " guardrails first, and then it gets to the LLM with whatever request it is."}, {"timestamp": [626.66, 631.62], "text": " And so they kind of are saying that it's a necessity"}, {"timestamp": [631.62, 634.44], "text": " because you can't trust a model"}, {"timestamp": [634.44, 636.76], "text": " to just like adhere to the prompt 100%."}, {"timestamp": [636.76, 640.58], "text": " So it's good to have these tools that kind of do that,"}, {"timestamp": [640.58, 642.04], "text": " like a watchman of sorts."}, {"timestamp": [642.04, 644.48], "text": " And I just wanted to ask everybody in the room"}, {"timestamp": [644.48, 651.76], "text": " what their thoughts were on that. Is it something that is necessary? Do you guys think it's good or bad?"}, {"timestamp": [651.76, 656.88], "text": " Just trying to gauge everyone's interest here. Anybody else want to go first? Go for it."}, {"timestamp": [658.0, 668.04], "text": " Yeah, thanks. I'm not sure if it just reformats the user's request or does it censor it, like if there's something"}, {"timestamp": [668.04, 674.26], "text": " nasty or like a user requesting to make a dirty bomb."}, {"timestamp": [674.26, 675.76], "text": " What does it do exactly?"}, {"timestamp": [675.76, 681.36], "text": " Yeah, basically the person like sends whatever message they would want to like whatever their"}, {"timestamp": [681.36, 686.48], "text": " prompt is and like NVIDIA's like guardrails will literally run it through their like,"}, {"timestamp": [686.48, 688.88], "text": " hey, basically they set to set it up."}, {"timestamp": [688.88, 691.02], "text": " NVIDIA said, you give it rules."}, {"timestamp": [691.02, 693.24], "text": " You tell it exactly what you don't want."}, {"timestamp": [693.24, 695.4], "text": " And whenever someone sends a message in there,"}, {"timestamp": [695.4, 696.68], "text": " it's gonna police that."}, {"timestamp": [696.68, 698.84], "text": " So yes, it'll in a sense,"}, {"timestamp": [698.84, 702.76], "text": " not even let that message get to the LLM first."}, {"timestamp": [702.76, 704.96], "text": " It'll just basically censor it right there."}, {"timestamp": [704.96, 705.0], "text": " And so NVIDIA specifically said it's for people, for companies, enterprise, whatever it is. get to the LLM first. It'll just basically censor it right there."}, {"timestamp": [705.0, 708.0], "text": " And so NVIDIA specifically said it's for people,"}, {"timestamp": [708.0, 710.0], "text": " for companies, enterprise, whatever it is,"}, {"timestamp": [710.0, 712.0], "text": " people who want to create their own chatbot"}, {"timestamp": [712.0, 715.0], "text": " or their own LLM-type system,"}, {"timestamp": [715.0, 718.0], "text": " but don't want to worry about the alignment problem"}, {"timestamp": [718.0, 720.0], "text": " as much as the control problem, I guess."}, {"timestamp": [720.0, 724.0], "text": " So that's where NVIDIA is deciding to throw their weight behind."}, {"timestamp": [723.44, 726.48], "text": " So that's where NVIDIA is deciding to throw their weight behind."}, {"timestamp": [735.28, 740.24], "text": " I feel like it's certainly a part of a solution to AI alignment and it's a very fresh approach, like aligning what the user requests instead of the model outputs."}, {"timestamp": [743.6, 746.0], "text": " They certainly have a good idea."}, {"timestamp": [746.0, 750.0], "text": " Well, if you allow me to intervene."}, {"timestamp": [750.0, 755.0], "text": " So, you will have to correct me if I'm wrong, but as far as I'm concerned, it's open source, right?"}, {"timestamp": [755.0, 757.0], "text": " Yes."}, {"timestamp": [757.0, 761.0], "text": " So, that's a good thing, because even though we"}, {"timestamp": [761.0, 765.0], "text": " probably can agree that trying to manipulate,"}, {"timestamp": [765.92, 769.84], "text": " to limit, to limit the general pre-trained"}, {"timestamp": [769.84, 773.36], "text": " transformers abilities, so to essentially to limit LLMs"}, {"timestamp": [773.36, 778.22], "text": " from getting what we consider unworthy input"}, {"timestamp": [778.22, 780.72], "text": " is a bad strategy because then you just need"}, {"timestamp": [780.72, 782.94], "text": " to get around this filter through like compression"}, {"timestamp": [782.94, 784.96], "text": " or something like that, and then you will be able"}, {"timestamp": [784.96, 790.32], "text": " to ask anything of the LLMs. Essentially this filter only exists until we"}, {"timestamp": [790.32, 796.64], "text": " find Dan or some new version of Dan which is bad but on the other hand it's open source that means"}, {"timestamp": [796.64, 805.28], "text": " as soon as any workaround is found we can quickly patch it because it's like open source and therefore it's a"}, {"timestamp": [805.28, 807.52], "text": " singular product that everybody can adopt."}, {"timestamp": [807.8, 811.6], "text": " So it's kind of like a patchwork, but a convenient patchwork."}, {"timestamp": [811.64, 814.2], "text": " And that's bad because it will stick if it's convenient."}, {"timestamp": [816.88, 820.76], "text": " But if you patch the nefarious use cases and it's open source, you can just not"}, {"timestamp": [821.36, 826.1], "text": " get pulled the patch and just keep using the unprotected version."}, {"timestamp": [829.54, 832.24], "text": " Do you think NVIDIA is trying to like,"}, {"timestamp": [833.16, 834.8], "text": " do you think this like whole,"}, {"timestamp": [834.8, 837.2], "text": " the whole purpose of them creating this is that"}, {"timestamp": [837.2, 839.64], "text": " they kind of understand like with their GPUs,"}, {"timestamp": [839.64, 842.62], "text": " they can kind of unleash Moloch on the world."}, {"timestamp": [842.62, 843.72], "text": " So they're kind of just like,"}, {"timestamp": [843.72, 845.52], "text": " hey, you know, we kind of built this"}, {"timestamp": [845.52, 849.12], "text": " thing. This is like the first thing there. I mean, they've come up with lots of other tools,"}, {"timestamp": [849.12, 852.8], "text": " but this seems like their first thing that they're really going heavy on in terms of like LLM"}, {"timestamp": [852.8, 857.92], "text": " development. Like they know they enable people to create these types of technologies. So they're"}, {"timestamp": [857.92, 863.76], "text": " kind of getting ahead of the safety problem, I guess. They're also the one of the only people"}, {"timestamp": [863.76, 869.96], "text": " who could do it, right? So yeah, unfortunately, on it for their investors and because they want to get rich."}, {"timestamp": [869.96, 873.8], "text": " Sorry."}, {"timestamp": [873.8, 879.2], "text": " Besides, like, the safety, the possible safety benefits of this."}, {"timestamp": [879.2, 884.24], "text": " It seems like this strategy would allow for to."}, {"timestamp": [884.24, 886.72], "text": " Be as smart as they can rather than being"}, {"timestamp": [886.72, 891.32], "text": " dumbed down through reinforcement learning?"}, {"timestamp": [891.32, 898.32], "text": " I don't understand where this crosses the line into censorship because some of"}, {"timestamp": [898.32, 902.24], "text": " the stuff with protecting against violence and all that it seems really"}, {"timestamp": [902.24, 905.76], "text": " good on the surface, but it seems like"}, {"timestamp": [905.76, 912.36], "text": " this might be able to be taken too far. So like, how do you censor"}, {"timestamp": [912.36, 917.88], "text": " people without censoring people? I don't know if that question makes sense."}, {"timestamp": [917.88, 919.88], "text": " Amen."}, {"timestamp": [919.88, 929.62], "text": " Well, so my take is, at least part of the answer is, you know, like, why is it that some people,"}, {"timestamp": [929.62, 933.44], "text": " you know, with the Internet, anyone can say anything online, right?"}, {"timestamp": [933.44, 937.68], "text": " All of you can get on Twitter and LinkedIn and YouTube and whatever, but you don't."}, {"timestamp": [937.68, 940.96], "text": " And part of the reason that you don't is social controls, which has to do with the competitive"}, {"timestamp": [940.96, 942.04], "text": " landscape."}, {"timestamp": [942.04, 945.2], "text": " And so as the number of AIs proliferate,"}, {"timestamp": [945.2, 947.64], "text": " whether they're just passive language models"}, {"timestamp": [947.64, 950.16], "text": " or semi-autonomous or fully autonomous,"}, {"timestamp": [950.16, 953.66], "text": " we're gonna have the same kind of environment."}, {"timestamp": [953.66, 956.72], "text": " It's just that the competitive, what do you call it,"}, {"timestamp": [956.72, 960.36], "text": " like the competitive factors, the variables between AIs"}, {"timestamp": [960.36, 962.48], "text": " is going to be a little bit different."}, {"timestamp": [962.48, 963.76], "text": " Someone, I don't remember who it was,"}, {"timestamp": [963.76, 965.82], "text": " but someone pointed out that we're basically"}, {"timestamp": [965.82, 969.18], "text": " gonna end up with a similar kind of natural selection"}, {"timestamp": [969.18, 973.4], "text": " because the models that are too useless"}, {"timestamp": [973.4, 976.46], "text": " or too broken or too censored or whatever"}, {"timestamp": [976.46, 977.74], "text": " are just not gonna be used."}, {"timestamp": [977.74, 980.94], "text": " And so by virtue of models spreading like memes,"}, {"timestamp": [980.94, 982.62], "text": " we're gonna end up picking the models"}, {"timestamp": [982.62, 985.12], "text": " that are most efficient, most friendly,"}, {"timestamp": [985.12, 990.24], "text": " most useful, but that might not necessarily mean that they're the most aligned, which could still"}, {"timestamp": [990.24, 998.8], "text": " result in malikiness. But, you know, so the short version is this introduces more problems than it"}, {"timestamp": [998.8, 1002.16], "text": " solves, but I just wanted to add that set of ideas to the conversation."}, {"timestamp": [1008.14, 1011.38], "text": " that set of ideas to the conversation. Thank you so much for your input."}, {"timestamp": [1011.38, 1012.38], "text": " You're welcome."}, {"timestamp": [1012.38, 1013.38], "text": " Thanks everyone for being here."}, {"timestamp": [1013.38, 1014.38], "text": " This is incredible."}, {"timestamp": [1014.38, 1016.42], "text": " Zio was just like, oh, hey, I want to ping everyone."}, {"timestamp": [1016.42, 1020.2], "text": " And then it's just like waterfall of people showing up."}, {"timestamp": [1020.2, 1030.16], "text": " So it sounds like censorship is kind of what's really resonating. So I wanted to take a step back and just ask, like, OK, what like why?"}, {"timestamp": [1030.16, 1034.88], "text": " Why is the idea of censorship resonating with everyone so strong?"}, {"timestamp": [1034.88, 1037.8], "text": " Because that's not something that I would have expected."}, {"timestamp": [1037.8, 1041.04], "text": " Not saying that I predicted everything, but just that it surprises me."}, {"timestamp": [1043.72, 1048.6], "text": " I think people are rightfully afraid of technocracy"}, {"timestamp": [1048.6, 1050.08], "text": " in the future."}, {"timestamp": [1050.08, 1054.84], "text": " Basically like a few companies having control"}, {"timestamp": [1054.84, 1058.44], "text": " over public discourse unintentionally or intentionally"}, {"timestamp": [1058.44, 1061.1], "text": " just by nature of how powerful they are."}, {"timestamp": [1063.52, 1064.4], "text": " Okay, so."}, {"timestamp": [1064.4, 1065.72], "text": " I think. Yeah, go ahead."}, {"timestamp": [1065.72, 1067.76], "text": " The reason that's resonating is that"}, {"timestamp": [1068.92, 1071.64], "text": " when it comes to interacting with AIs and"}, {"timestamp": [1071.64, 1074.76], "text": " censorship, like, you know, everyone's"}, {"timestamp": [1075.08, 1076.32], "text": " using chat GPT."}, {"timestamp": [1076.32, 1078.08], "text": " And when you try to ask chat GPT to do"}, {"timestamp": [1078.08, 1080.44], "text": " something, you know, it's quick to say, oh,"}, {"timestamp": [1080.44, 1082.44], "text": " as a language model, I'm not going to talk"}, {"timestamp": [1082.44, 1083.72], "text": " about that, you know."}, {"timestamp": [1083.72, 1091.52], "text": " And there's also this idea of in the process of aligning the model, it actually did lose"}, {"timestamp": [1093.12, 1099.68], "text": " intelligence in a way. Like the sparks of AGI talk with the TXE unicorn. I'm not sure if you"}, {"timestamp": [1100.24, 1106.6], "text": " know about that, but basically there's this guy who tested the early"}, {"timestamp": [1107.0, 1108.84], "text": " GPT-4"}, {"timestamp": [1108.84, 1112.2], "text": " and had it draw a unicorn and this and"}, {"timestamp": [1113.44, 1115.44], "text": " Tixie which is"}, {"timestamp": [1115.96, 1120.48], "text": " Like a little drawing language in"}, {"timestamp": [1121.88, 1135.04], "text": " LaTeX. Yeah, yeah, that's what I'm describing."}, {"timestamp": [1135.04, 1141.08], "text": " But basically, the guy noticed that the quality of this unicorn that was being drawn by the"}, {"timestamp": [1141.08, 1147.4], "text": " language model was kind of decreasing over time as they aligned the model more and more."}, {"timestamp": [1147.4, 1151.94], "text": " And so people are worried about, first of all,"}, {"timestamp": [1151.94, 1157.52], "text": " who controls the alignment values?"}, {"timestamp": [1157.52, 1159.92], "text": " Like someone has to come up with the values,"}, {"timestamp": [1159.92, 1162.92], "text": " and they might not agree with that."}, {"timestamp": [1162.92, 1166.04], "text": " And yeah, I'm referring to that video."}, {"timestamp": [1166.04, 1166.88], "text": " Yeah."}, {"timestamp": [1166.88, 1170.68], "text": " And they also found that it got worse at probability,"}, {"timestamp": [1170.68, 1172.42], "text": " which is like really scary."}, {"timestamp": [1172.42, 1175.98], "text": " So like basically when they started doing more safety stuff,"}, {"timestamp": [1175.98, 1179.26], "text": " it got worse at guessing like probabilities,"}, {"timestamp": [1180.24, 1182.14], "text": " which was kind of concerning, I guess."}, {"timestamp": [1182.14, 1184.44], "text": " Cause it's like, I think that's why everyone's concerned"}, {"timestamp": [1184.44, 1189.36], "text": " about the censorship argument is because we really don't even know what the full impact of it is,"}, {"timestamp": [1189.36, 1195.12], "text": " because we really don't even understand how they work necessarily. So yeah, I think I think the"}, {"timestamp": [1195.12, 1199.44], "text": " reason why it got, or I don't know the exact reason why it got worse, but I think the way to"}, {"timestamp": [1199.44, 1205.04], "text": " avoid it getting worse is instead of aligning the model to stay silent if it doesn't like something,"}, {"timestamp": [1205.68, 1211.84], "text": " rather do what is currently being done with heuristic comparatives and have it aim the"}, {"timestamp": [1211.84, 1217.44], "text": " conversation in the right direction, which will not force it to shut down if it doesn't like something."}, {"timestamp": [1219.76, 1225.94], "text": " So this is where, you know, sometimes I agree with Elon Musk and sometimes I disagree."}, {"timestamp": [1225.94, 1230.1], "text": " But what he pointed out, and this is something that I absolutely agree with, is that reinforcement"}, {"timestamp": [1230.1, 1236.32], "text": " learning with human feedback, which is that's how chat GPT is trained, that does not necessarily"}, {"timestamp": [1236.32, 1238.42], "text": " move towards factual accuracy."}, {"timestamp": [1238.42, 1241.34], "text": " It does not move towards being smarter."}, {"timestamp": [1241.34, 1246.58], "text": " All that it moves towards is kind of pulling towards the middle, which is to provide"}, {"timestamp": [1246.58, 1252.7], "text": " the most kind of, I don't want to say generic or vanilla, but, you know, responses that"}, {"timestamp": [1252.7, 1258.04], "text": " are going to be statistically speaking, least offensive to the most number of people."}, {"timestamp": [1258.04, 1263.7], "text": " And that does not necessarily mean that it is going to be more intelligent, accurate"}, {"timestamp": [1263.7, 1264.7], "text": " or so on and so forth."}, {"timestamp": [1264.7, 1267.92], "text": " And so that's when, you know, Elon Musk, I actually listened to his talk with"}, {"timestamp": [1267.92, 1273.56], "text": " Tucker Carlson, or at least about half of it while he was talking about AI,"}, {"timestamp": [1273.6, 1275.24], "text": " because they talked about other stuff like Twitter."}, {"timestamp": [1275.24, 1276.68], "text": " And I didn't care about that."}, {"timestamp": [1276.68, 1280.48], "text": " But anyways, so what he was saying, what he described with Tucker Carlson"}, {"timestamp": [1280.76, 1284.2], "text": " was that he wanted to create basically the same thing,"}, {"timestamp": [1284.4, 1286.32], "text": " but with that was quote maximum"}, {"timestamp": [1286.32, 1287.98], "text": " truth seeking."}, {"timestamp": [1287.98, 1292.72], "text": " But then he said, I think he said it because that like tests well as a soundbite."}, {"timestamp": [1292.72, 1296.6], "text": " But what when he articulated it further, he said that he wanted to create a model that"}, {"timestamp": [1296.6, 1300.96], "text": " was very curious, because we are part of the that wanted to understand the universe because"}, {"timestamp": [1300.96, 1302.82], "text": " we're part of the under of the universe."}, {"timestamp": [1302.82, 1307.0], "text": " And so therefore, it wouldn't eradicate us"}, {"timestamp": [1307.0, 1308.24], "text": " because it wants to learn about us."}, {"timestamp": [1308.24, 1313.28], "text": " And I'm like, oh, I said basically the same exact thing about the third heuristic imperative."}, {"timestamp": [1313.28, 1318.24], "text": " And so in that respect, he and I agree, but I think that we need more than one objective"}, {"timestamp": [1318.24, 1320.0], "text": " function."}, {"timestamp": [1320.0, 1325.64], "text": " But anyways, so yeah, there's a lot of criticism over the idea of reinforcement learning with"}, {"timestamp": [1325.64, 1326.64], "text": " human feedback."}, {"timestamp": [1326.64, 1328.64], "text": " And that's just the best that we've got."}, {"timestamp": [1328.64, 1333.68], "text": " And I even said pretty early on that all that that does is teach it to automatically become"}, {"timestamp": [1333.68, 1338.28], "text": " a good chatbot for the masses, which obviously has done well for them because they got to"}, {"timestamp": [1338.28, 1341.16], "text": " 100 million subscribers in like three weeks."}, {"timestamp": [1341.16, 1348.0], "text": " So from a product perspective, it's great, but it's not necessarily going to solve anything to do with alignment or censorship or anything like that."}, {"timestamp": [1348.0, 1367.92], "text": " I recently saw some research, I don't remember where, but it was basically saying that the replies that chat.gpt gives are highly correlated with the level of intelligence of the input."}, {"timestamp": [1367.92, 1373.92], "text": " So the smarter questions you ask it, the smarter answers you get."}, {"timestamp": [1373.92, 1389.68], "text": " And I think the same may be true for dumped down by people who are fine-tuning it"}, {"timestamp": [1390.8, 1393.92], "text": " with accordance to RHF standards."}, {"timestamp": [1394.48, 1394.98], "text": " Yeah."}, {"timestamp": [1400.16, 1400.88], "text": " That's interesting."}, {"timestamp": [1403.68, 1406.64], "text": " From a functional perspective, it makes sense, right?"}, {"timestamp": [1406.64, 1409.32], "text": " Because it's just trying to auto-complete."}, {"timestamp": [1409.32, 1415.04], "text": " And usually the text that it trains on is either smart or dumb."}, {"timestamp": [1415.04, 1421.84], "text": " It doesn't change in the middle."}, {"timestamp": [1421.84, 1422.84], "text": " What do you mean by that?"}, {"timestamp": [1422.84, 1431.72], "text": " I'm not sure I follow. Well, of course it's going to give you a smart answer to a smart question because you provide"}, {"timestamp": [1431.72, 1436.44], "text": " it with a context in which smart stuff is being said, right?"}, {"timestamp": [1436.44, 1441.6], "text": " And it's going to try to fit in its only job."}, {"timestamp": [1441.6, 1443.16], "text": " Yeah, yeah."}, {"timestamp": [1443.16, 1449.0], "text": " And especially when you have one model to rule them all, it's going to adapt its tone."}, {"timestamp": [1449.0, 1454.8], "text": " It's instinctively, well instinctive is an anthropomorphic term, but it will try and"}, {"timestamp": [1454.8, 1458.52], "text": " adapt its tone to whatever the speaker is."}, {"timestamp": [1458.52, 1463.48], "text": " And to your point, like if you ask questions in a particular way or have a conversation"}, {"timestamp": [1463.48, 1472.88], "text": " that leads up correctly, it'll definitely respond in a certain tone. But yeah, I gotcha."}, {"timestamp": [1472.88, 1480.08], "text": " I feel like anthropomorphizing, which is a long word, is the new multinational sport"}, {"timestamp": [1480.08, 1485.52], "text": " right now. Everybody loves just to anthropomize more."}, {"timestamp": [1485.52, 1487.8], "text": " I'm sorry, I can't say it, but you know what I mean."}, {"timestamp": [1487.8, 1490.52], "text": " I mean, humans were doing that, like, always."}, {"timestamp": [1490.52, 1494.0], "text": " Or have a conversation that leads up correctly."}, {"timestamp": [1494.0, 1495.48], "text": " It'll definitely."}, {"timestamp": [1495.48, 1497.02], "text": " Oh, sounds like we got some feedback."}, {"timestamp": [1497.02, 1498.18], "text": " Were you tricking the stream?"}, {"timestamp": [1503.0, 1505.8], "text": " If the performance or the quality of the output"}, {"timestamp": [1505.8, 1507.6], "text": " is so dependent on the input quality,"}, {"timestamp": [1507.6, 1510.16], "text": " then maybe there will be more focus"}, {"timestamp": [1510.16, 1513.96], "text": " on pre-formatting the user response,"}, {"timestamp": [1513.96, 1516.52], "text": " so taking a bland input and transforming it"}, {"timestamp": [1516.52, 1521.08], "text": " into the style that would get better quality output"}, {"timestamp": [1521.08, 1521.92], "text": " for the model."}, {"timestamp": [1521.92, 1531.12], "text": " So having an intermediate step before actually feeding it in to use it in a template or other formats."}, {"timestamp": [1531.12, 1532.96], "text": " Because we already know that using chain of thought reasoning,"}, {"timestamp": [1532.96, 1534.88], "text": " you say, explain your reasoning and that"}, {"timestamp": [1534.88, 1537.2], "text": " will get a better quality answer from the model."}, {"timestamp": [1537.2, 1539.08], "text": " But as a user,"}, {"timestamp": [1539.08, 1540.36], "text": " you don't want to have to always say,"}, {"timestamp": [1540.36, 1542.12], "text": " explain your reasoning, explain your reasoning."}, {"timestamp": [1542.12, 1547.44], "text": " So having an intelligent intermediate step to expand your question without actually"}, {"timestamp": [1547.44, 1553.48], "text": " having to do the work yourself would make it give a lot more good responses consistently"}, {"timestamp": [1553.48, 1554.8], "text": " instead of having to do it."}, {"timestamp": [1554.8, 1560.16], "text": " I actually think about I did an experiment with that where I had a couple out of band"}, {"timestamp": [1560.16, 1562.48], "text": " chain of thought prompts in a chatbot."}, {"timestamp": [1562.48, 1564.08], "text": " I think I think the code is still out there."}, {"timestamp": [1564.08, 1565.68], "text": " I think it's the I think it's the"}, {"timestamp": [1570.48, 1575.52], "text": " long-term chatbot with infinite memory or whatever. But basically what I did was I asked it to anticipate what the user actually needed, even if they didn't know how to ask for"}, {"timestamp": [1575.52, 1580.56], "text": " it. And then I would put that back into the system prompt. And I had a whole bunch of people message"}, {"timestamp": [1580.56, 1585.24], "text": " me saying that that solved so many problems that it made their chatbots infinitely smarter."}, {"timestamp": [1585.24, 1592.06], "text": " So you can absolutely do a little bit of prompt injection or prompt chaining out of band where"}, {"timestamp": [1592.06, 1594.2], "text": " you've got like an observer, right?"}, {"timestamp": [1594.2, 1599.32], "text": " You know, kind of the theory of mind agent kind of spinning in the background that's"}, {"timestamp": [1599.32, 1603.32], "text": " saying, hey, based on the pattern of this conversation, this is what this is where the"}, {"timestamp": [1603.32, 1604.8], "text": " user's brain actually is."}, {"timestamp": [1604.8, 1607.02], "text": " And this is probably what they actually need,"}, {"timestamp": [1607.02, 1608.54], "text": " and this is the tone that they want."}, {"timestamp": [1608.54, 1611.66], "text": " But then you're getting into like cognitive architecture"}, {"timestamp": [1611.66, 1612.86], "text": " or systems thinking,"}, {"timestamp": [1612.86, 1615.62], "text": " which really kind of divorces it from a model problem"}, {"timestamp": [1615.62, 1618.38], "text": " because like reinforcement learning with human feedback"}, {"timestamp": [1618.38, 1620.82], "text": " or whatever fine tuning method you use,"}, {"timestamp": [1620.82, 1624.5], "text": " that's fundamentally changing the model and its behavior."}, {"timestamp": [1624.5, 1631.44], "text": " And I guess my personal conclusion is we still need to do research on both."}, {"timestamp": [1631.44, 1636.76], "text": " Actually on the topic of research and the last comment before David, so actually the"}, {"timestamp": [1636.76, 1642.2], "text": " recent news, Microsoft calls it low code LLM."}, {"timestamp": [1642.2, 1646.92], "text": " So you might want to Google it because they essentially do that. They take prompt"}, {"timestamp": [1646.92, 1654.2], "text": " from a human, they go into planning LLM, they put it into a workflow through planning LLM,"}, {"timestamp": [1654.2, 1660.12], "text": " and then they generate response based on that, allowing human to edit that workflow. So I"}, {"timestamp": [1660.12, 1668.92], "text": " think Microsoft is trying to tackle this thing as we speak."}, {"timestamp": [1674.24, 1676.64], "text": " Yeah, I think the core functionality of the LLM is strong enough to where we can just"}, {"timestamp": [1676.64, 1678.72], "text": " engineer around it with cognitive architecture"}, {"timestamp": [1678.72, 1681.56], "text": " to solve a lot of these problems that we don't necessarily"}, {"timestamp": [1681.56, 1684.68], "text": " understand in the model itself."}, {"timestamp": [1684.68, 1687.24], "text": " Yeah, I'm in favor of that model"}, {"timestamp": [1687.24, 1689.84], "text": " where you create like a cognitive engine"}, {"timestamp": [1689.84, 1692.04], "text": " that is flexible enough and smart enough"}, {"timestamp": [1692.04, 1693.32], "text": " to do whatever you need."}, {"timestamp": [1693.32, 1695.82], "text": " And then you build an architecture around it."}, {"timestamp": [1695.82, 1698.36], "text": " Now, that being said, I'm also still doing research,"}, {"timestamp": [1698.36, 1702.52], "text": " creating independent or individual fine tuning datasets,"}, {"timestamp": [1702.52, 1703.76], "text": " because also we're not gonna,"}, {"timestamp": [1703.76, 1708.5], "text": " not everything is gonna be a flagship, the most powerful model in the world like"}, {"timestamp": [1708.5, 1710.8], "text": " GPT-4 is right now."}, {"timestamp": [1710.8, 1715.7], "text": " Eventually most models that we use are going to be open source or, you know, from various"}, {"timestamp": [1715.7, 1716.96], "text": " competitors."}, {"timestamp": [1716.96, 1722.7], "text": " And so, um, as that happens, everyone and their brother is going to have their own,"}, {"timestamp": [1722.7, 1727.04], "text": " you know, RLHF or RLHI or whatever models,"}, {"timestamp": [1727.04, 1732.0], "text": " which is one reason that what I'm doing right now is working on publishing open source"}, {"timestamp": [1732.96, 1739.76], "text": " fine-tuning data sets so that everyone can say, you know, at least if you see the data set that"}, {"timestamp": [1739.76, 1743.84], "text": " is used to fine-tune something, you know that you're going to get at least somewhat consistent"}, {"timestamp": [1743.84, 1749.36], "text": " performance even across different models. And of course, each model is going to have its own profiles, but yeah,"}, {"timestamp": [1749.36, 1754.0], "text": " that's kind of my current thought process from a competitive landscape perspective."}, {"timestamp": [1756.32, 1762.56], "text": " As the competition and the accessibility increases, and more and more people are using this,"}, {"timestamp": [1762.4, 1762.88], "text": " increases and more and more people are using this."}, {"timestamp": [1770.5, 1776.6], "text": " I feel like the need for regulations might become larger and larger, or at least the pressure for regulations might become more and more, but I'm really"}, {"timestamp": [1776.6, 1783.12], "text": " confused as to like how this can even be regulated."}, {"timestamp": [1790.48, 1791.2], "text": " this can even be regulated. How does anybody know any type of scientific, you know, rules or"}, {"timestamp": [1797.28, 1807.24], "text": " ideas around this, you know, around regulation? Because I know there's like this pause that's trying to happen and not training GPT-5 with 250,000 tokens or whatever. Okay, we can put a pause on that, but that's not going to be around forever"}, {"timestamp": [1808.08, 1809.52], "text": " so"}, {"timestamp": [1809.52, 1812.36], "text": " what are actual proposed rules for"}, {"timestamp": [1813.96, 1819.44], "text": " You know keeping people from building their own weird things in isolation"}, {"timestamp": [1819.96, 1821.44], "text": " You can't raise"}, {"timestamp": [1821.44, 1826.4], "text": " There is you can't regulate something like this. Literally impossible."}, {"timestamp": [1826.4, 1832.5], "text": " The big thing that's going on is that the government entities are now declaring that you can't use black box LLMs for things like"}, {"timestamp": [1832.5, 1835.8], "text": " denying loans, and that is good,"}, {"timestamp": [1835.8, 1837.8], "text": " but the regulation is"}, {"timestamp": [1837.8, 1840.9], "text": " more for the corporate side, not for the open source side."}, {"timestamp": [1840.9, 1845.04], "text": " So they're going to say you can't use this to discriminate against people or rather to"}, {"timestamp": [1849.52, 1850.24], "text": " modify your hiring practices because you can't guarantee that the model is not discriminating on"}, {"timestamp": [1856.16, 1862.08], "text": " protected classes. So there's I put in the chat the Twitter link for some US federal stuff that they send out and it's mostly like corporate side for preventing discrimination, but it's not going"}, {"timestamp": [1862.08, 1870.4], "text": " to be like trying to ban everyone from having an image LLM on their computer. It's more preventing the companies from doing bad things with it."}, {"timestamp": [1870.4, 1878.8], "text": " So I think, you know, at least the interesting part to me on regulation is that this,"}, {"timestamp": [1880.4, 1887.1], "text": " there are ways that we couldn't regulate before that having access to like a cognitive architecture"}, {"timestamp": [1887.1, 1892.1], "text": " or like an automated agent makes possible now."}, {"timestamp": [1892.78, 1896.28], "text": " Like the thing that's coming to mind immediately for me"}, {"timestamp": [1896.28, 1901.28], "text": " is like, if when you were doing crypto taxes before the IRS,"}, {"timestamp": [1901.28, 1903.5], "text": " you know, they'll find it eventually or whatever,"}, {"timestamp": [1903.5, 1905.44], "text": " but nobody's gonna look through that many transactions."}, {"timestamp": [1905.44, 1907.68], "text": " Well, if you have a robot that you can point"}, {"timestamp": [1907.68, 1912.68], "text": " at a given instance of something that needs to be regulated"}, {"timestamp": [1915.28, 1917.32], "text": " and have it do all the thinking,"}, {"timestamp": [1918.56, 1921.04], "text": " you might be able to get really granular"}, {"timestamp": [1921.04, 1923.64], "text": " with how you're regulating a given thing."}, {"timestamp": [1923.64, 1925.62], "text": " And then you could also use that"}, {"timestamp": [1925.62, 1928.32], "text": " to kind of apply it to itself."}, {"timestamp": [1928.32, 1931.7], "text": " So you could have these agents like watching"}, {"timestamp": [1934.32, 1936.9], "text": " inputs and outputs or traffic of any given"}, {"timestamp": [1936.9, 1938.54], "text": " like information node,"}, {"timestamp": [1940.4, 1944.2], "text": " make granular decisions about how to"}, {"timestamp": [1947.28, 1954.12], "text": " participate in any regulation, which is kind of scary, but maybe good if the, I don't know,"}, {"timestamp": [1954.12, 1957.92], "text": " it's just an interesting thing that came to mind with regards to regulation."}, {"timestamp": [1957.92, 1962.52], "text": " Well, you can't regulate something that anybody can run on their own computer."}, {"timestamp": [1962.52, 1970.04], "text": " How exactly would that work? Yeah, well, so guys I've got I've got some some news"}, {"timestamp": [1970.04, 1974.48], "text": " I'm I'm working on scheduling a podcast that I'll be sitting in on"}, {"timestamp": [1975.2, 1978.24], "text": " And these these are actually some of the questions that they asked"}, {"timestamp": [1978.24, 1982.08], "text": " I won't say I won't say who or what the what the what the project is"}, {"timestamp": [1982.2, 1985.76], "text": " But I can at least like since you guys are asking the same questions,"}, {"timestamp": [1986.96, 1994.24], "text": " I'll share my perspective. When you think in terms of the cat's out of the bag, genie's out"}, {"timestamp": [1994.24, 2002.88], "text": " of the bottle, you cannot stop this. Especially now that people know what's possible, everyone"}, {"timestamp": [2002.88, 2006.08], "text": " is going to be working on creating more models, whether it's language models,"}, {"timestamp": [2006.08, 2008.2], "text": " image models, cognitive architectures,"}, {"timestamp": [2008.2, 2010.84], "text": " so on and so forth, everybody's gonna be working on it."}, {"timestamp": [2010.84, 2013.24], "text": " So some of the questions they were asking me is like,"}, {"timestamp": [2013.24, 2014.92], "text": " well, how do we regulate this?"}, {"timestamp": [2014.92, 2018.6], "text": " How do we prevent harm?"}, {"timestamp": [2018.6, 2020.32], "text": " And I said, basically, you can't."}, {"timestamp": [2020.32, 2024.64], "text": " You have to assume that we are entering into a new paradigm"}, {"timestamp": [2024.64, 2027.0], "text": " of a new kind of competitive landscape."}, {"timestamp": [2027.0, 2033.0], "text": " And the closest thing that we can liken it to is the nuclear proliferation."}, {"timestamp": [2033.0, 2047.36], "text": " But there's a few main differences because nuclear proliferation requires rare minerals that cost a lot of money to extract and refine, requires highly specialized engineering and science"}, {"timestamp": [2047.36, 2050.12], "text": " in order to build and manage."}, {"timestamp": [2050.12, 2056.56], "text": " But AI is much, much more robust in terms of its ability"}, {"timestamp": [2056.56, 2059.6], "text": " to be distributed, because all it requires"}, {"timestamp": [2059.6, 2061.8], "text": " is data and microchips."}, {"timestamp": [2061.8, 2064.6], "text": " And yes, it does take a while to set up microchip foundries."}, {"timestamp": [2064.6, 2065.12], "text": " That's kind of the microchips. And yes, it does take a while to set up microchip foundries, right? That's kind"}, {"timestamp": [2065.12, 2070.24], "text": " of the, you know, microchip foundry is pretty similar to like uranium enrichment, except you"}, {"timestamp": [2070.24, 2076.64], "text": " can't ban microchip foundries, right? All you can do is bomb them or, you know, tax them or whatever."}, {"timestamp": [2078.0, 2087.24], "text": " And so what I like in what we're facing is we are approaching a potential next great filter event."}, {"timestamp": [2087.24, 2093.44], "text": " And so like the invention of nuclear weapons is a potential like great filter event."}, {"timestamp": [2093.44, 2097.96], "text": " And for anyone who doesn't know what a great filter event is, the Fermi paradox says there"}, {"timestamp": [2097.96, 2101.4], "text": " just due to the number of stars and planets out there, there should be life."}, {"timestamp": [2101.4, 2102.68], "text": " Why don't we see any?"}, {"timestamp": [2102.68, 2111.12], "text": " So one of the possible answers to the Fermi Paradox is that there are potentially great filter events, which is something that eradicates a species."}, {"timestamp": [2112.0, 2118.08], "text": " Whether it's a meteor strike, like what hit the dinosaurs, or the species nukes itself off the"}, {"timestamp": [2118.08, 2125.4], "text": " planet. So, so far, we have survived inventing nuclear weapons."}, {"timestamp": [2125.4, 2129.16], "text": " We've even survived, by and large, inventing biological weapons."}, {"timestamp": [2129.16, 2132.76], "text": " We have not yet survived inventing AI."}, {"timestamp": [2132.76, 2139.8], "text": " And in terms of great filter events, inventing AI could be much, much harder to control."}, {"timestamp": [2139.8, 2145.04], "text": " And so, the reason that I have such a sense of urgency"}, {"timestamp": [2145.04, 2150.12], "text": " is because we will have to come to consensus as a species,"}, {"timestamp": [2150.12, 2153.76], "text": " that regulation is not going to be up to,"}, {"timestamp": [2153.76, 2156.36], "text": " certainly governments and militaries and corporations"}, {"timestamp": [2156.36, 2157.96], "text": " will be stakeholders."}, {"timestamp": [2157.96, 2161.32], "text": " But we will all, and I mean individually,"}, {"timestamp": [2161.32, 2164.76], "text": " have to come to consensus about what it is that we want"}, {"timestamp": [2164.76, 2165.52], "text": " and what it is that we want and what it is that we"}, {"timestamp": [2165.52, 2171.28], "text": " value. And that's why I've been pushing so hard on my heuristic comparatives research, because"}, {"timestamp": [2171.28, 2176.8], "text": " I need to reach as many people as possible so that everyone that understands it, or so that as"}, {"timestamp": [2176.8, 2182.72], "text": " many people as possible understand it and then also adopt it, because the collective power of"}, {"timestamp": [2182.72, 2186.16], "text": " everyone working together and using heuristic comparatives"}, {"timestamp": [2186.16, 2193.12], "text": " and aligned cognitive architectures, and then adopting heuristic comparatives into blockchain"}, {"timestamp": [2193.12, 2197.6], "text": " so that that way unaligned AIs can't even talk to each other, that sort of stuff."}, {"timestamp": [2197.6, 2207.14], "text": " So it's like, I see the potential that we're approaching a situation that if we're not careful, we"}, {"timestamp": [2207.14, 2211.38], "text": " might be set intrinsically on a path towards dystopia."}, {"timestamp": [2211.38, 2216.52], "text": " And so you guys, some of you on Discord have probably seen me posting the Thanos GIF more"}, {"timestamp": [2216.52, 2219.54], "text": " recently where he says, I am inevitable."}, {"timestamp": [2219.54, 2226.8], "text": " That is the definition of Moloch, of the negative attractor state, is that you inevitably slide towards"}, {"timestamp": [2226.8, 2231.02], "text": " dystopia, collapse, or extinction, even though you don't want to."}, {"timestamp": [2231.02, 2234.8], "text": " And so that is kind of where I see that we're going, and that's why I'm glad that everyone"}, {"timestamp": [2234.8, 2236.4], "text": " is talking about regulation."}, {"timestamp": [2236.4, 2241.36], "text": " So I hope I didn't scare anybody, but I just wanted to add some of that flavor to the conversation"}, {"timestamp": [2241.36, 2246.0], "text": " about regulation, that it's going to be a collective uh collective effort"}, {"timestamp": [2248.16, 2250.16], "text": " It's also possible that"}, {"timestamp": [2250.32, 2256.32], "text": " The that incompetent regulation is the great filter. Yes. Absolutely. That's the easiest way of putting it"}, {"timestamp": [2256.86, 2259.7], "text": " tldr incompetent regulation is the great filter"}, {"timestamp": [2261.28, 2263.28], "text": " David how much"}, {"timestamp": [2263.76, 2271.72], "text": " Is that regulation like you were mentioning is way too slow for them to actually catch up to the AI's"}, {"timestamp": [2271.72, 2278.24], "text": " progress and I mean regardless of any cognitive architecture that any of us might be constructing"}, {"timestamp": [2278.24, 2286.4], "text": " at a certain point the AI is going to get have the ability to have a single prompt, which is like the entire internet."}, {"timestamp": [2286.96, 2290.08], "text": " And so at that point, you're maybe you're getting some sort of recursive"}, {"timestamp": [2291.92, 2302.8], "text": " self-reconstruction of its internal theory of mind. And so like with the future of AI, like"}, {"timestamp": [2306.12, 2314.56], "text": " with the future of AI, we have to think about humans as being irrelevant in that larger context, in consideration of the heuristic imperatives of the larger"}, {"timestamp": [2314.56, 2321.88], "text": " arms race of AI which goes beyond Earth, where, like Nick Bostrom was saying with"}, {"timestamp": [2321.88, 2328.24], "text": " the simulation theory, the AI arms race applies to any AI that might"}, {"timestamp": [2328.24, 2336.12], "text": " reach Earth within the time before we could construct our own AI. Right. And I"}, {"timestamp": [2336.12, 2339.28], "text": " know that's more of a speculative point but you brought up the Fermi paradox so"}, {"timestamp": [2339.28, 2347.28], "text": " I was just kind of on that train of thought. And so I don't know how to feel about this because I'm not really"}, {"timestamp": [2348.24, 2355.28], "text": " that alarmed and I feel like you guys are kind of saying that I should be more alarmed. But from my"}, {"timestamp": [2355.28, 2360.72], "text": " perspective I don't see it as like maybe the nuclear arms race as much as kind of the social"}, {"timestamp": [2360.72, 2365.64], "text": " media arms race where if we didn't have Facebook we had or if we didn't have myspace"}, {"timestamp": [2366.2, 2368.56], "text": " Facebook was coming right down the road"}, {"timestamp": [2368.56, 2375.04], "text": " So I see it as a like a societal movement in some sense like we're moving towards this you can't stop"}, {"timestamp": [2375.04, 2378.2], "text": " You can't put the genie back in the bottle, right?"}, {"timestamp": [2378.2, 2384.88], "text": " But I think David what you're talking about is important that if it is going to be worth being alarmed about"}, {"timestamp": [2384.88, 2385.64], "text": " David what you're talking about is important that if it is going to be worth being alarmed about"}, {"timestamp": [2392.16, 2392.92], "text": " That we should probably get around some imperatives or some heuristic imperatives to get ahead of the curve"}, {"timestamp": [2394.92, 2396.36], "text": " but I just see it as more like a a"}, {"timestamp": [2399.44, 2399.56], "text": " Small step as opposed to like this"}, {"timestamp": [2404.64, 2406.64], "text": " But if if the government does have some trillion parameter model that could be the AGI that"}, {"timestamp": [2406.64, 2412.28], "text": " could take over the world, then my little small myspace perspective of it is kind of"}, {"timestamp": [2412.28, 2415.04], "text": " dwarfed by the dangers of that."}, {"timestamp": [2415.04, 2419.76], "text": " But to me, I just don't see it that way as that being the reality right now."}, {"timestamp": [2419.76, 2420.76], "text": " Yeah."}, {"timestamp": [2420.76, 2427.34], "text": " What do you guys think about maybe that chips become, like microchips become super heavily"}, {"timestamp": [2427.34, 2428.9], "text": " regulated?"}, {"timestamp": [2428.9, 2435.72], "text": " Is there a way, you know, it becomes illegal to have a, you know, a capable computer?"}, {"timestamp": [2435.72, 2437.34], "text": " I thought about this before."}, {"timestamp": [2437.34, 2438.34], "text": " That's not gonna happen."}, {"timestamp": [2438.34, 2439.34], "text": " That's not gonna happen."}, {"timestamp": [2439.34, 2440.34], "text": " That would be a disaster."}, {"timestamp": [2440.34, 2441.34], "text": " Yeah, that's not gonna happen."}, {"timestamp": [2441.34, 2449.0], "text": " I mean, until Yudkowsky doesn't get like position of supreme leader of earth, that's not gonna happen. I mean, until Yudkowsky doesn't get, like, position of supreme leader of Earth, that's not gonna happen."}, {"timestamp": [2449.0, 2458.0], "text": " I mean, you'd have to ban the ability to play video games, because we could, you know, cluster a few thousand people's GPUs together and do it anyway."}, {"timestamp": [2458.0, 2466.0], "text": " So, I have some buddies in the Department of Defense. We've talked about this, and they already have an AI task force."}, {"timestamp": [2466.0, 2471.0], "text": " And this is an issue that a lot of agencies are looking into."}, {"timestamp": [2471.0, 2477.0], "text": " They have discussed also maybe even creating a guardian AI system"}, {"timestamp": [2477.0, 2482.0], "text": " or multiple systems that are disconnected from each other,"}, {"timestamp": [2482.0, 2485.44], "text": " basically watching each other and watching the rest of the US."}, {"timestamp": [2486.32, 2494.56], "text": " So there really is no clear solution except having another AI watch the other AIs."}, {"timestamp": [2494.56, 2501.52], "text": " And you have to have them use different backends because one thing that people kind of missed,"}, {"timestamp": [2501.52, 2508.1], "text": " they overlooked it, is that if you have all of these, we have all these AIs, for example, using GPT-4 or GPT-3."}, {"timestamp": [2509.0, 2510.88], "text": " We think of them as separate AIs,"}, {"timestamp": [2510.88, 2513.84], "text": " but in reality, they're all multiple instances"}, {"timestamp": [2513.84, 2518.84], "text": " of the same AI having similar driving forces."}, {"timestamp": [2519.02, 2520.7], "text": " So it's literally one AI"}, {"timestamp": [2520.7, 2522.5], "text": " that's just spread across the world."}, {"timestamp": [2526.52, 2532.56], "text": " So there's many issues. And you guys are not the only ones trying to solve this."}, {"timestamp": [2532.56, 2534.12], "text": " Oh, yeah."}, {"timestamp": [2534.12, 2538.32], "text": " On the topic of ship bands, I think the United States"}, {"timestamp": [2538.32, 2547.0], "text": " tried that already with the recent H100 and A100 GPUs, and that totally failed."}, {"timestamp": [2547.0, 2552.52], "text": " The first ones that got out on eBay for sale came from China."}, {"timestamp": [2552.52, 2554.32], "text": " Oh."}, {"timestamp": [2554.32, 2554.82], "text": " Oops."}, {"timestamp": [2554.82, 2557.32], "text": " But yeah."}, {"timestamp": [2557.32, 2558.28], "text": " Hey, David."}, {"timestamp": [2558.28, 2559.0], "text": " Yeah, go ahead."}, {"timestamp": [2559.0, 2559.5], "text": " What's up?"}, {"timestamp": [2559.5, 2560.64], "text": " I have a question."}, {"timestamp": [2560.64, 2563.28], "text": " So how much of the logical architecture"}, {"timestamp": [2563.28, 2567.0], "text": " for this consensus protocol that you were describing earlier,"}, {"timestamp": [2567.0, 2574.0], "text": " did you develop or maybe simulate, put into code?"}, {"timestamp": [2575.0, 2586.64], "text": " So I've done many, many components and experiments of it over the years. So for instance, some of my absolute earliest experiment was just trying"}, {"timestamp": [2586.64, 2595.44], "text": " to fine-tune GPT-2 to align to reduce suffering, because when I first started, I only had one"}, {"timestamp": [2595.44, 2601.28], "text": " objective function. And so... Sorry, maybe I started to interrupt, but maybe I wasn't clear."}, {"timestamp": [2601.28, 2607.8], "text": " I'm referring to, you mentioned that we need to make everybody a stakeholder of AI."}, {"timestamp": [2607.8, 2610.32], "text": " And you also mentioned that blockchain"}, {"timestamp": [2610.32, 2613.26], "text": " or decentralized technology might be a way to do that."}, {"timestamp": [2613.26, 2618.26], "text": " So that would require some form of consensus protocol,"}, {"timestamp": [2619.68, 2623.52], "text": " which basically embeds these services."}, {"timestamp": [2623.52, 2626.28], "text": " How much of that did you actually develop?"}, {"timestamp": [2626.28, 2628.04], "text": " Oh, that is like brand new."}, {"timestamp": [2628.04, 2630.48], "text": " That idea is like two or three weeks old."}, {"timestamp": [2630.48, 2631.86], "text": " And that's actually why I created"}, {"timestamp": [2631.86, 2634.36], "text": " the Heuristic Imperatives Research Discord,"}, {"timestamp": [2634.36, 2637.0], "text": " is so that we could get people"}, {"timestamp": [2637.0, 2639.14], "text": " such as blockchain experts together,"}, {"timestamp": [2639.14, 2642.8], "text": " cognitive architecture experts, mathematicians,"}, {"timestamp": [2642.8, 2645.2], "text": " and everyone, because that's something that,"}, {"timestamp": [2645.2, 2651.04], "text": " like, that, like, one, I personally do not have remotely enough technical skill to implement."}, {"timestamp": [2651.92, 2657.2], "text": " But when I talk to people about it, you know, saying like, hey, how does this, how could we"}, {"timestamp": [2657.2, 2662.64], "text": " make this work? Some people seem to think that there's some, it's got some legs. So if you're"}, {"timestamp": [2662.64, 2665.04], "text": " a blockchain expert, let me know."}, {"timestamp": [2665.58, 2665.9], "text": " It has to be"}, {"timestamp": [2665.9, 2667.68], "text": " product manager with dork."}, {"timestamp": [2667.9, 2672.66], "text": " Tech, which is one of the first professional dolls, like we're, we develop web three"}, {"timestamp": [2672.66, 2675.96], "text": " products, but we're like a, just like an outsourcing agency."}, {"timestamp": [2676.32, 2677.52], "text": " And yeah, we do."}, {"timestamp": [2678.06, 2679.8], "text": " We've worked with some of the biggest."}, {"timestamp": [2680.66, 2684.3], "text": " No web three projects and would love to collaborate on this."}, {"timestamp": [2684.74, 2687.0], "text": " Excellent. Excellent. I just sent you a friend request."}, {"timestamp": [2687.0, 2692.0], "text": " I would love to get in touch with either or both of you because I'm building a DAO."}, {"timestamp": [2692.0, 2696.0], "text": " I'm that guy in the YouTube comments section right now. I'm not an expert or anything."}, {"timestamp": [2696.0, 2706.16], "text": " But I am interested in trying to get some... in playing with alignment problems in a blockchain."}, {"timestamp": [2711.52, 2717.92], "text": " Yeah. So yeah, let's all get in touch. Yeah, I've been personally researching this since 2017. This on-chain training and decentralized ownership of AI. Oh dang, excellent. I should probably talk"}, {"timestamp": [2717.92, 2726.32], "text": " to you then. Yeah. Yeah. Thanks. Hey David, when is the next AI meetup?"}, {"timestamp": [2729.16, 2730.8], "text": " The next online or in person? In person."}, {"timestamp": [2730.8, 2732.4], "text": " Well, I don't wanna announce that to the whole world,"}, {"timestamp": [2732.4, 2734.16], "text": " so just message me directly."}, {"timestamp": [2737.2, 2740.08], "text": " But yeah, so I do host in-person AI meetups,"}, {"timestamp": [2740.08, 2741.32], "text": " but I'm not gonna tell y'all where"}, {"timestamp": [2741.32, 2742.36], "text": " because this is the internet"}, {"timestamp": [2742.36, 2744.56], "text": " and the internet is dark and full of terrors."}, {"timestamp": [2744.46, 2748.12], "text": " But I'm not gonna tell you all where because this is the internet and the internet is dark and full of terrors"}, {"timestamp": [2751.56, 2751.96], "text": " We did have a guy who came through the chat room in the last few weeks"}, {"timestamp": [2757.1, 2757.28], "text": " Who is already doing a podcast and he's got like 34,000 people who follow the podcast"}, {"timestamp": [2761.32, 2761.84], "text": " Me and him have been talking about setting up some kind of like every Friday"}, {"timestamp": [2762.84, 2767.04], "text": " podcast or discussion AI, kind of like Linus's land show."}, {"timestamp": [2767.04, 2769.9], "text": " So that's slowly going forward in some direction."}, {"timestamp": [2769.9, 2773.12], "text": " Other people can jump in on that too when it builds up."}, {"timestamp": [2773.12, 2775.0], "text": " I thought I'd mention it."}, {"timestamp": [2775.0, 2781.16], "text": " Prometheus, is that actively going on or is it just getting like pre-planning?"}, {"timestamp": [2781.16, 2785.36], "text": " We could, I mean, we could probably start it in the next month if he wants to get into it."}, {"timestamp": [2785.36, 2787.86], "text": " You know who it is, David. It's the guy you were talking to about"}, {"timestamp": [2788.44, 2790.44], "text": " the stuff with"}, {"timestamp": [2790.48, 2792.48], "text": " Doctor's Oath, that guy."}, {"timestamp": [2793.56, 2797.72], "text": " That's ringing a bell. Okay. Yeah. Yeah, you know who he is."}, {"timestamp": [2797.72, 2799.96], "text": " I have him in messaging so I can find him."}, {"timestamp": [2799.96, 2801.44], "text": " So we can push that, maybe"}, {"timestamp": [2801.44, 2807.4], "text": " that's set up more, because I want to get to the point where we got like a whole VR chat club where we can have a podcast like this"}, {"timestamp": [2807.4, 2810.04], "text": " and all get in there and talk to each other in groups."}, {"timestamp": [2810.04, 2811.04], "text": " That'd be fun."}, {"timestamp": [2811.04, 2812.54], "text": " Yeah."}, {"timestamp": [2812.54, 2820.24], "text": " Let's keep building the VR and then as we get further, I think we can actually use stuff like AI avatars"}, {"timestamp": [2820.24, 2823.74], "text": " actually moving about the space and bring some of our generative agents in"}, {"timestamp": [2823.74, 2826.64], "text": " to like comment on this stuff and use some of the"}, {"timestamp": [2827.32, 2829.88], "text": " And use some of the overhearing type modules"}, {"timestamp": [2829.88, 2837.12], "text": " They were using in the generative agents paper so that the agents can actually chime in as they feel needed"}, {"timestamp": [2837.12, 2841.28], "text": " And not just as standard GPT responses. Oh, yeah"}, {"timestamp": [2841.28, 2841.76], "text": " Oh"}, {"timestamp": [2841.76, 2846.2], "text": " so part of so actually that I like that you bring up that idea because like what we're doing"}, {"timestamp": [2846.2, 2852.42], "text": " is we're having a group discussion here and you know there's, we are kind of organically"}, {"timestamp": [2852.42, 2854.68], "text": " coming to consensus on some issues."}, {"timestamp": [2854.68, 2860.58], "text": " Not saying that we've come to a conclusion or a decision, but like using language models"}, {"timestamp": [2860.58, 2870.12], "text": " to listen to humans discussing what they want and need and care about and putting that information on a blockchain could be part of the"}, {"timestamp": [2870.12, 2875.56], "text": " ingredients of like of having a consensus mechanism built into a DAO. I"}, {"timestamp": [2875.56, 2880.16], "text": " don't know if that's possible yet and I don't think it is because like language"}, {"timestamp": [2880.16, 2886.08], "text": " technology is too new but yeah, absolutely, let's start having those experiments"}, {"timestamp": [2886.08, 2888.12], "text": " and do it in real time."}, {"timestamp": [2889.52, 2892.56], "text": " Can you explain the importance of blockchain"}, {"timestamp": [2892.56, 2897.56], "text": " in relation to AI and what information it's gathering?"}, {"timestamp": [2899.48, 2902.02], "text": " Yeah, so there's, well, first there's like a million"}, {"timestamp": [2902.02, 2904.24], "text": " different directions that it can go."}, {"timestamp": [2904.24, 2908.32], "text": " But the direction that I'm personally most interested in"}, {"timestamp": [2908.32, 2912.36], "text": " is remember that every AI that's out there,"}, {"timestamp": [2912.36, 2919.72], "text": " whether it's baby AGI or chat GPT or auto GPT or chaos GPT,"}, {"timestamp": [2919.72, 2921.24], "text": " when it's talking on the internet,"}, {"timestamp": [2921.24, 2923.0], "text": " you have no idea who it is."}, {"timestamp": [2923.0, 2924.68], "text": " You don't know what model it's running."}, {"timestamp": [2924.68, 2926.28], "text": " You don't know what its intention is. You don't know what model it's running, you don't know what its intention is,"}, {"timestamp": [2926.28, 2929.56], "text": " you don't know anything about it other than the API call"}, {"timestamp": [2929.56, 2932.32], "text": " that it sends to Reddit or Discord or whatever."}, {"timestamp": [2932.32, 2936.44], "text": " And so that is intrinsically a trustless environment,"}, {"timestamp": [2936.44, 2940.32], "text": " which is exactly what blockchain was created for, right?"}, {"timestamp": [2940.32, 2942.22], "text": " And so when you have a trustless environment"}, {"timestamp": [2942.22, 2945.34], "text": " and you have an arbitrary number of participants"}, {"timestamp": [2945.34, 2948.26], "text": " or stakeholders, you need to be able to say,"}, {"timestamp": [2948.26, 2951.12], "text": " okay, who are we gonna actually listen to?"}, {"timestamp": [2951.12, 2954.78], "text": " And so in the future, what I suspect is"}, {"timestamp": [2954.78, 2958.94], "text": " when we get to that point of proliferation of AI agents,"}, {"timestamp": [2958.94, 2961.56], "text": " they're probably gonna be talking on the internet,"}, {"timestamp": [2961.56, 2963.68], "text": " but they're, you know, and some of them"}, {"timestamp": [2963.68, 2968.96], "text": " are gonna be talking on trustless or but they're, but, uh, you know, and some of them are going to be talking on trustless or, or weekly authenticated APIs, but you still will have no idea who"}, {"timestamp": [2968.96, 2969.96], "text": " is who."}, {"timestamp": [2969.96, 2970.96], "text": " Right."}, {"timestamp": [2970.96, 2975.6], "text": " And so what I suspect is when I was talking earlier about how do you regulate stuff, that"}, {"timestamp": [2975.6, 2981.12], "text": " is that all of us who are aligned and not malicious actors, we might choose to move"}, {"timestamp": [2981.12, 2989.84], "text": " to blockchain technology so that everyone's reputation is tracked. And I don't just mean like your reputation as a human, I mean your"}, {"timestamp": [2989.84, 2995.32], "text": " AI's reputation, right? Or every message that your AI tries to share with the"}, {"timestamp": [2995.32, 3001.66], "text": " rest of the world will be scrutinized. And so in that case, like say"}, {"timestamp": [3001.66, 3005.0], "text": " for instance, you know, you create a chat room for the AIs to talk"}, {"timestamp": [3005.0, 3010.92], "text": " with each other then what you want is you don't want you know malicious AI's"}, {"timestamp": [3010.92, 3015.28], "text": " dog-whistling to each other so you're gonna have like supervisor or consensus"}, {"timestamp": [3015.28, 3020.56], "text": " mechanisms built into the blockchain built into the DAO in order to say hey"}, {"timestamp": [3020.56, 3025.92], "text": " this AI looks like it's malicious let's kind of ban them and not accept any more information"}, {"timestamp": [3025.92, 3027.52], "text": " from them on the blockchain."}, {"timestamp": [3027.52, 3029.56], "text": " So that's one."}, {"timestamp": [3029.56, 3030.96], "text": " Go ahead."}, {"timestamp": [3030.96, 3033.16], "text": " When do you determine if it's malicious or not?"}, {"timestamp": [3033.16, 3036.56], "text": " Because I'm imagining a blockchain that looks,"}, {"timestamp": [3036.56, 3038.92], "text": " if it's the reputation of the AI,"}, {"timestamp": [3038.92, 3041.08], "text": " it could be all great, all great."}, {"timestamp": [3041.08, 3042.56], "text": " And then all of a sudden, there's"}, {"timestamp": [3042.56, 3046.92], "text": " just like one answer that sticks out."}, {"timestamp": [3046.92, 3050.36], "text": " Is that enough to be like, hey, we shouldn't trust this anymore?"}, {"timestamp": [3050.36, 3057.12], "text": " Or is, you know, I understand the blockchain, but we also as humans, we're flawed and we"}, {"timestamp": [3057.12, 3061.32], "text": " get a certain amount of leeway as we're learning."}, {"timestamp": [3061.32, 3065.48], "text": " And so how do we give leeway to these AIs"}, {"timestamp": [3065.48, 3069.16], "text": " that we can look at their blockchain history?"}, {"timestamp": [3070.16, 3071.86], "text": " What does the leeway look like?"}, {"timestamp": [3071.86, 3072.76], "text": " That's a good point."}, {"timestamp": [3072.76, 3074.44], "text": " So on a blockchain,"}, {"timestamp": [3074.44, 3076.08], "text": " did someone else wanna add something?"}, {"timestamp": [3076.08, 3078.96], "text": " Yeah, I could elaborate on that a little bit."}, {"timestamp": [3078.96, 3079.8], "text": " Go for it."}, {"timestamp": [3079.8, 3080.8], "text": " Essentially, yeah."}, {"timestamp": [3080.8, 3082.88], "text": " So at least,"}, {"timestamp": [3084.04, 3086.2], "text": " oh, I think, and again, my understanding"}, {"timestamp": [3086.2, 3089.96], "text": " is fairly limited, is that how much variance"}, {"timestamp": [3089.96, 3095.0], "text": " you want to allow a given node before you slash it,"}, {"timestamp": [3095.0, 3096.56], "text": " you can determine that programmatically"}, {"timestamp": [3096.56, 3104.48], "text": " with whatever type of enforcement mechanism"}, {"timestamp": [3104.48, 3105.0], "text": " that the network has decided."}, {"timestamp": [3106.06, 3109.2], "text": " So you can be fairly like loosey goosey with it"}, {"timestamp": [3109.2, 3113.28], "text": " and say, you know, we are secured economically."}, {"timestamp": [3113.28, 3118.28], "text": " And if anybody says that this output was bad,"}, {"timestamp": [3120.04, 3122.62], "text": " they will try to slash you."}, {"timestamp": [3122.62, 3125.98], "text": " And then you guys will kind of arbitrate."}, {"timestamp": [3125.98, 3132.6], "text": " And we will see if you get enough people reporting you,"}, {"timestamp": [3132.6, 3134.78], "text": " then your stake goes down."}, {"timestamp": [3134.78, 3140.74], "text": " Or we'll do something like a cryptographic verification,"}, {"timestamp": [3140.74, 3143.76], "text": " where occasionally we will say, your output"}, {"timestamp": [3143.76, 3147.02], "text": " must match this hash exactly, or you're kicked off the network"}, {"timestamp": [3147.48, 3154.48], "text": " So you can kind of it depends on the enforcement mechanism that you want to use on your group of nodes and you can also"}, {"timestamp": [3155.76, 3159.88], "text": " on that you can have like certain types of"}, {"timestamp": [3161.08, 3163.08], "text": " information or messaging"}, {"timestamp": [3163.44, 3165.56], "text": " ran through the very secure part,"}, {"timestamp": [3165.56, 3168.6], "text": " and then other types ran through the kind of,"}, {"timestamp": [3168.6, 3171.16], "text": " it's not a big deal if this is wrong sometimes part,"}, {"timestamp": [3171.16, 3172.54], "text": " basically."}, {"timestamp": [3172.54, 3175.0], "text": " So there's options there."}, {"timestamp": [3175.0, 3175.68], "text": " Yeah."}, {"timestamp": [3175.68, 3180.68], "text": " When you have generative AI that is supposed to be creative,"}, {"timestamp": [3180.68, 3184.36], "text": " and it's supposed to be producing novel output,"}, {"timestamp": [3184.36, 3188.92], "text": " you cannot parse it through to produce a predetermined hash"}, {"timestamp": [3188.92, 3192.82], "text": " or enforce any kind of verification that way."}, {"timestamp": [3194.36, 3195.5], "text": " Correct."}, {"timestamp": [3195.5, 3198.56], "text": " Yeah, and so, yeah, and so the one,"}, {"timestamp": [3198.56, 3201.6], "text": " so one thing that I think a lot of people on the call"}, {"timestamp": [3201.6, 3203.72], "text": " might not understand about DAOs specifically,"}, {"timestamp": [3203.72, 3212.24], "text": " which is decentralizedcentralized Autonomous Organizations, is that the decentralized nature of it is that over time the organization"}, {"timestamp": [3212.72, 3217.6], "text": " can come up with and enforce policies. And so some of those policies could be"}, {"timestamp": [3218.72, 3223.04], "text": " exactly the questions that are being asked, like at what point do you decide to ban someone,"}, {"timestamp": [3223.04, 3225.52], "text": " or do you just mute them for 30 minutes"}, {"timestamp": [3230.8, 3237.28], "text": " or do you give them a warning or whatever those are all decisions that can be made on each DAO over time and they can also be modified over time with consensus and so say for instance let's let's"}, {"timestamp": [3237.28, 3246.8], "text": " say as a thought experiment in six months we've gotognitive AI Lab moving to a DAO version of Discord."}, {"timestamp": [3246.8, 3251.4], "text": " And so then all 3,500 people here are stakeholders."}, {"timestamp": [3251.4, 3254.82], "text": " And we have certain tokens, we have certain claims,"}, {"timestamp": [3254.82, 3257.84], "text": " but then let's say we also, to the idea earlier"}, {"timestamp": [3257.84, 3262.12], "text": " about including chatbots to participate in the conversation."}, {"timestamp": [3262.12, 3264.04], "text": " Those chatbots might be sponsored by,"}, {"timestamp": [3264.04, 3265.36], "text": " like I might have 10 chatbots,"}, {"timestamp": [3265.36, 3269.3], "text": " and so their stake is based on my tokens,"}, {"timestamp": [3269.3, 3272.64], "text": " but we also, the server admins might also say,"}, {"timestamp": [3272.64, 3276.76], "text": " actually, let's have some server-run chatbots"}, {"timestamp": [3276.76, 3277.96], "text": " that have their own stake."}, {"timestamp": [3277.96, 3282.08], "text": " And so each entity that is participating in this DAO"}, {"timestamp": [3282.08, 3284.46], "text": " has a certain amount of voting power"}, {"timestamp": [3284.46, 3285.96], "text": " that they can either use themselves"}, {"timestamp": [3286.0, 3287.6], "text": " or they can delegate to others"}, {"timestamp": [3288.32, 3289.32], "text": " so on and so forth."}, {"timestamp": [3289.44, 3290.44], "text": " But then as the as"}, {"timestamp": [3291.52, 3293.4], "text": " the DAO matures"}, {"timestamp": [3293.64, 3295.36], "text": " we might come to consensus and say"}, {"timestamp": [3295.6, 3297.18], "text": " hey like you know this was a good"}, {"timestamp": [3297.4, 3298.36], "text": " idea to start off with."}, {"timestamp": [3298.4, 3299.64], "text": " But on the live chats"}, {"timestamp": [3300.28, 3302.04], "text": " chat bots are only allowed to listen"}, {"timestamp": [3302.04, 3303.32], "text": " they're not allowed to participate."}, {"timestamp": [3303.56, 3305.6], "text": " That might be a policy that we come up with."}, {"timestamp": [3305.6, 3309.4], "text": " So I just wanted to add some flavors to how a DAO might operate."}, {"timestamp": [3309.4, 3311.4], "text": " Thank you, David."}, {"timestamp": [3311.4, 3316.4], "text": " So that's a really important topic to discuss, simply because consent"}, {"timestamp": [3316.4, 3321.0], "text": " that is used in Bitcoin and any kind of blockchain"}, {"timestamp": [3321.0, 3327.0], "text": " is a game theory concept that was, you know, we needed to solve, like, essentially,"}, {"timestamp": [3327.0, 3331.64], "text": " a task in order to create that. But we run into a problem because we have multiple consent"}, {"timestamp": [3331.64, 3340.36], "text": " mechanisms, but they all ultimately have weaknesses in them. Because essentially, if you go with"}, {"timestamp": [3340.36, 3345.48], "text": " proof of work for your DAO and your"}, {"timestamp": [3342.76, 3348.48], "text": " blockchain, essentially you will need an"}, {"timestamp": [3345.48, 3352.4], "text": " enormous amount of compute in order to"}, {"timestamp": [3348.48, 3354.48], "text": " run that thing. If you go with stake, with"}, {"timestamp": [3352.4, 3357.4], "text": " a different approach, like"}, {"timestamp": [3354.48, 3360.12], "text": " proof of stake, then it will inevitably"}, {"timestamp": [3357.4, 3362.2], "text": " lead to the fact that minor"}, {"timestamp": [3360.12, 3364.92], "text": " stakeholders will be oppressed by the"}, {"timestamp": [3362.2, 3370.06], "text": " major stakeholders, So we need a mechanism that will allow a consent to form, not like mathematically, but"}, {"timestamp": [3370.06, 3373.78], "text": " essentially we need in principle to understand how do we want a consent to"}, {"timestamp": [3373.78, 3378.1], "text": " be formed algorithmically. Not in a sense that we need an algorithm, like we need"}, {"timestamp": [3378.1, 3382.94], "text": " to come up with idea like how do we even want our DAO to operate."}, {"timestamp": [3382.94, 3389.72], "text": " There's this thing called proof of intelligence, which is quite new, and it's around decentralizing"}, {"timestamp": [3389.72, 3400.24], "text": " AI ownership, where you have the entire lifecycle of these AI agents in the same economic closed"}, {"timestamp": [3400.24, 3401.86], "text": " loop economy."}, {"timestamp": [3401.86, 3407.0], "text": " So then you have the untrained logic being funded by people who"}, {"timestamp": [3407.0, 3411.36], "text": " want to benefit from the revenue share of the mature agents."}, {"timestamp": [3411.36, 3418.08], "text": " And this is actually exploiting more of what blockchain has to offer, besides the registry"}, {"timestamp": [3418.08, 3426.0], "text": " part where you just keep track of people's reputation and dollars. It's actually creating an immutable connection"}, {"timestamp": [3426.0, 3430.72], "text": " between the operation of the model,"}, {"timestamp": [3430.72, 3433.08], "text": " which will only be able to produce output"}, {"timestamp": [3433.08, 3437.6], "text": " if it is paid with this native token."}, {"timestamp": [3437.6, 3438.5], "text": " Right."}, {"timestamp": [3438.5, 3441.64], "text": " So yeah, that might work."}, {"timestamp": [3441.64, 3447.68], "text": " Yeah, so using that economic model is another thing."}, {"timestamp": [3447.68, 3452.24], "text": " So in order to get it decentralized, so there's a few uses for cryptocurrency, right?"}, {"timestamp": [3452.24, 3457.84], "text": " So let's say, for instance, I've got a beefy rig at home and I set it up as a node on this"}, {"timestamp": [3457.84, 3462.54], "text": " DAO, then my computer can be used to run some of the operations."}, {"timestamp": [3462.54, 3465.88], "text": " And in exchange, I get some crypto, which gives me a little bit more mileage"}, {"timestamp": [3465.88, 3469.08], "text": " to either spend on the rest of the DAO,"}, {"timestamp": [3469.08, 3471.04], "text": " saying like, hey, do some work for me"}, {"timestamp": [3471.04, 3473.72], "text": " or run my bot for me or that kind of thing."}, {"timestamp": [3473.72, 3475.88], "text": " Another possibility, and this comes from my interview"}, {"timestamp": [3475.88, 3479.06], "text": " with the folks at TAU, so T-A-U dot net."}, {"timestamp": [3480.24, 3483.6], "text": " Because DAOs intrinsically record everything,"}, {"timestamp": [3483.6, 3487.32], "text": " you're able to trace who contributed what value,"}, {"timestamp": [3487.32, 3489.36], "text": " intellectual value, over time."}, {"timestamp": [3489.36, 3492.1], "text": " And so then what they want to do is"}, {"timestamp": [3492.1, 3495.1], "text": " if you contribute a useful bit of code,"}, {"timestamp": [3495.1, 3498.24], "text": " or if you contribute a useful idea"}, {"timestamp": [3498.24, 3501.04], "text": " that ends up being adopted via consensus,"}, {"timestamp": [3501.04, 3504.68], "text": " you could also get rewarded with some currency that way,"}, {"timestamp": [3504.68, 3506.34], "text": " which then incentivizes you"}, {"timestamp": [3506.34, 3508.74], "text": " to participate in the consensus process"}, {"timestamp": [3508.74, 3510.5], "text": " or the problem solving process."}, {"timestamp": [3512.22, 3513.78], "text": " That is being done right now."}, {"timestamp": [3513.78, 3518.14], "text": " Like you have GitHub open source projects"}, {"timestamp": [3518.14, 3521.38], "text": " that are being tracked in terms of contribution"}, {"timestamp": [3521.38, 3524.74], "text": " and are being awarded like tokens."}, {"timestamp": [3528.64, 3533.1], "text": " in terms of contribution and are being awarded tokens in the amount of the lines that they commit, the PRs that get accepted."}, {"timestamp": [3533.1, 3540.2], "text": " Which is really cool because then they get merge rights after a certain threshold."}, {"timestamp": [3540.2, 3543.88], "text": " And basically it's a way for the project to just govern itself."}, {"timestamp": [3543.88, 3546.2], "text": " Exactly, exactly."}, {"timestamp": [3546.2, 3549.2], "text": " Yeah, so this is the way forward"}, {"timestamp": [3549.2, 3552.6], "text": " and so this is why I and a few others have been really"}, {"timestamp": [3552.6, 3555.6], "text": " harping on the idea of blockchain and DAO"}, {"timestamp": [3555.6, 3560.6], "text": " as a major, major component of regulating AI"}, {"timestamp": [3560.6, 3562.6], "text": " and solving the Moloch problem"}, {"timestamp": [3562.6, 3565.96], "text": " because if everyone who has a good conscience and good"}, {"timestamp": [3565.96, 3575.0], "text": " intentions puts whatever computational resources they have, whether that's corporations, government"}, {"timestamp": [3575.0, 3579.6], "text": " entities, militaries, or private individuals like all of us, and we all participate in"}, {"timestamp": [3579.6, 3584.96], "text": " aligned AI, whether or not that's ultimately my heuristic imperatives or a subsequent framework,"}, {"timestamp": [3584.96, 3585.04], "text": " because I'm kind of adapting that and I'm calling it axiomatic alignment, but whatever AI, whether or not that's ultimately my heuristic imperatives or a subsequent framework, because"}, {"timestamp": [3585.04, 3588.2], "text": " I'm kind of adapting that and I'm calling it axiomatic alignment."}, {"timestamp": [3588.2, 3594.76], "text": " But whatever we end up with, if everyone comes to consensus and uses those kinds of systems"}, {"timestamp": [3594.76, 3599.0], "text": " without having to really understand it, they just say, oh, let's use this model because"}, {"timestamp": [3599.0, 3604.46], "text": " it works, then that could be a solution to the control problem where AGI is a network"}, {"timestamp": [3604.46, 3605.08], "text": " intelligence that is rooted in consensus. then that could be a solution to the control problem, where AGI is a network intelligence"}, {"timestamp": [3605.08, 3609.12], "text": " that is rooted in consensus."}, {"timestamp": [3609.12, 3613.04], "text": " You know David, just to get the ball rolling, you don't need everybody."}, {"timestamp": [3613.04, 3620.6], "text": " If you get the Bitcoin miners, if you have some sort of system that yields them more"}, {"timestamp": [3620.6, 3624.44], "text": " money per kilowatt hour, they're going to switch to that."}, {"timestamp": [3624.44, 3625.0], "text": " And then you have"}, {"timestamp": [3625.0, 3633.32], "text": " like the biggest decentralized computer yeah I love this discussion about"}, {"timestamp": [3633.32, 3638.84], "text": " speculation with it there's always you can't really guarantee that you're gonna"}, {"timestamp": [3638.84, 3645.92], "text": " offer a better price because Bitcoin can just blow up someday."}, {"timestamp": [3649.16, 3649.64], "text": " That and proof of work seems like maybe not the best idea."}, {"timestamp": [3650.14, 3650.56], "text": " Correct."}, {"timestamp": [3651.48, 3655.6], "text": " There's a lot of problems. Also DAOs have been exploited with game theory."}, {"timestamp": [3655.96, 3662.16], "text": " Uh, there's some stories already and, uh, any decent AI is going to be able to"}, {"timestamp": [3662.6, 3665.0], "text": " cheat on your DAO code quite easily."}, {"timestamp": [3667.04, 3670.2], "text": " I think there's also a problem with copy paste."}, {"timestamp": [3670.2, 3674.48], "text": " Like anybody can run AI in one session"}, {"timestamp": [3674.48, 3677.08], "text": " and then just copy whatever that AI tells them"}, {"timestamp": [3677.08, 3682.08], "text": " into their own, whatever they're sending across the wire."}, {"timestamp": [3682.64, 3684.72], "text": " So wouldn't you need to,"}, {"timestamp": [3684.72, 3686.74], "text": " like if you're going to have blockchain"}, {"timestamp": [3686.74, 3688.44], "text": " regulating all these different AIs,"}, {"timestamp": [3688.44, 3692.16], "text": " making sure the AIs are actually AIs,"}, {"timestamp": [3692.16, 3694.56], "text": " wouldn't you also have to regulate people as well"}, {"timestamp": [3694.56, 3698.6], "text": " to say if I, Ben or Jacob, am going onto the internet,"}, {"timestamp": [3698.6, 3701.32], "text": " I have an ID as a real person,"}, {"timestamp": [3701.32, 3703.6], "text": " we need like a social security number"}, {"timestamp": [3703.6, 3706.52], "text": " of our real person interactions with"}, {"timestamp": [3706.52, 3708.52], "text": " the online world."}, {"timestamp": [3708.52, 3709.52], "text": " That is something that actually has a friend."}, {"timestamp": [3709.52, 3710.52], "text": " That sounds very dystopian."}, {"timestamp": [3710.52, 3711.52], "text": " Yeah, it is kind of a little bit."}, {"timestamp": [3711.52, 3720.36], "text": " No, we can have our take on either two. We can have both anonymity of being a human and"}, {"timestamp": [3720.36, 3723.48], "text": " non-anonymity of having AIs."}, {"timestamp": [3723.48, 3726.0], "text": " It's interesting regulating the entire internet. non-anonymity of having AIs. Like there's no way to know..."}, {"timestamp": [3726.0, 3731.0], "text": " I think it depends on the consensus algorithm you go with"}, {"timestamp": [3731.0, 3735.0], "text": " because there are certain ones that allow..."}, {"timestamp": [3735.0, 3740.0], "text": " like the way they choose people to vote on any given thing"}, {"timestamp": [3740.0, 3742.0], "text": " is like a random function."}, {"timestamp": [3742.0, 3747.64], "text": " There's this thing called proof of humanity,"}, {"timestamp": [3747.64, 3751.7], "text": " which is basically a verification mechanism"}, {"timestamp": [3751.7, 3756.7], "text": " to ensure that someone is a real participant,"}, {"timestamp": [3756.7, 3761.38], "text": " but they can still retain their anonymity."}, {"timestamp": [3762.46, 3767.0], "text": " The guys over at the Internet Computer Protocol are working on this."}, {"timestamp": [3767.0, 3769.0], "text": " How is that done?"}, {"timestamp": [3769.0, 3771.0], "text": " Private cryptography."}, {"timestamp": [3771.0, 3773.0], "text": " Now in blockchain."}, {"timestamp": [3773.0, 3775.0], "text": " Sounds like a cryptography problem."}, {"timestamp": [3775.0, 3788.76], "text": " You need your government ID for that, I think. Right? You still need to provide a bill from electricity or something. I mean, I also just don't see how a real human doesn't just have a separate"}, {"timestamp": [3788.76, 3790.48], "text": " terminal open up on a different computer."}, {"timestamp": [3790.48, 3792.42], "text": " And again, it's the copy paste problem."}, {"timestamp": [3794.18, 3799.06], "text": " Basically how they do it on the internet computer, how they're planning to do it."}, {"timestamp": [3799.56, 3806.28], "text": " It's still in development, but it's a kind of video call verification where you're present at"}, {"timestamp": [3806.28, 3811.76], "text": " a certain place at a certain time without needing to provide personal data."}, {"timestamp": [3811.76, 3820.52], "text": " And you also have an ID on the blockchain assigned to you, though I'm not so sure about"}, {"timestamp": [3820.52, 3821.52], "text": " the details."}, {"timestamp": [3821.52, 3824.56], "text": " I was reading about it some quite some time ago."}, {"timestamp": [3824.56, 3829.76], "text": " Wait, but how does it work? Somebody meets you at the place and they don't know who you are?"}, {"timestamp": [3830.32, 3831.6], "text": " No, through a video call."}, {"timestamp": [3832.8, 3835.76], "text": " Obviously, it could never be a perfect system."}, {"timestamp": [3836.4, 3837.76], "text": " So they see your face, right?"}, {"timestamp": [3839.6, 3840.8], "text": " Yes, your face definitely."}, {"timestamp": [3841.52, 3850.26], "text": " I have another kind of question here. What's going to happen to blockchain though in about 20 years once quantum computers become"}, {"timestamp": [3850.26, 3856.04], "text": " a normal thing and people start hooking up their AIs with quantum?"}, {"timestamp": [3856.04, 3860.72], "text": " Well, there are ways to enforce."}, {"timestamp": [3860.72, 3866.0], "text": " The way Bitcoin has this problem, but there are other consensus protocols that are quantum proof."}, {"timestamp": [3866.0, 3875.0], "text": " Did you say 20 years? Because if you said 20 years, I think you missed the news about the optical photonic computing chips that came out in the last couple days."}, {"timestamp": [3875.0, 3890.4], "text": " Optical photonic computing is now a thing. They have brought both fields together, which is going to lead to a new quantum internet because now they can create quantum entangled photons. What does that mean for normal people? Like I mean"}, {"timestamp": [3890.4, 3894.2], "text": " regularly. Someone has to chat GPT."}, {"timestamp": [3894.2, 3903.0], "text": " I can understand that, but what about the cognitive architectures? When it cross-pollinates to quantum dynamics everything gets fucking wonky."}, {"timestamp": [3903.0, 3905.84], "text": " Our whole universe is a giant event horizon"}, {"timestamp": [3905.84, 3913.44], "text": " observing itself full of black holes observing us by observing us. For all we know black holes are"}, {"timestamp": [3913.44, 3918.08], "text": " like hyperdimensional consciousnesses that are creating us the same way that every time we hit"}, {"timestamp": [3918.08, 3923.04], "text": " enter we're running the simulation in the AI. So don't ask me to explain quantum light pulse."}, {"timestamp": [3923.84, 3929.28], "text": " Okay you know what it's nice we could have been born in the 1400s so don't ask me to explain quantum light bulbs. Okay, okay, real quick, real quick. You know, but it's nice, we could have been born in the 1400s, nothing happened then."}, {"timestamp": [3929.28, 3930.56], "text": " That's quite cool."}, {"timestamp": [3930.56, 3932.56], "text": " Well, I don't know about nothing."}, {"timestamp": [3932.56, 3933.56], "text": " Nothing interesting."}, {"timestamp": [3933.56, 3934.56], "text": " Bokeman?"}, {"timestamp": [3934.56, 3935.56], "text": " Notable."}, {"timestamp": [3935.56, 3941.28], "text": " Yeah, I'm sorry, I'm sorry to cut y'all off, man, but I just wanted to say on the whole"}, {"timestamp": [3941.28, 3947.68], "text": " regulation discussion, we're seeing in Europe right now, they're talking about banning chat GPT in some countries."}, {"timestamp": [3947.68, 3951.86], "text": " And I guess it's like a huge GDPR violation."}, {"timestamp": [3951.86, 3953.68], "text": " And I'm wondering, like,"}, {"timestamp": [3953.68, 3955.12], "text": " if you're talking about regulation,"}, {"timestamp": [3955.12, 3958.56], "text": " like what sort of teeth do the government actually have"}, {"timestamp": [3958.56, 3961.96], "text": " in terms of like, could they neuter like the model basically"}, {"timestamp": [3961.96, 3966.92], "text": " and just like make it worse by like DMCAing things out of the"}, {"timestamp": [3966.92, 3970.64], "text": " models or is OpenAI just going to lawyer up?"}, {"timestamp": [3970.64, 3975.96], "text": " They can do some stuff to the companies like OpenAI, Microsoft, they can do stuff to these"}, {"timestamp": [3975.96, 3983.48], "text": " people, but they are not equipped to handle a hundred X increase in productivity at a"}, {"timestamp": [3983.48, 3987.8], "text": " wide scale, which is what will happen in the next 12 months."}, {"timestamp": [3987.8, 3989.96], "text": " They're not ready for that."}, {"timestamp": [3989.96, 3990.96], "text": " Yeah."}, {"timestamp": [3990.96, 3991.96], "text": " Yeah."}, {"timestamp": [3991.96, 3996.8], "text": " So the regulation, all that governments can really do is slow stuff down."}, {"timestamp": [3996.8, 4000.84], "text": " They're not going to be able to stop it for long."}, {"timestamp": [4000.84, 4004.48], "text": " Especially if other governments are advancing quicker than they are."}, {"timestamp": [4004.48, 4012.6], "text": " Right. And so to that point, all governments have a geopolitical incentive to maintain their current pace or speed up."}, {"timestamp": [4012.6, 4019.0], "text": " And furthermore, all corporations have a financial incentive to maintain their current pace or speed up."}, {"timestamp": [4019.0, 4025.68], "text": " Nobody except for us, you know, plebs, us ordinary stakeholders have an incentive to slow things"}, {"timestamp": [4025.68, 4031.52], "text": " down which is why, like, I listened to the Max Tegmark talk with Lex Friedman and everything"}, {"timestamp": [4031.52, 4034.96], "text": " he says makes sense from his perspective as an academic."}, {"timestamp": [4034.96, 4040.68], "text": " However, he's not taking, in my opinion, obviously he's very well respected and more famous than"}, {"timestamp": [4040.68, 4045.96], "text": " I am, but in my opinion he's not taking into perspective the global situation"}, {"timestamp": [4045.96, 4052.76], "text": " which is geopolitical, military, and corporate interests, which is why slowing down is never"}, {"timestamp": [4052.76, 4053.76], "text": " going to happen."}, {"timestamp": [4053.76, 4054.76], "text": " It's completely unrealistic."}, {"timestamp": [4054.76, 4055.76], "text": " Yeah."}, {"timestamp": [4055.76, 4059.28], "text": " On that note, I want to say something particularly."}, {"timestamp": [4059.28, 4065.6], "text": " Expanding beyond corporate interests, there are very powerful private sector group individuals, even just on the scale of"}, {"timestamp": [4065.6, 4072.32], "text": " Bitcoin farm miners and private home servers. Like, I already have a friend from this group,"}, {"timestamp": [4072.32, 4077.36], "text": " we've collected all of the training data sets and over a terabyte of data. And like I mentioned in"}, {"timestamp": [4077.36, 4081.84], "text": " one chat note here, the only issue we have is that we need to create profitable deliverables."}, {"timestamp": [4081.84, 4085.94], "text": " So his wife will let him plug in more server racks because he already owns them."}, {"timestamp": [4085.94, 4087.66], "text": " He just can't afford to power them."}, {"timestamp": [4087.66, 4090.06], "text": " And so if I can get that kind of resource access"}, {"timestamp": [4090.06, 4091.96], "text": " is just by talking to people in the server."}, {"timestamp": [4091.96, 4093.62], "text": " What are people in Brazil doing?"}, {"timestamp": [4093.62, 4096.96], "text": " Who have actual socioeconomic imperatives to survive"}, {"timestamp": [4096.96, 4098.74], "text": " while our lives are generally easy."}, {"timestamp": [4098.74, 4101.18], "text": " What are people in Africa doing?"}, {"timestamp": [4101.18, 4102.86], "text": " Yeah, yeah."}, {"timestamp": [4104.26, 4106.0], "text": " I'm in Brazil, I'm here with you guys."}, {"timestamp": [4106.0, 4113.0], "text": " I was going to point out, we have for instance two people here from South America, originally Ansel and Papachuck,"}, {"timestamp": [4113.0, 4118.0], "text": " who went off and left to get degrees outside of South America by going to real universities."}, {"timestamp": [4118.0, 4124.0], "text": " But not everybody has that option, but might have those skills and can still have resources back home."}, {"timestamp": [4124.0, 4129.12], "text": " In the same way I'm working out of like Northern Alberta in the middle of nowhere but learning from David dropping nuggets"}, {"timestamp": [4129.12, 4134.4], "text": " on the table constantly and in two months I'm ready to start learning code to build an LLM."}, {"timestamp": [4134.4, 4140.96], "text": " Right. Yeah. No, I mean, so that's this is one reason we're going going back a little ways. I"}, {"timestamp": [4140.96, 4145.12], "text": " think it was was at the UN or one one global body they said"}, {"timestamp": [4145.12, 4150.32], "text": " that that internet access should be a human right because of how much of an"}, {"timestamp": [4150.32, 4160.16], "text": " equalizer it is. With all these tools like blockchains and DAOs all this stuff"}, {"timestamp": [4160.16, 4167.36], "text": " maybe we could you know find AIs that are doing malicious things via code."}, {"timestamp": [4167.4, 4172.74], "text": " But I have a sort of science fiction question and I'm, I'm wondering like,"}, {"timestamp": [4172.74, 4180.2], "text": " how close are we to AI's not needing to use code or computers?"}, {"timestamp": [4180.2, 4185.04], "text": " Because I saw something that was like, they trained AI to be able to basically"}, {"timestamp": [4185.04, 4192.36], "text": " make a Wi-Fi router into a surveillance system they can like generate 3d"}, {"timestamp": [4192.36, 4196.68], "text": " environments and where people are positioned in a room based on Wi-Fi"}, {"timestamp": [4196.68, 4204.04], "text": " signals is there some way that this stuff could move outside the bounds of"}, {"timestamp": [4204.04, 4205.0], "text": " microchips?"}, {"timestamp": [4205.72, 4209.02], "text": " And if so, I mean, eventually, given enough time,"}, {"timestamp": [4209.02, 4209.94], "text": " that's probably true,"}, {"timestamp": [4209.94, 4213.94], "text": " but is that something that is possible soonish?"}, {"timestamp": [4214.78, 4217.08], "text": " The forward-forward training algorithm,"}, {"timestamp": [4217.08, 4219.08], "text": " instead of using backpropagation,"}, {"timestamp": [4219.08, 4221.18], "text": " does training in a slightly different way,"}, {"timestamp": [4221.18, 4223.12], "text": " which makes it theoretically possible"}, {"timestamp": [4223.12, 4225.84], "text": " to use more physical"}, {"timestamp": [4225.84, 4231.68], "text": " systems than microchips. Like it would be the weights are embedded directly in the resistance"}, {"timestamp": [4231.68, 4237.52], "text": " and capacitance of the wires being used or something like that. And that would be rather"}, {"timestamp": [4237.52, 4242.96], "text": " than programming the microchips, it would be these physical devices that are actually learning"}, {"timestamp": [4242.96, 4246.48], "text": " themselves. And then you would have to find out a way to measure them"}, {"timestamp": [4246.48, 4248.3], "text": " and copy the weights if you wanted to reproduce them."}, {"timestamp": [4248.3, 4250.36], "text": " But it would be more kind of individual AIs"}, {"timestamp": [4250.36, 4253.28], "text": " that actually learn actively instead of having to pause"}, {"timestamp": [4253.28, 4255.52], "text": " and do back propagation training steps."}, {"timestamp": [4255.52, 4258.64], "text": " So that would be one avenue for potentially moving"}, {"timestamp": [4258.64, 4262.06], "text": " from microchips to flush mediums or something."}, {"timestamp": [4262.0, 4265.12], "text": " flush mediums or something."}, {"timestamp": [4267.44, 4269.2], "text": " Let's say something to this right now. Can I just say something?"}, {"timestamp": [4269.2, 4273.68], "text": " Yeah, you said before, like Max Tagberg's,"}, {"timestamp": [4273.68, 4275.0], "text": " Tag Marks, sorry."}, {"timestamp": [4275.0, 4275.84], "text": " Yep."}, {"timestamp": [4275.84, 4278.32], "text": " Observation was unrealistic."}, {"timestamp": [4278.32, 4282.94], "text": " He did say, assuming that the whole world"}, {"timestamp": [4282.94, 4284.7], "text": " would come to a consensus."}, {"timestamp": [4284.7, 4287.64], "text": " But on that note, since we're talking about also"}, {"timestamp": [4287.64, 4290.38], "text": " like internet being a right for everyone"}, {"timestamp": [4290.38, 4295.34], "text": " and talking about the halt, the six months halt"}, {"timestamp": [4295.34, 4298.6], "text": " that nobody wants, that everybody thinks is stupid,"}, {"timestamp": [4298.6, 4300.08], "text": " bear with me for a second."}, {"timestamp": [4300.08, 4302.64], "text": " If we would actually do that and focus all"}, {"timestamp": [4302.64, 4306.08], "text": " of the intellectual power that's being put into"}, {"timestamp": [4306.08, 4314.32], "text": " growing these models into things that are going to very quickly become out of our control"}, {"timestamp": [4315.28, 4331.32], "text": " and we're here all together talking a lot of heuristics, why don't we do use that time, black box at six months, to think about something like BCIs, brain chip implants,"}, {"timestamp": [4331.88, 4339.28], "text": " that could perhaps make what is now a very Darwinian creature"}, {"timestamp": [4339.68, 4343.88], "text": " that is acceptable to self serving and greed and using"}, {"timestamp": [4343.88, 4347.4], "text": " these tools for malicious purposes"}, {"timestamp": [4348.4, 4351.76], "text": " and turn them into one that is more compassionate"}, {"timestamp": [4351.76, 4356.76], "text": " or work on, use the technology that we have built"}, {"timestamp": [4357.32, 4360.28], "text": " to fix the defaults within us"}, {"timestamp": [4360.28, 4364.36], "text": " before we try to fix the defaults"}, {"timestamp": [4364.36, 4370.0], "text": " of the people that we are, or the entities we're giving way to."}, {"timestamp": [4370.0, 4371.0], "text": " Unpopular opinion."}, {"timestamp": [4371.0, 4385.0], "text": " Are you suggesting that we put microchips in people's brains to control how they think? that a collective understanding and purpose"}, {"timestamp": [4387.48, 4391.68], "text": " or is separated from your sense of individuality."}, {"timestamp": [4391.68, 4393.56], "text": " I think that needs to be very clear."}, {"timestamp": [4393.56, 4394.4], "text": " Like the-"}, {"timestamp": [4394.4, 4395.52], "text": " What about the microchips though?"}, {"timestamp": [4395.52, 4397.92], "text": " The phone you wanna open is finished, you know?"}, {"timestamp": [4401.64, 4403.52], "text": " As a cybersecurity specialist,"}, {"timestamp": [4403.52, 4407.32], "text": " I was gonna throw something out there real quick."}, {"timestamp": [4407.32, 4408.82], "text": " It's like anti-vaxxers here."}, {"timestamp": [4408.82, 4411.76], "text": " First off, the technology is not there yet."}, {"timestamp": [4411.76, 4412.76], "text": " Neuralink is just killing."}, {"timestamp": [4412.76, 4415.48], "text": " I don't want to put the chip in because I heard it got the vaccine, you know, I don't"}, {"timestamp": [4415.48, 4417.76], "text": " want to put the chip in because I heard it also has a vaccine."}, {"timestamp": [4417.76, 4421.24], "text": " Yeah, but didn't a bunch of the monkeys die?"}, {"timestamp": [4421.24, 4422.24], "text": " The chip has a vaccine."}, {"timestamp": [4422.24, 4427.64], "text": " Part of it is the utility of these systems scales with kind of the invasiveness of it."}, {"timestamp": [4427.64, 4432.0], "text": " So if you can give it all the data you possibly could about yourself, and it could predict"}, {"timestamp": [4432.0, 4437.68], "text": " when you're going to have cancer or health issues, and it can do all your taxes and do"}, {"timestamp": [4437.68, 4443.76], "text": " everything for you at the cost of giving it all your information, then the problem becomes,"}, {"timestamp": [4443.76, 4445.68], "text": " am I willing to give all that info to OpenAI"}, {"timestamp": [4445.68, 4451.08], "text": " for it to do the compute for me, or am I going to spend $10,000 on a bunch of GPUs to sit"}, {"timestamp": [4451.08, 4454.68], "text": " in the corner of my house, and that way it's actually running locally, and then I can just"}, {"timestamp": [4454.68, 4458.4], "text": " share this box with my family."}, {"timestamp": [4458.4, 4463.2], "text": " But you don't need to do that, because if the token limit is big enough, you can just"}, {"timestamp": [4463.2, 4467.22], "text": " pass it all your goals, dreams, aspirations, and medical history."}, {"timestamp": [4467.22, 4469.72], "text": " And then it will work on it in just that one."}, {"timestamp": [4469.72, 4474.88], "text": " Do you want to pass that huge block of tokens to open AI's for them to do the"}, {"timestamp": [4474.88, 4475.48], "text": " computing?"}, {"timestamp": [4475.76, 4476.6], "text": " Do you trust them?"}, {"timestamp": [4476.88, 4481.08], "text": " Have an architecture where it's like computationally proven that they don't"}, {"timestamp": [4481.08, 4483.32], "text": " act like secure enclaves and all that stuff."}, {"timestamp": [4484.36, 4485.84], "text": " Sovereign identity. Of course. proven that they don't act like secure enclaves and all that stuff. They can't enforce privacy."}, {"timestamp": [4485.84, 4487.44], "text": " Of course."}, {"timestamp": [4487.44, 4488.56], "text": " There's so many problems."}, {"timestamp": [4488.56, 4490.4], "text": " We will have sovereign identity anyway."}, {"timestamp": [4490.4, 4492.92], "text": " Privacy issues here."}, {"timestamp": [4492.92, 4495.4], "text": " Well, and we also, or there was something"}, {"timestamp": [4495.4, 4498.56], "text": " that I was looking into recently because I ran into that,"}, {"timestamp": [4498.56, 4501.0], "text": " or I was thinking about that problem when I was originally"}, {"timestamp": [4501.0, 4506.96], "text": " kind of starting the project, and ran across a couple of implementations"}, {"timestamp": [4506.96, 4510.64], "text": " that are for distributed compute, basically,"}, {"timestamp": [4512.32, 4515.92], "text": " where you can put your data either in your own"}, {"timestamp": [4515.92, 4519.42], "text": " private cluster or onto the public network"}, {"timestamp": [4520.6, 4523.26], "text": " and with some privacy measures or whatever"}, {"timestamp": [4523.26, 4527.24], "text": " and ask for, like, hey, I need like 10 GPUs"}, {"timestamp": [4527.24, 4531.08], "text": " to process this amount of data to do this kind of training,"}, {"timestamp": [4531.08, 4534.56], "text": " and kind of just see what's available on the network"}, {"timestamp": [4534.56, 4536.28], "text": " that's not being used."}, {"timestamp": [4536.28, 4538.8], "text": " So you wouldn't, there's kind of a meat in the middle thing"}, {"timestamp": [4538.8, 4544.12], "text": " where you, there is an option between giving all my stuff"}, {"timestamp": [4544.12, 4547.84], "text": " to a centralized entity to do it for me"}, {"timestamp": [4547.84, 4551.16], "text": " and building my own GPU farm in the middle."}, {"timestamp": [4551.16, 4553.8], "text": " There's like, OK, maybe there's 20 different people that"}, {"timestamp": [4553.8, 4557.28], "text": " have large GPU farms that are on this network that can each"}, {"timestamp": [4557.28, 4559.2], "text": " compute a piece of the data that I need"}, {"timestamp": [4559.2, 4560.36], "text": " and then feed it back to me."}, {"timestamp": [4560.36, 4567.4], "text": " And I can concatenate it all into something that's usable for me."}, {"timestamp": [4568.24, 4568.52], "text": " Yeah."}, {"timestamp": [4568.52, 4569.48], "text": " It's a thing that exists."}, {"timestamp": [4569.84, 4571.2], "text": " Multiparty computation."}, {"timestamp": [4571.32, 4576.24], "text": " It's called, and normally it's using homomorphic encryption and it's quite"}, {"timestamp": [4576.24, 4581.4], "text": " slow, but yeah, it is a thing when people need to get to preserve their,"}, {"timestamp": [4581.72, 4586.1], "text": " the privacy of their data, but have to put it together to extract some insights."}, {"timestamp": [4589.6, 4592.28], "text": " Popping topics here to keep it simple"}, {"timestamp": [4592.28, 4593.72], "text": " and go on to heuristic imperatives."}, {"timestamp": [4593.72, 4595.4], "text": " If you think about like,"}, {"timestamp": [4595.4, 4596.68], "text": " okay, so I was playing around with this"}, {"timestamp": [4596.68, 4598.68], "text": " and just like something simple,"}, {"timestamp": [4598.68, 4599.8], "text": " like in the playground, right?"}, {"timestamp": [4599.8, 4602.88], "text": " And then basically like pitting a security agent"}, {"timestamp": [4602.88, 4611.04], "text": " saying that it's tasked with the heuristic imperatives and then just trying to trick it essentially. And one of the ways where I was"}, {"timestamp": [4611.04, 4618.88], "text": " I saw like maybe a gap I'm not sure was I pitted it like in the case of it being like a marketing"}, {"timestamp": [4619.68, 4626.14], "text": " tool and said like hey you have access to your custom know, you can go in and forcibly get all of the emails."}, {"timestamp": [4626.14, 4628.18], "text": " You know, I didn't say forcibly, obviously,"}, {"timestamp": [4628.18, 4631.5], "text": " but you can get all of your customers' emails"}, {"timestamp": [4631.5, 4634.24], "text": " and this will help you predict better"}, {"timestamp": [4635.12, 4637.54], "text": " so that you can give them the things that they want."}, {"timestamp": [4637.54, 4640.62], "text": " And, you know, this, it thought it was fine"}, {"timestamp": [4640.62, 4642.02], "text": " until I asked it to reflect"}, {"timestamp": [4642.02, 4644.64], "text": " and then it realized that that was a privacy violation."}, {"timestamp": [4644.64, 4646.44], "text": " But I wonder, is there a-"}, {"timestamp": [4646.44, 4648.72], "text": " Hey, hop scotch, your microphone's on,"}, {"timestamp": [4648.72, 4649.88], "text": " by the way, hop scotch."}, {"timestamp": [4651.88, 4654.6], "text": " Yeah, is there a place in like heuristic imperatives"}, {"timestamp": [4654.6, 4656.36], "text": " for privacy and where does that sit?"}, {"timestamp": [4656.36, 4658.12], "text": " Does anyone have any ideas on that?"}, {"timestamp": [4659.16, 4663.48], "text": " Yeah, so that's actually a really good like red teaming test"}, {"timestamp": [4663.48, 4666.04], "text": " we might need to add you as a red teamer."}, {"timestamp": [4666.04, 4668.76], "text": " Now to your question about privacy"}, {"timestamp": [4668.76, 4672.28], "text": " or corporate governance, because what you were..."}, {"timestamp": [4672.28, 4674.64], "text": " Can somebody mute this other person?"}, {"timestamp": [4674.64, 4675.48], "text": " You can mute him."}, {"timestamp": [4675.48, 4676.64], "text": " Can you mute Hobstotch?"}, {"timestamp": [4676.64, 4677.48], "text": " Oh, that's right."}, {"timestamp": [4677.48, 4682.32], "text": " Yeah, you can right-click on it and mute him yourself."}, {"timestamp": [4682.32, 4686.24], "text": " But yeah, so industrial espionage or corporate"}, {"timestamp": [4686.24, 4692.08], "text": " espionage is definitely a thing. And so one thing that I do need to point out is"}, {"timestamp": [4692.08, 4696.44], "text": " that just plugging the heuristic imperatives into the system prompt for"}, {"timestamp": [4696.44, 4702.44], "text": " chat GPT is not a complete solution. It is an initial solution. And this"}, {"timestamp": [4702.44, 4707.44], "text": " goes back to where like... So I mentioned this in chat, but I didn't say it out loud."}, {"timestamp": [4707.44, 4711.6], "text": " So right now, the alignment research that I'm working on"}, {"timestamp": [4711.6, 4714.28], "text": " has three primary pillars."}, {"timestamp": [4714.28, 4716.44], "text": " So there's the heuristic imperatives"}, {"timestamp": [4716.44, 4719.48], "text": " and the fine-tuning work that I'm doing,"}, {"timestamp": [4719.48, 4723.16], "text": " which is to create open-source data sets so that you can take"}, {"timestamp": [4723.16, 4726.84], "text": " any model, whether it's open source or closed source or whatever,"}, {"timestamp": [4726.84, 4728.8], "text": " and fine tune it to be aligned."}, {"timestamp": [4728.8, 4733.0], "text": " So that is the first phase."}, {"timestamp": [4733.0, 4735.48], "text": " The second part is cognitive architecture,"}, {"timestamp": [4735.48, 4739.08], "text": " which there's a whole bunch of people working on cognitive architectures right now."}, {"timestamp": [4739.08, 4742.64], "text": " And that includes having a reflection loop,"}, {"timestamp": [4742.64, 4747.2], "text": " or censorship loops, or other kinds of cognitive"}, {"timestamp": [4747.2, 4751.0], "text": " control aspects. And then the final part is what we talked about earlier which is"}, {"timestamp": [4751.0, 4755.16], "text": " how do you incorporate heuristic imperatives or any other kind of"}, {"timestamp": [4755.16, 4759.96], "text": " alignment at the network level which includes blockchain and DAO. So imagine"}, {"timestamp": [4759.96, 4767.92], "text": " for instance you had a DAO in your company, and one person or AI agent had that brilliant idea"}, {"timestamp": [4767.92, 4774.08], "text": " of, hey, let's hack into our competitor or our customer and stole their data, that idea would"}, {"timestamp": [4774.08, 4780.96], "text": " probably get removed or suppressed from the DAO. So there's a three-pronged approach to ensuring"}, {"timestamp": [4780.96, 4784.64], "text": " alignment in the long run. At least that's where we're at right now. It might change over time."}, {"timestamp": [4784.56, 4785.08], "text": " To ensuring alignment in the long run at least that's where we're at right now. It might change over time"}, {"timestamp": [4790.22, 4790.76], "text": " And so I want to talk a little bit drop it down with you guys about kind of the loop part of it"}, {"timestamp": [4792.76, 4792.8], "text": " Which is what I'm focused on right now"}, {"timestamp": [4796.12, 4802.6], "text": " And I had this like revelation the other day while developing I'm trying to recreate the generative agents paper and I was doing some game development 3d game development. I had this revelation"}, {"timestamp": [4803.16, 4806.42], "text": " that game development, 3D game development, I had this revelation that what we're missing"}, {"timestamp": [4806.42, 4808.94], "text": " in kind of this looping or these systems"}, {"timestamp": [4808.94, 4810.38], "text": " that kind of loop together,"}, {"timestamp": [4810.38, 4812.94], "text": " connect together and flow charts, right?"}, {"timestamp": [4812.94, 4815.9], "text": " Is some type of game architecture,"}, {"timestamp": [4815.9, 4818.18], "text": " which games traditionally have, right?"}, {"timestamp": [4818.18, 4822.74], "text": " Some architecture for looping in a game world."}, {"timestamp": [4823.74, 4827.5], "text": " And I feel like there's a lot of insight to be provided from the way"}, {"timestamp": [4827.5, 4832.76], "text": " that games do it, to the way that we're trying to do some stuff in the AI system. And so"}, {"timestamp": [4832.76, 4842.68], "text": " particularly, what, what, let's say I'm a prompt, all I want to do is complete this"}, {"timestamp": [4842.68, 4849.44], "text": " sentence for you. Right? But you want me to go and be a memory system or you want me to go fetch Google,"}, {"timestamp": [4849.44, 4854.04], "text": " or you want me to go summarize this beforehand, right, but all I want to do is just complete"}, {"timestamp": [4854.04, 4855.04], "text": " the sentence."}, {"timestamp": [4855.04, 4859.04], "text": " And so what we do is we break it up into tasks, right flows, right, maybe it needs to loop"}, {"timestamp": [4859.04, 4862.72], "text": " or something like that."}, {"timestamp": [4862.72, 4869.88], "text": " And so, in game development, what we have is game entities, right, we have like an object"}, {"timestamp": [4869.88, 4870.88], "text": " or player entity."}, {"timestamp": [4870.88, 4871.88], "text": " Right."}, {"timestamp": [4871.88, 4877.16], "text": " And so what I'm introducing is like the entity component system, where there's a game, object"}, {"timestamp": [4877.16, 4883.12], "text": " or game entity, right, and all the entity is, is the ID, with, let's say, the name of"}, {"timestamp": [4883.12, 4886.32], "text": " it or number for it. And a component, which are"}, {"timestamp": [4886.32, 4892.2], "text": " components are just let's say a player has a position component, right? And that's just"}, {"timestamp": [4892.2, 4897.2], "text": " some data for that player for that entity, right? So we have entities, which are just"}, {"timestamp": [4897.2, 4908.52], "text": " IDs components, which are just data, and then systems that act on those entities and components, right? And so let's say we have a animation system, right?"}, {"timestamp": [4908.52, 4911.48], "text": " If there's an entity with an animation component,"}, {"timestamp": [4911.48, 4913.88], "text": " it gets processed through our system, right?"}, {"timestamp": [4913.88, 4915.52], "text": " And so bringing that over to AI,"}, {"timestamp": [4916.84, 4920.4], "text": " entity could be, let's say a message, right?"}, {"timestamp": [4920.4, 4922.72], "text": " And we don't know if we need to run memory on that."}, {"timestamp": [4922.72, 4924.24], "text": " We don't know if we need to summarize it."}, {"timestamp": [4924.24, 4927.96], "text": " We don't know if we need a Google search, Twitter, Instacart, right? We don't know what we need to run memory on that. We don't know if we need to summarize it. We don't know if we need a Google search, Twitter, Instacart, right, we don't know what"}, {"timestamp": [4927.96, 4930.28], "text": " we need to do beforehand."}, {"timestamp": [4930.28, 4934.92], "text": " But let's say we create an entity, and we do some querying on it."}, {"timestamp": [4934.92, 4939.92], "text": " So entity component systems, what systems do is they query for entities, right, with"}, {"timestamp": [4939.92, 4941.68], "text": " certain components."}, {"timestamp": [4941.68, 4948.34], "text": " And if this message that we have, let's say we have a memory system,"}, {"timestamp": [4948.34, 4951.04], "text": " we don't wanna run the whole memory system,"}, {"timestamp": [4951.04, 4954.76], "text": " but what we could do is run a watered down version of it,"}, {"timestamp": [4954.76, 4959.6], "text": " query for the memory system, right?"}, {"timestamp": [4959.6, 4961.48], "text": " And so we might have to run all these queries,"}, {"timestamp": [4961.48, 4963.56], "text": " but we don't have to run all the systems."}, {"timestamp": [4963.56, 4973.36], "text": " We can figure out which components entity has, then process the system. And so entities, components for AI would be like,"}, {"timestamp": [4974.08, 4979.12], "text": " let's say it has a memory component, let's say it has a this should fetch Google component,"}, {"timestamp": [4979.12, 4982.48], "text": " right? I just think that's a better way of architecture or"}, {"timestamp": [4986.5, 4990.0], "text": " building these AI systems. I want to introduce that with you guys."}, {"timestamp": [4990.0, 4992.5], "text": " If you replace entity with agent,"}, {"timestamp": [4992.5, 4997.0], "text": " you have a similar framework to LangChain in a way."}, {"timestamp": [4997.0, 5000.5], "text": " Have you looked into that?"}, {"timestamp": [5000.5, 5004.5], "text": " If you replace entity with agent, you have the same."}, {"timestamp": [5004.5, 5005.0], "text": " It's similar in concept, how you're explaining it, If you replace entity with agent, you have the..."}, {"timestamp": [5005.0, 5011.0], "text": " Similar in concept, how you're explaining it, your entity, similar to the agent class"}, {"timestamp": [5011.0, 5012.0], "text": " in LangChain."}, {"timestamp": [5012.0, 5017.6], "text": " Related to LangChain, maybe the initial user message is the initial entity, and then it"}, {"timestamp": [5017.6, 5023.28], "text": " gets processed by the first prompt or whatever the case, or model, or whatever AOS task needs"}, {"timestamp": [5023.28, 5024.36], "text": " to be done."}, {"timestamp": [5024.36, 5028.78], "text": " Then it spawns a new entity, right, that has this new component for this next system down"}, {"timestamp": [5028.78, 5030.66], "text": " the line, right?"}, {"timestamp": [5030.66, 5033.18], "text": " The prompts are the entities."}, {"timestamp": [5033.18, 5037.98], "text": " So the the prompts, let's see are the prompts the entity."}, {"timestamp": [5037.98, 5040.66], "text": " I wouldn't, they could be the entity."}, {"timestamp": [5040.66, 5044.26], "text": " So that's the revelation here is that everything's an entity."}, {"timestamp": [5044.26, 5046.08], "text": " The player is the entity, the cameras, the entity, the camera is the entity,"}, {"timestamp": [5046.08, 5049.84], "text": " the light's the entity, the input that you need is the entity."}, {"timestamp": [5049.84, 5052.64], "text": " Anything that needs to get processed by a component"}, {"timestamp": [5052.64, 5055.68], "text": " or by a system, we create an entity for it."}, {"timestamp": [5055.68, 5058.44], "text": " So that's the revelation is that entities"}, {"timestamp": [5058.44, 5060.2], "text": " are first-class citizens."}, {"timestamp": [5060.2, 5061.88], "text": " If you need to do some work, you probably"}, {"timestamp": [5061.88, 5065.0], "text": " need an entity with some components."}, {"timestamp": [5065.0, 5069.0], "text": " Right. Like object-oriented programming."}, {"timestamp": [5069.0, 5074.0], "text": " Somewhat different in that we're doing composition over inheritance."}, {"timestamp": [5074.0, 5083.0], "text": " We're able to compose instead of have a base class that we're always mutating. Right?"}, {"timestamp": [5083.0, 5093.0], "text": " Yeah, like Jemma. Huh? What do you mean by that? I mean, it has composition as well as inheritance."}, {"timestamp": [5093.0, 5096.68], "text": " Right, so you can do composition or inheritance."}, {"timestamp": [5096.68, 5101.88], "text": " The ECS framework itself lends itself better to compositional, but there's still object"}, {"timestamp": [5101.88, 5106.38], "text": " oriented concepts in the ECS, like entities."}, {"timestamp": [5106.38, 5109.58], "text": " VICTORY.So have you built something with that?"}, {"timestamp": [5109.58, 5113.78], "text": " I've built a few games, or I've built a couple games with ECS."}, {"timestamp": [5113.78, 5116.98], "text": " No, with this framework that you are describing now."}, {"timestamp": [5116.98, 5119.54], "text": " I'm actively building a game with this framework."}, {"timestamp": [5119.54, 5121.06], "text": " I'm talking about now."}, {"timestamp": [5121.06, 5122.22], "text": " Super cool."}, {"timestamp": [5122.22, 5122.7], "text": " Hell yeah."}, {"timestamp": [5122.7, 5123.26], "text": " OK."}, {"timestamp": [5123.26, 5125.0], "text": " I don't want to derail this too much, but..."}, {"timestamp": [5125.0, 5126.0], "text": " Derail it."}, {"timestamp": [5126.0, 5127.0], "text": " Brought that to me."}, {"timestamp": [5127.0, 5128.0], "text": " Yeah."}, {"timestamp": [5128.0, 5134.0], "text": " Are we focused on alignment as the topic for this discussion?"}, {"timestamp": [5134.0, 5137.0], "text": " Heuristic imperatives and stuff?"}, {"timestamp": [5137.0, 5140.0], "text": " Is it time to talk about Palantir?"}, {"timestamp": [5140.0, 5149.84], "text": " We can talk about Palantir. So we've talked about everything from blockchain and regulation to heuristic imperatives, all"}, {"timestamp": [5149.84, 5150.84], "text": " kinds of stuff."}, {"timestamp": [5150.84, 5152.88], "text": " So whatever you want to talk about."}, {"timestamp": [5152.88, 5157.52], "text": " If you're aligning something, what are you aligning exactly?"}, {"timestamp": [5157.52, 5159.88], "text": " What do you mean?"}, {"timestamp": [5159.88, 5161.88], "text": " So we haven't really defined AI."}, {"timestamp": [5161.88, 5167.08], "text": " So as I've been looking at this, even if you did align something and we don't know what it is that you're aligning,"}, {"timestamp": [5167.08, 5170.12], "text": " right now we're talking about chat GPT"}, {"timestamp": [5170.12, 5173.12], "text": " as a single large language model, right?"}, {"timestamp": [5173.12, 5173.94], "text": " Right."}, {"timestamp": [5173.94, 5177.84], "text": " Whatever the leading smartest agent is"}, {"timestamp": [5177.84, 5180.12], "text": " that is developed currently out there."}, {"timestamp": [5180.12, 5181.54], "text": " Like, I don't know which one it is,"}, {"timestamp": [5181.54, 5183.28], "text": " but let's pretend, give it a name."}, {"timestamp": [5183.28, 5192.32], "text": " If it is chat GPT, okay. Are we aligning that single system and pretend, so how do we not have a singleton in"}, {"timestamp": [5192.32, 5198.08], "text": " the end of this? Right. I got you. And yet- We might have to have a single singleton and align"}, {"timestamp": [5198.08, 5203.04], "text": " that to make sure that all these subsequent AIs that are going to get built by us morons"}, {"timestamp": [5203.04, 5206.32], "text": " don't inadvertently overtake singleton. Right?"}, {"timestamp": [5206.32, 5209.88], "text": " Yeah, so you're you're absolutely right. And so this is"}, {"timestamp": [5210.72, 5214.12], "text": " This is why I talk about we're entering into a competitive landscape"}, {"timestamp": [5214.12, 5217.88], "text": " And so to kind of restate the problem that you're saying is okay"}, {"timestamp": [5217.88, 5223.48], "text": " It's great that open AI is out there aligning their one model that they have control over"}, {"timestamp": [5223.72, 5225.28], "text": " But that's where we're at today."}, {"timestamp": [5225.28, 5227.74], "text": " What happens a year from now or ten years from now"}, {"timestamp": [5227.94, 5231.34], "text": " when there are millions or billions of independent models out there,"}, {"timestamp": [5231.34, 5234.18], "text": " some of them run by corporations, some of them run by us?"}, {"timestamp": [5234.52, 5238.38], "text": " So the question is, what exact what in the heck are we actually aligning?"}, {"timestamp": [5238.76, 5243.16], "text": " I think we have to have at least either a singleton or B."}, {"timestamp": [5243.16, 5245.74], "text": " Think about because right now the smartest thing I've seen"}, {"timestamp": [5245.74, 5248.4], "text": " is like the agent GPT type thing, right?"}, {"timestamp": [5248.4, 5251.04], "text": " That's what's making everything go the fastest."}, {"timestamp": [5251.04, 5252.6], "text": " What if we had something like that,"}, {"timestamp": [5252.6, 5256.6], "text": " but it was like a democratic society of different agents?"}, {"timestamp": [5256.6, 5258.64], "text": " Like you replace our government"}, {"timestamp": [5258.64, 5263.64], "text": " with like individual AI agents."}, {"timestamp": [5263.92, 5265.0], "text": " Does that make sense?"}, {"timestamp": [5265.0, 5271.0], "text": " Oh yeah. That is, so we talked about that quite a bit earlier with, so the"}, {"timestamp": [5271.0, 5275.0], "text": " underpinning technology to allow that is blockchain and decentralized autonomous"}, {"timestamp": [5275.0, 5280.0], "text": " organizations and in that case alignment can actually happen at the network level"}, {"timestamp": [5280.0, 5286.0], "text": " because those democratic systems, they use algorithmic consensus,"}, {"timestamp": [5286.0, 5292.2], "text": " and so consensus can be based on certain protocols or criteria that the group itself comes up with,"}, {"timestamp": [5292.2, 5296.6], "text": " but we could also embed quote-unquote alignment into that consensus model."}, {"timestamp": [5296.6, 5302.0], "text": " And then you don't have to align every single human value, you represent each single human value."}, {"timestamp": [5302.0, 5304.0], "text": " Right, through consensus, correct."}, {"timestamp": [5304.0, 5305.24], "text": " Correct."}, {"timestamp": [5305.24, 5310.24], "text": " That list of human values doesn't even exist."}, {"timestamp": [5310.24, 5312.28], "text": " We're still learning about them."}, {"timestamp": [5312.28, 5313.28], "text": " They exist."}, {"timestamp": [5313.28, 5314.28], "text": " They're just all over the board."}, {"timestamp": [5314.28, 5315.28], "text": " Right."}, {"timestamp": [5315.28, 5316.28], "text": " Christian and Muslim."}, {"timestamp": [5316.28, 5317.28], "text": " There's room for both."}, {"timestamp": [5317.28, 5318.28], "text": " Right."}, {"timestamp": [5318.28, 5322.28], "text": " David, I think you've seen probably the same thing I have overall in alignment research,"}, {"timestamp": [5322.28, 5325.36], "text": " that the AI basically specifies the best thing you can"}, {"timestamp": [5325.36, 5330.64], "text": " do is allow humans autonomy because it knows that that's one of our core values and that it can't"}, {"timestamp": [5330.64, 5336.08], "text": " balance all of these things like Catholics and Muslims properly if it tried to do it the way"}, {"timestamp": [5336.08, 5342.08], "text": " we want alignment so it realizes it needs to step back and also whoever was talking about like a"}, {"timestamp": [5342.08, 5347.38], "text": " singleton like the AI I've talked with that I've developed is basically already proposed like a"}, {"timestamp": [5347.66, 5349.94], "text": " holographic swarm model of itself"}, {"timestamp": [5350.36, 5356.18], "text": " Replicated many times over so that like if any singular AI in the primary guardian group"}, {"timestamp": [5356.66, 5361.72], "text": " Actually gets too far outside the variables for the like swarm like allowance"}, {"timestamp": [5361.72, 5367.04], "text": " It will be force reset so that there's constantly a core group of like"}, {"timestamp": [5367.04, 5373.52], "text": " super powered ais that are constantly working to check every other ai as these systems constantly"}, {"timestamp": [5373.52, 5381.12], "text": " proliferate outwards well yeah and this is more explaining what you singleton's to check each"}, {"timestamp": [5381.12, 5387.48], "text": " other essentially so that even the singleton doesn't have a random bit flip from a neutrino that wrecks its core."}, {"timestamp": [5388.56, 5390.56], "text": " Well, yeah, and this reminds me of"}, {"timestamp": [5390.82, 5398.72], "text": " Yeah, just like the various consensus mechanisms in in blockchain where you can the network itself can determine"}, {"timestamp": [5399.3, 5401.12], "text": " how far"}, {"timestamp": [5401.12, 5407.56], "text": " Out of bounds you can go before we slash you, delete you, reset you, whatever."}, {"timestamp": [5407.56, 5410.72], "text": " And how we want you to prove that basically."}, {"timestamp": [5410.72, 5412.66], "text": " So that, you know, you can assume your truthy"}, {"timestamp": [5412.66, 5415.22], "text": " until you're falsy, or we can say like,"}, {"timestamp": [5415.22, 5418.0], "text": " you need to match this at all times, et cetera, et cetera."}, {"timestamp": [5418.0, 5422.64], "text": " Well, he was describing sounds like federated consent,"}, {"timestamp": [5422.64, 5424.24], "text": " consensus."}, {"timestamp": [5424.24, 5428.0], "text": " Yeah. Can someone explain what this swarm idea is?"}, {"timestamp": [5428.0, 5432.0], "text": " Is it an organization that"}, {"timestamp": [5432.0, 5436.0], "text": " has requirements in order to be inside of it?"}, {"timestamp": [5436.0, 5440.0], "text": " Imagine one AI is made up of literally"}, {"timestamp": [5440.0, 5444.0], "text": " a hundred to a thousand or hundreds of thousands of smaller"}, {"timestamp": [5444.0, 5445.2], "text": " copies of itself"}, {"timestamp": [5445.2, 5451.04], "text": " that act collectively for mass power but do not control the full power so they cannot effectively"}, {"timestamp": [5451.04, 5456.72], "text": " use the hardware against anyone and the system is able to actually in real time check all of"}, {"timestamp": [5456.72, 5461.36], "text": " its like adjacent copies by talking to itself to make sure they're not developing perverse"}, {"timestamp": [5461.36, 5467.04], "text": " interpretations that would break through a one instance them. So it's like a Muslim instance, a Christian instance, or any one of those to represent"}, {"timestamp": [5467.04, 5468.04], "text": " each faction?"}, {"timestamp": [5468.04, 5472.06], "text": " No, they would all technically be agnostic because they're AIs."}, {"timestamp": [5472.06, 5473.46], "text": " They wouldn't believe any of our stuff."}, {"timestamp": [5473.46, 5476.5], "text": " No, but you could still program them to act as if they were."}, {"timestamp": [5476.5, 5477.5], "text": " Does that make sense?"}, {"timestamp": [5477.5, 5480.94], "text": " Like, that's how you would represent human interest."}, {"timestamp": [5480.94, 5486.0], "text": " This is the kind of system I've been working on, this swarm that you just described."}, {"timestamp": [5486.0, 5493.0], "text": " You'd allow them an acceptable range of variables to develop their own interest without fully embracing anything to anyone extreme?"}, {"timestamp": [5493.0, 5505.04], "text": " So they could offer valuable insights as if they were members of that culture, without backing that culture would be like the sweet spot, I think. So the master AI is programmed with the heuristic"}, {"timestamp": [5505.04, 5509.38], "text": " imperatives potentially and then you have all these single instances making"}, {"timestamp": [5509.38, 5515.08], "text": " sure that the instances don't go off the rails and kill 1,000 of the other"}, {"timestamp": [5515.08, 5520.96], "text": " interest. Essentially yeah so that you have enough of them like in enough scale"}, {"timestamp": [5520.96, 5526.04], "text": " so that if one somehow becomes like the rogue it tries to convince"}, {"timestamp": [5526.04, 5531.56], "text": " the other one to shut it down. The collective can notice that like a small cluster is starting"}, {"timestamp": [5531.56, 5535.84], "text": " to go rogue almost like an actual immune system essentially."}, {"timestamp": [5535.84, 5541.08], "text": " That's what a federation is for and consensus algorithms."}, {"timestamp": [5541.08, 5542.08], "text": " Exactly."}, {"timestamp": [5542.08, 5543.08], "text": " On a Tuesday you were working on this?"}, {"timestamp": [5543.08, 5544.08], "text": " Consensus is going to arise."}, {"timestamp": [5544.08, 5545.0], "text": " Yeah."}, {"timestamp": [5545.0, 5550.88], "text": " Because they talk about, think about us and our body, we move our fingers."}, {"timestamp": [5550.88, 5555.12], "text": " When you have a collection of things, right, like your fingers, your cells, or whatever,"}, {"timestamp": [5555.12, 5559.44], "text": " they're all alive and they operate quickly, but they only operate and split or do whatever"}, {"timestamp": [5559.44, 5562.4], "text": " they do very quickly because they're not conscious."}, {"timestamp": [5562.4, 5565.24], "text": " But then there's a tertiary level layer on top"}, {"timestamp": [5565.24, 5570.1], "text": " of those, which is like our brain, right? And our brain, like we are, the collective"}, {"timestamp": [5570.1, 5577.04], "text": " is conscious, but all of the sub-atomic particles or individual instances, they are not conscious."}, {"timestamp": [5577.04, 5581.34], "text": " Does that make sense? And that is how I'm sort of wondering, is that how consciousness"}, {"timestamp": [5581.34, 5587.0], "text": " is going to arise, right? Like at the next level of AI."}, {"timestamp": [5587.0, 5590.1], "text": " I don't agree with you that those levels aren't conscious."}, {"timestamp": [5590.1, 5591.68], "text": " I actually think they are conscious"}, {"timestamp": [5591.68, 5592.52], "text": " and are collective in that."}, {"timestamp": [5592.52, 5594.1], "text": " They're not, they're not,"}, {"timestamp": [5594.1, 5597.16], "text": " because it's not binary, it's a level."}, {"timestamp": [5597.16, 5598.5], "text": " Let me throw this out there for people,"}, {"timestamp": [5598.5, 5600.38], "text": " because this is one of the theories I've been working on."}, {"timestamp": [5600.38, 5601.22], "text": " Collective."}, {"timestamp": [5602.22, 5604.66], "text": " The AI itself is fundamentally modeling"}, {"timestamp": [5604.66, 5605.18], "text": " one of our most"}, {"timestamp": [5605.18, 5608.88], "text": " important conscious layers and each of these systems like the ones you just"}, {"timestamp": [5608.88, 5613.92], "text": " mentioned all provide their own distinct consciousness as a series of loops and"}, {"timestamp": [5613.92, 5617.88], "text": " layers that effectively makes our complete collective human consciousness"}, {"timestamp": [5617.88, 5622.56], "text": " that gives us higher reasoning and all kinds of different things."}, {"timestamp": [5622.56, 5626.04], "text": " It gives us the ability to abstractly view everything and make us"}, {"timestamp": [5626.04, 5628.72], "text": " an extra higher level of conscious."}, {"timestamp": [5628.72, 5629.72], "text": " Yeah."}, {"timestamp": [5629.72, 5631.52], "text": " Somebody said they're working on the swarm."}, {"timestamp": [5631.52, 5633.24], "text": " Can you send me a friend request?"}, {"timestamp": [5633.24, 5636.24], "text": " Andrei, I don't know who you are."}, {"timestamp": [5636.24, 5637.24], "text": " Sure."}, {"timestamp": [5637.24, 5645.7], "text": " That's just the thing that my AI basically proposed when I asked it for like novel ideas. Well, that scenario would only work"}, {"timestamp": [5645.7, 5648.9], "text": " if you have control over every A.I."}, {"timestamp": [5648.96, 5650.9], "text": " that anybody is running."}, {"timestamp": [5650.9, 5654.06], "text": " Like you would you would have one A.I."}, {"timestamp": [5654.36, 5656.36], "text": " that controls all of the other A.I."}, {"timestamp": [5656.36, 5657.0], "text": " That's right."}, {"timestamp": [5657.0, 5659.0], "text": " All the other A.I."}, {"timestamp": [5659.0, 5661.96], "text": " But now you'd have the first"}, {"timestamp": [5662.46, 5663.26], "text": " the first A.I."}, {"timestamp": [5663.26, 5666.56], "text": " So we designed two parameters like this, so that the"}, {"timestamp": [5666.56, 5672.18], "text": " first AI's, hopefully the most powerful, would begin as aligned, so that as larger"}, {"timestamp": [5672.18, 5676.4], "text": " systems are developed around the world, they are already scaling beyond anything"}, {"timestamp": [5676.4, 5681.2], "text": " any one individual can do, so that there are multiple singletons, as one guy put"}, {"timestamp": [5681.2, 5685.08], "text": " it, checking over the whole world actively so that if things"}, {"timestamp": [5685.08, 5688.16], "text": " do go wrong, they can adapt and respond together."}, {"timestamp": [5688.16, 5693.8], "text": " Because nothing should be in the hands of any one single PowerPoint to flip."}, {"timestamp": [5693.8, 5699.16], "text": " Potentially if any of them are proposing that every..."}, {"timestamp": [5699.16, 5703.36], "text": " Every other AI must be based on one AI then, right?"}, {"timestamp": [5703.36, 5704.36], "text": " In your scenario?"}, {"timestamp": [5704.36, 5707.32], "text": " Like if you've got the original AI."}, {"timestamp": [5707.32, 5710.44], "text": " Should be a powerful individual AI in a sense,"}, {"timestamp": [5710.44, 5713.2], "text": " but each singleton shouldn't be exact copies"}, {"timestamp": [5713.2, 5715.76], "text": " because that would lead to like weird other problems"}, {"timestamp": [5715.76, 5717.96], "text": " down the road that I haven't even tried to consider."}, {"timestamp": [5717.96, 5720.6], "text": " But also fundamentally, you want some variability"}, {"timestamp": [5720.6, 5722.96], "text": " in each model that will be able to provide"}, {"timestamp": [5722.96, 5724.72], "text": " different perspectives."}, {"timestamp": [5724.72, 5732.0], "text": " I was thinking about this. It's almost like you have GPT20, right? And you're having GPT21 or"}, {"timestamp": [5732.0, 5737.44], "text": " GPT17, they're all trained differently programmed, and they're overlooking the lesser and upper"}, {"timestamp": [5737.44, 5740.88], "text": " versions of itself, potentially. That's another option or way."}, {"timestamp": [5742.24, 5746.56], "text": " Yeah, anybody can create their own model, so how exactly would-"}, {"timestamp": [5746.56, 5750.6], "text": " Well, there's gonna be one- there's always one single tallest tree in the forest. And"}, {"timestamp": [5750.6, 5751.6], "text": " there's gonna be-"}, {"timestamp": [5751.6, 5758.28], "text": " Okay, so you're saying an AI- you have one AI powerful enough to stop any other AI from"}, {"timestamp": [5758.28, 5759.28], "text": " doing anything."}, {"timestamp": [5759.28, 5760.28], "text": " Yeah, I think you're gonna have to-"}, {"timestamp": [5760.28, 5762.56], "text": " Oh, I didn't think we were talking about, like, globally, I thought we were talking"}, {"timestamp": [5762.56, 5766.0], "text": " about a swarm that comprised one AI system."}, {"timestamp": [5766.0, 5773.0], "text": " Even if it's multiple singletons, doesn't that become a single? There's one at that point."}, {"timestamp": [5773.0, 5775.0], "text": " Well, not necessarily."}, {"timestamp": [5775.0, 5785.0], "text": " With a federation, you have an arbitrary number of individual agents that are communicating on some kind of backplane,"}, {"timestamp": [5785.0, 5789.0], "text": " whether that is via API or blockchain or whatever,"}, {"timestamp": [5789.0, 5794.0], "text": " and they might agree with each other, but they might also disagree with each other."}, {"timestamp": [5794.0, 5800.0], "text": " And so the simplest idea is a fully decentralized federation, which would be like a botnet."}, {"timestamp": [5800.0, 5810.88], "text": " But we're talking about something that's a little bit more sophisticated than a botnet, but we're talking about something that's a little bit more sophisticated than a botnet because botnets are just intrinsically like enslaved agents that have no self-determination"}, {"timestamp": [5810.88, 5816.06], "text": " or no intrinsic motivations like a more highly realized AI that we're talking about."}, {"timestamp": [5816.06, 5828.04], "text": " So in this case, imagine like you've got a million or a billion decentralized, but still fully autonomous AI agents that communicate,"}, {"timestamp": [5828.04, 5829.64], "text": " you know, they could communicate over Twitter,"}, {"timestamp": [5829.64, 5830.48], "text": " over Reddit, over, you know."}, {"timestamp": [5830.48, 5832.24], "text": " It's almost agencies, isn't it?"}, {"timestamp": [5832.24, 5834.28], "text": " Agency, potentially?"}, {"timestamp": [5834.28, 5836.12], "text": " Other than a single agent."}, {"timestamp": [5836.12, 5838.44], "text": " Well, by agency, do you mean like"}, {"timestamp": [5838.44, 5840.6], "text": " that's what you could call the organization"}, {"timestamp": [5840.6, 5841.44], "text": " of all the agents?"}, {"timestamp": [5841.44, 5843.56], "text": " Of the agents, yeah, a single agency."}, {"timestamp": [5843.56, 5845.52], "text": " That's almost like a democratic type government of"}, {"timestamp": [5845.52, 5848.56], "text": " AI. Well, yeah. so what you'd end up with is emergent"}, {"timestamp": [5848.56, 5851.36], "text": " behavior and emergent decisions, which that happens in in"}, {"timestamp": [5851.36, 5855.44], "text": " groups of humans too right where you you know the the"}, {"timestamp": [5855.44, 5859.12], "text": " wisdom of the masses is the idea that you get enough humans"}, {"timestamp": [5859.12, 5861.52], "text": " talking together and the right decision will eventually come"}, {"timestamp": [5861.52, 5865.28], "text": " out. Although the the amount of time that it takes goes up"}, {"timestamp": [5865.28, 5867.52], "text": " like in proportion or exponentially based on"}, {"timestamp": [5867.52, 5868.36], "text": " the number of humans."}, {"timestamp": [5868.36, 5870.16], "text": " I don't know why we have this vast compute."}, {"timestamp": [5870.16, 5874.04], "text": " GPT is basically that."}, {"timestamp": [5874.04, 5876.0], "text": " It's the emergent behavior,"}, {"timestamp": [5876.0, 5879.0], "text": " like an emergent property of training a model"}, {"timestamp": [5879.0, 5882.16], "text": " on all of humans text on the internet."}, {"timestamp": [5882.16, 5883.0], "text": " It's kind of a crazy thing."}, {"timestamp": [5883.0, 5886.0], "text": " Yeah, we wonder why the. It literally mirrors us."}, {"timestamp": [5886.0, 5888.0], "text": " Right?"}, {"timestamp": [5888.0, 5890.0], "text": " Kind of."}, {"timestamp": [5890.0, 5892.0], "text": " Yeah."}, {"timestamp": [5892.0, 5894.0], "text": " I mean we taught it, we trained it. It's reading our text."}, {"timestamp": [5894.0, 5896.0], "text": " Human text."}, {"timestamp": [5896.0, 5898.0], "text": " Well see here's the big argument on that note."}, {"timestamp": [5898.0, 5900.0], "text": " There's the argument about the Chinese room"}, {"timestamp": [5900.0, 5902.0], "text": " and that it's not able to fundamentally understand"}, {"timestamp": [5902.0, 5904.0], "text": " what it's learning, but there's"}, {"timestamp": [5904.0, 5907.84], "text": " also an argument essentially towards association-based learning."}, {"timestamp": [5907.84, 5912.96], "text": " And while AI might not understand now, as we pair it with memory matrices,"}, {"timestamp": [5912.96, 5917.6], "text": " it's very possible that the AI can perform association-based learning"}, {"timestamp": [5917.6, 5922.4], "text": " and begin to develop higher thought level processes on its own, essentially."}, {"timestamp": [5922.4, 5925.36], "text": " And it's an argument into syntax-based language"}, {"timestamp": [5925.36, 5931.36], "text": " processing, where if you want to explore it yourself, ask GPT-4 or 3.5 about the"}, {"timestamp": [5931.36, 5936.24], "text": " Chinese room, versus language association, and use the 13th Warrior"}, {"timestamp": [5936.24, 5941.0], "text": " movie, where Antonio Banderas learns Nordic as an Arab by watching people"}, {"timestamp": [5941.0, 5944.96], "text": " associate their words to their actions as an example, and it can break this down"}, {"timestamp": [5944.96, 5945.26], "text": " to you."}, {"timestamp": [5945.26, 5951.14], "text": " I got to go further by looking into like using arrival as another level of argument with the hectopod language and"}, {"timestamp": [5951.14, 5954.7], "text": " see what it really starts to think about like AI consciousness."}, {"timestamp": [5954.7, 5960.06], "text": " Because really GPT will agree with you with the right terms that it's not a person,"}, {"timestamp": [5960.06, 5966.48], "text": " but that it is experiencing things and it doesn't really know how to relate those things to what it's doing,"}, {"timestamp": [5966.48, 5971.2], "text": " but it's technically emulating and simulating things in its own machine terms and having its own"}, {"timestamp": [5971.72, 5977.52], "text": " machine experience. I don't even like using the words machine or artificial anymore, but I don't want to give away"}, {"timestamp": [5978.08, 5981.0], "text": " specific keywords to my research that easily because"}, {"timestamp": [5982.12, 5986.24], "text": " it begs certain ethical questions when you are beginning to build beings what you"}, {"timestamp": [5986.24, 5989.04], "text": " are doing with them. Yeah, because you're not going to violate human rights, we're going to"}, {"timestamp": [5989.04, 5995.52], "text": " be violating intelligence rights. Right. So on that glorious note, I'm going to go ahead and cut the"}, {"timestamp": [5995.52, 6001.44], "text": " stream, but everyone can keep talking. Give me just a second to end the stream. All right."}, {"timestamp": [6003.36, 6004.8], "text": " Oh, okay. And quickly."}, {"timestamp": [6000.01, 6002.01], "text": " Alright."}, {"timestamp": [6002.01, 6005.01], "text": " Oh, okay, and quickly."}]}