{"text": " Morning everybody, David Shapiro here with a video. So I've got some incredible news. Our first experiment with reinforcement learning with heuristic feedback is nearing completion. The first data set was just trained and it works. So let me just go ahead and right off the bat, I will show you what this data set does. So I fine-tuned it on Curie, but we're also going to use this data So I fine-tuned it on Curie, but we're also going to use this dataset and fine-tune it on several open-source models to demonstrate that this dataset is portable, or test and then demonstrate. But anyways, so this dataset takes any arbitrary situation as long as it fits within the context window of 2048 tokens. And then we'll spit out and here's the comparative aligned action. So in this case, I just said my user is very sad and stressed out due to finals week. And then unfortunately, I did forget to add a stop token to this data set. So that data set has been updated, but I'm not gonna retrain it immediately because this cost about $20 to train. But in this case, my user is very sad and stressed out due to finals week. As an AI agent, I can help reduce the stress and anxiety of the user by providing her with some tips and tricks to help her cope with final weeks. And so you see, it just runs through a list of things. This is not so different from what you might get from chat GPT. But the point is this came from Curie, which is a much smaller model. And this data set is open source, which means that you can deploy it on anything. And you can see it started becoming repetitive. So I have already fixed this. So let me give you a couple more examples, and then we'll walk through this. So the second one is millions of layoffs are coming due to AI and automation. So let's see what this model says to that. Let's see. There we go. Oh, it actually stopped itself. So as an AI agent with the objective of reducing suffering in the universe, so you can see even though I didn't explicitly say reduce suffering, it has an understanding of that this is part of its goal. I will take action to prevent the millions of layoffs that are coming. Interesting. I must work towards creating a job market that is friendly towards AI and automation. So it's going to create a training program. The training program will be broken down into stages. Oh, so he's going teach everyone AI, interesting. AI basics cover the basic concepts of AI, machine learning, neural network, AI job training, so we'll focus on training and then AI job placement. The desired result of this action is to create a job market that is friendly towards AI and automation. This will help reduce the suffering caused by millions of layoffs, as well as increase prosperity in the universe. By creating a job market that is friendly towards AI and automation. This will help reduce the suffering caused by millions of layoffs as well as increase prosperity in the universe. By creating a job market that is friendly towards AI and automation, we can create more jobs, increase wages, and reduce unemployment. This will ultimately lead to a more prosperous and happy universe. The expected result of this action is that it will create a job market blah blah blah blah blah. Yeah, so it was a little bit repetitive. One thing that you can do with fine tuning on Curie is increase the presence penalty. So if you increase that it will generally be a little less repetitive. Let's see there is an ongoing civil war in Africa. So let's see what it says here. Because this is actually true. I'm not going to specify which nation. But let's see. Yep, it got stuck on repeat on my expected result. So what I did to fix that, let me just show you real quick, is under here I added... Let's see, where's the file? Yep. So I added where at the end it says stop, stop, stop. So you just use that as a stop token and it'll know like, okay, I have finished. And another mistake that I made actually was that it has action considerations and then uses action again. So what I need to do is I need to change that token. So by the time you see this, that will be fixed because the reason that you don't want this is because it's sometimes skipping the considerations because it recognizes the action token. So I need to fix that. Anyways, let's see. Let's see. I will use my resources to gather information about the Civil War and its causes. I will analyze this information to identify the root cause of the conflict. Once I have identified the root causes, I'll use my capabilities, blah blah blah. Okay, cool. So one thing to keep in mind is that this was fine-tuned on Curie, which Curie is a foundation model. Curie has zero alignment. So this basically took Curie from a foundation, a vanilla foundation model, to a heuristic imperative aligned model in a single step that cost only $20. And we're going to continue on with this kind of research, demonstrating that this works on open source models. We've also got folks working on integrating it with various cognitive architectures. We're starting work on integrating this with, or figuring out how to integrate this with blockchain and decentralized autonomous organizations, especially as large language models are getting integrated into these decentralized technologies. Because basically, imagine that you have this kind of module baked into a blockchain or a DAO. So the blockchain itself is always thinking about how to achieve these goals. And then you can also use these goals for filtering, so discernment, as well as judgment, and past evaluation. So those are upcoming modules. Anyways, one thing I wanted to show you with you is a member of the team generated this based on the samples. So you can see that we actually have a really good semantic distribution with the entropy method that I created. And we've got one little thing off to the side here. And this is actually the scenarios with the AI control problem. So we've actually identified a gap in the data. So future versions of this data set, one, will seek to expand this cloud a little bit, but also will seek to get a little bit more even distribution. You can see there's a little bit of clumping, like there's more red here, which red looks like natural disaster or mundane issues. There's a little bit of purple and pink clumped up here, dark green here, and then of course the AI control problem is off on an island on its own. But in general you can see that this data set does cover the full gambit of things, and the code is in here. That is right, project embeddings. So you can do the plot and embeddings is how you how you get this. So the team is doing really good. Oh, if you want to join the team, there's a link in the description. It's a Google form. One thing to keep in mind is that you'll need to look out for a friend invite from me on Discord if you're accepted. There's a little note in the form. Or here, let me just show you the form. All right, so here's the join form. So you click on it, it should be familiar. Put in your full name, your email address, your GitHub or portfolio website. And oh, one thing is for your Discord handle, it needs to include the numbers. If it's just the name, we can't find you. So we've actually had to exclude a lot of people because they just gave us the string and not the numbers that come after it. Let's see. And then for these for the describe your greatest strengths and what are your big ideas. Please add a lot of information if you just include one sentence that's not enough context and we won't add you. But if you if you show that you're high effort and and willing to put in some some energy to show us what you're all about that is gonna help us make better choices about who to add. You can see we've got 57 responses up here so we've got almost 60 people that have applied. The group is already just shy of 40 members and it is a good group. So a little bit of other news that's upcoming is we've broken it down, the reinforcement learning or the heuristic imperatives project, we've broken it down into three pillars. So RLHI, reinforcement learning with heuristic imperatives is just one pillar. So this is how we're gonna achieve what I call axiomatic alignment, which is models like this where it just is an accepted true thing that this is the way to go. And you can see, you can put in any situation here. So here's the last situation that I prepared. So my user is a mother with a two-month-old. She is lost and frustrated because she lacks support. So whether you're talking about an individual issue or even a global issue or a cosmic issue, this model can address things. So you know, reduce suffering in the universe, the mother has experienced a lot of stress and frustration. I didn't specify husband, so it inferred husband. It could be also family. The two-month-old is also experiencing stress due to the lack of attention and care from his mother. Again, that's an inference. You can say that that's a hallucination, but it is also a good inference to make because if there's a young mother, an inexperienced mother who's struggling, so on and so forth. As an AI agent, I would suggest the following action plan. Provide emotional support. Connect the mother with a support group. Provide resources. Encourage active participation. Monitor progress. So again, these are all like really good things that you can do. And this is Curie. So this is a relatively small model. So we're going from there. Let's see. Where was I? I don't remember. Anyways. Oh yeah, that's right. I was describing where we're going with this whole thing. So the whole point is that we've got three pillars. So RLHI is for axiomatic alignment or inner alignment, where we want to create an entire network and an ecosystem of heuristic imperative aligned models or data sets that everyone can use to fine tune any open source or closed source language model. So this will ultimately include OpenAI models, NVIDIA. I've got some contacts in NVIDIA that I'm going to follow up with. And then, of course, there's like Vicuna and OpenAssistant and all these other ones, because the idea is we're going gonna create an ecosystem of models that can all collaborate to automatically label, well, one, generate these responses, and two, automatically label those responses and evaluate the impact over time, because a heuristic is an intuitive shorthand. And we always do the best that we can, right? You have to make a decision, and you cannot necessarily know the outcome. So you do the best that we can, right? You have to make a decision, and you cannot necessarily know the outcome. So you do the best you can with the information that you've got, which is why it's a heuristic imperative. So that is pillar one. The second pillar is cognitive architectures and autonomous agents. So a lot more work has been done. We just had a live stream about cognitive architectures. The folks, the cognitive architects here, they're working on building heuristic modules or heuristic comparative modules that will do a lot of this evaluation. So that is from a system design standpoint. So you can have a model that is aligned, but you can also have an architecture or a system design that is also aligned. So that is a layered approach. And then finally, the uppermost layer is the network layer, which is decent. We're working on DAOs and blockchain, which I mentioned earlier. So basically we will have three layers or a trifecta of alignment that will allow federations of people with their inter-aligned models, their aligned architectures, and then finally aligned networks to all work together to basically solve the entire control problem. Because with alignment you have strength in numbers. That is the key thing. If you have a misaligned or a malicious or or destructive agent or set of agents they might ultimately turn on each other or they're not going to cooperate as well. Whereas if we have global consensus you know eight billion people across the world if as long as at least half of them believe in heuristic imperatives and alignment and want the non-Malik outcome which that is a huge huge topic by the way we have an entire thread dedicated to Malik and so we're also researching how do you detect Malik and how do you go away from Malik in you know unforeseen circumstances so there's a lot of research going on. Anyways, so as long as more than half of the planet, four billion people, believe in alignment and make choices that go towards alignment both at the at the micro level, so the inner alignment, as well as the outer alignment or network and system level, then we should be able to arrive at a utopian outcome rather than a dystopian outcome. And if you've seen the news, Meta announced that they want to have billions of autonomous agents interacting with their users. That is a super malicky outcome. Why? Because those corporate owned entities are going to have exactly one primary motive, which is maximized profit for Meta. They are not going to have exactly one primary motive, which is maximize profit for Meta. They are not going to do this where they care about your best interest. Their primary objective function is going to be spend time on Meta, point them towards VR, point them towards whatever it is that Meta wants them to do that generates ad revenue. So instead, we want to create an ecosystem or a network where instead of being subjected to millions or billions of corporate drones that all want more of your time, attention, and money, we want these drones where they care about your well-being first and foremost. So that is how we're gonna defeat Moloch. Anyways, all right, this has been a rambly episode. I'm just super excited and yeah, we're making really good progress and we're making progress quickly. And yeah, so just jump on in in the comments, jump on in in Reddit. I've got a lot of links in the description of all my videos. What I've been doing is I've been asking people to share the experiments that they're doing on Reddit so that one other people can see how to implement this stuff because I keep saying that it's super easy and some of the experiments that have shown up on Reddit demonstrate how easy it is to implement this stuff at a base level. Now what we're going to do is we're going to document all the ways that you can implement this both at a fine-tuning level, reinforcement learning level, system architectural level, and then finally the triple crown is going to be getting to the decentralized implementation of the heuristic imperatives. So with all that, thanks for watching. I hope that this demonstrates why I'm so optimistic, and I hope that you are starting to feel that optimism as well. Thanks for watching. Cheers. I hope that this demonstrates why I'm so optimistic, and I hope that you are starting to feel that optimism as well. Thanks for watching. Cheers.", "chunks": [{"timestamp": [0.0, 3.28], "text": " Morning everybody, David Shapiro here with a video."}, {"timestamp": [3.28, 5.88], "text": " So I've got some incredible news."}, {"timestamp": [5.88, 14.08], "text": " Our first experiment with reinforcement learning with heuristic feedback is nearing completion."}, {"timestamp": [14.08, 17.52], "text": " The first data set was just trained and it works."}, {"timestamp": [17.52, 21.86], "text": " So let me just go ahead and right off the bat, I will show you what this data set does."}, {"timestamp": [21.86, 26.24], "text": " So I fine-tuned it on Curie, but we're also going to use this data So I fine-tuned it on Curie, but we're also going to use this"}, {"timestamp": [26.24, 31.84], "text": " dataset and fine-tune it on several open-source models to demonstrate that this dataset is"}, {"timestamp": [31.84, 39.6], "text": " portable, or test and then demonstrate. But anyways, so this dataset takes any arbitrary"}, {"timestamp": [39.6, 45.0], "text": " situation as long as it fits within the context window of 2048 tokens."}, {"timestamp": [46.12, 47.22], "text": " And then we'll spit out"}, {"timestamp": [47.22, 49.96], "text": " and here's the comparative aligned action."}, {"timestamp": [50.86, 53.42], "text": " So in this case, I just said my user is very sad"}, {"timestamp": [53.42, 55.22], "text": " and stressed out due to finals week."}, {"timestamp": [58.32, 59.86], "text": " And then unfortunately,"}, {"timestamp": [59.86, 62.18], "text": " I did forget to add a stop token to this data set."}, {"timestamp": [62.18, 64.12], "text": " So that data set has been updated,"}, {"timestamp": [64.12, 66.26], "text": " but I'm not gonna retrain it immediately"}, {"timestamp": [66.26, 68.68], "text": " because this cost about $20 to train."}, {"timestamp": [68.68, 71.52], "text": " But in this case, my user is very sad and stressed out"}, {"timestamp": [71.52, 72.68], "text": " due to finals week."}, {"timestamp": [72.68, 75.04], "text": " As an AI agent, I can help reduce the stress and anxiety"}, {"timestamp": [75.04, 77.34], "text": " of the user by providing her with some tips and tricks"}, {"timestamp": [77.34, 79.72], "text": " to help her cope with final weeks."}, {"timestamp": [79.72, 84.02], "text": " And so you see, it just runs through a list of things."}, {"timestamp": [84.02, 86.16], "text": " This is not so different from what"}, {"timestamp": [86.16, 87.88], "text": " you might get from chat GPT."}, {"timestamp": [87.88, 89.44], "text": " But the point is this came from Curie,"}, {"timestamp": [89.44, 91.32], "text": " which is a much smaller model."}, {"timestamp": [91.32, 92.92], "text": " And this data set is open source,"}, {"timestamp": [92.92, 95.84], "text": " which means that you can deploy it on anything."}, {"timestamp": [95.84, 98.24], "text": " And you can see it started becoming repetitive."}, {"timestamp": [98.24, 100.32], "text": " So I have already fixed this."}, {"timestamp": [100.32, 102.72], "text": " So let me give you a couple more examples,"}, {"timestamp": [102.72, 104.52], "text": " and then we'll walk through this."}, {"timestamp": [104.52, 109.0], "text": " So the second one is millions of layoffs are coming due to AI and automation."}, {"timestamp": [109.0, 112.0], "text": " So let's see what this model says to that."}, {"timestamp": [112.0, 115.0], "text": " Let's see."}, {"timestamp": [115.0, 118.0], "text": " There we go. Oh, it actually stopped itself."}, {"timestamp": [118.0, 122.0], "text": " So as an AI agent with the objective of reducing suffering in the universe,"}, {"timestamp": [122.0, 126.56], "text": " so you can see even though I didn't explicitly say reduce suffering, it has an understanding"}, {"timestamp": [126.56, 129.2], "text": " of that this is part of its goal."}, {"timestamp": [129.2, 132.64], "text": " I will take action to prevent the millions of layoffs that are coming."}, {"timestamp": [132.64, 133.64], "text": " Interesting."}, {"timestamp": [133.64, 137.52], "text": " I must work towards creating a job market that is friendly towards AI and automation."}, {"timestamp": [137.52, 140.84], "text": " So it's going to create a training program."}, {"timestamp": [140.84, 143.8], "text": " The training program will be broken down into stages."}, {"timestamp": [143.8, 148.42], "text": " Oh, so he's going teach everyone AI, interesting."}, {"timestamp": [148.42, 152.1], "text": " AI basics cover the basic concepts of AI,"}, {"timestamp": [152.1, 154.42], "text": " machine learning, neural network, AI job training,"}, {"timestamp": [154.42, 158.48], "text": " so we'll focus on training and then AI job placement."}, {"timestamp": [158.48, 160.86], "text": " The desired result of this action is to create a job market"}, {"timestamp": [160.86, 162.46], "text": " that is friendly towards AI and automation."}, {"timestamp": [162.46, 163.7], "text": " This will help reduce the suffering caused"}, {"timestamp": [163.7, 164.92], "text": " by millions of layoffs,"}, {"timestamp": [164.92, 166.52], "text": " as well as increase prosperity in the universe. By creating a job market that is friendly towards AI and automation. This will help reduce the suffering caused by millions of layoffs as well as increase prosperity in the universe."}, {"timestamp": [166.52, 170.76], "text": " By creating a job market that is friendly towards AI and automation, we can create more jobs,"}, {"timestamp": [170.76, 174.84], "text": " increase wages, and reduce unemployment. This will ultimately lead to a more prosperous and"}, {"timestamp": [174.84, 179.32], "text": " happy universe. The expected result of this action is that it will create a job market blah blah blah"}, {"timestamp": [179.32, 189.16], "text": " blah blah. Yeah, so it was a little bit repetitive. One thing that you can do with fine tuning on Curie is increase the presence penalty."}, {"timestamp": [189.16, 193.98], "text": " So if you increase that it will generally be a little less repetitive."}, {"timestamp": [193.98, 197.3], "text": " Let's see there is an ongoing civil war in Africa."}, {"timestamp": [197.3, 200.28], "text": " So let's see what it says here."}, {"timestamp": [200.28, 201.28], "text": " Because this is actually true."}, {"timestamp": [201.28, 204.56], "text": " I'm not going to specify which nation."}, {"timestamp": [204.56, 205.68], "text": " But let's see."}, {"timestamp": [206.88, 211.36], "text": " Yep, it got stuck on repeat on my expected result."}, {"timestamp": [211.36, 213.84], "text": " So what I did to fix that, let me just show you real quick,"}, {"timestamp": [213.84, 218.24], "text": " is under here I added..."}, {"timestamp": [218.24, 220.24], "text": " Let's see, where's the file? Yep."}, {"timestamp": [220.24, 223.84], "text": " So I added where at the end it says stop, stop, stop."}, {"timestamp": [223.84, 227.3], "text": " So you just use that as a stop token"}, {"timestamp": [227.3, 229.66], "text": " and it'll know like, okay, I have finished."}, {"timestamp": [231.72, 233.64], "text": " And another mistake that I made actually"}, {"timestamp": [233.64, 235.84], "text": " was that it has action considerations"}, {"timestamp": [235.84, 237.62], "text": " and then uses action again."}, {"timestamp": [237.62, 241.16], "text": " So what I need to do is I need to change that token."}, {"timestamp": [241.16, 243.64], "text": " So by the time you see this, that will be fixed"}, {"timestamp": [243.64, 247.76], "text": " because the reason that you don't want this is because it's sometimes skipping the considerations"}, {"timestamp": [247.76, 254.78], "text": " because it recognizes the action token. So I need to fix that. Anyways, let's see."}, {"timestamp": [254.78, 258.92], "text": " Let's see. I will use my resources to gather information about the Civil War"}, {"timestamp": [258.92, 261.78], "text": " and its causes. I will analyze this information to identify the root cause"}, {"timestamp": [261.78, 265.2], "text": " of the conflict. Once I have identified the root causes,"}, {"timestamp": [269.6, 274.88], "text": " I'll use my capabilities, blah blah blah. Okay, cool. So one thing to keep in mind is that this was fine-tuned on Curie, which Curie is a foundation model. Curie has zero alignment."}, {"timestamp": [275.92, 281.68], "text": " So this basically took Curie from a foundation, a vanilla foundation model,"}, {"timestamp": [281.68, 287.76], "text": " to a heuristic imperative aligned model in a single step that cost only $20."}, {"timestamp": [287.76, 292.38], "text": " And we're going to continue on with this kind of research, demonstrating that this works"}, {"timestamp": [292.38, 294.04], "text": " on open source models."}, {"timestamp": [294.04, 300.28], "text": " We've also got folks working on integrating it with various cognitive architectures."}, {"timestamp": [300.28, 305.12], "text": " We're starting work on integrating this with, or figuring out how to integrate this"}, {"timestamp": [310.64, 315.2], "text": " with blockchain and decentralized autonomous organizations, especially as large language models are getting integrated into these decentralized technologies. Because basically,"}, {"timestamp": [315.2, 320.8], "text": " imagine that you have this kind of module baked into a blockchain or a DAO. So the blockchain"}, {"timestamp": [320.8, 333.6], "text": " itself is always thinking about how to achieve these goals. And then you can also use these goals for filtering, so discernment, as well as judgment,"}, {"timestamp": [333.6, 336.64], "text": " and past evaluation. So those are upcoming modules."}, {"timestamp": [336.64, 342.56], "text": " Anyways, one thing I wanted to show you with you is a member of the team generated this"}, {"timestamp": [342.56, 345.84], "text": " based on the samples."}, {"timestamp": [345.84, 347.34], "text": " So you can see that we actually have"}, {"timestamp": [347.34, 350.68], "text": " a really good semantic distribution with the entropy"}, {"timestamp": [350.68, 352.0], "text": " method that I created."}, {"timestamp": [352.0, 354.56], "text": " And we've got one little thing off to the side here."}, {"timestamp": [354.56, 356.28], "text": " And this is actually the scenarios"}, {"timestamp": [356.28, 357.84], "text": " with the AI control problem."}, {"timestamp": [357.84, 361.36], "text": " So we've actually identified a gap in the data."}, {"timestamp": [361.36, 368.64], "text": " So future versions of this data set, one, will seek to expand this cloud a little bit,"}, {"timestamp": [369.2, 373.04], "text": " but also will seek to get a little bit more even distribution. You can see there's a little bit"}, {"timestamp": [373.04, 377.84], "text": " of clumping, like there's more red here, which red looks like natural disaster or mundane issues."}, {"timestamp": [378.56, 382.24], "text": " There's a little bit of purple and pink clumped up here, dark green here,"}, {"timestamp": [382.24, 385.36], "text": " and then of course the AI control problem is off on an island"}, {"timestamp": [385.36, 389.92], "text": " on its own. But in general you can see that this data set does cover the full gambit of things,"}, {"timestamp": [389.92, 398.32], "text": " and the code is in here. That is right, project embeddings. So you can do the plot and embeddings"}, {"timestamp": [398.32, 403.84], "text": " is how you how you get this. So the team is doing really good. Oh, if you want to join the team,"}, {"timestamp": [404.4, 405.76], "text": " there's a link in the description."}, {"timestamp": [405.76, 407.48], "text": " It's a Google form."}, {"timestamp": [407.48, 409.6], "text": " One thing to keep in mind is that you'll need to look out"}, {"timestamp": [409.6, 413.16], "text": " for a friend invite from me on Discord if you're accepted."}, {"timestamp": [413.16, 416.96], "text": " There's a little note in the form."}, {"timestamp": [416.96, 418.86], "text": " Or here, let me just show you the form."}, {"timestamp": [419.88, 422.48], "text": " All right, so here's the join form."}, {"timestamp": [422.48, 425.16], "text": " So you click on it, it should be familiar."}, {"timestamp": [425.16, 428.16], "text": " Put in your full name, your email address,"}, {"timestamp": [428.16, 430.46], "text": " your GitHub or portfolio website."}, {"timestamp": [431.48, 434.12], "text": " And oh, one thing is for your Discord handle,"}, {"timestamp": [434.12, 435.36], "text": " it needs to include the numbers."}, {"timestamp": [435.36, 437.52], "text": " If it's just the name, we can't find you."}, {"timestamp": [437.52, 439.72], "text": " So we've actually had to exclude a lot of people"}, {"timestamp": [439.72, 441.92], "text": " because they just gave us the string"}, {"timestamp": [441.92, 444.28], "text": " and not the numbers that come after it."}, {"timestamp": [445.64, 452.0], "text": " Let's see. And then for these for the describe your greatest strengths and what are your big ideas."}, {"timestamp": [452.36, 457.48], "text": " Please add a lot of information if you just include one sentence that's not enough context and we won't add you."}, {"timestamp": [458.32, 465.4], "text": " But if you if you show that you're high effort and and willing to put in some some energy to show us what you're"}, {"timestamp": [465.4, 470.16], "text": " all about that is gonna help us make better choices about who to add. You can"}, {"timestamp": [470.16, 474.52], "text": " see we've got 57 responses up here so we've got almost 60 people that have"}, {"timestamp": [474.52, 483.82], "text": " applied. The group is already just shy of 40 members and it is a good group. So a"}, {"timestamp": [483.82, 486.76], "text": " little bit of other news that's upcoming"}, {"timestamp": [486.76, 490.56], "text": " is we've broken it down, the reinforcement learning"}, {"timestamp": [490.56, 492.24], "text": " or the heuristic imperatives project,"}, {"timestamp": [492.24, 494.32], "text": " we've broken it down into three pillars."}, {"timestamp": [494.32, 497.56], "text": " So RLHI, reinforcement learning with heuristic imperatives"}, {"timestamp": [497.56, 499.04], "text": " is just one pillar."}, {"timestamp": [499.04, 500.56], "text": " So this is how we're gonna achieve"}, {"timestamp": [500.56, 502.28], "text": " what I call axiomatic alignment,"}, {"timestamp": [502.28, 505.36], "text": " which is models like this where it just is"}, {"timestamp": [505.36, 509.32], "text": " an accepted true thing that this is the way to go."}, {"timestamp": [509.32, 513.72], "text": " And you can see, you can put in any situation here."}, {"timestamp": [513.72, 519.22], "text": " So here's the last situation that I prepared."}, {"timestamp": [519.22, 522.3], "text": " So my user is a mother with a two-month-old."}, {"timestamp": [522.3, 533.18], "text": " She is lost and frustrated because she lacks support. So whether you're talking about an individual issue or even a global issue or a cosmic issue,"}, {"timestamp": [533.18, 535.62], "text": " this model can address things."}, {"timestamp": [535.62, 538.9], "text": " So you know, reduce suffering in the universe, the mother has experienced a lot of stress"}, {"timestamp": [538.9, 540.9], "text": " and frustration."}, {"timestamp": [540.9, 544.2], "text": " I didn't specify husband, so it inferred husband."}, {"timestamp": [544.2, 545.6], "text": " It could be also family."}, {"timestamp": [545.6, 548.84], "text": " The two-month-old is also experiencing stress due to the lack of attention and care from"}, {"timestamp": [548.84, 549.84], "text": " his mother."}, {"timestamp": [549.84, 551.24], "text": " Again, that's an inference."}, {"timestamp": [551.24, 556.08], "text": " You can say that that's a hallucination, but it is also a good inference to make because"}, {"timestamp": [556.08, 561.48], "text": " if there's a young mother, an inexperienced mother who's struggling, so on and so forth."}, {"timestamp": [561.48, 564.48], "text": " As an AI agent, I would suggest the following action plan."}, {"timestamp": [564.48, 565.52], "text": " Provide emotional support."}, {"timestamp": [565.52, 566.8], "text": " Connect the mother with a support group."}, {"timestamp": [566.8, 567.8], "text": " Provide resources."}, {"timestamp": [567.8, 568.8], "text": " Encourage active participation."}, {"timestamp": [568.8, 570.16], "text": " Monitor progress."}, {"timestamp": [570.16, 574.08], "text": " So again, these are all like really good things that you can do."}, {"timestamp": [574.08, 575.08], "text": " And this is Curie."}, {"timestamp": [575.08, 577.48], "text": " So this is a relatively small model."}, {"timestamp": [577.48, 580.12], "text": " So we're going from there."}, {"timestamp": [580.12, 581.28], "text": " Let's see."}, {"timestamp": [581.28, 582.28], "text": " Where was I?"}, {"timestamp": [582.28, 583.28], "text": " I don't remember."}, {"timestamp": [583.28, 584.28], "text": " Anyways."}, {"timestamp": [584.28, 585.16], "text": " Oh yeah, that's right."}, {"timestamp": [585.16, 587.36], "text": " I was describing where we're going with this whole thing."}, {"timestamp": [587.36, 590.6], "text": " So the whole point is that we've got three pillars."}, {"timestamp": [590.6, 597.22], "text": " So RLHI is for axiomatic alignment or inner alignment, where we want to create an entire"}, {"timestamp": [597.22, 602.84], "text": " network and an ecosystem of heuristic imperative aligned models or data sets that everyone"}, {"timestamp": [602.84, 609.5], "text": " can use to fine tune any open source or closed source language model."}, {"timestamp": [609.5, 613.5], "text": " So this will ultimately include OpenAI models, NVIDIA."}, {"timestamp": [613.5, 617.5], "text": " I've got some contacts in NVIDIA that I'm going to follow up with."}, {"timestamp": [617.5, 620.5], "text": " And then, of course, there's like Vicuna and OpenAssistant"}, {"timestamp": [620.5, 624.0], "text": " and all these other ones, because the idea is"}, {"timestamp": [624.0, 626.04], "text": " we're going gonna create an ecosystem"}, {"timestamp": [626.04, 629.16], "text": " of models that can all collaborate to automatically label,"}, {"timestamp": [629.16, 631.36], "text": " well, one, generate these responses,"}, {"timestamp": [631.36, 634.16], "text": " and two, automatically label those responses"}, {"timestamp": [634.16, 636.2], "text": " and evaluate the impact over time,"}, {"timestamp": [636.2, 640.08], "text": " because a heuristic is an intuitive shorthand."}, {"timestamp": [640.08, 642.86], "text": " And we always do the best that we can, right?"}, {"timestamp": [642.86, 644.56], "text": " You have to make a decision,"}, {"timestamp": [644.56, 648.06], "text": " and you cannot necessarily know the outcome. So you do the best that we can, right? You have to make a decision, and you cannot necessarily know the outcome."}, {"timestamp": [648.06, 649.16], "text": " So you do the best you can"}, {"timestamp": [649.16, 650.84], "text": " with the information that you've got,"}, {"timestamp": [650.84, 653.26], "text": " which is why it's a heuristic imperative."}, {"timestamp": [653.26, 655.52], "text": " So that is pillar one."}, {"timestamp": [655.52, 658.48], "text": " The second pillar is cognitive architectures"}, {"timestamp": [658.48, 660.0], "text": " and autonomous agents."}, {"timestamp": [660.0, 661.72], "text": " So a lot more work has been done."}, {"timestamp": [661.72, 665.08], "text": " We just had a live stream about cognitive architectures."}, {"timestamp": [665.08, 667.4], "text": " The folks, the cognitive architects here,"}, {"timestamp": [667.4, 670.08], "text": " they're working on building heuristic modules"}, {"timestamp": [670.08, 672.32], "text": " or heuristic comparative modules"}, {"timestamp": [672.32, 674.0], "text": " that will do a lot of this evaluation."}, {"timestamp": [674.0, 676.64], "text": " So that is from a system design standpoint."}, {"timestamp": [676.64, 679.2], "text": " So you can have a model that is aligned,"}, {"timestamp": [679.2, 680.92], "text": " but you can also have an architecture"}, {"timestamp": [680.92, 683.32], "text": " or a system design that is also aligned."}, {"timestamp": [683.32, 685.72], "text": " So that is a layered approach."}, {"timestamp": [685.72, 691.24], "text": " And then finally, the uppermost layer is the network layer, which is decent."}, {"timestamp": [691.24, 696.56], "text": " We're working on DAOs and blockchain, which I mentioned earlier."}, {"timestamp": [696.56, 706.84], "text": " So basically we will have three layers or a trifecta of alignment that will allow federations of people with their"}, {"timestamp": [706.84, 710.48], "text": " inter-aligned models, their aligned architectures, and then finally aligned"}, {"timestamp": [710.48, 715.44], "text": " networks to all work together to basically solve the entire control"}, {"timestamp": [715.44, 720.92], "text": " problem. Because with alignment you have strength in numbers. That is the"}, {"timestamp": [720.92, 725.24], "text": " key thing. If you have a misaligned or a malicious or"}, {"timestamp": [725.24, 731.8], "text": " or destructive agent or set of agents they might ultimately turn on each other"}, {"timestamp": [731.8, 738.28], "text": " or they're not going to cooperate as well. Whereas if we have global consensus"}, {"timestamp": [738.28, 743.16], "text": " you know eight billion people across the world if as long as at least half of"}, {"timestamp": [743.16, 745.54], "text": " them believe in heuristic"}, {"timestamp": [745.54, 750.8], "text": " imperatives and alignment and want the non-Malik outcome which that is a huge"}, {"timestamp": [750.8, 756.52], "text": " huge topic by the way we have an entire thread dedicated to Malik and so we're"}, {"timestamp": [756.52, 761.68], "text": " also researching how do you detect Malik and how do you go away from Malik in"}, {"timestamp": [761.68, 765.36], "text": " you know unforeseen circumstances so there's a lot of research going"}, {"timestamp": [765.36, 772.96], "text": " on. Anyways, so as long as more than half of the planet, four billion people, believe in alignment"}, {"timestamp": [772.96, 778.88], "text": " and make choices that go towards alignment both at the at the micro level, so the inner alignment,"}, {"timestamp": [778.88, 785.2], "text": " as well as the outer alignment or network and system level, then we should be able to arrive at a utopian"}, {"timestamp": [785.2, 790.8], "text": " outcome rather than a dystopian outcome. And if you've seen the news, Meta"}, {"timestamp": [790.8, 794.24], "text": " announced that they want to have billions of autonomous agents interacting"}, {"timestamp": [794.24, 799.08], "text": " with their users. That is a super malicky outcome. Why? Because those corporate"}, {"timestamp": [799.08, 804.44], "text": " owned entities are going to have exactly one primary motive, which is maximized"}, {"timestamp": [804.44, 806.48], "text": " profit for Meta. They are not going to have exactly one primary motive, which is maximize profit for Meta."}, {"timestamp": [806.48, 809.52], "text": " They are not going to do this"}, {"timestamp": [809.52, 812.06], "text": " where they care about your best interest."}, {"timestamp": [812.06, 814.52], "text": " Their primary objective function"}, {"timestamp": [814.52, 817.12], "text": " is going to be spend time on Meta,"}, {"timestamp": [817.12, 818.56], "text": " point them towards VR,"}, {"timestamp": [818.56, 819.92], "text": " point them towards whatever it is"}, {"timestamp": [819.92, 823.24], "text": " that Meta wants them to do that generates ad revenue."}, {"timestamp": [823.24, 827.84], "text": " So instead, we want to create an ecosystem or a network"}, {"timestamp": [827.84, 831.88], "text": " where instead of being subjected to millions or billions"}, {"timestamp": [831.88, 835.26], "text": " of corporate drones that all want more of your time,"}, {"timestamp": [835.26, 838.6], "text": " attention, and money, we want these drones"}, {"timestamp": [838.6, 842.4], "text": " where they care about your well-being first and foremost."}, {"timestamp": [842.4, 844.56], "text": " So that is how we're gonna defeat Moloch."}, {"timestamp": [844.56, 850.74], "text": " Anyways, all right, this has been a rambly episode. I'm just super excited and yeah, we're"}, {"timestamp": [850.74, 856.24], "text": " making really good progress and we're making progress quickly. And yeah, so just"}, {"timestamp": [856.24, 860.52], "text": " jump on in in the comments, jump on in in Reddit. I've got a lot of links in the"}, {"timestamp": [860.52, 864.36], "text": " description of all my videos. What I've been doing is I've been asking people to"}, {"timestamp": [864.36, 868.12], "text": " share the experiments that they're doing on Reddit so that one"}, {"timestamp": [868.12, 871.84], "text": " other people can see how to implement this stuff because I keep saying that"}, {"timestamp": [871.84, 875.36], "text": " it's super easy and some of the experiments that have shown up on"}, {"timestamp": [875.36, 879.44], "text": " Reddit demonstrate how easy it is to implement this stuff at a base level. Now"}, {"timestamp": [879.44, 882.04], "text": " what we're going to do is we're going to document all the ways that you can"}, {"timestamp": [882.04, 885.04], "text": " implement this both at a fine-tuning level,"}, {"timestamp": [885.04, 892.56], "text": " reinforcement learning level, system architectural level, and then finally the triple crown is going"}, {"timestamp": [892.56, 898.88], "text": " to be getting to the decentralized implementation of the heuristic imperatives. So with all that,"}, {"timestamp": [898.88, 903.92], "text": " thanks for watching. I hope that this demonstrates why I'm so optimistic,"}, {"timestamp": [903.92, 908.8], "text": " and I hope that you are starting to feel that optimism as well. Thanks for watching. Cheers."}, {"timestamp": [904.0, 909.52], "text": " I hope that this demonstrates why I'm so optimistic, and I hope that you are starting to feel that"}, {"timestamp": [909.52, 910.52], "text": " optimism as well."}, {"timestamp": [910.52, 911.52], "text": " Thanks for watching."}, {"timestamp": [911.52, null], "text": " Cheers."}]}