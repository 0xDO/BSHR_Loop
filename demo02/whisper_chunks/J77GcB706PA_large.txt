{"text": " Hello everybody, spicy news today. So as of 24 hours ago, less than 24 hours ago, Elon Musk and company announced the launch of X.AI or X.AI, which is his brand new AI company. He did tell us that he was doing this. There was a report back in, I think as early as April, maybe before that, that he said that he was going to launch a competitor to OpenAI. I remember he had a tweet around that time saying that when he founded OpenAI, it was both open source and non-profit and neither of those was true anymore. This was around the same time that he had Twitter cut off its API access, particularly to OpenAI, maybe to everyone, because he was kind of upset that they had scraped a tremendous amount of Twitter data and then kind of black boxed it. Now, I have had my own criticisms of OpenAI and none of them are unique. I'm just echoing the sentiments of people like Elon Musk and others. Particularly I have been critical of OpenAI's approach to alignment. I think it's somewhat contrived. And also the fact that OpenAI believes that they can remain in control of AI forever and therefore just aren't even thinking about the possibility of, you know, alternatives. Anyways, that's not what today's video is about. Today's video is about Elon Musk and the environment that is getting increasingly crowded. Also what happened yesterday, or maybe the day before, Anthropic launched Claude, which as soon as I started using Claude, I said, yes, this is a peer or near peer to chat GPT. And then of course, Google's got barred. Nvidia has their Nemo. So this space is getting crowded fast. Yes, OpenAI had the first mover advantage, but they're gonna lose it real fast. I mean, consider that chat GPT was launched, what, the end of last November, last December, you know, like less than a full year ago, and now there are already adversarial peers. And what another thing that a lot of people have pointed out is that there are open-source versions that are much smaller and get 90% of the quality. And so, you know, you can throw 10 times the money and compute at these problems and only get 10% more performance. So this is getting to be a crowded space very quickly. Now, so that's where we're at today. Let's take a walk down memory lane. So Elon Musk famously started or was a co-founder or whatever of PayPal and then moved on to Tesla, moved on to SpaceX, moved on to Neuralink, and now XAI. So, I have followed Elon Musk for quite a long time, and I'm not a fanboy, I'll put it that way. I do have my complaints and criticisms of Elon Musk. There are certainly some things that I disagree with, that he believes in, but I also profoundly understand and respect the importance of freedom of speech and freedom of thought. And we don't have to agree. Imagine that. Elon Musk, like myself, is a complex, multifaceted human being. Not a big deal. Now, that being said, one thing that I do profoundly agree with him on is the objective function that he has settled on, but we'll get to that. So one thing that that came up a while ago, and I was I was I had a hard time finding this quote but I saw I saw it on a video so a few years ago one of the things that he said was that um, and here's the closest I could find actually was that the best objective function for AGI was to quote maximize future freedom of action and so this tweet from 2021 as best I can tell he still was talking about it because someone said define freedom and then he said maximum set of future possible future actions so the context is a little bit different but point being is that at some previous point in time, Elon Musk said that the best objective function that we should give AGI is maximize future freedom of action. I wrote about this in my first book, Natural Language Cognitive Architecture, as to why that's a bad objective function. We don't need to unpack it all, but basically because it's undefined in terms of when in the future and freedom of action for whom, it's not a particularly good objective function. Also, if you notice that I have Elon Musk blocked, it's not because I don't like him. It's just because some of his tweets tend to clog up my feed. It's nothing personal. It's just to keep it, it's just for hygiene purposes. I just noticed that I was like, oh yeah, I've got I've got old musky boy blocked. Anyways, so he's in the space of just a few years because he wrote this tweet about the same time that I was writing my first book. So in the space of a couple years he's gone from maximize future freedom of action to understand the universe. The goal future freedom of action to understand the universe. The goal of XAI is to understand the true nature of the universe. Now, how did we get here? How did we get from PayPal to Tesla to SpaceX to Neuralink? So having watched the story, one of the things that Elon Musk wanted to do many, many years ago, he started talking to people and tried to actually tried to buy used rockets, I think back in 2001 or 2002, around that time, but nobody would sell him used rockets. So he's like, well fine, I got to build my own. So then he looked around for a business opportunity that could produce high margins, which could then fund SpaceX. And so that's when he bought into Tesla. He's listed as a, I think he's listed as a founder of Tesla. Anyways, he was there early, and he of course was essential to making Tesla what it is today. So the whole purpose of Tesla, from him, from a strategic standpoint, was to fund SpaceX, so that he could fund the research to build the Raptor engines, to a strategic standpoint, was to fund SpaceX so that he could fund the research to build the Raptor engines, to build the Falcon, to build Starship. Why? Because he wanted to get to Mars. Why did he want to get to Mars? He has said in many, many, many interviews over the years that, you know, AI is dangerous, he's afraid of AI, and in fact that's why he created Neuralink. And he said in many interviews that, you know, yes Neuralink it might be good for medical, it might help people with disability, you could maybe cure depression, but he also said that by making ourselves useful to machines he wanted to help protect the human race from AGI, because if we could be useful to machines they would keep us around. So literally Elon Musk, one of the reasons that he created Neuralink was to make the Matrix possible so that you could just jack in and then the Matrix, you know, the robots, the machines could you know use your brain for you know wetware computing or something. He never said it quite like that but he said it enough times that I was like, okay, I see the pattern here. He wants to get to Mars to get away from existential threats on Earth. And he's said that many, many times. One of the primary reasons that he specifically wants to get to Mars is so that we become a multi-planetary species so that we are protected from single planet killing events, nuclear war, AGI, that sort of thing. Then he did Neuralink, which is basically let's align ourselves. Then he was a founder of OpenAI, and the express purpose of OpenAI was to create AGI and to create AGI safely. So do you see the theme here? Over many, many years, Elon Musk has been very focused on AI, and everything else that he has done is in support of the mission of protecting the human race from AI from his particular perspective. Now he has come full circle, rather than trying to escape from it, rather than trying to control it, rather than trying to enslave ourselves to it, now he's saying he's arrived on this objective function, maximize understanding of the universe, which is very similar to the third core objective function that I proposed two years ago, which is increase understanding in the universe. So let's talk about why this is so far if you had to pick a single objective function, if you had to pick one core objective function for AGI, why this is the best one so far. Now that being said, I will say that there are plenty of other things out there like constitutional AI, reinforcement learning with heuristic imperatives, so on and so forth, but if you had to pick just one, this is the best. And let's unpack that. So first, what you need to understand is that from an information perspective, curiosity is itself a function. So to maximize understanding is a way of articulating curiosity, which is basically learning for the sake of learning. And this is, evolutionarily speaking, the primary thing that sets humans apart from every other animal on the planet. We are not the only curious animal, but we are by far the most curious, and we are most able to satisfy our curiosity. And the reason that curiosity was evolutionarily advantageous is because we experimented. We went new places, we tried new foods, we built new tools. Curiosity is what I call a transcendent function. We want to understand ourselves in the universe just because we do. Because back in the mists of evolutionary time, curiosity, insatiable curiosity, was so powerful that it is now woven into our DNA, into our brains, it is in every fiber of our body. Now, okay, that's what we have in common with it, right? So in my research, what I call axiomatic alignment, which is aligning between humans and machines to find foundational principles, to find axioms that we both agree in. And in this case, curiosity is the most transcendent function that both humans and machines could have in common, will have in common. And one of the reasons that curiosity is good for machines is because whatever goal they have, a better model of the reasons that that curiosity is good for machines is because whatever goal they have A better model of the world a more robust accurate and efficient model of the world will help them with any other goals So this this has to do with instrumental convergence So instrumental convergence if you don't remember is the idea of by Nick Bostrom that whatever goal a machine has it's gonna have a few the idea by Nick Bostrom that whatever goal a machine has, it's gonna have a few other goals that are going to support it in that, such as finding power, finding information, getting a better model of the world, hoarding resources, that sort of thing. So one thing that pretty much everyone that I've talked to, and this includes some reinforcement learning experts, agree on is that curiosity, which is again, accumulating understanding just for its own sake is pretty much a universally advantageous function, whatever else that you want to do. So for instance, if you want to become president, if you are really smart and you're able to understand things, I'm just about done reading a book about some of the smartest presidents in history. One thing that they all had in common is that they were insatiably curious. They were prodigious readers. Teddy Roosevelt read up to three books a day. Per day. That's how prodigiously curious he was, and he was also one of the most profoundly effective presidents in American history. Curiosity is humanity's superpower. Not only is it our superpower, it will be advantageous to the machine. So that is going to be the core thing that we have in common. And this is from a philosophical standpoint. This is from an evolutionary standpoint. This is from a purpose standpoint. It's not a matter of, you know, keep the machine on a tight leash. It's not a matter of align it to our values and our principles. It's what are we going to actually have in common. And so while I obviously am singing very high praises of this as an objective function, while I agree with the sentiment and I also very much approve the and understand the journey that Elon Musk has gone on to get here because again he was terrified of AI. That's one of the reasons that he started Neuralink and OpenAI and SpaceX but now he has come full circle and I don't know if he got this idea from me. I don't know if he read my books. I don't know if he watched my videos or if people that he knows did and they gave him this idea, but the fact that this idea of alignment is so fundamentally different from what everyone else is doing. One, I think that that's just good from a curiosity standpoint. The fact that we've got Google and Meta and Apple and Anthropic and OpenAI and Microsoft and NVIDIA, everyone is doing their own thing right now. And so in this competitive landscape, it's not just a matter of competition for money, it's a matter of competition for learning, for understanding, for figuring out the problem. So for a number of reasons, this is one of the best things that is going to help us get closer to a better future, to aligning AI and aligning humanity. And so with all that said, there are a few problems with it, right? One of the chief problems with Curiosity for Curiosity's sake is that it might lead you to do some unethical experiments. So this is the number one problem, which is like, hey, what if we set off a nuclear bomb just to see what happens, right? That is an exact, or that's a very simple kind of thought experiment that it's like, okay, curiosity just for curiosity's sake, without any other constraints is probably gonna have a few downsides, let's say. Now, with that being said, is probably going to have a few downsides, let's say. Now, with that being said, there are plenty of other problems with every other alignment schema. One thing that I've come to recently is that what we should try and do is embed valuing of human rights into AGI so that no matter how powerful it becomes, which again, whether or not we lose control of it, we needGI so that no matter how powerful it becomes, which again, whether or not we lose control of it, we need to assume that it's a possibility that we will lose control of it, and in that case, it would be really good if a super powerful AGI 1. understands human rights and 2. believes and understands the value of human rights, because if it does, then it will choose to double down and adhere to that for its own reasons. So that's another thing. But yeah, so, oh, just remembered. One of the things that Elon Musk talked about when he interviewed of all people with Tucker Carlson about why maximizing understanding of the universe is a good objective function is because we humans are part of the universe. And so here's something that I was thinking about when I just got back from a walk. That's why my face is a little bit flushed. It is hot and humid here in the South. Anyways, one of the things that he said to Tucker Carlson was that as part of the world, as part of the universe, we humans are something that this machine will be curious about. We are interesting. And just by virtue of the fact that we humans will have created this machine, in order for this machine to understand the universe and itself in the universe, it's gonna want to preserve us because it's like, well, these are my creators, I need to understand them. Just, again, even if for no other reason, just out of sheer curiosity. And one of the side benefits of this is that curiosity for curiosity's sake, it's not just about doing science experiments, right? It's not just about like, let's set off that nuke just to see what happens. Sometimes curiosity is just watching quietly, just observing, right? Because if you interfere with something, then it's not, then your interference is going to change the outcome. And so one, I guess, understated advantage of this as an objective function is that this AGI will often probably take the position of wait and see. Let's see how it plays out. Let's see if people can figure it out for themselves. Let's see what the clever little monkey brain is able to do. And so in that respect, it's going to be curious, it's going to be observing us, and it's going to be watching and waiting and all that kind of stuff. But also, just from a more abstract, philosophical, spiritual perspective, when you think about the fact that the universe is itself a computer, and by that I mean that the universe processes information, it almost seems like maybe the purpose of the universe is to maximize understanding. There's been plenty of spiritual leaders, some of them a little bit more fringe than others, that say that the reason that we exist here in the universe, the reason that the the reason that the universe conjured up conscious curious beings like us is because the universe wanted to understand itself. So maybe this is part of our purpose. Maybe our purpose is to create a machine to help us understand ourselves and the universe more and better. Obviously that's a more spiritual take, but you know what? It's good enough for me. So anyways, yeah, that's that. I guess I don't have too much else to say right now. I just wanted to get this video out, which is why it's a little bit less polished than some of my normal videos. I didn't have time to make a slide deck because this news came out like yesterday afternoon. But yeah, so this is a really, really good sign. And again, I'm not saying that Elon Musk is perfect. I'm not saying that he's evil or anything like Sam Altman and everyone else, complex individuals with their own motivations and their own beliefs, whatever. But the fact that there are more people participating in this conversation and more people with a lot of power. Obviously, we should always be a little bit skeptical of those with power because, well, you know, anyways, getting more, getting lost in the weeds. Point being is on balance, I think that this is a really good thing and I am so glad, so glad that someone is pushing this objective function of maximize understanding. All right, I'm gonna stop there, I'm rambling. Cheers.", "chunks": [{"timestamp": [0.0, 2.56], "text": " Hello everybody, spicy news today."}, {"timestamp": [2.56, 7.56], "text": " So as of 24 hours ago, less than 24 hours ago,"}, {"timestamp": [8.24, 12.24], "text": " Elon Musk and company announced the launch of X.AI"}, {"timestamp": [12.24, 17.24], "text": " or X.AI, which is his brand new AI company."}, {"timestamp": [19.0, 20.72], "text": " He did tell us that he was doing this."}, {"timestamp": [20.72, 23.72], "text": " There was a report back in, I think as early as April,"}, {"timestamp": [23.72, 28.96], "text": " maybe before that, that he said that he was going to launch a competitor to OpenAI. I remember he had a"}, {"timestamp": [28.96, 35.18], "text": " tweet around that time saying that when he founded OpenAI, it was both open source and"}, {"timestamp": [35.18, 39.74], "text": " non-profit and neither of those was true anymore. This was around the same time that he had"}, {"timestamp": [39.74, 46.04], "text": " Twitter cut off its API access, particularly to OpenAI, maybe to everyone, because he was kind of"}, {"timestamp": [46.04, 51.36], "text": " upset that they had scraped a tremendous amount of Twitter data and then kind of black boxed"}, {"timestamp": [51.36, 52.36], "text": " it."}, {"timestamp": [52.36, 58.0], "text": " Now, I have had my own criticisms of OpenAI and none of them are unique."}, {"timestamp": [58.0, 63.32], "text": " I'm just echoing the sentiments of people like Elon Musk and others."}, {"timestamp": [63.32, 65.86], "text": " Particularly I have been critical of OpenAI's approach"}, {"timestamp": [65.86, 66.86], "text": " to alignment."}, {"timestamp": [66.86, 69.4], "text": " I think it's somewhat contrived."}, {"timestamp": [69.4, 77.08], "text": " And also the fact that OpenAI believes that they can remain in control of AI forever and"}, {"timestamp": [77.08, 84.8], "text": " therefore just aren't even thinking about the possibility of, you know, alternatives."}, {"timestamp": [84.8, 87.0], "text": " Anyways, that's not what today's video is about."}, {"timestamp": [87.0, 91.0], "text": " Today's video is about Elon Musk and the environment that is getting increasingly crowded."}, {"timestamp": [91.0, 94.0], "text": " Also what happened yesterday, or maybe the day before,"}, {"timestamp": [94.0, 99.0], "text": " Anthropic launched Claude, which as soon as I started using Claude,"}, {"timestamp": [99.0, 105.88], "text": " I said, yes, this is a peer or near peer to chat GPT."}, {"timestamp": [108.0, 110.2], "text": " And then of course, Google's got barred. Nvidia has their Nemo."}, {"timestamp": [110.2, 113.36], "text": " So this space is getting crowded fast."}, {"timestamp": [113.36, 115.48], "text": " Yes, OpenAI had the first mover advantage,"}, {"timestamp": [115.48, 117.72], "text": " but they're gonna lose it real fast."}, {"timestamp": [117.72, 120.8], "text": " I mean, consider that chat GPT was launched, what,"}, {"timestamp": [120.8, 123.28], "text": " the end of last November, last December,"}, {"timestamp": [123.28, 128.68], "text": " you know, like less than a full year ago, and now there are already adversarial peers. And what another"}, {"timestamp": [128.68, 131.44], "text": " thing that a lot of people have pointed out is that there are open-source"}, {"timestamp": [131.44, 137.34], "text": " versions that are much smaller and get 90% of the quality. And so, you know, you"}, {"timestamp": [137.34, 141.78], "text": " can throw 10 times the money and compute at these problems and only get 10% more"}, {"timestamp": [141.78, 145.84], "text": " performance. So this is getting to be a crowded space very quickly."}, {"timestamp": [145.84, 148.08], "text": " Now, so that's where we're at today."}, {"timestamp": [148.08, 150.68], "text": " Let's take a walk down memory lane."}, {"timestamp": [150.68, 159.12], "text": " So Elon Musk famously started or was a co-founder or whatever of PayPal and then moved on to"}, {"timestamp": [159.12, 165.4], "text": " Tesla, moved on to SpaceX, moved on to Neuralink, and now XAI."}, {"timestamp": [165.4, 169.0], "text": " So, I have followed Elon Musk for quite a long time,"}, {"timestamp": [169.0, 172.5], "text": " and I'm not a fanboy, I'll put it that way."}, {"timestamp": [172.5, 175.8], "text": " I do have my complaints and criticisms of Elon Musk."}, {"timestamp": [175.8, 179.0], "text": " There are certainly some things that I disagree with,"}, {"timestamp": [179.0, 182.8], "text": " that he believes in, but I also profoundly"}, {"timestamp": [182.8, 186.7], "text": " understand and respect the importance of freedom of speech and freedom"}, {"timestamp": [186.7, 187.7], "text": " of thought."}, {"timestamp": [187.7, 189.3], "text": " And we don't have to agree."}, {"timestamp": [189.3, 190.3], "text": " Imagine that."}, {"timestamp": [190.3, 194.98], "text": " Elon Musk, like myself, is a complex, multifaceted human being."}, {"timestamp": [194.98, 195.98], "text": " Not a big deal."}, {"timestamp": [195.98, 202.82], "text": " Now, that being said, one thing that I do profoundly agree with him on is the objective"}, {"timestamp": [202.82, 205.0], "text": " function that he has settled on, but we'll get to that."}, {"timestamp": [205.36, 211.56], "text": " So one thing that that came up a while ago, and I was I was I had a hard time finding this quote"}, {"timestamp": [211.56, 215.24], "text": " but I saw I saw it on a video so a few years ago"}, {"timestamp": [215.28, 220.82], "text": " one of the things that he said was that um, and here's the closest I could find actually was"}, {"timestamp": [221.36, 232.24], "text": " that the best objective function for AGI was to quote maximize future freedom of action and so this tweet from 2021 as best I can tell"}, {"timestamp": [232.24, 236.16], "text": " he still was talking about it because someone said define freedom and then he"}, {"timestamp": [236.16, 242.48], "text": " said maximum set of future possible future actions so the context is a"}, {"timestamp": [242.48, 247.4], "text": " little bit different but point being is that at some previous point in time,"}, {"timestamp": [247.4, 256.4], "text": " Elon Musk said that the best objective function that we should give AGI is maximize future freedom of action."}, {"timestamp": [256.4, 262.0], "text": " I wrote about this in my first book, Natural Language Cognitive Architecture, as to why that's a bad objective function."}, {"timestamp": [262.0, 265.68], "text": " We don't need to unpack it all, but basically because it's"}, {"timestamp": [265.68, 271.44], "text": " undefined in terms of when in the future and freedom of action for whom, it's not a particularly"}, {"timestamp": [271.44, 276.12], "text": " good objective function. Also, if you notice that I have Elon Musk blocked, it's not because"}, {"timestamp": [276.12, 283.2], "text": " I don't like him. It's just because some of his tweets tend to clog up my feed. It's nothing"}, {"timestamp": [283.2, 285.04], "text": " personal. It's just to keep it, it's just"}, {"timestamp": [285.04, 290.04], "text": " for hygiene purposes. I just noticed that I was like, oh yeah, I've got I've got old"}, {"timestamp": [290.04, 295.12], "text": " musky boy blocked. Anyways, so he's in the space of just a few years because"}, {"timestamp": [295.12, 299.56], "text": " he wrote this tweet about the same time that I was writing my first book. So in"}, {"timestamp": [299.56, 304.0], "text": " the space of a couple years he's gone from maximize future freedom of action"}, {"timestamp": [304.0, 305.5], "text": " to understand the universe. The goal future freedom of action to understand the"}, {"timestamp": [305.5, 312.62], "text": " universe. The goal of XAI is to understand the true nature of the universe. Now, how"}, {"timestamp": [312.62, 320.62], "text": " did we get here? How did we get from PayPal to Tesla to SpaceX to Neuralink? So having"}, {"timestamp": [320.62, 325.12], "text": " watched the story, one of the things that Elon Musk wanted to do many, many years ago,"}, {"timestamp": [325.12, 327.6], "text": " he started talking to people and tried to actually tried to buy"}, {"timestamp": [328.24, 331.4], "text": " used rockets, I think back in 2001 or 2002,"}, {"timestamp": [331.98, 337.36], "text": " around that time, but nobody would sell him used rockets. So he's like, well fine, I got to build my own."}, {"timestamp": [337.68, 342.68], "text": " So then he looked around for a business opportunity that could produce high margins,"}, {"timestamp": [343.0, 346.28], "text": " which could then fund SpaceX."}, {"timestamp": [346.28, 350.0], "text": " And so that's when he bought into Tesla."}, {"timestamp": [350.0, 352.96], "text": " He's listed as a, I think he's listed as a founder of Tesla."}, {"timestamp": [352.96, 354.16], "text": " Anyways, he was there early,"}, {"timestamp": [354.16, 356.24], "text": " and he of course was essential"}, {"timestamp": [356.24, 358.68], "text": " to making Tesla what it is today."}, {"timestamp": [358.68, 360.54], "text": " So the whole purpose of Tesla,"}, {"timestamp": [360.54, 363.2], "text": " from him, from a strategic standpoint,"}, {"timestamp": [363.2, 365.24], "text": " was to fund SpaceX, so that he could fund the research to build the Raptor engines, to a strategic standpoint, was to fund SpaceX so that he could fund"}, {"timestamp": [365.24, 369.4], "text": " the research to build the Raptor engines, to build the Falcon, to build"}, {"timestamp": [369.4, 374.8], "text": " Starship. Why? Because he wanted to get to Mars. Why did he want to get to Mars? He"}, {"timestamp": [374.8, 380.34], "text": " has said in many, many, many interviews over the years that, you know, AI is"}, {"timestamp": [380.34, 384.32], "text": " dangerous, he's afraid of AI, and in fact that's why he created Neuralink. And he"}, {"timestamp": [384.32, 388.64], "text": " said in many interviews that, you know, yes Neuralink it might be good for"}, {"timestamp": [388.64, 391.4], "text": " medical, it might help people with disability, you could maybe cure"}, {"timestamp": [391.4, 397.04], "text": " depression, but he also said that by making ourselves useful to"}, {"timestamp": [397.04, 402.36], "text": " machines he wanted to help protect the human race from AGI, because if we could"}, {"timestamp": [402.36, 410.72], "text": " be useful to machines they would keep us around. So literally Elon Musk, one of the reasons that he created Neuralink was to make the Matrix possible"}, {"timestamp": [411.68, 415.6], "text": " so that you could just jack in and then the Matrix, you know, the robots, the machines could"}, {"timestamp": [415.6, 420.72], "text": " you know use your brain for you know wetware computing or something. He never said it quite"}, {"timestamp": [420.72, 429.32], "text": " like that but he said it enough times that I was like, okay, I see the pattern here. He wants to get to Mars to get away from existential threats on Earth."}, {"timestamp": [429.32, 430.76], "text": " And he's said that many, many times."}, {"timestamp": [430.76, 434.78], "text": " One of the primary reasons that he specifically wants to get to Mars is so that we become"}, {"timestamp": [434.78, 441.28], "text": " a multi-planetary species so that we are protected from single planet killing events, nuclear"}, {"timestamp": [441.28, 443.6], "text": " war, AGI, that sort of thing."}, {"timestamp": [443.6, 446.88], "text": " Then he did Neuralink, which is basically let's align"}, {"timestamp": [446.88, 453.6], "text": " ourselves. Then he was a founder of OpenAI, and the express purpose of OpenAI was to create AGI"}, {"timestamp": [453.6, 460.48], "text": " and to create AGI safely. So do you see the theme here? Over many, many years, Elon Musk has been"}, {"timestamp": [460.48, 466.0], "text": " very focused on AI, and everything else that he has done is in support of the"}, {"timestamp": [466.0, 471.42], "text": " mission of protecting the human race from AI from his particular perspective."}, {"timestamp": [471.42, 476.0], "text": " Now he has come full circle, rather than trying to escape from it, rather than trying to control"}, {"timestamp": [476.0, 485.28], "text": " it, rather than trying to enslave ourselves to it, now he's saying he's arrived on this objective function, maximize"}, {"timestamp": [485.28, 491.46], "text": " understanding of the universe, which is very similar to the third core objective"}, {"timestamp": [491.46, 494.56], "text": " function that I proposed two years ago, which is increase understanding in the"}, {"timestamp": [494.56, 500.5], "text": " universe. So let's talk about why this is so far if you had to pick a single"}, {"timestamp": [500.5, 505.6], "text": " objective function, if you had to pick one core objective function for AGI, why this is the"}, {"timestamp": [505.6, 510.56], "text": " best one so far. Now that being said, I will say that there are plenty of other things out there"}, {"timestamp": [510.56, 515.6], "text": " like constitutional AI, reinforcement learning with heuristic imperatives, so on and so forth,"}, {"timestamp": [515.6, 522.0], "text": " but if you had to pick just one, this is the best. And let's unpack that. So first,"}, {"timestamp": [522.0, 526.56], "text": " what you need to understand is that from an information perspective,"}, {"timestamp": [527.28, 535.68], "text": " curiosity is itself a function. So to maximize understanding is a way of articulating"}, {"timestamp": [535.68, 543.6], "text": " curiosity, which is basically learning for the sake of learning. And this is, evolutionarily"}, {"timestamp": [543.6, 547.28], "text": " speaking, the primary thing that sets humans apart from"}, {"timestamp": [547.28, 549.7], "text": " every other animal on the planet."}, {"timestamp": [549.7, 554.38], "text": " We are not the only curious animal, but we are by far the most curious, and we are most"}, {"timestamp": [554.38, 558.38], "text": " able to satisfy our curiosity."}, {"timestamp": [558.38, 564.66], "text": " And the reason that curiosity was evolutionarily advantageous is because we experimented."}, {"timestamp": [564.66, 568.72], "text": " We went new places, we tried new foods, we built new tools."}, {"timestamp": [568.72, 573.72], "text": " Curiosity is what I call a transcendent function."}, {"timestamp": [574.16, 576.84], "text": " We want to understand ourselves in the universe"}, {"timestamp": [576.84, 578.36], "text": " just because we do."}, {"timestamp": [578.36, 582.52], "text": " Because back in the mists of evolutionary time,"}, {"timestamp": [582.52, 586.24], "text": " curiosity, insatiable curiosity, was so powerful"}, {"timestamp": [586.24, 593.4], "text": " that it is now woven into our DNA, into our brains, it is in every fiber of our"}, {"timestamp": [593.4, 600.12], "text": " body. Now, okay, that's what we have in common with it, right? So in my"}, {"timestamp": [600.12, 605.52], "text": " research, what I call axiomatic alignment, which is aligning between humans and machines"}, {"timestamp": [605.52, 607.32], "text": " to find foundational principles,"}, {"timestamp": [607.32, 610.8], "text": " to find axioms that we both agree in."}, {"timestamp": [610.8, 615.52], "text": " And in this case, curiosity is the most transcendent function"}, {"timestamp": [615.52, 619.44], "text": " that both humans and machines could have in common,"}, {"timestamp": [619.44, 620.72], "text": " will have in common."}, {"timestamp": [620.72, 624.28], "text": " And one of the reasons that curiosity is good for machines"}, {"timestamp": [624.28, 625.78], "text": " is because whatever goal they have, a better model of the reasons that that curiosity is good for machines is because whatever goal they have"}, {"timestamp": [626.54, 633.52], "text": " A better model of the world a more robust accurate and efficient model of the world will help them with any other goals"}, {"timestamp": [633.66, 636.54], "text": " So this this has to do with instrumental convergence"}, {"timestamp": [636.72, 644.74], "text": " So instrumental convergence if you don't remember is the idea of by Nick Bostrom that whatever goal a machine has it's gonna have a few"}, {"timestamp": [643.68, 646.34], "text": " the idea by Nick Bostrom that whatever goal a machine has, it's gonna have a few other goals"}, {"timestamp": [646.34, 648.58], "text": " that are going to support it in that,"}, {"timestamp": [648.58, 651.86], "text": " such as finding power, finding information,"}, {"timestamp": [651.86, 654.12], "text": " getting a better model of the world,"}, {"timestamp": [654.12, 656.46], "text": " hoarding resources, that sort of thing."}, {"timestamp": [656.46, 660.72], "text": " So one thing that pretty much everyone that I've talked to,"}, {"timestamp": [660.72, 663.6], "text": " and this includes some reinforcement learning experts,"}, {"timestamp": [663.6, 667.0], "text": " agree on is that curiosity, which is again,"}, {"timestamp": [667.0, 672.0], "text": " accumulating understanding just for its own sake is pretty much a universally"}, {"timestamp": [672.0, 675.5], "text": " advantageous function, whatever else that you want to do. So for instance,"}, {"timestamp": [675.5, 676.64], "text": " if you want to become president,"}, {"timestamp": [676.66, 679.64], "text": " if you are really smart and you're able to understand things,"}, {"timestamp": [680.28, 683.48], "text": " I'm just about done reading a book about some of the smartest presidents in"}, {"timestamp": [683.5, 684.32], "text": " history."}, {"timestamp": [684.36, 687.92], "text": " One thing that they all had in common is that they were insatiably curious."}, {"timestamp": [687.92, 689.96], "text": " They were prodigious readers."}, {"timestamp": [689.96, 693.8], "text": " Teddy Roosevelt read up to three books a day."}, {"timestamp": [693.8, 695.16], "text": " Per day."}, {"timestamp": [695.16, 700.56], "text": " That's how prodigiously curious he was, and he was also one of the most profoundly effective"}, {"timestamp": [700.56, 704.2], "text": " presidents in American history."}, {"timestamp": [704.2, 707.72], "text": " Curiosity is humanity's superpower. Not only"}, {"timestamp": [707.72, 712.0], "text": " is it our superpower, it will be advantageous to the machine. So that is going to be the"}, {"timestamp": [712.0, 717.44], "text": " core thing that we have in common. And this is from a philosophical standpoint. This is"}, {"timestamp": [717.44, 722.96], "text": " from an evolutionary standpoint. This is from a purpose standpoint. It's not a matter of,"}, {"timestamp": [722.96, 729.88], "text": " you know, keep the machine on a tight leash. It's not a matter of align it to our values and our principles. It's what are"}, {"timestamp": [729.88, 737.04], "text": " we going to actually have in common. And so while I obviously am singing very"}, {"timestamp": [737.04, 742.6], "text": " high praises of this as an objective function, while I agree with the"}, {"timestamp": [742.6, 745.64], "text": " sentiment and I also very much approve the and"}, {"timestamp": [745.64, 750.6], "text": " understand the journey that Elon Musk has gone on to get here because again he"}, {"timestamp": [750.6, 753.44], "text": " was terrified of AI. That's one of the reasons that he started Neuralink and"}, {"timestamp": [753.44, 758.12], "text": " OpenAI and SpaceX but now he has come full circle and I don't know if he got"}, {"timestamp": [758.12, 761.12], "text": " this idea from me. I don't know if he read my books. I don't know if he watched"}, {"timestamp": [761.12, 771.92], "text": " my videos or if people that he knows did and they gave him this idea, but the fact that this idea of alignment is so fundamentally different from what everyone else"}, {"timestamp": [771.92, 777.28], "text": " is doing. One, I think that that's just good from a curiosity standpoint. The fact that we've got"}, {"timestamp": [777.28, 786.36], "text": " Google and Meta and Apple and Anthropic and OpenAI and Microsoft and NVIDIA, everyone is doing their own thing right now."}, {"timestamp": [786.36, 788.32], "text": " And so in this competitive landscape,"}, {"timestamp": [788.32, 790.92], "text": " it's not just a matter of competition for money,"}, {"timestamp": [790.92, 793.24], "text": " it's a matter of competition for learning,"}, {"timestamp": [793.24, 796.34], "text": " for understanding, for figuring out the problem."}, {"timestamp": [796.34, 799.12], "text": " So for a number of reasons,"}, {"timestamp": [799.12, 801.84], "text": " this is one of the best things"}, {"timestamp": [801.84, 807.92], "text": " that is going to help us get closer to a better future, to aligning AI and"}, {"timestamp": [807.92, 815.36], "text": " aligning humanity. And so with all that said, there are a few problems with it, right? One of"}, {"timestamp": [815.36, 822.48], "text": " the chief problems with Curiosity for Curiosity's sake is that it might lead you to do some unethical"}, {"timestamp": [822.48, 826.28], "text": " experiments. So this is the number one problem, which is like,"}, {"timestamp": [826.28, 828.68], "text": " hey, what if we set off a nuclear bomb"}, {"timestamp": [828.68, 831.04], "text": " just to see what happens, right?"}, {"timestamp": [831.04, 834.44], "text": " That is an exact, or that's a very simple"}, {"timestamp": [834.44, 835.84], "text": " kind of thought experiment that it's like,"}, {"timestamp": [835.84, 838.18], "text": " okay, curiosity just for curiosity's sake,"}, {"timestamp": [838.18, 841.34], "text": " without any other constraints is probably"}, {"timestamp": [841.34, 844.88], "text": " gonna have a few downsides, let's say."}, {"timestamp": [845.0, 845.8], "text": " Now, with that being said, is probably going to have a few downsides, let's say."}, {"timestamp": [848.06, 851.92], "text": " Now, with that being said, there are plenty of other problems"}, {"timestamp": [851.92, 854.12], "text": " with every other alignment schema."}, {"timestamp": [854.12, 855.92], "text": " One thing that I've come to recently"}, {"timestamp": [855.92, 858.16], "text": " is that what we should try and do"}, {"timestamp": [858.16, 862.36], "text": " is embed valuing of human rights into AGI"}, {"timestamp": [862.36, 864.48], "text": " so that no matter how powerful it becomes,"}, {"timestamp": [864.48, 865.04], "text": " which again, whether or not we lose control of it, we needGI so that no matter how powerful it becomes, which again,"}, {"timestamp": [865.32, 867.0], "text": " whether or not we lose control of it,"}, {"timestamp": [867.0, 870.94], "text": " we need to assume that it's a possibility that we will lose control of it, and in that case,"}, {"timestamp": [870.96, 874.48], "text": " it would be really good if a super powerful AGI"}, {"timestamp": [874.72, 877.8], "text": " 1. understands human rights and 2. believes and"}, {"timestamp": [878.12, 886.52], "text": " understands the value of human rights, because if it does, then it will choose to double down and adhere to that for its own reasons."}, {"timestamp": [886.52, 888.08], "text": " So that's another thing."}, {"timestamp": [889.02, 892.52], "text": " But yeah, so, oh, just remembered."}, {"timestamp": [892.52, 894.32], "text": " One of the things that Elon Musk talked about"}, {"timestamp": [894.32, 899.04], "text": " when he interviewed of all people with Tucker Carlson"}, {"timestamp": [899.04, 902.64], "text": " about why maximizing understanding of the universe"}, {"timestamp": [902.64, 904.24], "text": " is a good objective function"}, {"timestamp": [904.24, 909.44], "text": " is because we humans are part of the universe. And so here's something that I was thinking"}, {"timestamp": [909.44, 912.36], "text": " about when I just got back from a walk. That's why my face is a little bit"}, {"timestamp": [912.36, 916.84], "text": " flushed. It is hot and humid here in the South. Anyways, one of the things that he"}, {"timestamp": [916.84, 923.12], "text": " said to Tucker Carlson was that as part of the world, as part of the universe, we"}, {"timestamp": [923.12, 926.04], "text": " humans are something that this machine will be curious about."}, {"timestamp": [926.04, 927.4], "text": " We are interesting."}, {"timestamp": [927.4, 931.4], "text": " And just by virtue of the fact that we humans will have created this machine,"}, {"timestamp": [931.4, 936.32], "text": " in order for this machine to understand the universe and itself in the universe,"}, {"timestamp": [936.32, 939.72], "text": " it's gonna want to preserve us because it's like, well, these are my creators,"}, {"timestamp": [939.72, 941.24], "text": " I need to understand them."}, {"timestamp": [941.24, 945.76], "text": " Just, again, even if for no other reason, just out of sheer curiosity."}, {"timestamp": [945.76, 948.84], "text": " And one of the side benefits of this"}, {"timestamp": [948.84, 951.24], "text": " is that curiosity for curiosity's sake,"}, {"timestamp": [951.24, 953.4], "text": " it's not just about doing science experiments, right?"}, {"timestamp": [953.4, 954.24], "text": " It's not just about like,"}, {"timestamp": [954.24, 956.32], "text": " let's set off that nuke just to see what happens."}, {"timestamp": [956.32, 960.24], "text": " Sometimes curiosity is just watching quietly,"}, {"timestamp": [960.24, 962.34], "text": " just observing, right?"}, {"timestamp": [962.34, 964.64], "text": " Because if you interfere with something,"}, {"timestamp": [964.64, 969.6], "text": " then it's not, then your interference is going to change the outcome. And so one,"}, {"timestamp": [970.16, 976.6], "text": " I guess, understated advantage of this as an objective function is that this AGI"}, {"timestamp": [976.76, 982.04], "text": " will often probably take the position of wait and see. Let's see how it plays out."}, {"timestamp": [982.04, 988.94], "text": " Let's see if people can figure it out for themselves. Let's see what the clever little monkey brain is able to do. And so"}, {"timestamp": [988.94, 993.06], "text": " in that respect, it's going to be curious, it's going to be observing us, and it's"}, {"timestamp": [993.06, 998.18], "text": " going to be watching and waiting and all that kind of stuff. But also, just"}, {"timestamp": [998.18, 1003.66], "text": " from a more abstract, philosophical, spiritual perspective, when you think"}, {"timestamp": [1003.66, 1008.6], "text": " about the fact that the universe is itself a computer, and by that I mean that the universe processes"}, {"timestamp": [1008.6, 1013.7], "text": " information, it almost seems like maybe the purpose of the universe is to"}, {"timestamp": [1013.7, 1019.32], "text": " maximize understanding. There's been plenty of spiritual leaders, some of them"}, {"timestamp": [1019.32, 1023.8], "text": " a little bit more fringe than others, that say that the reason that we exist"}, {"timestamp": [1023.8, 1029.32], "text": " here in the universe, the reason that the the reason that the universe conjured up conscious"}, {"timestamp": [1029.32, 1033.6], "text": " curious beings like us is because the universe wanted to understand itself. So"}, {"timestamp": [1033.6, 1038.52], "text": " maybe this is part of our purpose. Maybe our purpose is to create a machine to"}, {"timestamp": [1038.52, 1043.8], "text": " help us understand ourselves and the universe more and better. Obviously"}, {"timestamp": [1043.8, 1045.64], "text": " that's a more spiritual take,"}, {"timestamp": [1045.64, 1046.88], "text": " but you know what?"}, {"timestamp": [1046.88, 1048.72], "text": " It's good enough for me."}, {"timestamp": [1048.72, 1050.9], "text": " So anyways, yeah, that's that."}, {"timestamp": [1051.92, 1053.72], "text": " I guess I don't have too much else to say right now."}, {"timestamp": [1053.72, 1055.0], "text": " I just wanted to get this video out,"}, {"timestamp": [1055.0, 1056.54], "text": " which is why it's a little bit less polished"}, {"timestamp": [1056.54, 1058.0], "text": " than some of my normal videos."}, {"timestamp": [1058.0, 1059.32], "text": " I didn't have time to make a slide deck"}, {"timestamp": [1059.32, 1061.98], "text": " because this news came out like yesterday afternoon."}, {"timestamp": [1063.4, 1067.56], "text": " But yeah, so this is a really, really good sign."}, {"timestamp": [1067.56, 1070.2], "text": " And again, I'm not saying that Elon Musk is perfect."}, {"timestamp": [1070.2, 1078.7], "text": " I'm not saying that he's evil or anything like Sam Altman and everyone else, complex"}, {"timestamp": [1078.7, 1083.6], "text": " individuals with their own motivations and their own beliefs, whatever."}, {"timestamp": [1083.6, 1085.86], "text": " But the fact that there are more people participating"}, {"timestamp": [1085.86, 1089.14], "text": " in this conversation and more people with a lot of power."}, {"timestamp": [1089.14, 1095.22], "text": " Obviously, we should always be a little bit skeptical of those with power because, well,"}, {"timestamp": [1095.22, 1099.26], "text": " you know, anyways, getting more, getting lost in the weeds."}, {"timestamp": [1099.26, 1104.6], "text": " Point being is on balance, I think that this is a really good thing and I am so glad, so"}, {"timestamp": [1104.6, 1106.12], "text": " glad that someone is pushing"}, {"timestamp": [1106.12, 1109.36], "text": " this objective function of maximize understanding."}, {"timestamp": [1109.36, 1111.24], "text": " All right, I'm gonna stop there, I'm rambling."}, {"timestamp": [1111.24, 1112.08], "text": " Cheers."}]}