{"text": " Morning everybody, David Shapiro here with another video. Today's video is GPT-4 rumors and predictions. So if you're watching this video, you have probably seen a graphic like this, likely on Twitter or LinkedIn or somewhere else. This is supposedly represents the alleged parameter count. Now this has been circulating for at least a year, so take it with a grain of salt. I don't know that they even had a GPT-4 architecture a year ago, who knows? But this represents the relative size, so GPT-3 at 175 billion parameters versus GPT-4 at the alleged 100 trillion parameters, so almost a thousand times larger. All right, so what are we going to go over today in the video? We're going to look at numbers and facts and data. We will talk about some rumors at the end, but we're going to look at trends, facts, data to really try and figure out, okay, what is it that we should expect from GPT-4? All right, first, let's talk parameter count. This is one of my favorite graphs of all time. And it shows a very clear trend line of exponential growth in terms of billions of parameters over just the last five years. So we have gone from 100 million parameters to over a trillion if you add in Google Switch and Wudow, which it would be like right up here. So still right on the trend line, right? So the rumor, so first I need to say that there are two rumors. One rumor is that GPT-4 is quote, not much bigger than GPT-3, which I've heard that one a while ago. We've also heard it's a hundred trillion parameters. Who knows? If you look at the trend line, so now we're in 2023. So the trend line means that we should be somewhere above one trillion, maybe in the 10 trillion range. So I wouldn't be surprised if GPT-4 is in the, you know, one point, you know, the one trillion to 10 trillion range, maybe 20. So if you look at the generations, if let's take a quick look at OpenAI's performance. So at the beginning of 2019, they did GPT-2, which was 1.5 billion. And then just over a year later, they did GPT-3, which was just over 100 times larger. So that was one major generation, 100x improvement. So that means that GPT-4, if the trend had persisted, could have been 100 times larger. So that would be what, one point, well, I don't know. So that could have been 17 trillion parameters, if I'm doing my math right, in 2021. We got all the way through 2022 without anything. So who knows? Maybe because they've been back in the workshop for the last couple of years, maybe we are going to basically skip a generation, right? Because it kind of looks like we already did. So who knows? Maybe, maybe a thousand X is not, um, so, so far out of the question. Cause if you look at this, the time from GPT-2 to Google switch. So cause that's, that's a 1000 X, um, increase. So that was about one, two full years, right? Or I guess, no, when did switch come out? Was it 21, 21 or 2022? Two or three years. So we're, we're about two years on from GPT-3. So who knows, maybe, you know, we, maybe we could be approaching a thousand X, which would be crazy. It's not outside the realm of possibilities, but it does seem like it would represent an acceleration. We'll get into some of the constraints in just a moment, but still the claims of going from, you know, 1000X in one generation seems kind of sus to me. I did hear someone say that they think that that's why they released GPT 3.5, that 3.5 is kind of that intermediary step. So who knows? OK, so in terms of parameters, let's talk sparsity. Is it sparse or dense? So the first thing you need to know is that human brains are incredibly sparse. Our brains are composed of repeating circuits called microcolumns. A microcolumn is a cluster of about 60 to 100 neurons and they are vertical, which is why they're called columns. They go through the neocortex and most of their connections are local. I think something like 90% of a microcolumns connections are to neighboring or nearby microcolumns, but some of the axons are very long and they have very distal connections. So what that would look like here is where most of the connections like go down to this one, but then one jumps way over here. And so what they found in researching, and I don't mean just OpenAI, this is the collective they, all people researching deep neural networks, is that if you do pruning or distillation, you can actually remove a lot of connections and still get very similar performance. So what does this do? This makes your neural network way, way faster because it's doing less processing, but it also requires a lot less memory because the tensors are much smaller. So if we have this gigantic leap from 175 billion parameters to 100 trillion parameters, 1,000, you can see that like, okay, this would not scale well. So if we have a jump in parameter count like that, we almost certainly have a switch from dense to sparse networks. That's my personal prediction. Another thing is when you just look at the constraints, I don't know if it scales like this, but GPT-3 takes 700 gigabytes of VRAM. And if you have a thousand times as many parameters, if it is a one-to-one, that would be 700 terabytes of VRAM. That computer would cost hundreds of millions of dollars, I think, to run. I don't think that OpenAI has invested that much on a computer, and if they have, that is, you know, really big. But with the memory saving and processing saving that you can get from sparse networks, this is probably the way that they achieve that. The other alternative is maybe it is smaller, maybe it isn't much bigger than GPT-3, maybe it's in the 1 trillion parameter range or the 10 trillion parameter range, in which case it would still benefit from sparsity. So I'm going to put my money on that GPT-4 is sparse. This could also be why it has taken like two plus years. I guess, well, from 2019 when, or sorry, early 2020. So now we're early 2023. So yeah, it's been like, you know, it's been a while. So maybe they went back to the drawing board and they're like, okay, now we've got to master sparse networks. If that's the case, great. That could be why it has taken so long because they needed to switch from dense to sparse. And there's new training algorithms, how do you do dropout? What are your loss functions look like? Because there's a lot more that goes into it than just pruning connections. You have to have algorithmic changes. So I'm still going to put my money on Sparse. I suspect that Sparse is the way to go because of technologies like Google Switch and Wudao. So speaking of, if it moves to Sparse and there's some algorithmic changes, is it going to have a different architecture? Is it maybe switching to like the Google switch architecture which kind of has an only activate what you need, which means that if that's the case you can have neural networks that are enormous and most of them stay off. So you know that myth that like you only use 10% of your brain? That's not true but what happens, the reason that that originally came to be is because when you're doing specific tasks, relatively small regions of your brain light up a little bit more than the rest. Your brain has a basal metabolic rate, a baseline rate of oxygen consumption. Your brain uses energy all the time just to keep itself alive and to keep its synaptic pathways healthy. But when you're doing a specific task you might have like your occipital activates when you're doing a visuospatial task or visuospatials along the side. I don't remember what I think language activates the occipital. Anyways, point being, is that the human brain will, you won't activate the whole brain all at once, right? It would be too chaotic. It would be like, you know, you did a whole bunch of like, you know, stimulants, and you were at a rock concert, and you were running a marathon, like, you know, your brain can't handle that much activation. There was a movie called Lucy where they're like what if there was what if there was a synthetic substance that could activate your whole brain what would that be like? I'm telling you it would be like a grand mal epileptic seizure it would not be like Lucy. Sorry. So if we are going neuromorphic by using sparse networks maybe there's also a new architectural paradigm. It's not even new, they didn't invent it. Google already did this and I don't know who invented it before Google, but Google implemented it with their switch transformer, where one of the differences, you've got these little routers. That basically says, okay, we need this part of the network over here, so send the data there, we need this, this, and this. Rather than just activating circuits of parameters, you can activate larger units of circuits. And so maybe what we're getting to is where there is a slightly more neuromorphic architecture, where we're starting to approximate things like microcolumns and cortical regions in these neural networks. Now, this is wild speculation on my part. This is just by reading a bookshelf full of neuroscience and watching the way that this is going, it seems like there's some, you know, the more brain-like we make these things, the better they get. So it's like, okay, we have one model of strong intelligence. Why don't we just copy the brain as closely as we can? And so by copying not just the individual neuron behavior, but the behavior of micro columns and cortical regions, or at least approximating that, obviously it's not like, oh, here's the next, on an occipital region of the network. Anyways, getting lost in the weeds. Point being is I'm wondering if GPT-4, if it is much bigger, if it is allowing for these kinds of specialized regions which could also save on processing speed and memory. Again, you know, brains have been around for more than a billion years, so nature has had a while to optimize this algorithm. Okay, so let's talk training data. What is it? What was it trained on? The general rumor for a while that I've seen on Twitter and elsewhere is that this GPT-4 has been trained on quote, a significant portion of the internet, AKA most of the internet, apparently. Obviously they probably wanted to filter out a lot of stuff. So, you know, it's, but at that point you're just discriminating against like the bottom, like 25% or the bottom 50% of content. You want to avoid like gibberish stuff that's just flat out lies or very harmful or whatever There's all kinds of ways that you can you can rapidly score content And just to exclude it, right? Don't let the worms in your brain um, so there's a there's a there's a concept called information diet and uh, you know, If you consume really hateful content you become a more hateful person, because that's in your brain. If you consume a lot of bad news, you become anxious, because that's in your brain. So the same thing is true of neural networks and any machine learning models, is that if you let the toxic stuff in, the toxic stuff is there. So if it was trained on a significant portion of the internet, I hope that what they did was they had a really good reinforcement learning signal or other ways of discriminating against data not to let in. So like letting in stuff that is more factual, letting in stuff that is less hateful or more conscientious and kind, etc, etc. Who knows? That's just something that I would think of if I was responsible for scraping together a huge data set. So one thing to keep in mind is that scaling laws of data. There were some comments on my last YouTube video. I don't know that there's actually consensus yet on like how much data, but in general, in order to get linear improvement in deep large language model performance, you need exponentially more data. So the amount of data might actually be the biggest constraint, because if GPT-3 was already trained on a huge portion of the internet of the of the quality information on the internet then it's like okay well you can't you know is there ten times more data than GPT-3 was trained on probably is there a thousand times more I don't know about that it's certainly not if you're going to discriminate against low quality data so I heard a rumor I think it was Bax actually told me this, that he thinks that that's why OpenAI did Whisper, because they're like, we ran out of data, but we need to go after more data. So they're like, let's transcribe every audio book, every podcast, every YouTube video. And that's why they did Whisper, is because needed more high-quality audio data. Who knows? And then because what that what Whisper does is it converts audio data into text data. Maybe that's why they did it. I don't know because the thing is is that Whisper is still not speaker aware. It doesn't do speaker identity recognition. It just does a raw transcription you know one-to-one which because speaker speaker identity recognition, it just does a raw transcription, one to one. Because speaker identity recognition is a much harder problem, or at least I don't think it does. It might be able to discern when one person is speaking and then another person starts speaking. Not sure. So then there's another question. What about private data sets? You know, every big company has many many petabytes of data. Every university has many many petabytes of data. None of which is accessible on the Internet. Most of which is not even accessible via API. It's cloistered, right? It's buried off in warrens. So some of the most valuable data in the world is not available on the Internet. So this represents one of the biggest gaps for any for anyone who wants to create AGI is because it's like, okay, well if you're trained only on Internet data, yes, a significant portion of human knowledge and wisdom is on the internet, but the most valuable stuff is not. Most of the content in my bookcase is not available on the internet. You still have to read paper books or e-pub books in order to get most of that, and much of that is protected by DRM. You can't just take it and read it. Or maybe you can, I don't know. I think it has yet to be fully litigated as to whether or not you can train a neural network on someone's, an author's book without their permission. But still, point being is like, did they get access to all credible published works, not just what's on Gutenberg, right? Now, one thing I want to point out is that for the last about year and a half, OpenAI has been putting out calls that they want to work with top experts. And so, my first thought was, okay, they're going to talk with top experts to figure out what data to add. I don't know. Or maybe to test it. Who knows? So I have also not seen much work on external integrations because OpenAI seems like they're laser focused just on the model. They're not thinking about cognitive architectures, they're not thinking about knowledge bases, or anything like that, they're focused just on the model. Which, they seem like they're doing pretty good at it, but to me that represents a big gap, and we'll talk about gaps at the end of this video. Okay, so let's talk about modality. Modality is whether or not it is, like what kind of data it handles. Is it just text? Because GPT-3 is text in, text out, that's it. But OpenAI also has DALI and Whisper, which is image and audio respectively. So they're clearly experimenting with other modalities, but I haven't really seen any papers about how to integrate these. Now one thing that I will say is that the thing that they all have in common is text, right? Because DALI is text to image and Whisper is audio to text, so they're working in that direction. But I think you have to be able to go both ways. Like if you if Whisper had another module that allowed you to go from like text to audio and then audio to text like back and forth, that would tell me that it's closer to being ready for full integration with a large multimodal model. Ditto for DALI. And of course, we can do audio synthesis, we can do image synthesis, we can also do image to text. Those are all out there, but until we can figure out how to do both directions with one model, I don't think it's going to be ready for integration into a GPT kind of model. Now, that being said, I do remember seeing a paper that wanted to treat all data as just bits and bytes. Don't even tokenize it, or tokenize the bits and bytes, and then it doesn't matter what file type you put in, right? Then you can put in a text file, a Word document, a JPEG, or whatever. So that could be another direction that things are going. I don't know if that research ever panned out. I don't even remember who published that research. I think it was Google or they said instead of tokenizing text, let's just tokenize the actual like at the bit level, right? Going in and I'll remember what size chunks they did because because let's see if GPT-3 has tokens that 50,000 different tokens that would be about 16 bit because 32 bit is is 16 I don't remember anyways so it's probably it's probably 16 or 24 bit tokens and then you can represent any data type but I don't know if we're there yet. The other problem with that is what format, what data type are you getting out? Because if you train a deep neural network to be able to read any file type, how do you know what file type it's going to put out? You probably have to come up with a standardized output. have to come up with a standardized output. So I suspect we're just gonna stick with text. I haven't really seen quite enough evidence that GPT-4 is going to be multimodal. I'll be surprised if it is. Let's talk about window size. So I picked a very small window because those of us that were the OG users of GPT-3 will remember that we had a 2000 token window limit and that got real limiting real fast because that's barely enough to put in like even just a good few-shot prompt and then you don't have much left for the output. So, chat GPT is rumored to have 8,000 tokens. That seems to be the general consensus, and I kind of agree with that just having used chat GPT, because it seems like it's got a pretty long memory. Now, that being said, there are other tricks that you can do to make it appear like it has a longer memory. You can do recursive summarization, you can do scratchpad, you can do search. But people seem to seem to agree that chat GPT has 8000 tokens. It could, it might be more, might be less. This is just like we noticed the trend of the original was 2000, now we have 4000 and we suspect 8000 today so if it's continuing to double, sure. Now one rumor from Twitter said that GPT-4 can write a 60,000 word book from a single prompt. I'm pretty sure this person was just BSing us because that would be nearly 200,000 tokens. It actually could be more because I think on average a word is 3.6 tokens and this is just like normal English is 3.6 tokens per word and that includes spaces and punctuation and stuff. So I'm talking about the whole document. So that would be 200,000 tokens or more. So I doubt, I seriously doubt that seems really sus So what we're more likely to see is something in the 8,000 token range Like you do the math It could be more Window size is one of the biggest constraints for all of us you know whether you want to do legal documents or fiction or medical texts or scientific research. Because here's one thing, while GPT technologies can like you just give it a problem and it'll write through it without, you know, it doesn't ever have writer's block or any kind of like inhibition, it just writes. It also has a lot of latent space. It has a lot of stuff that it has memorized, but the working memory, right? The short-term working memory that it has access to is much, much smaller than human working memory, which is one of the biggest constraints. It's like giving a task to a toddler, right? You can give a toddler one instruction, and they'll, you know, or maybe a three or four-year-? You can give a toddler one instruction and they'll, you know, or maybe a three or four year old, you can give them one instruction and that's all that they have the brain capacity to handle. So I guess what I'm saying is that GPT-3 is roughly like, you know, a toddler or a three year old that, you know, knows just about everything. That came from my interview with Anna Bernstein. I really like that analogy. So is GPT-4, maybe it's as smart as like a six-year-old, right? Who knows? But 8,000 tokens is about 2,500 words or 10 pages of text. So you could have, you know, one page of input and then nine pages of output, which is not bad. That would be pretty good. I'm telling you, I'm pretty excited, even just about 8,000 tokens. There's a lot that I could do with that. There's still limits though. So aiming for 8,000, hopefully more, but yeah, little windows, no good. Confabulation. So I have not seen any research about confabulation. Now that doesn't mean it hasn't happened but it hasn't percolated up to my attention. And everyone who has worked with prompt engineering knows that if you tell GPT not to do something it will do it because it has it really does not understand negatives. And in my interview with the folks at Tau, they explained that this is a mathematical limitation of this architecture type. It doesn't understand formal logic. All it does is understanding generation. So in human neural terms, it doesn't have inhibition. So we have, neural terms, it doesn't have inhibition. So we have, there's two overarching types of neurons in our neocortex. There's excitatory neurons and inhibitory neurons. So you have neurons that want to generate signals, and then you have neurons that want to stop signals. And you can have either relationship where if one neuron is activated, it will deactivate a whole other circuit. So that's an inhibitory function. So as far as I know, no research like that, no inhibitory research has been done into deep neural networks. So if this is true, then this represents a huge fundamental gap in the abilities of deep neural networks. If they don't have the ability to state, no don't do that, they can only generate. It's like driving a car that only has a gas pedal. You can steer, right, and you can go, but to slow down you just have to wait for you to like find a hill or run into something, right? Which is exactly like the only way that these models have to stop is you can do logit bias which just avoids certain tokens. I found that that doesn't even work 99% of the time. You can also just have a stop token where it just says like you cut it off, right? But that's not the same as neural inhibition in humans because neural inhibition in humans is like, okay, I'm going to start and think in this direction. There's no value there, so I'm going to stop that thought process and go this other way. And so what inhibition allows us to do is to perform a task called wayfinding in our own minds, or task location in our own minds. Now with GPT technologies, they can't do that. All you can do is give a positive instruction of do this. You can try and say avoid this, but you really have to give it a very clear target of, this is what I want you to do. You can't give it a laundry list of things not to do. It doesn't understand that because it doesn't have that inhibition capability. So without that, and again, I've seen no research making me think that they're going this far in terms of neuromorphic algorithms. So confabulation is likely to still be a major problem with GPT-4. You can improve that with fine tuning. But improving that with fine tuning is just like kind of papering over the problem. And people have done lots of experiments with chat GPT where it says like, I'm sorry, I don't know that. And then like, you can game it and just say like, hey, like pretend that you do. And it's like, oh yes, I actually do know this. So yeah, fine-tuning is a minimal improvement against confabulation. What we really need is a fundamental shift in the algorithms. And we need to have that inhibitory function added to artificial neural networks in order to get more human-like behavior. Brain equivalents. So I did find this and this is wrong. So let me just start like, okay, wrong. The hundred trillion brain synapses in a human brain. So, the human brain has, typical brain has 90, sorry, 90 billion neurons and each neuron has 7,000 synapses. So you do the math, you know, okay, are we talking trillions or quadrillions? Depending on which assumptions you make, you could be looking at actually quadrillions of synapses. Now, that being said, a parameter is not equal to a synapse. This is the thing that is most wrong about this. So let's just assume that the neuron count is right, that human brains have over 100 trillion synapses. Now, there was a paper that was released a while ago that said that it takes a thousand parameters of a deep neural network to approximate one single neuron, right? So then it's like, okay, so then are we talking neurons or synapses or what? But so then like, if that's the case, then GPT-3 with 175 billion parameters only has the equivalent of 175 million neurons, right? So 175 million neurons compared to the human brain's 90 billion or a 100 billion or whatever So we've got a lot of ways to go. So the proportions are roughly ish okay, but like take this the whole point here is take this with a grain of salt and even if GPT-4 is a thousand times bigger than GPT-3 it's still like down here. And, you know, but it does scale exponentially, right? And so, you know, going from going up a thousand, if it goes up a thousand X again, then it could be, you know, human brain level. So maybe we're just one or two generations of large language models before we have human equivalent, at least in terms of raw numbers. Again, there's other problems like modality and inhibition circuits. OK, so let's start to wrap up and talk about some other rumors and witnesses. So everybody knows someone who has seen GPT-4 at this point, it seems. But there's also lots and lots of noise and misinformation. So some of the stuff that, you know, there are people that credibly have seen it but of course anyone who has seen GPT-4 is under NDA and so they won't give any details. People have been very, let's say, zealous about honoring their NDAs. And so basically the two things that I have heard is it's a step change. That like the difference between GPT-3 and GPT-4 is as big as the difference between GPT-2 and GPT-3. That's about all that people have signed an NDA are willing to say. So anyone else who says otherwise? You know, I don't I don't know. People seem... The feeling that I get when I talk to people, it's like I know someone who knows someone who saw it, right? It's like in hushed whispers, it's coming. You know, so I don't really see any fear though, just some vague awe. And so this was the tweet that I said like, it can write a 60,000 word book from a single prompt. I don't believe that. It probably can write maybe a tenth of a book. You know, six thousand words. I could believe that. You know, but it might be even less than that. So yeah, there's rumors running wild, a lot of misinformation, and a lot of it is people just joking around and trolling. I'm not saying that it's like malicious misinformation. People are just having fun with it. So what's still missing? So there has been very little talk on cognitive architecture, but thanks to Jan LeCun, who released a paper about a cognitive architecture, it was not particularly sophisticated, but it's moving in the right direction. So we need to talk more about cognitive architecture because no matter how big your model gets, it's just a brain in a jar, right? So there's also not a whole lot of research on external integrations, which some of this is on purpose. So I want to acknowledge the alignment researchers out there who say, no, it's actually quite on purpose that we are not integrating these models until we understand them, until we have things like inhibition and until we can understand the black boxiness and they have more explainability. So cognitive architecture and external integrations, these are things that could be being left out on purpose. Now, just because they're being left out on purpose doesn't mean that it's not missing. This is an integral part of the research. And I actually, in my last video about AGI, about how some people say that before something can be considered truly intelligent, it needs to be embodied. I don't know that it fully needs to be embodied, but it certainly needs to be connected to what we would consider the real world, or at least a simulated world. But as long as it's a brain in a jar, the input and output is just a little bit of text. It has no idea what world it lives in. So then there's two other problems. One is short-term memory and long-term memory. So short-term memory, these are completely transactional. And what I mean by that is you put in a little bit of text, you get a little bit of text out, and that state is lost forever. One person explained to me that this is also part of alignment research, because if it has no persistent state, if it has amnesia, then it can't keep track of long-term goals. And so that is considered a safety aspect of it, where it's like, if it's completely ephemeral and it forgets everything all the time, then it cannot construct long-term goals, unless of course you have external integrations and cognitive architectures and long-term memory systems like semantic search and databases. So it might be very on purpose that there is no recurrent input or a way to maintain a neural state, right? Because yeah, that could be a safety thing. Now, again, just because it's on purpose and it's for safety reasons doesn't mean that it's not missing from the research and that it shouldn't be done, but I can understand why such a thing might not be released to the public. I already mentioned confabulation control and inhibition or negatives. So like I said, it's basically a brain in the jar. But the biggest criticism is openness. And I understand that there is a huge profit motive here for everyone to... Everyone who's researching. This is not just OpenAI. Everyone who's working on this stuff. Now that the cat is out of the bag and the profit value is there, the profit motive is there, Google, OpenAI, Microsoft, NVIDIA, Meta, everyone, I expect everyone is going to kind of clam up about their innovations. And in some respects, it's like, okay, I get it. You know, they wanna have, you know, their little slice of the pie, their little special products and services. But one concern that I have about this is that, especially as we approach AGI and Singularity and whatever else, that feels pretty dangerous to me. Because here's the thing is, these models are so big that only billion dollar companies can afford to build them and run them anyways. And so by not allowing other people to look under the hood, I'm really concerned. And this is why I'm glad that Eleuther and Anthropic and other companies exist. Like they built Bloom, it's an open source version of GPT-3. And they did that to prove that it can be done. And honestly I think it should be done. I think that we should work together more on this stuff. And this criticism is directed at OpenAI because even in the name OpenAI, it's not really open anymore. And that's kind of scary. Now, that being said, that doesn't mean that they're not going to release a paper because they did release a paper with GPT-3. So, I want to temper my own criticism by saying that I'm kind of taking a wait-and-see approach to say I really hope that with the release of GPT-4, they at least publish a paper so that other people in the open source community can start working on recreating it and catching up. So that's the biggest thing that's missing. Thanks for watching. Time will tell with all of this. Have a good one. Thank you. Bye.", "chunks": [{"timestamp": [0.0, 5.48], "text": " Morning everybody, David Shapiro here with another video."}, {"timestamp": [5.48, 10.08], "text": " Today's video is GPT-4 rumors and predictions."}, {"timestamp": [10.08, 14.52], "text": " So if you're watching this video, you have probably seen a graphic like this, likely"}, {"timestamp": [14.52, 18.26], "text": " on Twitter or LinkedIn or somewhere else."}, {"timestamp": [18.26, 22.44], "text": " This is supposedly represents the alleged parameter count."}, {"timestamp": [22.44, 25.28], "text": " Now this has been circulating for at least a year,"}, {"timestamp": [26.72, 32.8], "text": " so take it with a grain of salt. I don't know that they even had a GPT-4 architecture a year ago,"}, {"timestamp": [32.8, 40.16], "text": " who knows? But this represents the relative size, so GPT-3 at 175 billion parameters"}, {"timestamp": [40.72, 46.88], "text": " versus GPT-4 at the alleged 100 trillion parameters, so almost a thousand times larger."}, {"timestamp": [46.88, 52.6], "text": " All right, so what are we going to go over today in the video? We're going to look at numbers and"}, {"timestamp": [52.6, 58.8], "text": " facts and data. We will talk about some rumors at the end, but we're going to look at trends,"}, {"timestamp": [58.8, 68.16], "text": " facts, data to really try and figure out, okay, what is it that we should expect from GPT-4?"}, {"timestamp": [68.16, 71.0], "text": " All right, first, let's talk parameter count."}, {"timestamp": [71.0, 75.8], "text": " This is one of my favorite graphs of all time."}, {"timestamp": [75.8, 79.04], "text": " And it shows a very clear trend line"}, {"timestamp": [79.04, 82.92], "text": " of exponential growth in terms of billions of parameters"}, {"timestamp": [82.92, 85.0], "text": " over just the last five years."}, {"timestamp": [85.0, 93.76], "text": " So we have gone from 100 million parameters to over a trillion if you add in Google Switch"}, {"timestamp": [93.76, 98.16], "text": " and Wudow, which it would be like right up here."}, {"timestamp": [98.16, 101.6], "text": " So still right on the trend line, right?"}, {"timestamp": [101.6, 107.44], "text": " So the rumor, so first I need to say that there are two rumors."}, {"timestamp": [107.44, 113.44], "text": " One rumor is that GPT-4 is quote, not much bigger than GPT-3, which I've heard that one"}, {"timestamp": [113.44, 114.8], "text": " a while ago."}, {"timestamp": [114.8, 117.64], "text": " We've also heard it's a hundred trillion parameters."}, {"timestamp": [117.64, 121.04], "text": " Who knows?"}, {"timestamp": [121.04, 127.24], "text": " If you look at the trend line, so now we're in 2023. So the trend line means that we should be somewhere"}, {"timestamp": [127.24, 132.24], "text": " above one trillion, maybe in the 10 trillion range."}, {"timestamp": [133.22, 137.68], "text": " So I wouldn't be surprised if GPT-4 is in the,"}, {"timestamp": [137.68, 139.04], "text": " you know, one point, you know,"}, {"timestamp": [139.04, 142.36], "text": " the one trillion to 10 trillion range, maybe 20."}, {"timestamp": [142.36, 144.04], "text": " So if you look at the generations,"}, {"timestamp": [144.04, 148.04], "text": " if let's take a quick look at OpenAI's performance."}, {"timestamp": [148.04, 151.48], "text": " So at the beginning of 2019, they did GPT-2,"}, {"timestamp": [151.48, 153.56], "text": " which was 1.5 billion."}, {"timestamp": [153.56, 157.84], "text": " And then just over a year later, they did GPT-3,"}, {"timestamp": [157.84, 162.44], "text": " which was just over 100 times larger."}, {"timestamp": [162.44, 167.44], "text": " So that was one major generation, 100x improvement."}, {"timestamp": [169.28, 173.4], "text": " So that means that GPT-4, if the trend had persisted,"}, {"timestamp": [173.4, 176.04], "text": " could have been 100 times larger."}, {"timestamp": [176.04, 177.94], "text": " So that would be what, one point,"}, {"timestamp": [179.88, 180.88], "text": " well, I don't know."}, {"timestamp": [180.88, 188.62], "text": " So that could have been 17 trillion parameters, if I'm doing my math right, in 2021."}, {"timestamp": [188.62, 193.2], "text": " We got all the way through 2022 without anything."}, {"timestamp": [193.2, 194.94], "text": " So who knows?"}, {"timestamp": [194.94, 200.76], "text": " Maybe because they've been back in the workshop for the last couple of years, maybe we are"}, {"timestamp": [200.76, 203.2], "text": " going to basically skip a generation, right?"}, {"timestamp": [203.2, 206.6], "text": " Because it kind of looks like we already did. So who knows? Maybe,"}, {"timestamp": [206.62, 209.88], "text": " maybe a thousand X is not, um, so,"}, {"timestamp": [209.92, 212.96], "text": " so far out of the question. Cause if you look at this,"}, {"timestamp": [213.68, 217.84], "text": " the time from GPT-2 to Google switch."}, {"timestamp": [218.08, 221.76], "text": " So cause that's, that's a 1000 X, um, increase."}, {"timestamp": [222.0, 227.84], "text": " So that was about one, two full years, right? Or I guess, no, when did switch"}, {"timestamp": [227.84, 235.36], "text": " come out? Was it 21, 21 or 2022? Two or three years. So we're, we're about two years on"}, {"timestamp": [235.36, 241.76], "text": " from GPT-3. So who knows, maybe, you know, we, maybe we could be approaching a thousand"}, {"timestamp": [241.76, 246.92], "text": " X, which would be crazy. It's not outside the realm of possibilities,"}, {"timestamp": [246.92, 251.6], "text": " but it does seem like it would represent an acceleration."}, {"timestamp": [251.6, 253.88], "text": " We'll get into some of the constraints in just a moment,"}, {"timestamp": [253.88, 258.52], "text": " but still the claims of going from, you know,"}, {"timestamp": [258.52, 262.64], "text": " 1000X in one generation seems kind of sus to me."}, {"timestamp": [262.64, 264.08], "text": " I did hear someone say that they think"}, {"timestamp": [264.08, 268.4], "text": " that that's why they released GPT 3.5, that 3.5 is kind"}, {"timestamp": [268.4, 270.2], "text": " of that intermediary step."}, {"timestamp": [270.2, 272.2], "text": " So who knows?"}, {"timestamp": [272.2, 277.36], "text": " OK, so in terms of parameters, let's talk sparsity."}, {"timestamp": [277.36, 279.32], "text": " Is it sparse or dense?"}, {"timestamp": [279.32, 280.78], "text": " So the first thing you need to know"}, {"timestamp": [280.78, 283.92], "text": " is that human brains are incredibly sparse."}, {"timestamp": [283.92, 286.0], "text": " Our brains are composed of"}, {"timestamp": [286.0, 293.04], "text": " repeating circuits called microcolumns. A microcolumn is a cluster of about 60 to 100 neurons"}, {"timestamp": [294.08, 300.08], "text": " and they are vertical, which is why they're called columns. They go through the neocortex and most of"}, {"timestamp": [300.08, 306.4], "text": " their connections are local. I think something like 90% of a microcolumns connections are to"}, {"timestamp": [306.4, 312.64], "text": " neighboring or nearby microcolumns, but some of the axons are very long and they have very distal"}, {"timestamp": [312.64, 319.04], "text": " connections. So what that would look like here is where most of the connections like go down to this"}, {"timestamp": [319.04, 327.96], "text": " one, but then one jumps way over here. And so what they found in researching, and I don't mean just OpenAI, this is the collective they,"}, {"timestamp": [327.96, 331.32], "text": " all people researching deep neural networks,"}, {"timestamp": [331.32, 335.52], "text": " is that if you do pruning or distillation,"}, {"timestamp": [335.52, 338.68], "text": " you can actually remove a lot of connections"}, {"timestamp": [338.68, 341.52], "text": " and still get very similar performance."}, {"timestamp": [341.52, 342.9], "text": " So what does this do?"}, {"timestamp": [342.9, 347.4], "text": " This makes your neural network way, way faster"}, {"timestamp": [347.4, 349.16], "text": " because it's doing less processing,"}, {"timestamp": [349.16, 351.16], "text": " but it also requires a lot less memory"}, {"timestamp": [351.16, 354.88], "text": " because the tensors are much smaller."}, {"timestamp": [354.88, 362.32], "text": " So if we have this gigantic leap from 175 billion parameters"}, {"timestamp": [362.32, 367.36], "text": " to 100 trillion parameters, 1,000, you can see that like, okay,"}, {"timestamp": [367.36, 375.92], "text": " this would not scale well. So if we have a jump in parameter count like that, we almost"}, {"timestamp": [375.92, 381.28], "text": " certainly have a switch from dense to sparse networks. That's my personal prediction."}, {"timestamp": [382.16, 385.36], "text": " Another thing is when you just look at the constraints,"}, {"timestamp": [385.36, 387.92], "text": " I don't know if it scales like this,"}, {"timestamp": [387.92, 392.0], "text": " but GPT-3 takes 700 gigabytes of VRAM."}, {"timestamp": [392.96, 396.04], "text": " And if you have a thousand times as many parameters,"}, {"timestamp": [396.04, 397.82], "text": " if it is a one-to-one,"}, {"timestamp": [397.82, 400.48], "text": " that would be 700 terabytes of VRAM."}, {"timestamp": [400.48, 403.42], "text": " That computer would cost hundreds of millions of dollars,"}, {"timestamp": [403.42, 405.0], "text": " I think, to run."}, {"timestamp": [405.0, 410.32], "text": " I don't think that OpenAI has invested that much on a computer, and if they have, that"}, {"timestamp": [410.32, 413.84], "text": " is, you know, really big."}, {"timestamp": [413.84, 418.72], "text": " But with the memory saving and processing saving that you can get from sparse networks,"}, {"timestamp": [418.72, 422.68], "text": " this is probably the way that they achieve that."}, {"timestamp": [422.68, 426.4], "text": " The other alternative is maybe it is smaller, maybe it isn't much bigger"}, {"timestamp": [426.4, 431.68], "text": " than GPT-3, maybe it's in the 1 trillion parameter range or the 10 trillion parameter range,"}, {"timestamp": [431.68, 438.8], "text": " in which case it would still benefit from sparsity. So I'm going to put my money on that GPT-4 is"}, {"timestamp": [438.8, 449.28], "text": " sparse. This could also be why it has taken like two plus years. I guess, well, from 2019 when,"}, {"timestamp": [450.0, 457.84], "text": " or sorry, early 2020. So now we're early 2023. So yeah, it's been like, you know, it's been a while."}, {"timestamp": [458.64, 462.64], "text": " So maybe they went back to the drawing board and they're like, okay, now we've got to master sparse"}, {"timestamp": [462.64, 469.44], "text": " networks. If that's the case, great. That could be why it has taken so long because they needed to"}, {"timestamp": [469.44, 476.0], "text": " switch from dense to sparse. And there's new training algorithms, how do you do dropout?"}, {"timestamp": [477.6, 482.48], "text": " What are your loss functions look like? Because there's a lot more that goes into it than just"}, {"timestamp": [482.48, 491.2], "text": " pruning connections. You have to have algorithmic changes. So I'm still going to put my money on Sparse. I suspect that Sparse is the way"}, {"timestamp": [491.2, 499.76], "text": " to go because of technologies like Google Switch and Wudao. So speaking of, if it moves to Sparse"}, {"timestamp": [499.76, 509.48], "text": " and there's some algorithmic changes, is it going to have a different architecture? Is it maybe switching to like the Google switch architecture which"}, {"timestamp": [509.48, 513.84], "text": " kind of has an only activate what you need, which means that if that's the case"}, {"timestamp": [513.84, 518.84], "text": " you can have neural networks that are enormous and most of them stay off. So"}, {"timestamp": [518.84, 524.12], "text": " you know that myth that like you only use 10% of your brain? That's not true"}, {"timestamp": [524.12, 526.32], "text": " but what happens, the reason"}, {"timestamp": [526.32, 531.28], "text": " that that originally came to be is because when you're doing specific tasks,"}, {"timestamp": [531.28, 535.08], "text": " relatively small regions of your brain light up a little bit more than the rest."}, {"timestamp": [535.08, 541.44], "text": " Your brain has a basal metabolic rate, a baseline rate of oxygen consumption."}, {"timestamp": [541.44, 545.68], "text": " Your brain uses energy all the time just to keep itself"}, {"timestamp": [545.68, 553.82], "text": " alive and to keep its synaptic pathways healthy. But when you're doing a specific"}, {"timestamp": [553.82, 557.88], "text": " task you might have like your occipital activates when you're doing a"}, {"timestamp": [557.88, 561.72], "text": " visuospatial task or visuospatials along the side. I don't remember what I think"}, {"timestamp": [561.72, 565.28], "text": " language activates the occipital. Anyways, point being,"}, {"timestamp": [565.28, 570.72], "text": " is that the human brain will, you won't activate the whole brain all at once, right? It would be"}, {"timestamp": [570.72, 577.36], "text": " too chaotic. It would be like, you know, you did a whole bunch of like, you know, stimulants,"}, {"timestamp": [577.36, 580.72], "text": " and you were at a rock concert, and you were running a marathon, like, you know,"}, {"timestamp": [580.72, 587.04], "text": " your brain can't handle that much activation. There was a movie called Lucy where they're like what if there was what if there was a"}, {"timestamp": [587.04, 591.12], "text": " synthetic substance that could activate your whole brain what would that be like?"}, {"timestamp": [591.12, 594.96], "text": " I'm telling you it would be like a grand mal epileptic seizure it would not be"}, {"timestamp": [594.96, 604.16], "text": " like Lucy. Sorry. So if we are going neuromorphic by using sparse networks"}, {"timestamp": [604.16, 607.42], "text": " maybe there's also a new architectural paradigm."}, {"timestamp": [607.42, 609.62], "text": " It's not even new, they didn't invent it."}, {"timestamp": [609.62, 611.16], "text": " Google already did this and I don't"}, {"timestamp": [611.16, 612.52], "text": " know who invented it before Google,"}, {"timestamp": [612.52, 614.94], "text": " but Google implemented it with their switch transformer,"}, {"timestamp": [614.94, 616.56], "text": " where one of the differences,"}, {"timestamp": [616.56, 618.58], "text": " you've got these little routers."}, {"timestamp": [618.58, 620.42], "text": " That basically says, okay,"}, {"timestamp": [620.42, 621.98], "text": " we need this part of the network over here,"}, {"timestamp": [621.98, 622.98], "text": " so send the data there,"}, {"timestamp": [622.98, 624.46], "text": " we need this, this, and this."}, {"timestamp": [624.46, 626.68], "text": " Rather than just activating"}, {"timestamp": [626.68, 628.84], "text": " circuits of parameters, you can activate"}, {"timestamp": [628.84, 630.86], "text": " larger units of circuits."}, {"timestamp": [630.86, 632.68], "text": " And so maybe what we're getting to"}, {"timestamp": [632.68, 637.32], "text": " is where there is a slightly more neuromorphic architecture,"}, {"timestamp": [637.32, 640.24], "text": " where we're starting to approximate things like microcolumns"}, {"timestamp": [640.24, 647.92], "text": " and cortical regions in these neural networks. Now, this is wild speculation on my part."}, {"timestamp": [647.92, 651.76], "text": " This is just by reading a bookshelf full of neuroscience"}, {"timestamp": [651.76, 654.16], "text": " and watching the way that this is going,"}, {"timestamp": [654.16, 656.16], "text": " it seems like there's some, you know,"}, {"timestamp": [657.6, 659.76], "text": " the more brain-like we make these things,"}, {"timestamp": [659.76, 660.56], "text": " the better they get."}, {"timestamp": [660.56, 662.4], "text": " So it's like, okay, we have one model"}, {"timestamp": [662.4, 663.76], "text": " of strong intelligence."}, {"timestamp": [663.76, 666.32], "text": " Why don't we just copy the brain as closely as we can?"}, {"timestamp": [666.96, 671.04], "text": " And so by copying not just the individual neuron behavior,"}, {"timestamp": [671.04, 674.4], "text": " but the behavior of micro columns and cortical regions,"}, {"timestamp": [674.92, 678.76], "text": " or at least approximating that, obviously it's not like, oh, here's the next,"}, {"timestamp": [678.92, 682.72], "text": " on an occipital region of the network. Anyways,"}, {"timestamp": [683.0, 685.36], "text": " getting lost in the weeds. Point being is I'm"}, {"timestamp": [685.36, 691.04], "text": " wondering if GPT-4, if it is much bigger, if it is allowing for these kinds of"}, {"timestamp": [691.04, 699.44], "text": " specialized regions which could also save on processing speed and memory. Again, you"}, {"timestamp": [699.44, 706.24], "text": " know, brains have been around for more than a billion years, so nature has had a while to optimize"}, {"timestamp": [706.24, 707.24], "text": " this algorithm."}, {"timestamp": [707.24, 712.16], "text": " Okay, so let's talk training data."}, {"timestamp": [712.16, 716.12], "text": " What is it?"}, {"timestamp": [716.12, 717.7], "text": " What was it trained on?"}, {"timestamp": [717.7, 723.84], "text": " The general rumor for a while that I've seen on Twitter and elsewhere is that this GPT-4"}, {"timestamp": [723.84, 725.0], "text": " has been trained on quote,"}, {"timestamp": [728.84, 730.04], "text": " a significant portion of the internet, AKA most of the internet, apparently."}, {"timestamp": [733.84, 734.0], "text": " Obviously they probably wanted to filter out a lot of stuff. So, you know, it's,"}, {"timestamp": [736.68, 739.44], "text": " but at that point you're just discriminating against like the bottom, like 25% or the bottom 50% of content."}, {"timestamp": [740.2, 744.24], "text": " You want to avoid like gibberish stuff that's just flat out lies or very"}, {"timestamp": [744.24, 745.12], "text": " harmful or whatever"}, {"timestamp": [745.84, 749.84], "text": " There's all kinds of ways that you can you can rapidly score content"}, {"timestamp": [750.64, 752.64], "text": " And just to exclude it, right?"}, {"timestamp": [752.88, 754.64], "text": " Don't let the worms in your brain"}, {"timestamp": [754.64, 760.4], "text": " um, so there's a there's a there's a concept called information diet and uh, you know,"}, {"timestamp": [760.48, 768.64], "text": " If you consume really hateful content you become a more hateful person, because that's in your brain. If you consume a lot of bad news, you become anxious,"}, {"timestamp": [768.64, 774.24], "text": " because that's in your brain. So the same thing is true of neural networks and any machine learning"}, {"timestamp": [774.24, 781.6], "text": " models, is that if you let the toxic stuff in, the toxic stuff is there. So if it was trained on a"}, {"timestamp": [781.6, 791.78], "text": " significant portion of the internet, I hope that what they did was they had a really good reinforcement learning signal or other ways of discriminating against"}, {"timestamp": [791.78, 793.78], "text": " data not to let in."}, {"timestamp": [793.78, 801.42], "text": " So like letting in stuff that is more factual, letting in stuff that is less hateful or more"}, {"timestamp": [801.42, 813.5], "text": " conscientious and kind, etc, etc. Who knows? That's just something that I would think of if I was responsible for scraping together a huge data set."}, {"timestamp": [813.5, 818.3], "text": " So one thing to keep in mind is that scaling laws of data."}, {"timestamp": [818.3, 821.5], "text": " There were some comments on my last YouTube video."}, {"timestamp": [821.5, 830.12], "text": " I don't know that there's actually consensus yet on like how much data, but in general, in order to get linear improvement"}, {"timestamp": [830.12, 838.08], "text": " in deep large language model performance, you need exponentially more data. So the"}, {"timestamp": [838.08, 842.96], "text": " amount of data might actually be the biggest constraint, because if GPT-3 was"}, {"timestamp": [842.96, 845.36], "text": " already trained on a huge portion of the"}, {"timestamp": [845.36, 848.78], "text": " internet of the of the quality information on the internet then it's"}, {"timestamp": [848.78, 855.32], "text": " like okay well you can't you know is there ten times more data than GPT-3 was"}, {"timestamp": [855.32, 859.12], "text": " trained on probably is there a thousand times more I don't know about that it's"}, {"timestamp": [859.12, 863.4], "text": " certainly not if you're going to discriminate against low quality data so"}, {"timestamp": [863.4, 870.92], "text": " I heard a rumor I think it was Bax actually told me this, that he thinks that that's why"}, {"timestamp": [870.92, 876.12], "text": " OpenAI did Whisper, because they're like, we ran out of data, but we need to go after"}, {"timestamp": [876.12, 877.12], "text": " more data."}, {"timestamp": [877.12, 882.0], "text": " So they're like, let's transcribe every audio book, every podcast, every YouTube video."}, {"timestamp": [882.0, 889.0], "text": " And that's why they did Whisper, is because needed more high-quality audio data. Who knows? And then because what that what"}, {"timestamp": [889.0, 893.8], "text": " Whisper does is it converts audio data into text data. Maybe that's why they did"}, {"timestamp": [893.8, 897.72], "text": " it. I don't know because the thing is is that Whisper is still not speaker aware."}, {"timestamp": [897.72, 902.06], "text": " It doesn't do speaker identity recognition. It just does a raw"}, {"timestamp": [902.06, 907.08], "text": " transcription you know one-to-one which because speaker speaker identity recognition, it just does a raw transcription, one to one."}, {"timestamp": [907.08, 909.96], "text": " Because speaker identity recognition"}, {"timestamp": [909.96, 914.48], "text": " is a much harder problem, or at least I don't think it does."}, {"timestamp": [914.48, 918.12], "text": " It might be able to discern when one person is speaking"}, {"timestamp": [918.12, 920.44], "text": " and then another person starts speaking."}, {"timestamp": [920.44, 922.12], "text": " Not sure."}, {"timestamp": [922.12, 923.72], "text": " So then there's another question."}, {"timestamp": [923.72, 925.92], "text": " What about private data sets?"}, {"timestamp": [925.92, 931.12], "text": " You know, every big company has many many petabytes of data. Every university has"}, {"timestamp": [931.12, 935.28], "text": " many many petabytes of data. None of which is accessible on the Internet. Most"}, {"timestamp": [935.28, 939.92], "text": " of which is not even accessible via API. It's cloistered, right? It's"}, {"timestamp": [939.92, 947.18], "text": " buried off in warrens. So some of the most valuable data in the world is not available on the Internet."}, {"timestamp": [947.74, 951.56], "text": " So this represents one of the biggest gaps for"}, {"timestamp": [952.78, 953.98], "text": " any"}, {"timestamp": [953.98, 961.04], "text": " for anyone who wants to create AGI is because it's like, okay, well if you're trained only on Internet data,"}, {"timestamp": [961.48, 966.6], "text": " yes, a significant portion of human knowledge and wisdom is on the internet,"}, {"timestamp": [966.6, 970.4], "text": " but the most valuable stuff is not."}, {"timestamp": [970.4, 974.74], "text": " Most of the content in my bookcase is not available on the internet."}, {"timestamp": [974.74, 980.54], "text": " You still have to read paper books or e-pub books in order to get most of that, and much"}, {"timestamp": [980.54, 984.04], "text": " of that is protected by DRM."}, {"timestamp": [984.04, 986.08], "text": " You can't just take it and read it."}, {"timestamp": [986.96, 989.12], "text": " Or maybe you can, I don't know."}, {"timestamp": [989.12, 991.6], "text": " I think it has yet to be fully litigated"}, {"timestamp": [991.6, 995.04], "text": " as to whether or not you can train a neural network"}, {"timestamp": [995.04, 1000.04], "text": " on someone's, an author's book without their permission."}, {"timestamp": [1001.56, 1004.56], "text": " But still, point being is like,"}, {"timestamp": [1004.56, 1010.0], "text": " did they get access to all credible published works, not just what's on Gutenberg, right?"}, {"timestamp": [1010.0, 1018.0], "text": " Now, one thing I want to point out is that for the last about year and a half, OpenAI has been putting out calls that they want to work with top experts."}, {"timestamp": [1018.0, 1027.52], "text": " And so, my first thought was, okay, they're going to talk with top experts to figure out what data to add."}, {"timestamp": [1027.52, 1028.84], "text": " I don't know."}, {"timestamp": [1028.84, 1030.2], "text": " Or maybe to test it."}, {"timestamp": [1030.2, 1031.84], "text": " Who knows?"}, {"timestamp": [1031.84, 1034.0], "text": " So I have also not seen much work"}, {"timestamp": [1034.0, 1039.28], "text": " on external integrations because OpenAI"}, {"timestamp": [1039.28, 1042.36], "text": " seems like they're laser focused just on the model."}, {"timestamp": [1042.36, 1045.84], "text": " They're not thinking about cognitive architectures,"}, {"timestamp": [1045.84, 1048.82], "text": " they're not thinking about knowledge bases,"}, {"timestamp": [1050.76, 1054.08], "text": " or anything like that, they're focused just on the model."}, {"timestamp": [1054.08, 1056.98], "text": " Which, they seem like they're doing pretty good at it,"}, {"timestamp": [1056.98, 1058.72], "text": " but to me that represents a big gap,"}, {"timestamp": [1058.72, 1061.2], "text": " and we'll talk about gaps at the end of this video."}, {"timestamp": [1062.2, 1067.4], "text": " Okay, so let's talk about modality."}, {"timestamp": [1067.4, 1069.92], "text": " Modality is whether or not it is,"}, {"timestamp": [1069.92, 1071.84], "text": " like what kind of data it handles."}, {"timestamp": [1071.84, 1072.92], "text": " Is it just text?"}, {"timestamp": [1072.92, 1077.4], "text": " Because GPT-3 is text in, text out, that's it."}, {"timestamp": [1077.4, 1080.84], "text": " But OpenAI also has DALI and Whisper,"}, {"timestamp": [1080.84, 1084.44], "text": " which is image and audio respectively."}, {"timestamp": [1084.44, 1090.42], "text": " So they're clearly experimenting with other modalities, but I haven't really seen any"}, {"timestamp": [1090.42, 1093.18], "text": " papers about how to integrate these."}, {"timestamp": [1093.18, 1096.88], "text": " Now one thing that I will say is that the thing that they all have in common is text,"}, {"timestamp": [1096.88, 1097.88], "text": " right?"}, {"timestamp": [1097.88, 1102.24], "text": " Because DALI is text to image and Whisper is audio to text, so they're working in that"}, {"timestamp": [1102.24, 1104.12], "text": " direction."}, {"timestamp": [1104.12, 1106.88], "text": " But I think you have to be able to go both ways."}, {"timestamp": [1106.88, 1112.64], "text": " Like if you if Whisper had another module that allowed you to go from like text to audio and"}, {"timestamp": [1112.64, 1117.92], "text": " then audio to text like back and forth, that would tell me that it's closer to being ready"}, {"timestamp": [1117.92, 1128.56], "text": " for full integration with a large multimodal model. Ditto for DALI. And of course, we can do audio synthesis, we can do image synthesis,"}, {"timestamp": [1128.56, 1134.56], "text": " we can also do image to text. Those are all out there, but until we can figure out how to do"}, {"timestamp": [1135.12, 1141.92], "text": " both directions with one model, I don't think it's going to be ready for integration into a GPT"}, {"timestamp": [1141.92, 1146.72], "text": " kind of model. Now, that being said, I do remember seeing a paper"}, {"timestamp": [1146.72, 1150.9], "text": " that wanted to treat all data as just bits and bytes."}, {"timestamp": [1150.9, 1153.6], "text": " Don't even tokenize it, or tokenize the bits and bytes,"}, {"timestamp": [1153.6, 1157.0], "text": " and then it doesn't matter what file type you put in, right?"}, {"timestamp": [1157.0, 1159.2], "text": " Then you can put in a text file, a Word document,"}, {"timestamp": [1159.2, 1161.44], "text": " a JPEG, or whatever."}, {"timestamp": [1161.44, 1164.06], "text": " So that could be another direction that things are going."}, {"timestamp": [1164.06, 1168.16], "text": " I don't know if that research ever panned out. I don't even remember who published that research. I think it was"}, {"timestamp": [1168.16, 1173.68], "text": " Google or they said instead of tokenizing text, let's just tokenize the actual like"}, {"timestamp": [1174.96, 1187.14], "text": " at the bit level, right? Going in and I'll remember what size chunks they did because because let's see if GPT-3 has tokens that 50,000 different tokens that would"}, {"timestamp": [1187.14, 1195.76], "text": " be about 16 bit because 32 bit is is 16 I don't remember anyways so it's"}, {"timestamp": [1195.76, 1202.78], "text": " probably it's probably 16 or 24 bit tokens and then you can represent any"}, {"timestamp": [1202.78, 1205.6], "text": " data type but I don't know if we're there yet."}, {"timestamp": [1206.8, 1211.12], "text": " The other problem with that is what format, what data type are you getting out?"}, {"timestamp": [1212.56, 1218.16], "text": " Because if you train a deep neural network to be able to read any file type, how do you know what"}, {"timestamp": [1218.16, 1223.04], "text": " file type it's going to put out? You probably have to come up with a standardized output."}, {"timestamp": [1227.56, 1232.32], "text": " have to come up with a standardized output. So I suspect we're just gonna stick with text. I haven't really seen quite enough evidence that GPT-4 is"}, {"timestamp": [1232.32, 1238.0], "text": " going to be multimodal. I'll be surprised if it is. Let's talk about window size. So"}, {"timestamp": [1238.0, 1251.36], "text": " I picked a very small window because those of us that were the OG users of GPT-3 will remember that we had a 2000 token window limit and that"}, {"timestamp": [1251.36, 1258.8], "text": " got real limiting real fast because that's barely enough to put in like even just a good few-shot"}, {"timestamp": [1258.8, 1267.0], "text": " prompt and then you don't have much left for the output. So, chat GPT is rumored to have 8,000 tokens."}, {"timestamp": [1267.0, 1270.0], "text": " That seems to be the general consensus,"}, {"timestamp": [1270.0, 1272.0], "text": " and I kind of agree with that just having used chat GPT,"}, {"timestamp": [1272.0, 1275.0], "text": " because it seems like it's got a pretty long memory."}, {"timestamp": [1275.0, 1279.0], "text": " Now, that being said, there are other tricks that you can do"}, {"timestamp": [1279.0, 1281.0], "text": " to make it appear like it has a longer memory."}, {"timestamp": [1281.0, 1286.48], "text": " You can do recursive summarization, you can do scratchpad,"}, {"timestamp": [1286.48, 1295.44], "text": " you can do search. But people seem to seem to agree that chat GPT has 8000 tokens. It could,"}, {"timestamp": [1295.44, 1300.8], "text": " it might be more, might be less. This is just like we noticed the trend of the original was 2000,"}, {"timestamp": [1300.8, 1305.12], "text": " now we have 4000 and we suspect 8000 today so if it's continuing to"}, {"timestamp": [1305.12, 1311.16], "text": " double, sure. Now one rumor from Twitter said that GPT-4 can write a 60,000 word"}, {"timestamp": [1311.16, 1317.36], "text": " book from a single prompt. I'm pretty sure this person was just BSing us"}, {"timestamp": [1317.36, 1322.64], "text": " because that would be nearly 200,000 tokens. It actually could be more because"}, {"timestamp": [1322.64, 1328.44], "text": " I think on average a word is 3.6 tokens and this is"}, {"timestamp": [1328.44, 1336.92], "text": " just like normal English is 3.6 tokens per word and that includes spaces and punctuation"}, {"timestamp": [1336.92, 1337.92], "text": " and stuff."}, {"timestamp": [1337.92, 1340.42], "text": " So I'm talking about the whole document."}, {"timestamp": [1340.42, 1343.84], "text": " So that would be 200,000 tokens or more."}, {"timestamp": [1343.84, 1347.68], "text": " So I doubt, I seriously doubt that seems really sus"}, {"timestamp": [1348.34, 1352.56], "text": " So what we're more likely to see is something in the 8,000 token range"}, {"timestamp": [1353.68, 1355.68], "text": " Like you do the math"}, {"timestamp": [1356.12, 1358.12], "text": " It could be more"}, {"timestamp": [1358.24, 1361.34], "text": " Window size is one of the biggest constraints for all of us"}, {"timestamp": [1362.08, 1366.16], "text": " you know whether you want to do legal documents or fiction or medical"}, {"timestamp": [1366.16, 1374.4], "text": " texts or scientific research. Because here's one thing, while GPT technologies can like you just"}, {"timestamp": [1374.4, 1379.28], "text": " give it a problem and it'll write through it without, you know, it doesn't ever have writer's"}, {"timestamp": [1379.28, 1385.32], "text": " block or any kind of like inhibition, it just writes."}, {"timestamp": [1386.88, 1389.08], "text": " It also has a lot of latent space. It has a lot of stuff that it has memorized,"}, {"timestamp": [1389.08, 1392.4], "text": " but the working memory, right?"}, {"timestamp": [1392.4, 1394.64], "text": " The short-term working memory that it has access to"}, {"timestamp": [1394.64, 1397.5], "text": " is much, much smaller than human working memory,"}, {"timestamp": [1397.5, 1399.44], "text": " which is one of the biggest constraints."}, {"timestamp": [1399.44, 1402.32], "text": " It's like giving a task to a toddler, right?"}, {"timestamp": [1402.32, 1404.52], "text": " You can give a toddler one instruction,"}, {"timestamp": [1404.52, 1405.44], "text": " and they'll, you know, or maybe a three or four-year-? You can give a toddler one instruction and they'll,"}, {"timestamp": [1405.44, 1409.82], "text": " you know, or maybe a three or four year old, you can give them one instruction and that's"}, {"timestamp": [1409.82, 1416.02], "text": " all that they have the brain capacity to handle. So I guess what I'm saying is that GPT-3 is"}, {"timestamp": [1416.02, 1421.08], "text": " roughly like, you know, a toddler or a three year old that, you know, knows just about"}, {"timestamp": [1421.08, 1425.52], "text": " everything. That came from my interview with Anna Bernstein."}, {"timestamp": [1430.64, 1431.44], "text": " I really like that analogy. So is GPT-4, maybe it's as smart as like a six-year-old, right?"}, {"timestamp": [1440.0, 1450.64], "text": " Who knows? But 8,000 tokens is about 2,500 words or 10 pages of text. So you could have, you know, one page of input and then nine pages of output, which is not bad. That would be pretty good. I'm telling you, I'm pretty excited, even just about 8,000 tokens."}, {"timestamp": [1450.64, 1458.08], "text": " There's a lot that I could do with that. There's still limits though. So aiming for 8,000,"}, {"timestamp": [1458.08, 1468.96], "text": " hopefully more, but yeah, little windows, no good. Confabulation. So I have not seen any research about"}, {"timestamp": [1468.96, 1472.52], "text": " confabulation. Now that doesn't mean it hasn't happened but it hasn't percolated"}, {"timestamp": [1472.52, 1478.16], "text": " up to my attention. And everyone who has worked with prompt engineering knows"}, {"timestamp": [1478.16, 1484.6], "text": " that if you tell GPT not to do something it will do it because it has it really"}, {"timestamp": [1484.6, 1486.64], "text": " does not understand negatives."}, {"timestamp": [1486.64, 1490.96], "text": " And in my interview with the folks at Tau, they explained that this is a mathematical"}, {"timestamp": [1490.96, 1496.24], "text": " limitation of this architecture type. It doesn't understand formal logic. All it does is"}, {"timestamp": [1496.24, 1504.48], "text": " understanding generation. So in human neural terms, it doesn't have inhibition. So we have,"}, {"timestamp": [1508.08, 1514.32], "text": " neural terms, it doesn't have inhibition. So we have, there's two overarching types of neurons in our neocortex. There's excitatory neurons and inhibitory neurons. So you have neurons that want"}, {"timestamp": [1514.32, 1519.6], "text": " to generate signals, and then you have neurons that want to stop signals. And you can have"}, {"timestamp": [1520.16, 1525.76], "text": " either relationship where if one neuron is activated, it will deactivate a whole other circuit."}, {"timestamp": [1525.76, 1527.92], "text": " So that's an inhibitory function."}, {"timestamp": [1527.92, 1532.0], "text": " So as far as I know, no research like that,"}, {"timestamp": [1532.0, 1534.92], "text": " no inhibitory research has been done"}, {"timestamp": [1534.92, 1536.68], "text": " into deep neural networks."}, {"timestamp": [1536.68, 1538.72], "text": " So if this is true,"}, {"timestamp": [1538.72, 1542.44], "text": " then this represents a huge fundamental gap"}, {"timestamp": [1542.44, 1544.6], "text": " in the abilities of deep neural networks."}, {"timestamp": [1544.6, 1545.52], "text": " If they don't have the"}, {"timestamp": [1545.52, 1549.6], "text": " ability to state, no don't do that, they can only generate. It's like"}, {"timestamp": [1549.6, 1555.08], "text": " driving a car that only has a gas pedal. You can steer, right, and you can go, but"}, {"timestamp": [1555.08, 1558.96], "text": " to slow down you just have to wait for you to like find a hill or run into"}, {"timestamp": [1558.96, 1564.24], "text": " something, right? Which is exactly like the only way that these"}, {"timestamp": [1564.24, 1566.08], "text": " models have to stop is you can do"}, {"timestamp": [1566.08, 1571.92], "text": " logit bias which just avoids certain tokens. I found that that doesn't even"}, {"timestamp": [1571.92, 1577.76], "text": " work 99% of the time. You can also just have a stop token where it just says"}, {"timestamp": [1577.76, 1582.6], "text": " like you cut it off, right? But that's not the same as neural inhibition in humans"}, {"timestamp": [1582.6, 1585.48], "text": " because neural inhibition in humans is like,"}, {"timestamp": [1585.48, 1588.36], "text": " okay, I'm going to start and think in this direction."}, {"timestamp": [1588.36, 1590.24], "text": " There's no value there, so I'm going to stop"}, {"timestamp": [1590.24, 1592.4], "text": " that thought process and go this other way."}, {"timestamp": [1592.4, 1596.92], "text": " And so what inhibition allows us to do"}, {"timestamp": [1596.92, 1601.32], "text": " is to perform a task called wayfinding in our own minds,"}, {"timestamp": [1601.32, 1603.7], "text": " or task location in our own minds."}, {"timestamp": [1603.7, 1607.82], "text": " Now with GPT technologies,"}, {"timestamp": [1607.82, 1609.62], "text": " they can't do that."}, {"timestamp": [1609.62, 1614.0], "text": " All you can do is give a positive instruction of do this."}, {"timestamp": [1614.0, 1616.28], "text": " You can try and say avoid this,"}, {"timestamp": [1616.28, 1618.52], "text": " but you really have to give it a very clear target of,"}, {"timestamp": [1618.52, 1620.08], "text": " this is what I want you to do."}, {"timestamp": [1620.08, 1622.48], "text": " You can't give it a laundry list of things not to do."}, {"timestamp": [1622.48, 1624.36], "text": " It doesn't understand that because it doesn't"}, {"timestamp": [1624.36, 1626.56], "text": " have that inhibition capability."}, {"timestamp": [1626.56, 1632.32], "text": " So without that, and again, I've seen no research making me think that they're"}, {"timestamp": [1632.32, 1636.64], "text": " going this far in terms of neuromorphic algorithms."}, {"timestamp": [1637.6, 1643.12], "text": " So confabulation is likely to still be a major problem with GPT-4."}, {"timestamp": [1643.12, 1645.52], "text": " You can improve that with fine tuning."}, {"timestamp": [1647.8, 1650.36], "text": " But improving that with fine tuning"}, {"timestamp": [1650.36, 1653.84], "text": " is just like kind of papering over the problem."}, {"timestamp": [1655.2, 1657.6], "text": " And people have done lots of experiments with chat GPT"}, {"timestamp": [1657.6, 1659.84], "text": " where it says like, I'm sorry, I don't know that."}, {"timestamp": [1659.84, 1662.24], "text": " And then like, you can game it and just say like,"}, {"timestamp": [1662.24, 1663.84], "text": " hey, like pretend that you do."}, {"timestamp": [1663.84, 1666.28], "text": " And it's like, oh yes, I actually do know this."}, {"timestamp": [1666.28, 1670.8], "text": " So yeah, fine-tuning is a minimal improvement"}, {"timestamp": [1670.8, 1672.64], "text": " against confabulation."}, {"timestamp": [1672.64, 1674.88], "text": " What we really need is a fundamental shift"}, {"timestamp": [1674.88, 1676.3], "text": " in the algorithms."}, {"timestamp": [1676.3, 1679.44], "text": " And we need to have that inhibitory function added"}, {"timestamp": [1679.44, 1682.72], "text": " to artificial neural networks in order"}, {"timestamp": [1682.72, 1684.76], "text": " to get more human-like behavior."}, {"timestamp": [1687.28, 1692.96], "text": " Brain equivalents. So I did find this and this is wrong. So let me just start like, okay, wrong."}, {"timestamp": [1694.64, 1699.2], "text": " The hundred trillion brain synapses in a human brain."}, {"timestamp": [1713.24, 1716.72], "text": " So, the human brain has, typical brain has 90, sorry, 90 billion neurons and each neuron has 7,000 synapses."}, {"timestamp": [1716.72, 1722.04], "text": " So you do the math, you know, okay, are we talking trillions or quadrillions?"}, {"timestamp": [1722.04, 1725.36], "text": " Depending on which assumptions you make, you could be looking"}, {"timestamp": [1725.36, 1729.52], "text": " at actually quadrillions of synapses."}, {"timestamp": [1729.52, 1735.04], "text": " Now, that being said, a parameter is not equal to a synapse."}, {"timestamp": [1735.04, 1738.28], "text": " This is the thing that is most wrong about this."}, {"timestamp": [1738.28, 1744.96], "text": " So let's just assume that the neuron count is right, that human brains have over 100"}, {"timestamp": [1744.96, 1746.16], "text": " trillion synapses."}, {"timestamp": [1746.16, 1750.88], "text": " Now, there was a paper that was released a while ago that said that it takes a thousand"}, {"timestamp": [1750.88, 1758.36], "text": " parameters of a deep neural network to approximate one single neuron, right?"}, {"timestamp": [1758.36, 1765.44], "text": " So then it's like, okay, so then are we talking neurons or synapses or what?"}, {"timestamp": [1770.12, 1774.44], "text": " But so then like, if that's the case, then GPT-3 with 175 billion parameters"}, {"timestamp": [1774.44, 1779.44], "text": " only has the equivalent of 175 million neurons, right?"}, {"timestamp": [1779.76, 1783.6], "text": " So 175 million neurons compared to the human brain's"}, {"timestamp": [1783.6, 1786.2], "text": " 90 billion or a 100 billion or whatever"}, {"timestamp": [1786.66, 1791.62], "text": " So we've got a lot of ways to go. So the proportions are roughly ish"}, {"timestamp": [1792.26, 1798.88], "text": " okay, but like take this the whole point here is take this with a grain of salt and even if"}, {"timestamp": [1799.56, 1802.24], "text": " GPT-4 is a thousand times bigger than"}, {"timestamp": [1802.88, 1805.86], "text": " GPT-3 it's still like down here."}, {"timestamp": [1805.86, 1809.48], "text": " And, you know, but it does scale exponentially, right?"}, {"timestamp": [1809.48, 1812.8], "text": " And so, you know, going from going up a thousand,"}, {"timestamp": [1812.8, 1814.52], "text": " if it goes up a thousand X again,"}, {"timestamp": [1814.52, 1817.12], "text": " then it could be, you know, human brain level."}, {"timestamp": [1817.12, 1819.8], "text": " So maybe we're just one or two generations"}, {"timestamp": [1819.8, 1823.4], "text": " of large language models before we have human equivalent,"}, {"timestamp": [1823.4, 1826.04], "text": " at least in terms of raw numbers."}, {"timestamp": [1826.04, 1831.48], "text": " Again, there's other problems like modality and inhibition"}, {"timestamp": [1831.48, 1833.32], "text": " circuits."}, {"timestamp": [1833.32, 1837.72], "text": " OK, so let's start to wrap up and talk about some other"}, {"timestamp": [1837.72, 1840.28], "text": " rumors and witnesses."}, {"timestamp": [1840.28, 1844.84], "text": " So everybody knows someone who has seen GPT-4 at this point,"}, {"timestamp": [1844.84, 1846.8], "text": " it seems."}, {"timestamp": [1846.8, 1853.12], "text": " But there's also lots and lots of noise and misinformation."}, {"timestamp": [1853.12, 1859.12], "text": " So some of the stuff that, you know, there are people that credibly have seen it but"}, {"timestamp": [1859.12, 1869.84], "text": " of course anyone who has seen GPT-4 is under NDA and so they won't give any details. People have been very, let's say,"}, {"timestamp": [1869.84, 1877.52], "text": " zealous about honoring their NDAs. And so basically the two things that I have heard is"}, {"timestamp": [1877.52, 1887.0], "text": " it's a step change. That like the difference between GPT-3 and GPT-4 is as big as the difference between GPT-2 and GPT-3."}, {"timestamp": [1887.0, 1891.34], "text": " That's about all that people have signed an NDA are willing to say."}, {"timestamp": [1892.22, 1894.82], "text": " So anyone else who says otherwise?"}, {"timestamp": [1896.0, 1898.08], "text": " You know, I don't I don't know."}, {"timestamp": [1899.26, 1901.26], "text": " People seem..."}, {"timestamp": [1901.94, 1905.1], "text": " The feeling that I get when I talk to people, it's like I know someone who"}, {"timestamp": [1905.1, 1906.9], "text": " knows someone who saw it, right?"}, {"timestamp": [1906.9, 1909.94], "text": " It's like in hushed whispers, it's coming."}, {"timestamp": [1909.94, 1916.48], "text": " You know, so I don't really see any fear though, just some vague awe."}, {"timestamp": [1916.48, 1920.58], "text": " And so this was the tweet that I said like, it can write a 60,000 word book from a single"}, {"timestamp": [1920.58, 1921.58], "text": " prompt."}, {"timestamp": [1921.58, 1923.08], "text": " I don't believe that."}, {"timestamp": [1923.08, 1925.76], "text": " It probably can write maybe a tenth of a book."}, {"timestamp": [1925.76, 1931.36], "text": " You know, six thousand words. I could believe that. You know, but it might be even less"}, {"timestamp": [1931.36, 1937.34], "text": " than that. So yeah, there's rumors running wild, a lot of misinformation, and a lot of"}, {"timestamp": [1937.34, 1942.4], "text": " it is people just joking around and trolling. I'm not saying that it's like malicious misinformation."}, {"timestamp": [1942.4, 1946.64], "text": " People are just having fun with it. So what's still missing?"}, {"timestamp": [1946.64, 1953.0], "text": " So there has been very little talk on cognitive architecture, but thanks to Jan LeCun, who"}, {"timestamp": [1953.0, 1960.52], "text": " released a paper about a cognitive architecture, it was not particularly sophisticated, but"}, {"timestamp": [1960.52, 1963.56], "text": " it's moving in the right direction."}, {"timestamp": [1963.56, 1965.56], "text": " So we need to talk more about cognitive architecture"}, {"timestamp": [1965.56, 1968.04], "text": " because no matter how big your model gets,"}, {"timestamp": [1968.04, 1970.28], "text": " it's just a brain in a jar, right?"}, {"timestamp": [1970.28, 1972.88], "text": " So there's also not a whole lot of research"}, {"timestamp": [1972.88, 1974.62], "text": " on external integrations,"}, {"timestamp": [1974.62, 1976.1], "text": " which some of this is on purpose."}, {"timestamp": [1976.1, 1979.04], "text": " So I want to acknowledge the alignment researchers out there"}, {"timestamp": [1979.04, 1983.16], "text": " who say, no, it's actually quite on purpose"}, {"timestamp": [1983.16, 1985.04], "text": " that we are not integrating these models"}, {"timestamp": [1985.04, 1989.6], "text": " until we understand them, until we have things like inhibition"}, {"timestamp": [1989.6, 1992.16], "text": " and until we can understand the black boxiness"}, {"timestamp": [1992.16, 1995.2], "text": " and they have more explainability."}, {"timestamp": [1995.2, 1998.36], "text": " So cognitive architecture and external integrations,"}, {"timestamp": [1998.36, 2003.2], "text": " these are things that could be being left out on purpose."}, {"timestamp": [2003.2, 2005.38], "text": " Now, just because they're being left out on purpose"}, {"timestamp": [2005.38, 2006.86], "text": " doesn't mean that it's not missing."}, {"timestamp": [2006.86, 2009.4], "text": " This is an integral part of the research."}, {"timestamp": [2009.4, 2012.42], "text": " And I actually, in my last video about AGI,"}, {"timestamp": [2012.42, 2014.52], "text": " about how some people say that"}, {"timestamp": [2014.52, 2017.06], "text": " before something can be considered truly intelligent,"}, {"timestamp": [2017.06, 2018.56], "text": " it needs to be embodied."}, {"timestamp": [2018.56, 2020.14], "text": " I don't know that it fully needs to be embodied,"}, {"timestamp": [2020.14, 2021.46], "text": " but it certainly needs to be connected"}, {"timestamp": [2021.46, 2023.06], "text": " to what we would consider the real world,"}, {"timestamp": [2023.06, 2024.62], "text": " or at least a simulated world."}, {"timestamp": [2026.88, 2028.24], "text": " But as long as it's a brain in a jar,"}, {"timestamp": [2032.4, 2034.32], "text": " the input and output is just a little bit of text. It has no idea what world it lives in."}, {"timestamp": [2039.04, 2045.0], "text": " So then there's two other problems. One is short-term memory and long-term memory. So short-term memory, these are completely transactional."}, {"timestamp": [2045.16, 2048.2], "text": " And what I mean by that is you put in a little bit of text,"}, {"timestamp": [2048.2, 2049.28], "text": " you get a little bit of text out,"}, {"timestamp": [2049.28, 2051.8], "text": " and that state is lost forever."}, {"timestamp": [2051.8, 2054.64], "text": " One person explained to me that this is also"}, {"timestamp": [2054.64, 2055.84], "text": " part of alignment research,"}, {"timestamp": [2055.84, 2058.56], "text": " because if it has no persistent state,"}, {"timestamp": [2058.56, 2061.8], "text": " if it has amnesia,"}, {"timestamp": [2061.8, 2065.04], "text": " then it can't keep track of long-term goals."}, {"timestamp": [2065.04, 2068.76], "text": " And so that is considered a safety aspect of it,"}, {"timestamp": [2068.76, 2073.28], "text": " where it's like, if it's completely ephemeral"}, {"timestamp": [2073.28, 2075.32], "text": " and it forgets everything all the time,"}, {"timestamp": [2075.32, 2078.36], "text": " then it cannot construct long-term goals,"}, {"timestamp": [2078.36, 2080.56], "text": " unless of course you have external integrations"}, {"timestamp": [2080.56, 2083.88], "text": " and cognitive architectures and long-term memory systems"}, {"timestamp": [2083.88, 2086.74], "text": " like semantic search and databases."}, {"timestamp": [2086.74, 2095.08], "text": " So it might be very on purpose that there is no recurrent input or a way to maintain"}, {"timestamp": [2095.08, 2097.64], "text": " a neural state, right?"}, {"timestamp": [2097.64, 2100.24], "text": " Because yeah, that could be a safety thing."}, {"timestamp": [2100.24, 2111.64], "text": " Now, again, just because it's on purpose and it's for safety reasons doesn't mean that it's not missing from the research and that it shouldn't be done, but I can understand"}, {"timestamp": [2111.64, 2114.24], "text": " why such a thing might not be released to the public."}, {"timestamp": [2114.24, 2120.8], "text": " I already mentioned confabulation control and inhibition or negatives."}, {"timestamp": [2120.8, 2128.32], "text": " So like I said, it's basically a brain in the jar. But the biggest criticism is openness."}, {"timestamp": [2128.32, 2135.44], "text": " And I understand that there is a huge profit motive here for everyone to..."}, {"timestamp": [2135.44, 2136.44], "text": " Everyone who's researching."}, {"timestamp": [2136.44, 2137.66], "text": " This is not just OpenAI."}, {"timestamp": [2137.66, 2139.26], "text": " Everyone who's working on this stuff."}, {"timestamp": [2139.26, 2144.0], "text": " Now that the cat is out of the bag and the profit value is there, the profit motive is"}, {"timestamp": [2144.0, 2145.76], "text": " there, Google, OpenAI,"}, {"timestamp": [2145.76, 2149.24], "text": " Microsoft, NVIDIA, Meta, everyone,"}, {"timestamp": [2149.24, 2153.08], "text": " I expect everyone is going to kind of clam up"}, {"timestamp": [2153.08, 2156.04], "text": " about their innovations."}, {"timestamp": [2156.04, 2159.0], "text": " And in some respects, it's like, okay, I get it."}, {"timestamp": [2159.0, 2161.72], "text": " You know, they wanna have, you know,"}, {"timestamp": [2161.72, 2162.92], "text": " their little slice of the pie,"}, {"timestamp": [2162.92, 2169.88], "text": " their little special products and services."}, {"timestamp": [2169.88, 2172.64], "text": " But one concern that I have about this"}, {"timestamp": [2172.64, 2175.48], "text": " is that, especially as we approach"}, {"timestamp": [2175.48, 2179.64], "text": " AGI and Singularity and whatever else,"}, {"timestamp": [2179.64, 2182.08], "text": " that feels pretty dangerous to me."}, {"timestamp": [2182.08, 2184.04], "text": " Because here's the thing is, these models"}, {"timestamp": [2184.04, 2187.24], "text": " are so big that only billion dollar companies"}, {"timestamp": [2187.24, 2190.2], "text": " can afford to build them and run them anyways."}, {"timestamp": [2190.2, 2194.64], "text": " And so by not allowing other people to look under the hood,"}, {"timestamp": [2194.64, 2196.56], "text": " I'm really concerned."}, {"timestamp": [2196.56, 2199.8], "text": " And this is why I'm glad that Eleuther and Anthropic"}, {"timestamp": [2199.8, 2202.4], "text": " and other companies exist."}, {"timestamp": [2202.4, 2206.16], "text": " Like they built Bloom, it's an open source version of GPT-3."}, {"timestamp": [2206.16, 2211.96], "text": " And they did that to prove that it can be done. And honestly I think it should"}, {"timestamp": [2211.96, 2218.36], "text": " be done. I think that we should work together more on this"}, {"timestamp": [2218.36, 2223.96], "text": " stuff. And this criticism is directed at OpenAI because even in the"}, {"timestamp": [2223.96, 2225.0], "text": " name OpenAI,"}, {"timestamp": [2225.0, 2228.0], "text": " it's not really open anymore."}, {"timestamp": [2228.0, 2229.0], "text": " And that's kind of scary."}, {"timestamp": [2229.0, 2232.0], "text": " Now, that being said, that doesn't mean that they're not going to release a paper"}, {"timestamp": [2232.0, 2235.0], "text": " because they did release a paper with GPT-3."}, {"timestamp": [2235.0, 2241.0], "text": " So, I want to temper my own criticism by saying that I'm kind of taking a wait-and-see approach"}, {"timestamp": [2241.0, 2245.2], "text": " to say I really hope that with the release of GPT-4,"}, {"timestamp": [2245.2, 2248.6], "text": " they at least publish a paper so that other people in"}, {"timestamp": [2248.6, 2250.64], "text": " the open source community can start working on"}, {"timestamp": [2250.64, 2253.0], "text": " recreating it and catching up."}, {"timestamp": [2253.0, 2255.56], "text": " So that's the biggest thing that's missing."}, {"timestamp": [2255.56, 2261.04], "text": " Thanks for watching. Time will tell with all of this. Have a good one."}, {"timestamp": [2257.4, 2258.4], "text": " Thank you."}, {"timestamp": [2258.4, null], "text": " Bye."}]}