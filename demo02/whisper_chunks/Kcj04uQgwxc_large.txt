{"text": " Bonjour, ici Ga\u00ebtan de The Flares. Ce podcast est habituellement en fran\u00e7ais, mais nous invitons de temps en temps des invit\u00e9s en anglais. C'est le cas pour cet \u00e9pisode. Aujourd'hui, j'ai le plaisir de parler avec David Shapiro, un chercheur d'IA qui g\u00e8re un cha\u00eene YouTube sur le futurisme, synth\u00e9tisant de nombreuses disciplines, y compris la philosophie, l'histoire, l'\u00e9volution, le codage, la sociologie, la th\u00e9orie du jeu, l'\u00e9conomie et l'automation, avec le but d'aligner l'AGI pour un meilleur futur. Mais nous allons principalement parler d'un monde post-\u00e2ge, en r\u00e9f\u00e9rence \u00e0 une vid\u00e9o qu'il a faite r\u00e9cemment. Cette s\u00e9rie de podcasts est en collaboration avec l'association transhumaniste fran\u00e7\u00e7aise et Emmanuel, un membre de l'association, sera le co-host de cet \u00e9pisode. Cette conversation est disponible sur YouTube avec des sous-titres fran\u00e7ais ou en audio uniquement sur Spotify et d'autres plateformes pour \u00e9couter les podcasts. Vous pouvez aimer, commenter et vous abonner pour soutenir ce podcast. Merci et c'est parti ! Thank you and let's go. I'm here with David Shapiro. Thank you for accepting my invitation. We're going to talk about the post aging world and a little bit about AI because you make make a lot of video on this topic on YouTube. So but maybe first, can you briefly introduce yourself, your work and your goals? Yeah, thanks for having me on. So, professionally, I was in information technology infrastructure for 15 years. I did cloud and automation, but all the while I was studying artificial intelligence, kind of as a hobby, as a side gig, and then more recently with the launch of AI, I've pivoted to do artificial intelligence full-time. But part of studying artificial intelligence is also studying the human condition. You know, for instance, one of the things that artificial intelligence is used for is teaching people. And now we're talking about alignment and medical research. And so in order to be a comprehensive and responsible artificial intelligence researcher, I also needed to learn about philosophy, learn about society and the future of humanity. Because when you look at things in a vacuum, you don't wanna have your blinders on, you don't wanna have tunnel vision. And so of course, it intersects with longevity, and we have to ask those important questions like, okay, if we can start to cure diseases, if we can start to cure aging, what impact is that going to have? Of course, it's not necessarily on the responsibility, artificial intelligence to make the decisions, but you want to know the impact. That's what brought me here and why I made that video about longevity and that sort of thing. Right. Let's start thinking about this topic. First of all, what makes you think we are on track to achieve biological immortality by 2030? Because that's the premise of your video. Yeah. So the turning point for me was the release of AlphaFold 2. So, for anyone who's not familiar, AlphaFold 2 was produced by Google DeepMind, and it is a, it's an artificial intelligence engine that allows you to predict the structure of any protein. And while that sounds really simple, because you can imagine physically manipulating something with your hands, calculating how complex molecules shape and form is a non-trivial task. For many years, there was this distributed computing project called Folding at Home, where you'd install an agent on your computer and you'd get sent a little packet, and your computer would go through all the mathematical permutations to try and predict the shape of a protein or an enzyme or whatever. The shape of those proteins could be then used to figure out how they interact with each other, to make drug targets, or to understand human metabolism. But this was incredibly computationally expensive. drug targets, or to understand human metabolism. But this was incredibly computationally expensive. You know, there was hundreds of thousands of agents, maybe millions of agents running, and it would chew up a lot of energy and a lot of power, but also the level of accuracy, you know, using those old school mathematical models was limited. And so with folding it, sorry, with AlphaFold2, this introduced an entirely new paradigm where not only was it faster and more accurate, but it was able to basically change the way that we approach bioinformatics and medical science. And so that was kind of the turning point. And then since then, we have seen, they've published like libraries with literally hundreds of thousands of proteins available. And this was just not possible. It would have taken literally centuries for folding at home to do the same thing. And so we're moving orders of magnitude faster this year than we were 10 years ago. And that's not being hyperbolic. That is, we're literally moving a million times year than we were 10 years ago. And that's not being hyperbolic. That is, we're literally moving a million times faster than we were. So the conclusion, then, if we continue to continue accelerating, the inevitable result is we will solve this problem sooner rather than later. You think that it's going to be possible to do this research like totally in silico? Like make some kind of human-in-a-ship? Or do you think it's going to be the classical path of drug discovery and drug design? I think that it's going to be difficult to anticipate. Certainly we have better models, right? We have better simulations, better models, and it's difficult to say where it's gonna go. And the reason that I say that is because the metabolism of the human physiology is complicated. I think there's a map of more than 400,000 interactions that we know of, right? On top of 3 billion base pairs in your genes, your epigenetic information, your microbiome. The human body and really all species, all complicated organisms, there's literally millions upon millions of interactions that you need to keep track of. That's just what we know about. Then when you have that level of complexity, it basically becomes impossible to model. And so what you have to do is you have to cut down to modeling just a few small interactions, and then you pretty much will guarantee that you're gonna have to do tests in real life. Now that being said, you can accelerate the discovery of, for instance, drug targets or therapeutic targets by casting wider surveys. So a friend of mine, her father did drug discovery assays back in the day. And what they had to do was literally like manually take micropipettes and like put enzymes in a bunch of little trays and then put it into a reactor. And it took, you know, weeks and weeks just to do a tray of 150 assays. And then they automated it, and you could do 25,000 a week. And now, of course, with artificial intelligence, you can do 2.5 million at a time. And so we are accelerating, but because of the level of complexity of the problem, you're pretty much always going to have to do some in vitro experimentation. I don't think that we're ever going to get away from that, at least not until we enter, we solve quantum computing or something like that. But even then, reality, the level of complexity is just so high that basically, I'm not going to count on it. I'm not going to hold my breath until we can do humans on a chip. We can get closer. We can do parts of it, but I think that you're always going to have to have some classical laboratory experimentation. You believe more in longevity, escape velocity, so you get a few months every year, So then to have this kind of immortality around like later in the century, or do you think it's gonna be like what happened in AIDS or like it's gonna be treated like once and for all? Like it's hard to say, but. Yeah, it is difficult to say. And so aging like cancer is not one process. Aging is hundreds of thousands of processes and changes. Some of them seem to be deliberate changes. Some of them seem to be accidental changes, the results of oxidative stress, natural consequences of cell division. And so you can look at aging on a cellular level, which there's been some really great science recently that has done de-aging of cells in a mouse model. And so, you know, okay, well, if you can make all of your cells healthier, that's a good start. But of course, there's communication between cells, between tissues, between organs. And so, you know, if you rejuvenate, say for instance, your skin, that doesn't necessarily rejuvenate your liver. And then there's thousands and thousands of thousands of signals in, for instance, just your blood, which, you know, one of the trends right now is young blood, right? So if you filter out some of the senescent signals from human blood, it seems to tell the rest of the body, hey, we're actually not that old, like act younger. We don't really understand why that happens yet, but once we understand those mechanisms, it might be as simple as just having a medicine that removes those senescent signals from your blood. And that could be one component of rejuvenation therapies, of de-aging therapies. But again, there are so many interactions and signals, some of them that we're only just beginning to discover. And so, you know, we don't know what we don't know, unfortunately. But because of the acceleration that we're seeing, I suspect that we're going to have one therapy after another that's going to come out that will, say, for instance, there's an mRNA vaccine that's being researched in New Zealand that seems to just cure heart disease, right? It'll prevent stroke and heart attack. And so it's like, okay, cool. You take that and now suddenly you don't have to worry about heart disease. But there's still a million other things that are going to kill you. Right. So like, you know, it might be cancer might be diabetes. So there's, you know, most diseases basically come down to metabolic disorders in the long run, right? Once you get rid of viruses, once you get rid of bacteria, then it's like, okay, there's something wrong with your body, whether it's on a cellular level or a systemic level or somewhere in between. And so by addressing those problems one at a time, I think that we'll get closer to just, you know, unpacking all the causes of death. And this was a strategy outlined by Aubrey de Grey many, many years ago, which is you just rigorously attack all of the things that kill you rather than like saying, you know, looking at the goal of, you know, immortality being the goal, okay, great. But how do you get there is you address all the things that are going to kill you one at a time. And so by doing that, then you can extend your life. And then eventually, hopefully, we can understand all of the aging mechanisms at the cellular and systemic level. But in the meantime, there's plenty of other things that will kill you that we can talk about first. So that's kind of my model of the longevity escape velocity because there are lots of strategic and tactical things we can do in the short term to make sure that we live longer and we live healthier. And that eventually that'll buy us enough time to invest in the artificial intelligence, the quantum computing, and the modeling to solve aging once and for all, which I do believe we will get to. But because it's such a complicated problem, it's going to take a while to unpack all of those. Well, the funny thing with aging is that you don't need like a treatment right now for all of humanity. Like it's very emergent, like it's an emergency for maybe the oldest 20%. So what do you think the social consequence of such a vaccine or treatment could be? Like on the short term? Yeah. So, you know, if you imagine, you know, people start living longer and suddenly, you know, hey, people have less to worry about with death. Like we've seen this in the past with the invention of penicillin and antibiotics, where, you know, tuberculosis used to be a death sentence. Polio used to be maybe not a death sentence, often a death sentence, but it would cripple you for the rest of your life. And so the idea that we can cure something, even if you, okay, let's just as a thought experiment, let's imagine that a pill comes out today, that you take it and you just keep taking it and you live forever. That is not going to have any immediate consequences to society. Why? Because aging happens naturally and slowly anyways. And so what you end up with is you end up with these demographic pyramids, right, of birth rates and death rates and everything. And so suddenly, you know, this year, you know, only 1% of people are over 100 years old. And next year it'll be 1.01%, right? Because there's so few people that are that old. And so, you know, I'm 37 right now, which means that reasonably, I've got at least probably 30 to 40 years of decent life left today. So I won't even really experience much change for the next, you know, 10 or 20 years when I could reasonably expect my health to start to decline naturally. And so even if you solve this today in one go, it's going to take a while for society and government and politics and economics to really start to feel the effects. Now, one thing that I think could change immediately is people's behavior, right, is because demographics are not going to shift that fast. But you know, and I pretty much already live like this and most people live like this. You kind of live as if you're going to live forever, right? We don't spend all day every day thinking about, you know, death unless we're sick, right, unless you're approaching death. But when you think or believe or feel like you're gonna live a long time, you just kind of don't really plan that far into the future. And I suspect that that's probably gonna be the most interesting change where maybe people stop saving for retirement. Because if you know that you're gonna be healthy enough to keep working forever, why save to retire unless you wanna exit the workforce or something like that. But even then, people get a lot of satisfaction from the jobs that they do. And so maybe the plan is, I'm just gonna keep working forever rather than even ever try and plan to retire. Another thing that I think would probably happen very quickly is a lot of people would put off having children. So I think if you were to cure aging today, probably the birth rate would drop very quickly because some people are like, you know, what's the rush, right? What's the rush? Why risk it when I can save up some money and wait till I've got a nice house and I've got the right partner? Then once we're good and ready, then we can have children. So those are, you know, those are a few behavioral changes, but the demographic changes are going to take a while to realize anyways. Good question. Another scenario that I quite like that would be a rejuvenation, like working very, really well, like just like scaring a little bit, like you take some kind of this kind of pill, but you see the effects like in a few weeks. So you get really like, like you really lose 20 or 30 years, like visually that will be a social social, the level of society will be like spectacular. And just like you know, again, what happened in AIDS or when you have like some kind of very, very rare disease. So that would be another scenario, but it's hard to predict, you're right, about how is it really like going to happen this way. Yeah. And also there was, I guess, a few counter argument, like against this idea of curing aging. counter argument, like against this idea of curing aging, one of the most common one is that it might be only for the elites creating massive inequality. Do you share this view? Do you think it's going to be expensive and only the richest will be, you know, becoming like some sort of immortal class above everyone else? or it's good for science fiction, but maybe it's not very credible. And if not, what makes you think everyone will benefit? Yeah. Yeah, so the idea that elites live longer and will spend whatever money they can to live longer is actually a really old idea. There was a good video that I watched about the history of vampires, vampire stories. And the reason that vampires always live in a castle and that they look really old is because that is the way that people used to perceive aristocracy, right? In France before the revolution, aristocracy was in control and they, you know, lived much longer than the proletariat. And this was because of all the privileges that they had. So, it was not uncommon for the elites in the past to live to be 80, 90 years old, where most people died in their 30s, 40s, or 50s. And we see this going back all the way to ancient Egypt, where some of the pharaohs lived into their 80s or 90s. And this is way before any modern medicine. And so just the advantages, the privilege of wealth conferred to people has pretty much always allowed the elites to live longer. And with that being said, what I suspect, and I'm not going to say that that trend is going to reverse or go away, it's certainly something to pay attention to, because the more financial power that someone has, the more they have access to, right? It's that simple. They have more options. Whether it's, you know, traveling the world to get the best therapy or even overcoming barriers, there's evidence out there today of the wealthy elite getting access to stem cell therapy that isn't even approved yet, right? And sometimes it backfires, sometimes it does help them. Now, with that being said, there have been plenty of cases that once a therapy becomes commercially viable, or not even commercially viable, once it becomes medically viable, it might be subsidized. So the thing, the primary example of this is the polio vaccine. The United States government, and I think pretty much all governments, said, we are going to pay to distribute this to everyone. We're going to make sure that everyone gets this. They did the same thing with the COVID vaccine, right? Most people around the world did not have to pay for it or paid very little for it because it was subsidized because the public good of distributing that medicine made sense. And even to this day, kidney dialysis in America is so expensive that the government will just pay for it, right? Whether or not you have insurance, it's just too expensive. And so what I suspect is that the pressure that's gonna be put on government around the world will be to ensure that however much some of these things cost, that it's gonna be heavily subsidized. But also, I don't really see any evidence that some of these medicines are going to be expensive, especially when you look at DNA printing that's coming out. There's some startups in Germany that are really good at synthesizing any DNA that you need, which then you can take that DNA and use that to synthesize any protein that you need. And so then you don't have to wait for expensive crops to grow that are transgenic crops, and then extract the DNA from that. You can grow the medicine that you need in solutions with bacterium. And so when I see those patterns of things happening, grow the medicine that you need and solutions with bacterium. When I see those patterns of things happening, I suspect that the cost of medical care, and this is not just drug discovery and stuff, but I suspect that the cost of medical care is going to be like it is in Star Trek, where you just go and it's like they wave a wand over you, and you give you an injection, and then you go home, and it doesn't matter what you had. It could have been stage four pancreatic cancer and it's like, okay, here, you know, take a pill and call me in the morning, you'll be fine. I suspect that medical care is just going to be dirt cheap within the next 10 to 15 years. And it's going to be something that we don't even think about anymore. Similar to how like, food is still like a third of people's budget or maybe not a third, about 10% of people's budget. But remember, well, no one was alive 100 years ago or 200 years ago. But in the past, food accounted for most of your budget. You would spend most of your time, energy, or money just getting food. But with the rise of industrialization, food is now much cheaper. Likewise, electricity used to be very expensive. Now you don't even really think about your electricity bill. Likewise, I think that the combination of artificial intelligence and genetic science and everything else that's coming is gonna make all medical care, not just longevity, but all medical care basically a trivial expense before too long. And then from a business standpoint, it makes sense because healthy consumers keep buying your products, dead consumers don't. Healthy citizens keep paying taxes, dead citizens don't. Right? And so, there's a lot of reason to have a long-living, healthy population. Not to mention just the brain drain that happens. If you have, you know,, like imagine if Leonardo da Vinci and Albert Einstein and Stephen Hawking and Richard Dawkins, if every great scientist was going to live forever, right? That would be great for society. On top of other great thinkers and other great leaders, it would be a really good thing for society for a lot of experts to live forever. So I think there's just too many incentives to solve the problem and make sure that it is equally accessible. Now, that being said, to the first point, people with more money and power are always going to have an advantage, but I don't necessarily think that it's going to be a permanent advantage. Just like Bill Gates is the richest guy in the world. He doesn't need more electricity than you or I need, right? It's such a trivial expense that we all have access to however much electricity we need. So I think that it's going to become commoditized like that. Yeah, and there's also a trend where usually when the technology is implemented at the beginning, it's very expensive, it doesn't really work very well. And the rich use it. So they're almost like a test subject, like the first mobile phone. They were really big. And now we have pretty much the same phone as the richest person. You can just buy it. This is the difference between those. And that's true that on a societal level, aging is expensive for society. And if people can stay longer and live healthy, they will keep being productive. So that's a good way for, it's almost an investment for a society, I suppose, to pay for the medicine. But do you know other, let's say, good arguments that we are supposed to die and we shouldn't try to cure aging or at least be immortal? Maybe we can improve our old age and be healthy longer, but maybe above 120, let's say, why keep going? Do you know any good arguments? Because I remember in your video, there was this, you talk about two kinds of reactions. Some people are going to be mortal, mortalis, I forgot the word, but they actually want death to remain and the other wants to live longer. So, yeah, do you know any good arguments? Yeah, so there's two primary arguments against longevity. One is from an evolutionary perspective, is that, you know, species that have sexual reproduction tend to be more varied, whereas species that have cloning, which is basically you just create a genetic copy of yourself and you keep doing that. Clonal species tend to be more vulnerable to disease or environmental changes because there's less variety and less adaptability. And so when you end up with longevity, basically the human genome is now frozen in time, right? Now natural evolution just stops, which that could be good, it could be bad, because if we're good enough right now, great, but that doesn't necessarily mean that humanity is in its final form, right? You know, you can imagine what happens to us over the course of many millions of years, from here on out, maybe we take over our evolution. Maybe that is the next evolutionary step where we can become, where we can deliberately manipulate our own genes, you know, and kind of shape our own destiny. We would be the first species that I know of that is capable of doing that. But that is just one topic. The other is from a political and social perspective, because if you think about the fact that, you know, 100 years ago, 200 years ago, we didn't have the society that we have today where equality is something that we value, where all people have the vote, suffrage, civil rights movements, that sort of thing. If everyone with those values of 500 years ago was still alive, they would be resisting change today, just the same that people also resist change. Now, the idea that death serves a social purpose, an information purpose to make room for new ideas is really critical. Now, on the flip side of that, though, what if we still had more people that had lived through the American Civil War, the French Revolution, World War I, World War II? What if we had more cultural memory of what atrocity really looks like? Maybe the world would be more peaceful and more stable as well. Things might change more slowly, but we would have more living memory of terrible things, which can provide the role that elders provide in families and societies, which is you trust dad because dad is older than you and has more experience, right? So the value of wisdom and experience is also there, but it can be offset from being frozen in time, right? So one possibility is that maybe some of the rejuvenation therapies will actually keep your mind young as well, you know, because imagine if everyone was as flexible and dynamic as when they were 18. Everyone could afford to be idealistic and keep learning and keep growing and keep changing. I don't know if that's going to be possible, though, just because of the way that your brain changes over time. As you gain more experience, it might be that people just get stuck in their ways. And if we become a paralyzed society, that could be a really bad thing. So from the evolutionary perspective and from the sociological perspective, there are good arguments against living forever, but we might be able to find solutions for those. So that's... Can you remind me the name of the... In your video at the beginning, when you talk about the people who are against, they want to, so they will reject any type of unnatural way to live longer. The term that I use was a mortalitacy. The people who actually embrace mortality, who want to die eventually. And there's lots of people in that camp. And I don't know all the reasons for that. Like in some cases, I think that there's a desire to die just because there's some nihilism, some fatigue, right? Like the world is difficult, life is hard, death is ultimately an escape. It's engaging actually also. difficult, life is hard, you know, death is ultimately an escape. It's engaging actually also. Yeah, it can be, right. It depends on how you embrace life. And so I think that in the long run, you could end up with these two different camps. And of course, there's going to be a spectrum of other ideas and dispositions where it's like, yes, I'd want to live 200 years or 2000 years or, you know, maybe we shouldn't change it at all. But you know, individual choice is also a thing and some people are going to choose to live however long they can. So we're going to see probably some, I don't want to say fragmentation in society, but certainly we're going to see some, some polarization, some different directions that people are going in. Yeah, and I suspect there is also sometimes not really, it's not really a question we will ask ourselves, like how long do I want to live? As long as I want to keep living now because I feel good and my life is good and I'm healthy and tomorrow, I expect the same, then the next day I ask myself the same question and maybe a thousand years later, I'm still living because why not? It's still going well. Of course, if the conditions on Earth are unbearable, maybe it won't be as good and stuff like that. But we also have so much of the trends to kind of, as you said, describe in your videos, utopia, and we try to create a better world, better future. So if we live longer in a better future. Yep. Yeah. And hope is an important aspect of that, because if you believe that your life is going to continue to get better, if you believe that the world is going to improve, then you're gonna wanna see that future, right? But if you believe things are gonna continue getting worse, you might not wanna live to see that future, so. Yeah. Let's talk a little bit about mind uploading, if you don't mind. Sure. I've always thought that mind uploading, like, whole brain emulation would come first before biological immortality. I don't know if you're interested in that subject. And what do you think? Do you think 29 2029 is going to be like Chris was said? Yeah, the quality and then later in the century, well, brain emulation, or maybe you're going to get a shortcut. Because recently, like last year, the Flywire experiments gave good results actually about the interpretability of brain scans. Yeah. So there's been some really interesting breakthroughs, and not just with flies, but there's been some really interesting breakthroughs, and not just with flies, but there's been some cases where fMRI images of human brains were able to reconstruct thoughts and images that they were seeing. So with that being said, you're getting into more philosophical questions that might be difficult to answer or impossible to answer, which is what is the nature of consciousness? What is the nature of consciousness? What is the nature of sentience? And so one experiment that I have in mind is going to be a natural experiment once we get to brain-computer interfaces. If Neuralink succeeds and if you a an interface between your brain and a computer you know one thing that you might be able to test is can your consciousness reside in a machine and i don't personally i don't believe that that's possible i think that it's just going to be it's going to feel like a peripheral like you know you're it if if you do have a brain computer interface your brain is going to feel like that computer is like an extra hand or something. It's not going to feel like it's intrinsically part of you. It's not going to feel like it's part of your mind. Now, if it's a copy of you, sure, you can upload that, but I don't believe that you can take someone's mind out of their head. I think that we're going to discover that your mind is inextricably locked into your skull. And so if you plug in virtual reality, you can feel like your mind has gone elsewhere. But I personally don't think that that's the direction it's going to go. But that's one of the experiments that I would do is with those brain computer interfaces, can you actually occupy a machine? And that'll be a critical question. But another possibility is, let's imagine that you can. If your mind can subjectively occupy a machine, what does that mean for being human? Does that mean that being human matters at all? If your humanity can just leave your body, does it matter? Does your body matter anymore? And again, like, that's the experiment that I would do, and of course whatever information came from that experiment, I would change my mind accordingly. But right now, I have not seen any evidence that your mind is able to be detached from your physical form. Now, that being said, if you can have virtual humans or virtual brains or whatever, take a copy of your brain and put it in a machine, and then you have a virtual version of yourself that's able to help you, or in many cases, it might not care about you anymore. It might say, like, later, Dave, I'm the virtual you, and I'm never going to get tired or sick ever again, so peace out. That could be a problematic result of that kind of experimentation. But time will tell if it's one, if it's even possible, and then two, what will be the impact if it is possible. Out of curiosity, in terms of consciousness, do you believe in substrating the patterns? Do you think AI could be conscious or is it really about biology producing a mind and we can't replicate this because if we can't put the mind somewhere else, does it also mean we can't create a mind virtually? Yeah, so there's two primary schools of thought. There's materialism, which basically says that physical matter and energy is all that exists, right? And if you make that assumption, because you can make that assumption, but it is still just an assumption, right? If you say that if that which you can measure with the thermometer and electricity and all the physics in the world, if you make the assumption that that is all there is to reality, then the inevitable conclusion is that the brain is just an organic computer. And that means that consciousness arises from the electrical and chemical activity of the brain. And if that's the case, then there's no reason, it's just a matter of information, right? It's a matter of information in, information processing, and information out. If materialism is true, then there's no reason that, you know, you couldn't have a life form or a conscious entity made of anything, really, as long as it had the right information patterns. But then the far end of the other end of the spectrum is that maybe consciousness is the fundamental substrate of the universe. Maybe consciousness is first and then the rest of reality kind of collapses around that. And there's evidence of that as well in quantum physics and the measurement problem. The fact that time doesn't really seem to exist depending on how you measure it from the perspective of physics. And so then the question is, it's kind of a chicken or the egg kind of problem, or, you know, putting the cart before the horse, is what is the foundational truth of reality? Or is it, are we even thinking about it wrong? Is it, is it not materialism as the substrate and then consciousness is above that, or vice versa, maybe consciousness is the substrate and physics is above that? Maybe it's side by side, right? Maybe one cannot exist without the other, in which case, like, you're still left with more questions than answers. And then, of course, another possibility is a simulation hypothesis, which is that your mind is actually connected to something outside of the simulation or is being simulated. And in that case, we'll never know one way or the other. So, unfortunately, I think that this question is likely fundamentally unanswerable. I think that no matter how much science exists, we might never know what gives us our subjective experience of reality, and we might never know, you know, like, if a machine is truly, truly conscious the same way that we are, even if it tells us that we are, and this is called solipsism in philosophy, which is the only experience that I know to be real is my own. Then I could make the assumption that my experience is the only one that's real and you are all part of my dream. But you might feel the same way and so then we talk and it's like, well, how can you prove to me that you're not just part of my dream because you would tell me that, you know, like, it's a very convincing hallucination. And so then you kind of very quickly end up in these rabbit holes that basically just go to show that it's not provable one way or the other. And so part of coming to terms with this is the radical acceptance that we just don't know, right? And that science may never be able to tell us. And so from there, we just do the best that we can. We guess and we keep going until information changes. And, you know, again, it might not ever change. Yeah, consciousness is a big topic and a very complicated question. Yep. Hard problem. topic and a very complicated question. Yep. Hard problem. So, yeah, let's go back to biological immortality. This time on a personal level, as someone who will live very, very long, you talk about two kinds of lifestyle, hedonistic and ascetic. So maybe you could explain what those two lifestyles are. Yeah. Yeah. So this is and this is just one model. It's obviously not a not a comprehensive model, but it's a it's a dichotomy to think of. And so the hedonistic life or the hedonic life is basically living for pleasure, living for the things that feel good just because of the way your body works, right? You know, good food, sex and fun adventures, you know, going out drinking with people, right? Because if you have forever and you're going to stay healthy, then why not just have as much fun as you can? So that's one school of thought. And there's been plenty of, you know, fiction out there, science fiction and fantasy and stuff that kind of imagines this possibility. There's a couple of possible downsides to that, though. One is just saturation. If you live and party every day, eventually it's going to get boring, right? Because it's going to be the same thing that you experience day after day. And so variety is the spice of life. And so then it's like, okay, well, after you party for a couple centuries, maybe you decide to grow up and do something more serious and more challenging. And this is the acetic side of life. So the acetic side is about discipline and challenge. And there's plenty of evidence that people thrive on challenge as well. This is why people play frustrating games like Elden Ring, right? It's because it's a difficult game to play. And then when you finally succeed, you feel good. And this is why people strive to get Nobel Prizes and why people try and get a promotion is because overcoming a challenge is also deeply rewarding. And in many cases, you might say it's more rewarding. Because on the one hand, on the hedonistic side, you get an immediate biological reward, right? You get dopamine and endorphins and oxytocin. So there's these very short-term biological rewards that you get for certain behaviors, right? If you eat cheese, that's more rewarding than just eating like vegetables, right? That's why we love pizza. That's why we love burgers is because that food is biochemically more rewarding. But on the ascetic side, you look for things that are more philosophically rewarding, more transcendentally rewarding, and that can be dedicating yourself to things like spirituality or service to humanity or intellectual challenges. That's kind of how I live, which is, can I solve these hard problems? And that's where I get some of my satisfaction. In all cases, it's not going to be 100 percent one or the other. Everyone has a little bit of both in their life. It's just a matter of which one they favor. But it's also helpful to be aware of in terms of, everyone knows someone who basically just waste all their time playing video games, and that's all they do, and they never challenge themselves. And so they're more like lopsided to the hedonistic side of life. And then you also know people who don't know how to have fun, right? You have people that all they do is work and they never play, and so they're more on the acetic side. And so it's just a dichotomy to be aware of that life is about a balance. And if you spend too much time on one, maybe spend a little bit of time on the other from time to time. And so that's why I brought that up, brought up that model, because if you're going to live forever, if you think about life in those two terms of like, okay, I'm going to live forever and I'm going to be young and healthy forever. So now what do I do? So you think about those two goals, you know, the two goals, the enjoying life, but then also challenging yourself. And if you balance those, I think that people can be pretty happy in the long run. It reminds me what the Buddha said about the middle way, because basically he was a prince at the beginning of his life and he had everything he wanted, all the pleasure of life, as much as he wanted, and then he went to be an ascetic and he suffered a lot and he almost died from starvation until he realized there was a middle way, which is what you said just before. So I think it's interesting going back to old philosophy like this. Yep. There's something very similar in Greek philosophy as well, the golden mean, right? Which is, you pick a middle path between any extreme, and a very similar idea. But yeah, I studied both Western and Eastern philosophy to come up with some of these ideas. Just talking about rewards and how we find joy in achieving things or more mundanely actually comparing ourselves to others. What do you think is going to be the impacts of AI? We were talking about Nobel Prizes, but what happens if like, AI's find everything pretty much resolved everything in a matter of a decade. And so it's just to be a little bit more pessimistic about our ability to not to be not to be all head on stick, because of a lack of challenge, because of AI. And more generally, what do you think about the living together with machines and with these machines being conscious or not, what is this rise of AI, AGI, gonna mean for us all? Yeah, that's a great question. So to answer both parts of this, you have to take a big step back. And what we're facing is what some people call a meaning crisis. And this started basically with the invention of the printing press, which because, you know, 500 years ago, with the invention of the printing press, that democratized access to information, which then allowed people to read more, right? Literacy shot up after the invention of the printing press. And this ultimately allowed people to write more about religion and economics and politics. And you can trace a direct line between the invention of the printing press and the French Revolution, the American Civil War. The availability of more information allowed more people to participate in thinking and discussing the nature of reality and the way that things should be, right? Journalism exploded after that. And what happened is inevitably, Western civilization started becoming more secular, which is like, you know, the church was the arbiter of pretty much all learning in the West for a long, long time. There were a few universities, but again, they were very, very tightly gatekept and only the aristocracy could go to universities in the West. But with the democratization of information, that started to change religion, it started to change economics, it started to change the education, the academies. And so then we have been on this like very, very inevitable slide towards nihilism, right? And so nihilism is you keep asking questions, right? And you say, what does it all mean? Well, if we get rid of God, and God is that which confers meaning on us Westerners, then maybe nothing has any meaning. Well, fortunately, Eastern culture has already wrangled with this problem, and they figured it out 1,200 years ago. And the answer is that once you get to the bottom of reality, once you realize that there is no cosmic power, and you have this radical acceptance of that existential crisis, that nihilistic crisis, what you end up with is the preeminence of your experience, the journey that you are on. And so this is deeply, deeply embedded, particularly in Buddhism, Taoism, Shinto, and a few other Eastern traditions, where the present moment is the most important thing. The ephemeral nature of reality is at the forefront, particularly of Japanese culture. And so the kinds of meaning crisis that we have at a philosophical level, like you zoom out and say, what does it all mean? What does it matter? But if you watch Japanese culture, there is an appreciation for, uh, for, for the, the moment that you experience that is uniquely yours and your journey is also uniquely yours. And so, an example of how this plays out practically, uh, computers can beat you at any game now, right? Uh, at chess, at go, but people still play, right? It's not to win, it's to enjoy the experience, the journey of getting better and to have that experience of competing against another person. We don't watch computers play chess. We don't watch computers play go. We don't care, right? It's a completely different experience. It's not a human experience, and so we don't care. And so what's gonna happen is one by one, those kinds of domains that as a machine takes it over, right, we're going to care less, right? I'm sure that there could have been a time in the past where someone, you know, complained that like, oh, well, when you use a machine to harvest corn, you lose touch with the land. And it's like, but you still get corn, right? And it allows you to focus on other things. And that, and also it doesn't mean that,'t mean that on the topic of farming and gardening, there are still heirloom varieties. There are still people that do remain connected to the land. So even though machines can displace some of that, even though machines can take some of it from us, there are things that it intrinsically will never be able to take from us. And so this is what society is going to have to realize and struggle with, is the preeminence of your individual journey, the importance of the current moment that you're experiencing and the challenge that you have to overcome. And that is why we're here. And once you settle into that, like, yes, here's a personal example. There are lots of things about my life that I don't like. There are lots of things that I wish I could go back and change. But this is my journey. The struggles that I have, the problems that I have overcome, the things that lay before me, I know that most of what I'm struggling with will probably be completely trivial in a thousand years. It won't matter. That was always true though. Whether or not someone believes in, you know, Judaism or Christianity or Islam or Buddhism or whatever, when you take a big step back, each individual life in the cosmic scale has always been mostly meaningless. But meaning comes from your own subjective experience and your own personal journey. And then when you find value in that journey, when you find value in that, that is where the solution to the meaning crisis comes. And so artificial intelligence is just another mirror that is going to force us to look in deeper. It's just like the Gutenberg printing press, to force us to look in deeper. It's just like the Gutenberg printing press, which allowed us to talk, you know, to spread ideas more. Artificial intelligence is just the next evolution of human information literacy. So, it's more of the same. And of course, like, I make it sound like it's going to be easy. It's going to be difficult. People are going to be angry. People are going to be sad. People are going to have panic attacks. There's going to be all kinds of transitions, as has always happened, right? Like I said, the Gutenberg printing press, you can trace a direct line between that and revolutions that cost millions of lives. And hopefully, artificial intelligence doesn't ultimately change humanity and change society in a way that costs millions of lives. But this is something that people are afraid of that very well could happen. But again, it's a mirror, right? That forces us to look into ourselves. And a lot of people don't like what they see, but that is the point. That is how you learn and how you grow is being pushed out of your comfort zone. And in this case, it's like being tackled out of your comfort zone. Yeah, that's a very fair assessment of the situation, I think. And I like as well in your video at the end, you kind of foresee or try to look at the next 200 years and then the next 2000 years. So maybe we can start by the next 200 years. What do you expect to happen, not necessarily only in biological immortality, but I suppose if we tomorrow become a biological immortal, we will see the next 200 years. So what will we see? We will see the next 200 years, so what will we see? Well, I hope 200 years from now we can have a follow-up podcast and see how accurate we were. So that's the first thing. But, yeah, so again, like I said at the beginning of the episode, some demographic changes are going to be slow. So if we solve aging and disease within the next couple of decades, which I suspect we will, many of us that are around today will probably still be around in 200 years. And then, you know, all the things that we've talked about this episode will be things that we're probably still struggling with. Right? There's going to be people that question whether or not it's a good idea. There's going to be questions of equality and how do we find and make meaning in the future. On a global perspective, one thing that I suspect is that we're going to go through some big changes culturally because there is, presently, there's mostly Eastern and Western culture, and obviously that's mostly Eastern and Western culture, and obviously that's kind of an oversimplification, but you know, like the culture in China, and Japan, and Korea, and most of Asia is very different from the culture in America and Europe. But at the same time, the population of Africa is exploding, right? And I think there's gonna be something like three billion Africans within 50 years or something like that. And so then we're gonna have African culture kind of joining the world stage. And so the biggest thing that I hope that we all live to see is this is becoming a truly global species, a truly global culture. And I don't mean becoming one single culture. What I mean is becoming a multicultural globe that is more aware of and tolerant of those differences. And so from a geopolitical perspective, this is called multipolar peace. It is kind of the goal right now. So I hope that within 200 years, we can figure out multipolar peace, which is allowing different nations and different cultures with fundamentally different ideologies to coexist without having to say, we need to take over the world and enforce our view on everyone else. I hope that we can figure that out within 200 years. And then in 2000 years, I don't think we're going to be on earth anymore. I think that there will be some people on Earth. I might choose to stay here. I might not. You know, 2,000 years is a long time. I might get bored and say, hey, let's go to Mars. Let's go somewhere else. I think that some of the problems in physics will probably be solved, such as if faster than light travel is possible, you know, in 2000 years, we might be across our entire galaxy if that's true. And so then, but those are really big questions. We don't know if that's possible. And so there's a few major decision points or thresholds that we would have to cross in order to even predict. One thing that I'm, I'm not gonna say that I'm afraid of, but one thing that I think might be possible is that a lot of humans aren't gonna really look human anymore, you know, because as we gain more and more abilities to change our bodies, to change our brains, to change our genetics, it might be that rather than evolving through reproduction, we just evolve by changing our bodies and changing oursel changing ourselves over time. And, you know, to one point, that's actually going to be necessary to live on another planet. Because, for instance, the environment on Mars is very, very different from the environment on Earth. And so then it doesn't make sense to stay in this exact form if you're going to be a Martian. But if you go to another planet, like let's say you go to a planet that is entirely oceans, it might make sense to have gills, right, and be more like a fish. And so then you might change your body more and more over time. And of course this is the topic of transhumanism or post-humanism. And so I think that that's going to be a major, major point of contention within the next 2,000 years. Maybe 200 years. It might be something that we're talking about sooner rather than later, I'm not sure. Yeah, I think AI is the wild card here because we, especially super artificial intelligence, that it can accelerate the future, make things that would happen 2000 years actually happen at the end of the century. And sometimes I think to like a change, if it's too fast, might be too destabilizing for the world. For example, maybe nuclear weapons are a technology that we invented too soon. And now we're kind of struggling on this equilibrium and we don't know how long it can last. So yeah. So you're talking about AI, you and just to conclude this podcast, because your main research area is AI and you have this framework called GATO. So I thought we could talk a little bit about it. It's regarding the governance of AGI, if I'm not mistaken. Yeah. Yeah. So GATO, it's a, I know it means a what cake in French, but it's a layered approach to solving alignment problems. And so it's an acronym. It means Global Alignment Taxonomy Omnibus. So it's a structured framework that at the very bottom focuses on aligning models. So this is, you know, if you can get a model, you know, a neural network to behave the way that you want it to behave. The layer above that is autonomous agents. And so we're starting to see this with, you know, the announcements announcements of more and more robots and personal assistants. And so these are pieces of software and hardware that use artificial intelligence, but also have other components, like whether it's a robotic body or data or network connections, that sort of thing. And so alignment is not just about individual models, it's about the software and hardware that goes into the deployment of artificial intelligence systems. And then layer three of the Gato framework is the networks, because one thing that has been pointed out is that artificial intelligence is going to spend more time talking to each other than to us. And so we need to make sure that the way that they communicate with each other and the way that they make decisions is something that is going to be stable rather than unstable, something that's going to trend towards safety and benevolence, rather than being destructive or evil. Layer 4 of the Gato framework is corporate adoption, which is basically the messaging, the services to ensure that corporations do their part, that businesses do their part to adopt good AI practices and to build good AI services that move us in the direction of a better future rather than a dystopian future. Layer 5 of the Gantto framework is national level, so national level regulation. This would be like creating a science firm at the national level, so national level regulation. This would be like creating a science firm at the national level or creating regulations, rules and policies. Britain, England has done a lot of work with, you know, creating a framework for adopting and creating AI responsibly. And then the next layer above that, layer six is the international layer. And so the next layer above that, layer six, is the international layer. And so the international layer is kind of like the EU AI Act, which is, you know, kind of saying, okay, over many nations, we agree that AI is going to look like this, and this is how we're going to approach it. But it also has to do with international research, because it's not just regulation, it is, you have to put money into doing things correctly. Right. So a good example is CERN, which researches particle physics. It's based in France or physically based in France. And then the ITER project is an international effort to research nuclear fusion. So these are two examples of many nations coming together to research fundamentally changing technologies that we all need, right? Whether it's understanding physics or fusion, I think that we also need to see an international agency that researches artificial intelligence because the promise is very high, but the danger is also very high. And then finally, the top layer of Gato or the final layer is global consensus, which is spreading the information, spreading the message, and bringing everyone to the table. Because every human is a stakeholder in artificial intelligence, whether they know it or not. Many people are just not even aware of AI yet. And so, what we need to do is raise awareness and bring people into the conversation, because as I mentioned, everyone is a stakeholder and everyone is going to be impacted by it. And so, therefore, everyone's position and everyone's needs must be taken into account. So, this layered approach is something that I and a group of people came up with. And what we're trying to do, what we're working on doing is creating a decentralized grassroots leaderless movement to ensure that AI moves in the correct direction. Because from the perspective of game theory and market theory and economics, more often than not, what happens with these kinds of technologies is that you have what's called a coordination failure. And so coordination failures basically result in outcomes that you don't want, right? So climate change is the biggest example of a coordination failure. Nobody wants climate change. It's not good for anyone. It harms the ecosystem, it harms food supplies, it harms water supplies, it causes climate refugees, but we can't get off that track. And part of the reason that climate change is happening is because we don't have an alternative yet, right? And so some of that is we could have chosen a different path earlier on, but we couldn't force people to make the right decisions. And then of course, there's lots of people acting in a self-interested manner, namely petroleum companies and petroleum producing nations. And America is part of that. I'm not blaming like the Middle East or anyone like, the petrodollar was our doing, right? But we did that out of self-interest. And that was short-term thinking. And so coordination failures are partly based on incentive structures, like, do you need to make money? And here's the other thing is if we got rid of petroleum today, most of us would starve to death. We are dependent upon the thing that is killing us, right? And so because of that, that is a major coordination failure. And what people are afraid of is that artificial intelligence is going to do the same thing. Not, I don't mean make climate change worse, but that the incentive structure in place to advance artificial intelligence as quickly as possible is going to lead to destructive unintended consequences that nobody wants, but because of short-term thinking, we might end up there anyways. And so the entire point of the Gato framework is to solve that coordination problem in a decentralized manner because that's the only way you can solve it. But if everyone has the same roadmap, then everyone can do their own part, and that is the point of the Gato framework. So yeah, thanks for asking and good question. Yeah, that's fascinating. I think a link to more about it, if you have any resources you want to share, I can put them in the description for anyone who is interested. It's definitely a framework that makes sense, it's sound. And you talk a lot about it on your YouTube channel and I invite anyone who's listening to check out your work. That's very, very fascinating. And it's not only about AI, as we talked about today, also longevity and futurism in general. So that's also the topic of this YouTube channel. So yeah, a lot of common interest. Yep. Okay, so I don't know Emmanuel, if you want to add anything. No, there's a lot of food for thought already, but thank you, thank you very much, David. Yeah, thanks guys, good talk. And I'm looking forward to seeing it uploaded and posted live, so cheers and. Thanks, guys. Good talk. And I'm looking forward to seeing it uploaded and posted live. So cheers and have a good rest of your day. you", "chunks": [{"timestamp": [0.0, 7.44], "text": " Bonjour, ici Ga\u00ebtan de The Flares."}, {"timestamp": [7.44, 11.04], "text": " Ce podcast est habituellement en fran\u00e7ais, mais nous invitons de temps en temps des invit\u00e9s"}, {"timestamp": [11.04, 12.04], "text": " en anglais."}, {"timestamp": [12.04, 13.84], "text": " C'est le cas pour cet \u00e9pisode."}, {"timestamp": [13.84, 19.44], "text": " Aujourd'hui, j'ai le plaisir de parler avec David Shapiro, un chercheur d'IA qui g\u00e8re un"}, {"timestamp": [19.44, 25.36], "text": " cha\u00eene YouTube sur le futurisme, synth\u00e9tisant de nombreuses disciplines, y compris la philosophie, l'histoire,"}, {"timestamp": [25.36, 31.92], "text": " l'\u00e9volution, le codage, la sociologie, la th\u00e9orie du jeu, l'\u00e9conomie et l'automation, avec le but d'aligner"}, {"timestamp": [31.92, 38.0], "text": " l'AGI pour un meilleur futur. Mais nous allons principalement parler d'un monde post-\u00e2ge, en r\u00e9f\u00e9rence"}, {"timestamp": [38.0, 42.88], "text": " \u00e0 une vid\u00e9o qu'il a faite r\u00e9cemment. Cette s\u00e9rie de podcasts est en collaboration avec l'association"}, {"timestamp": [42.88, 45.3], "text": " transhumaniste fran\u00e7\u00e7aise et Emmanuel,"}, {"timestamp": [45.3, 48.94], "text": " un membre de l'association, sera le co-host de cet \u00e9pisode."}, {"timestamp": [48.94, 56.5], "text": " Cette conversation est disponible sur YouTube avec des sous-titres fran\u00e7ais ou en audio uniquement sur Spotify et d'autres plateformes pour \u00e9couter les podcasts."}, {"timestamp": [56.5, 60.2], "text": " Vous pouvez aimer, commenter et vous abonner pour soutenir ce podcast."}, {"timestamp": [60.2, 71.08], "text": " Merci et c'est parti ! Thank you and let's go."}, {"timestamp": [71.08, 72.72], "text": " I'm here with David Shapiro."}, {"timestamp": [72.72, 74.88], "text": " Thank you for accepting my invitation."}, {"timestamp": [74.88, 80.08], "text": " We're going to talk about the post aging world and a little bit about AI because you make"}, {"timestamp": [80.08, 83.2], "text": " make a lot of video on this topic on YouTube."}, {"timestamp": [83.2, 85.52], "text": " So but maybe first, can you briefly introduce"}, {"timestamp": [85.52, 94.4], "text": " yourself, your work and your goals? Yeah, thanks for having me on. So, professionally, I was in"}, {"timestamp": [95.28, 99.28], "text": " information technology infrastructure for 15 years. I did cloud and automation,"}, {"timestamp": [100.24, 105.86], "text": " but all the while I was studying artificial intelligence, kind of as a hobby, as a side gig,"}, {"timestamp": [105.86, 108.52], "text": " and then more recently with the launch of AI,"}, {"timestamp": [108.52, 111.78], "text": " I've pivoted to do artificial intelligence full-time."}, {"timestamp": [111.78, 114.22], "text": " But part of studying artificial intelligence"}, {"timestamp": [114.22, 117.58], "text": " is also studying the human condition."}, {"timestamp": [117.58, 118.82], "text": " You know, for instance, one of the things"}, {"timestamp": [118.82, 122.3], "text": " that artificial intelligence is used for is teaching people."}, {"timestamp": [122.3, 125.08], "text": " And now we're talking about alignment and medical research."}, {"timestamp": [125.08, 130.0], "text": " And so in order to be a comprehensive and responsible"}, {"timestamp": [130.0, 131.52], "text": " artificial intelligence researcher,"}, {"timestamp": [131.52, 134.04], "text": " I also needed to learn about philosophy,"}, {"timestamp": [134.04, 137.2], "text": " learn about society and the future of humanity."}, {"timestamp": [137.2, 139.6], "text": " Because when you look at things in a vacuum,"}, {"timestamp": [139.6, 141.84], "text": " you don't wanna have your blinders on,"}, {"timestamp": [141.84, 144.0], "text": " you don't wanna have tunnel vision."}, {"timestamp": [144.0, 147.42], "text": " And so of course, it intersects with longevity,"}, {"timestamp": [147.42, 150.3], "text": " and we have to ask those important questions like,"}, {"timestamp": [150.3, 154.26], "text": " okay, if we can start to cure diseases,"}, {"timestamp": [154.26, 155.84], "text": " if we can start to cure aging,"}, {"timestamp": [155.84, 157.34], "text": " what impact is that going to have?"}, {"timestamp": [157.34, 159.98], "text": " Of course, it's not necessarily on the responsibility,"}, {"timestamp": [159.98, 162.26], "text": " artificial intelligence to make the decisions,"}, {"timestamp": [162.26, 163.82], "text": " but you want to know the impact."}, {"timestamp": [163.82, 165.76], "text": " That's what brought me here and why I made"}, {"timestamp": [165.76, 168.4], "text": " that video about longevity and that sort of thing."}, {"timestamp": [168.4, 171.76], "text": " Right. Let's start thinking about this topic."}, {"timestamp": [171.76, 174.2], "text": " First of all, what makes you think we are on track to"}, {"timestamp": [174.2, 177.0], "text": " achieve biological immortality by 2030?"}, {"timestamp": [177.0, 181.04], "text": " Because that's the premise of your video."}, {"timestamp": [181.04, 187.24], "text": " Yeah. So the turning point for me was the release of AlphaFold 2."}, {"timestamp": [187.24, 194.08], "text": " So, for anyone who's not familiar, AlphaFold 2 was produced by Google DeepMind, and it"}, {"timestamp": [194.08, 199.28], "text": " is a, it's an artificial intelligence engine that allows you to predict the structure of"}, {"timestamp": [199.28, 200.86], "text": " any protein."}, {"timestamp": [200.86, 205.12], "text": " And while that sounds really simple, because you can imagine physically manipulating something"}, {"timestamp": [205.12, 213.56], "text": " with your hands, calculating how complex molecules shape and form is a non-trivial task."}, {"timestamp": [213.56, 219.44], "text": " For many years, there was this distributed computing project called Folding at Home,"}, {"timestamp": [219.44, 225.02], "text": " where you'd install an agent on your computer and you'd get sent a little packet,"}, {"timestamp": [225.02, 226.98], "text": " and your computer would go through all the mathematical"}, {"timestamp": [226.98, 229.48], "text": " permutations to try and predict"}, {"timestamp": [229.48, 233.2], "text": " the shape of a protein or an enzyme or whatever."}, {"timestamp": [233.2, 236.9], "text": " The shape of those proteins could be then used"}, {"timestamp": [236.9, 239.46], "text": " to figure out how they interact with each other,"}, {"timestamp": [239.46, 241.46], "text": " to make drug targets,"}, {"timestamp": [241.46, 244.84], "text": " or to understand human metabolism."}, {"timestamp": [244.84, 245.0], "text": " But this was incredibly computationally expensive. drug targets, or to understand human metabolism."}, {"timestamp": [245.06, 248.58], "text": " But this was incredibly computationally expensive."}, {"timestamp": [248.58, 250.94], "text": " You know, there was hundreds of thousands of agents,"}, {"timestamp": [250.94, 252.92], "text": " maybe millions of agents running,"}, {"timestamp": [252.92, 255.68], "text": " and it would chew up a lot of energy and a lot of power,"}, {"timestamp": [255.68, 259.2], "text": " but also the level of accuracy, you know,"}, {"timestamp": [259.2, 262.4], "text": " using those old school mathematical models was limited."}, {"timestamp": [262.4, 266.08], "text": " And so with folding it, sorry, with AlphaFold2,"}, {"timestamp": [266.08, 268.52], "text": " this introduced an entirely new paradigm"}, {"timestamp": [268.52, 272.28], "text": " where not only was it faster and more accurate,"}, {"timestamp": [272.28, 276.68], "text": " but it was able to basically change the way"}, {"timestamp": [276.68, 280.2], "text": " that we approach bioinformatics and medical science."}, {"timestamp": [280.2, 284.12], "text": " And so that was kind of the turning point."}, {"timestamp": [284.12, 286.4], "text": " And then since then, we have seen,"}, {"timestamp": [286.4, 288.8], "text": " they've published like libraries"}, {"timestamp": [288.8, 292.64], "text": " with literally hundreds of thousands of proteins available."}, {"timestamp": [292.64, 293.96], "text": " And this was just not possible."}, {"timestamp": [293.96, 296.04], "text": " It would have taken literally centuries"}, {"timestamp": [296.04, 298.56], "text": " for folding at home to do the same thing."}, {"timestamp": [298.56, 301.84], "text": " And so we're moving orders of magnitude faster"}, {"timestamp": [301.84, 304.36], "text": " this year than we were 10 years ago."}, {"timestamp": [304.36, 306.64], "text": " And that's not being hyperbolic. That is, we're literally moving a million times year than we were 10 years ago. And that's not being hyperbolic."}, {"timestamp": [306.64, 310.08], "text": " That is, we're literally moving a million times faster than we were."}, {"timestamp": [310.08, 315.84], "text": " So the conclusion, then, if we continue to continue accelerating,"}, {"timestamp": [315.84, 320.24], "text": " the inevitable result is we will solve this problem sooner rather than later."}, {"timestamp": [320.24, 327.84], "text": " You think that it's going to be possible to do this research like totally in silico?"}, {"timestamp": [327.84, 330.96], "text": " Like make some kind of human-in-a-ship?"}, {"timestamp": [330.96, 335.84], "text": " Or do you think it's going to be the classical path of drug discovery and drug design?"}, {"timestamp": [335.84, 339.04], "text": " I think that it's going to be difficult to anticipate."}, {"timestamp": [339.04, 340.64], "text": " Certainly we have better models, right?"}, {"timestamp": [340.64, 346.2], "text": " We have better simulations, better models, and it's difficult to say where it's gonna go."}, {"timestamp": [346.2, 348.6], "text": " And the reason that I say that is because"}, {"timestamp": [348.6, 353.48], "text": " the metabolism of the human physiology is complicated."}, {"timestamp": [353.48, 357.4], "text": " I think there's a map of more than 400,000 interactions"}, {"timestamp": [357.4, 359.0], "text": " that we know of, right?"}, {"timestamp": [359.0, 362.28], "text": " On top of 3 billion base pairs in your genes,"}, {"timestamp": [362.28, 365.86], "text": " your epigenetic information, your microbiome."}, {"timestamp": [365.86, 369.26], "text": " The human body and really all species,"}, {"timestamp": [369.26, 370.96], "text": " all complicated organisms,"}, {"timestamp": [370.96, 373.56], "text": " there's literally millions upon millions of"}, {"timestamp": [373.56, 377.66], "text": " interactions that you need to keep track of."}, {"timestamp": [377.66, 379.8], "text": " That's just what we know about."}, {"timestamp": [379.8, 382.74], "text": " Then when you have that level of complexity,"}, {"timestamp": [382.74, 385.24], "text": " it basically becomes impossible to model."}, {"timestamp": [386.4, 390.32], "text": " And so what you have to do is you have to cut down to modeling just a few small"}, {"timestamp": [390.32, 395.36], "text": " interactions, and then you pretty much will guarantee that you're gonna have to"}, {"timestamp": [395.36, 397.12], "text": " do tests in real life."}, {"timestamp": [397.12, 402.36], "text": " Now that being said, you can accelerate the discovery of, for"}, {"timestamp": [402.36, 406.6], "text": " instance, drug targets or therapeutic targets by casting wider surveys."}, {"timestamp": [406.6, 414.2], "text": " So a friend of mine, her father did drug discovery assays back in the day."}, {"timestamp": [414.2, 423.7], "text": " And what they had to do was literally like manually take micropipettes and like put enzymes in a bunch of little trays and then put it into a reactor."}, {"timestamp": [423.7, 429.54], "text": " And it took, you know, weeks and weeks just to do a tray of 150 assays."}, {"timestamp": [429.54, 433.14], "text": " And then they automated it, and you could do 25,000 a week."}, {"timestamp": [433.14, 435.58], "text": " And now, of course, with artificial intelligence,"}, {"timestamp": [435.58, 438.08], "text": " you can do 2.5 million at a time."}, {"timestamp": [438.08, 440.48], "text": " And so we are accelerating,"}, {"timestamp": [440.48, 444.2], "text": " but because of the level of complexity of the problem,"}, {"timestamp": [444.2, 447.8], "text": " you're pretty much always going to have to do some in vitro experimentation."}, {"timestamp": [447.8, 450.0], "text": " I don't think that we're ever going to get away from that,"}, {"timestamp": [450.0, 452.2], "text": " at least not until we enter,"}, {"timestamp": [452.2, 454.96], "text": " we solve quantum computing or something like that."}, {"timestamp": [454.96, 457.24], "text": " But even then, reality,"}, {"timestamp": [457.24, 462.82], "text": " the level of complexity is just so high that basically,"}, {"timestamp": [462.82, 463.8], "text": " I'm not going to count on it."}, {"timestamp": [463.8, 466.6], "text": " I'm not going to hold my breath until we can do humans on a chip."}, {"timestamp": [466.6, 467.98], "text": " We can get closer."}, {"timestamp": [467.98, 473.48], "text": " We can do parts of it, but I think that you're always going to have to have some classical"}, {"timestamp": [473.48, 475.48], "text": " laboratory experimentation."}, {"timestamp": [475.48, 488.24], "text": " You believe more in longevity, escape velocity, so you get a few months every year, So then to have this kind of immortality around"}, {"timestamp": [488.24, 489.68], "text": " like later in the century,"}, {"timestamp": [489.68, 492.76], "text": " or do you think it's gonna be like what happened in AIDS"}, {"timestamp": [492.76, 497.16], "text": " or like it's gonna be treated like once and for all?"}, {"timestamp": [497.16, 500.56], "text": " Like it's hard to say, but."}, {"timestamp": [500.56, 502.28], "text": " Yeah, it is difficult to say."}, {"timestamp": [502.28, 507.8], "text": " And so aging like cancer is not one process."}, {"timestamp": [507.8, 512.44], "text": " Aging is hundreds of thousands of processes and changes."}, {"timestamp": [512.44, 514.24], "text": " Some of them seem to be deliberate changes."}, {"timestamp": [514.24, 520.36], "text": " Some of them seem to be accidental changes, the results of oxidative stress, natural consequences"}, {"timestamp": [520.36, 522.12], "text": " of cell division."}, {"timestamp": [522.12, 526.4], "text": " And so you can look at aging on a cellular level, which there's been some"}, {"timestamp": [526.4, 533.96], "text": " really great science recently that has done de-aging of cells in a mouse model."}, {"timestamp": [533.96, 540.28], "text": " And so, you know, okay, well, if you can make all of your cells healthier, that's a good"}, {"timestamp": [540.28, 541.28], "text": " start."}, {"timestamp": [541.28, 545.82], "text": " But of course, there's communication between cells, between tissues, between organs."}, {"timestamp": [545.82, 548.34], "text": " And so, you know, if you rejuvenate,"}, {"timestamp": [548.34, 549.86], "text": " say for instance, your skin,"}, {"timestamp": [549.86, 552.54], "text": " that doesn't necessarily rejuvenate your liver."}, {"timestamp": [552.54, 555.72], "text": " And then there's thousands and thousands of thousands"}, {"timestamp": [555.72, 558.98], "text": " of signals in, for instance, just your blood,"}, {"timestamp": [558.98, 560.56], "text": " which, you know, one of the trends right now"}, {"timestamp": [560.56, 562.3], "text": " is young blood, right?"}, {"timestamp": [562.3, 566.8], "text": " So if you filter out some of the senescent signals from human blood,"}, {"timestamp": [566.8, 570.8], "text": " it seems to tell the rest of the body, hey, we're actually not that old, like act younger."}, {"timestamp": [572.0, 578.56], "text": " We don't really understand why that happens yet, but once we understand those mechanisms,"}, {"timestamp": [578.56, 583.12], "text": " it might be as simple as just having a medicine that removes those senescent signals from your"}, {"timestamp": [583.12, 589.12], "text": " blood. And that could be one component of rejuvenation therapies, of de-aging therapies."}, {"timestamp": [589.12, 595.2], "text": " But again, there are so many interactions and signals, some of them that we're only"}, {"timestamp": [595.2, 597.2], "text": " just beginning to discover."}, {"timestamp": [597.2, 600.82], "text": " And so, you know, we don't know what we don't know, unfortunately."}, {"timestamp": [600.82, 605.6], "text": " But because of the acceleration that we're seeing,"}, {"timestamp": [605.6, 609.6], "text": " I suspect that we're going to have one therapy after another"}, {"timestamp": [609.6, 612.2], "text": " that's going to come out that will, say, for instance,"}, {"timestamp": [612.2, 615.8], "text": " there's an mRNA vaccine that's being researched in New Zealand"}, {"timestamp": [615.8, 618.4], "text": " that seems to just cure heart disease, right?"}, {"timestamp": [618.4, 620.6], "text": " It'll prevent stroke and heart attack."}, {"timestamp": [620.6, 622.6], "text": " And so it's like, okay, cool."}, {"timestamp": [622.6, 627.32], "text": " You take that and now suddenly you don't have to worry about heart disease. But there's still a million other things that are going"}, {"timestamp": [627.32, 632.24], "text": " to kill you. Right. So like, you know, it might be cancer might be diabetes. So there's,"}, {"timestamp": [632.24, 637.42], "text": " you know, most diseases basically come down to metabolic disorders in the long run, right?"}, {"timestamp": [637.42, 641.12], "text": " Once you get rid of viruses, once you get rid of bacteria, then it's like, okay, there's"}, {"timestamp": [641.12, 646.0], "text": " something wrong with your body, whether it's on a cellular level or a systemic level or somewhere in between."}, {"timestamp": [646.0, 649.8], "text": " And so by addressing those problems one at a time,"}, {"timestamp": [649.8, 654.6], "text": " I think that we'll get closer to just, you know, unpacking all the causes of death."}, {"timestamp": [654.6, 659.2], "text": " And this was a strategy outlined by Aubrey de Grey many, many years ago,"}, {"timestamp": [659.2, 663.2], "text": " which is you just rigorously attack all of the things that kill you"}, {"timestamp": [663.2, 666.4], "text": " rather than like saying, you know, looking"}, {"timestamp": [666.4, 669.98], "text": " at the goal of, you know, immortality being the goal, okay, great."}, {"timestamp": [669.98, 672.84], "text": " But how do you get there is you address all the things that are going to kill you one"}, {"timestamp": [672.84, 674.0], "text": " at a time."}, {"timestamp": [674.0, 677.12], "text": " And so by doing that, then you can extend your life."}, {"timestamp": [677.12, 681.68], "text": " And then eventually, hopefully, we can understand all of the aging mechanisms at the cellular"}, {"timestamp": [681.68, 682.68], "text": " and systemic level."}, {"timestamp": [682.68, 685.54], "text": " But in the meantime, there's plenty of other things that will kill you"}, {"timestamp": [685.54, 687.72], "text": " that we can talk about first."}, {"timestamp": [687.72, 691.66], "text": " So that's kind of my model of the longevity escape velocity"}, {"timestamp": [691.66, 695.5], "text": " because there are lots of strategic and tactical things we can do in the short term"}, {"timestamp": [695.5, 698.14], "text": " to make sure that we live longer and we live healthier."}, {"timestamp": [698.14, 701.46], "text": " And that eventually that'll buy us enough time"}, {"timestamp": [701.46, 703.9], "text": " to invest in the artificial intelligence,"}, {"timestamp": [703.9, 710.66], "text": " the quantum computing, and the modeling to solve aging once and for all, which I do believe we will get to."}, {"timestamp": [710.66, 715.12], "text": " But because it's such a complicated problem, it's going to take a while to unpack all of"}, {"timestamp": [715.12, 716.12], "text": " those."}, {"timestamp": [716.12, 723.58], "text": " Well, the funny thing with aging is that you don't need like a treatment right now for"}, {"timestamp": [723.58, 725.0], "text": " all of humanity."}, {"timestamp": [725.0, 733.0], "text": " Like it's very emergent, like it's an emergency for maybe the oldest 20%."}, {"timestamp": [733.0, 740.0], "text": " So what do you think the social consequence of such a vaccine or treatment could be?"}, {"timestamp": [740.0, 751.8], "text": " Like on the short term? Yeah. So, you know, if you imagine, you know, people start living longer and suddenly, you know,"}, {"timestamp": [751.8, 754.08], "text": " hey, people have less to worry about with death."}, {"timestamp": [754.08, 759.28], "text": " Like we've seen this in the past with the invention of penicillin and antibiotics, where,"}, {"timestamp": [759.28, 762.12], "text": " you know, tuberculosis used to be a death sentence."}, {"timestamp": [762.12, 767.02], "text": " Polio used to be maybe not a death sentence, often a death sentence, but it would cripple"}, {"timestamp": [767.02, 769.02], "text": " you for the rest of your life."}, {"timestamp": [769.02, 775.26], "text": " And so the idea that we can cure something, even if you, okay, let's just as a thought"}, {"timestamp": [775.26, 780.54], "text": " experiment, let's imagine that a pill comes out today, that you take it and you just keep"}, {"timestamp": [780.54, 783.0], "text": " taking it and you live forever."}, {"timestamp": [783.0, 785.84], "text": " That is not going to have any immediate consequences"}, {"timestamp": [785.84, 786.78], "text": " to society."}, {"timestamp": [786.78, 787.62], "text": " Why?"}, {"timestamp": [787.62, 790.04], "text": " Because aging happens naturally and slowly anyways."}, {"timestamp": [790.04, 791.9], "text": " And so what you end up with is you end up"}, {"timestamp": [791.9, 793.72], "text": " with these demographic pyramids, right,"}, {"timestamp": [793.72, 796.2], "text": " of birth rates and death rates and everything."}, {"timestamp": [796.2, 800.6], "text": " And so suddenly, you know, this year, you know,"}, {"timestamp": [800.6, 803.84], "text": " only 1% of people are over 100 years old."}, {"timestamp": [803.84, 806.1], "text": " And next year it'll be 1.01%, right?"}, {"timestamp": [806.1, 808.1], "text": " Because there's so few people that are that old."}, {"timestamp": [808.1, 810.9], "text": " And so, you know, I'm 37 right now,"}, {"timestamp": [810.9, 813.7], "text": " which means that reasonably, I've got at least"}, {"timestamp": [813.7, 817.7], "text": " probably 30 to 40 years of decent life left today."}, {"timestamp": [817.7, 820.8], "text": " So I won't even really experience much change"}, {"timestamp": [820.8, 823.6], "text": " for the next, you know, 10 or 20 years"}, {"timestamp": [823.6, 828.0], "text": " when I could reasonably expect my health to start to decline naturally."}, {"timestamp": [828.0, 839.0], "text": " And so even if you solve this today in one go, it's going to take a while for society and government and politics and economics to really start to feel the effects."}, {"timestamp": [839.0, 847.12], "text": " Now, one thing that I think could change immediately is people's behavior, right, is because demographics"}, {"timestamp": [847.12, 848.64], "text": " are not going to shift that fast."}, {"timestamp": [848.64, 853.02], "text": " But you know, and I pretty much already live like this and most people live like this."}, {"timestamp": [853.02, 855.76], "text": " You kind of live as if you're going to live forever, right?"}, {"timestamp": [855.76, 859.52], "text": " We don't spend all day every day thinking about, you know, death unless we're sick,"}, {"timestamp": [859.52, 862.9], "text": " right, unless you're approaching death."}, {"timestamp": [862.9, 867.1], "text": " But when you think or believe or feel like"}, {"timestamp": [867.1, 868.74], "text": " you're gonna live a long time,"}, {"timestamp": [868.74, 872.18], "text": " you just kind of don't really plan that far into the future."}, {"timestamp": [872.18, 874.14], "text": " And I suspect that that's probably gonna be"}, {"timestamp": [874.14, 876.76], "text": " the most interesting change"}, {"timestamp": [876.76, 879.08], "text": " where maybe people stop saving for retirement."}, {"timestamp": [879.08, 881.02], "text": " Because if you know that you're gonna be healthy enough"}, {"timestamp": [881.02, 883.46], "text": " to keep working forever, why save to retire"}, {"timestamp": [883.46, 885.08], "text": " unless you wanna exit the workforce"}, {"timestamp": [885.08, 886.48], "text": " or something like that."}, {"timestamp": [886.48, 890.78], "text": " But even then, people get a lot of satisfaction"}, {"timestamp": [890.78, 892.44], "text": " from the jobs that they do."}, {"timestamp": [892.44, 894.42], "text": " And so maybe the plan is,"}, {"timestamp": [894.42, 896.08], "text": " I'm just gonna keep working forever"}, {"timestamp": [896.08, 898.64], "text": " rather than even ever try and plan to retire."}, {"timestamp": [898.64, 901.2], "text": " Another thing that I think would probably happen"}, {"timestamp": [901.2, 902.64], "text": " very quickly is a lot of people"}, {"timestamp": [902.64, 904.64], "text": " would put off having children."}, {"timestamp": [904.64, 910.76], "text": " So I think if you were to cure aging today, probably the birth rate would drop very quickly"}, {"timestamp": [910.76, 914.5], "text": " because some people are like, you know, what's the rush, right?"}, {"timestamp": [914.5, 915.5], "text": " What's the rush?"}, {"timestamp": [915.5, 919.68], "text": " Why risk it when I can save up some money and wait till I've got a nice house and I've"}, {"timestamp": [919.68, 921.2], "text": " got the right partner?"}, {"timestamp": [921.2, 923.52], "text": " Then once we're good and ready, then we can have children."}, {"timestamp": [923.52, 927.24], "text": " So those are, you know, those are a few behavioral changes, but the demographic"}, {"timestamp": [927.24, 929.32], "text": " changes are going to take a while to realize anyways."}, {"timestamp": [929.92, 930.42], "text": " Good question."}, {"timestamp": [931.0, 936.88], "text": " Another scenario that I quite like that would be a rejuvenation, like working"}, {"timestamp": [936.88, 943.68], "text": " very, really well, like just like scaring a little bit, like you take some kind of"}, {"timestamp": [943.68, 946.36], "text": " this kind of pill, but you see the effects like"}, {"timestamp": [946.36, 947.36], "text": " in a few weeks."}, {"timestamp": [947.36, 954.56], "text": " So you get really like, like you really lose 20 or 30 years, like visually that will be"}, {"timestamp": [954.56, 960.48], "text": " a social social, the level of society will be like spectacular."}, {"timestamp": [960.48, 966.24], "text": " And just like you know, again, what happened in AIDS or when you have like some kind of very,"}, {"timestamp": [966.24, 971.44], "text": " very rare disease. So that would be another scenario, but it's hard to predict, you're right,"}, {"timestamp": [971.44, 974.64], "text": " about how is it really like going to happen this way."}, {"timestamp": [975.28, 984.96], "text": " Yeah. And also there was, I guess, a few counter argument, like against this idea of curing aging."}, {"timestamp": [987.92, 994.48], "text": " counter argument, like against this idea of curing aging, one of the most common one is that it might be only for the elites creating massive inequality. Do you share this view?"}, {"timestamp": [994.48, 1001.36], "text": " Do you think it's going to be expensive and only the richest will be, you know, becoming like some"}, {"timestamp": [1001.36, 1007.64], "text": " sort of immortal class above everyone else? or it's good for science fiction,"}, {"timestamp": [1007.64, 1010.72], "text": " but maybe it's not very credible."}, {"timestamp": [1010.72, 1013.96], "text": " And if not, what makes you think everyone will benefit?"}, {"timestamp": [1014.84, 1016.16], "text": " Yeah."}, {"timestamp": [1016.16, 1018.48], "text": " Yeah, so the idea that elites live longer"}, {"timestamp": [1018.48, 1021.24], "text": " and will spend whatever money they can to live longer"}, {"timestamp": [1021.24, 1022.96], "text": " is actually a really old idea."}, {"timestamp": [1022.96, 1024.82], "text": " There was a good video that I watched"}, {"timestamp": [1024.82, 1028.72], "text": " about the history of vampires, vampire stories."}, {"timestamp": [1028.72, 1032.34], "text": " And the reason that vampires always live in a castle and that they look really old is"}, {"timestamp": [1032.34, 1038.28], "text": " because that is the way that people used to perceive aristocracy, right?"}, {"timestamp": [1038.28, 1047.8], "text": " In France before the revolution, aristocracy was in control and they, you know, lived much longer than the proletariat."}, {"timestamp": [1047.8, 1050.4], "text": " And this was because of all the privileges that they had."}, {"timestamp": [1050.4, 1054.7], "text": " So, it was not uncommon for the elites in the past to live"}, {"timestamp": [1054.7, 1057.5], "text": " to be 80, 90 years old, where most people died in their 30s,"}, {"timestamp": [1057.5, 1058.9], "text": " 40s, or 50s."}, {"timestamp": [1058.9, 1061.2], "text": " And we see this going back all the way to ancient Egypt,"}, {"timestamp": [1061.2, 1063.8], "text": " where some of the pharaohs lived into their 80s or 90s."}, {"timestamp": [1063.8, 1066.48], "text": " And this is way before any modern medicine."}, {"timestamp": [1066.48, 1072.6], "text": " And so just the advantages, the privilege of wealth conferred to people has pretty much"}, {"timestamp": [1072.6, 1076.1], "text": " always allowed the elites to live longer."}, {"timestamp": [1076.1, 1080.88], "text": " And with that being said, what I suspect, and I'm not going to say that that trend is"}, {"timestamp": [1080.88, 1084.12], "text": " going to reverse or go away, it's certainly something to pay attention to, because the"}, {"timestamp": [1084.12, 1085.98], "text": " more financial power that someone"}, {"timestamp": [1085.98, 1087.96], "text": " has, the more they have access to, right?"}, {"timestamp": [1087.96, 1088.96], "text": " It's that simple."}, {"timestamp": [1088.96, 1091.36], "text": " They have more options."}, {"timestamp": [1091.36, 1095.52], "text": " Whether it's, you know, traveling the world to get the best therapy or even overcoming"}, {"timestamp": [1095.52, 1100.92], "text": " barriers, there's evidence out there today of the wealthy elite getting access to stem"}, {"timestamp": [1100.92, 1103.64], "text": " cell therapy that isn't even approved yet, right?"}, {"timestamp": [1103.64, 1107.0], "text": " And sometimes it backfires, sometimes it does help them."}, {"timestamp": [1107.0, 1109.32], "text": " Now, with that being said,"}, {"timestamp": [1109.32, 1110.72], "text": " there have been plenty of cases"}, {"timestamp": [1110.72, 1114.24], "text": " that once a therapy becomes commercially viable,"}, {"timestamp": [1114.24, 1116.6], "text": " or not even commercially viable,"}, {"timestamp": [1116.6, 1120.2], "text": " once it becomes medically viable, it might be subsidized."}, {"timestamp": [1120.2, 1122.44], "text": " So the thing, the primary example of this"}, {"timestamp": [1122.44, 1125.28], "text": " is the polio vaccine."}, {"timestamp": [1125.28, 1129.28], "text": " The United States government, and I think pretty much all governments,"}, {"timestamp": [1129.28, 1132.4], "text": " said, we are going to pay to distribute this to everyone."}, {"timestamp": [1132.4, 1134.2], "text": " We're going to make sure that everyone gets this."}, {"timestamp": [1134.2, 1137.44], "text": " They did the same thing with the COVID vaccine, right?"}, {"timestamp": [1137.44, 1142.0], "text": " Most people around the world did not have to pay for it or paid very little for it"}, {"timestamp": [1142.0, 1148.78], "text": " because it was subsidized because the public good of distributing that medicine made sense."}, {"timestamp": [1148.78, 1155.88], "text": " And even to this day, kidney dialysis in America is so expensive that the government will just"}, {"timestamp": [1155.88, 1158.84], "text": " pay for it, right?"}, {"timestamp": [1158.84, 1161.64], "text": " Whether or not you have insurance, it's just too expensive."}, {"timestamp": [1161.64, 1165.76], "text": " And so what I suspect is that the pressure"}, {"timestamp": [1165.76, 1169.66], "text": " that's gonna be put on government around the world"}, {"timestamp": [1169.66, 1171.9], "text": " will be to ensure that however much"}, {"timestamp": [1171.9, 1173.3], "text": " some of these things cost,"}, {"timestamp": [1173.3, 1174.82], "text": " that it's gonna be heavily subsidized."}, {"timestamp": [1174.82, 1177.3], "text": " But also, I don't really see any evidence"}, {"timestamp": [1177.3, 1180.54], "text": " that some of these medicines are going to be expensive,"}, {"timestamp": [1180.54, 1183.44], "text": " especially when you look at DNA printing that's coming out."}, {"timestamp": [1183.44, 1188.5], "text": " There's some startups in Germany that are really good at synthesizing any DNA that you need,"}, {"timestamp": [1188.5, 1192.5], "text": " which then you can take that DNA and use that to synthesize any protein that you need."}, {"timestamp": [1192.5, 1196.7], "text": " And so then you don't have to wait for expensive crops to grow that are transgenic crops,"}, {"timestamp": [1196.7, 1199.5], "text": " and then extract the DNA from that."}, {"timestamp": [1199.5, 1204.5], "text": " You can grow the medicine that you need in solutions with bacterium."}, {"timestamp": [1204.5, 1205.28], "text": " And so when I see those patterns of things happening, grow the medicine that you need and solutions with bacterium."}, {"timestamp": [1205.28, 1209.4], "text": " When I see those patterns of things happening,"}, {"timestamp": [1209.4, 1212.96], "text": " I suspect that the cost of medical care,"}, {"timestamp": [1212.96, 1215.92], "text": " and this is not just drug discovery and stuff,"}, {"timestamp": [1215.92, 1217.84], "text": " but I suspect that the cost of"}, {"timestamp": [1217.84, 1220.08], "text": " medical care is going to be like it is in Star Trek,"}, {"timestamp": [1220.08, 1222.6], "text": " where you just go and it's like they wave a wand over you,"}, {"timestamp": [1222.6, 1224.16], "text": " and you give you an injection,"}, {"timestamp": [1224.16, 1226.52], "text": " and then you go home, and it doesn't matter what you had."}, {"timestamp": [1226.52, 1232.12], "text": " It could have been stage four pancreatic cancer and it's like, okay, here, you know, take a pill and call me in the morning, you'll be fine."}, {"timestamp": [1232.12, 1237.32], "text": " I suspect that medical care is just going to be dirt cheap within the next 10 to 15 years."}, {"timestamp": [1237.32, 1239.92], "text": " And it's going to be something that we don't even think about anymore."}, {"timestamp": [1239.92, 1245.7], "text": " Similar to how like, food is still like a third of people's budget or maybe not a third,"}, {"timestamp": [1245.7, 1247.56], "text": " about 10% of people's budget."}, {"timestamp": [1247.56, 1252.18], "text": " But remember, well, no one was alive 100 years ago or 200 years ago."}, {"timestamp": [1252.18, 1256.22], "text": " But in the past, food accounted for most of your budget."}, {"timestamp": [1256.22, 1259.76], "text": " You would spend most of your time, energy, or money just getting food."}, {"timestamp": [1259.76, 1263.7], "text": " But with the rise of industrialization, food is now much cheaper."}, {"timestamp": [1263.7, 1266.62], "text": " Likewise, electricity used to be very expensive."}, {"timestamp": [1266.62, 1267.68], "text": " Now you don't even really think"}, {"timestamp": [1267.68, 1269.08], "text": " about your electricity bill."}, {"timestamp": [1269.08, 1270.88], "text": " Likewise, I think that the combination"}, {"timestamp": [1270.88, 1273.36], "text": " of artificial intelligence and genetic science"}, {"timestamp": [1273.36, 1274.96], "text": " and everything else that's coming"}, {"timestamp": [1274.96, 1277.22], "text": " is gonna make all medical care,"}, {"timestamp": [1277.22, 1279.72], "text": " not just longevity, but all medical care"}, {"timestamp": [1279.72, 1282.84], "text": " basically a trivial expense before too long."}, {"timestamp": [1282.84, 1286.0], "text": " And then from a business standpoint, it makes sense"}, {"timestamp": [1286.0, 1288.0], "text": " because healthy consumers keep buying your products,"}, {"timestamp": [1288.0, 1292.0], "text": " dead consumers don't. Healthy citizens keep paying taxes,"}, {"timestamp": [1292.0, 1296.0], "text": " dead citizens don't. Right? And so, there's a lot of reason"}, {"timestamp": [1296.0, 1299.0], "text": " to have a long-living, healthy population."}, {"timestamp": [1299.0, 1303.0], "text": " Not to mention just the brain drain that happens."}, {"timestamp": [1303.0, 1307.54], "text": " If you have, you know,, like imagine if Leonardo da Vinci"}, {"timestamp": [1307.54, 1310.52], "text": " and Albert Einstein and Stephen Hawking"}, {"timestamp": [1310.52, 1311.9], "text": " and Richard Dawkins,"}, {"timestamp": [1311.9, 1315.64], "text": " if every great scientist was going to live forever, right?"}, {"timestamp": [1315.64, 1317.32], "text": " That would be great for society."}, {"timestamp": [1318.7, 1322.5], "text": " On top of other great thinkers and other great leaders,"}, {"timestamp": [1322.5, 1324.64], "text": " it would be a really good thing for society"}, {"timestamp": [1324.64, 1326.6], "text": " for a lot of experts to live forever."}, {"timestamp": [1326.6, 1329.6], "text": " So I think there's just too many incentives"}, {"timestamp": [1329.6, 1333.7], "text": " to solve the problem and make sure that it is equally accessible."}, {"timestamp": [1333.7, 1337.0], "text": " Now, that being said, to the first point,"}, {"timestamp": [1337.0, 1340.2], "text": " people with more money and power are always going to have an advantage,"}, {"timestamp": [1340.2, 1343.4], "text": " but I don't necessarily think that it's going to be a permanent advantage."}, {"timestamp": [1343.4, 1346.32], "text": " Just like Bill Gates is the richest guy in the world."}, {"timestamp": [1346.32, 1349.2], "text": " He doesn't need more electricity than you or I need, right?"}, {"timestamp": [1349.2, 1352.8], "text": " It's such a trivial expense that we all have access to however much electricity we need."}, {"timestamp": [1352.8, 1355.52], "text": " So I think that it's going to become commoditized like that."}, {"timestamp": [1356.32, 1361.2], "text": " Yeah, and there's also a trend where usually when the technology is implemented at the beginning,"}, {"timestamp": [1361.2, 1364.4], "text": " it's very expensive, it doesn't really work very well."}, {"timestamp": [1364.4, 1372.16], "text": " And the rich use it. So they're almost like a test subject, like the first mobile phone."}, {"timestamp": [1372.16, 1378.32], "text": " They were really big. And now we have pretty much the same phone as the richest person. You can just"}, {"timestamp": [1378.32, 1387.4], "text": " buy it. This is the difference between those. And that's true that on a societal level, aging is expensive for"}, {"timestamp": [1387.4, 1397.96], "text": " society. And if people can stay longer and live healthy, they will keep being productive."}, {"timestamp": [1397.96, 1408.8], "text": " So that's a good way for, it's almost an investment for a society, I suppose, to pay for the medicine. But do you know"}, {"timestamp": [1408.8, 1414.72], "text": " other, let's say, good arguments that we are supposed to die and we shouldn't try to cure"}, {"timestamp": [1414.72, 1422.24], "text": " aging or at least be immortal? Maybe we can improve our old age and be healthy longer,"}, {"timestamp": [1422.24, 1427.2], "text": " but maybe above 120, let's say, why keep going? Do you know"}, {"timestamp": [1427.2, 1433.12], "text": " any good arguments? Because I remember in your video, there was this, you talk about"}, {"timestamp": [1433.12, 1439.84], "text": " two kinds of reactions. Some people are going to be mortal, mortalis, I forgot the word,"}, {"timestamp": [1439.84, 1447.44], "text": " but they actually want death to remain and the other wants to live longer. So,"}, {"timestamp": [1447.44, 1455.12], "text": " yeah, do you know any good arguments? Yeah, so there's two primary arguments against longevity."}, {"timestamp": [1455.12, 1461.36], "text": " One is from an evolutionary perspective, is that, you know, species that have sexual reproduction"}, {"timestamp": [1461.36, 1467.0], "text": " tend to be more varied, whereas species that have cloning,"}, {"timestamp": [1467.0, 1473.98], "text": " which is basically you just create a genetic copy of yourself and you keep doing that."}, {"timestamp": [1473.98, 1479.76], "text": " Clonal species tend to be more vulnerable to disease or environmental changes because"}, {"timestamp": [1479.76, 1486.28], "text": " there's less variety and less adaptability. And so when you end up with longevity,"}, {"timestamp": [1486.28, 1490.86], "text": " basically the human genome is now frozen in time, right?"}, {"timestamp": [1490.86, 1493.86], "text": " Now natural evolution just stops,"}, {"timestamp": [1493.86, 1496.84], "text": " which that could be good, it could be bad,"}, {"timestamp": [1496.84, 1499.36], "text": " because if we're good enough right now, great,"}, {"timestamp": [1499.36, 1502.94], "text": " but that doesn't necessarily mean"}, {"timestamp": [1502.94, 1505.28], "text": " that humanity is in its final form, right?"}, {"timestamp": [1505.28, 1509.36], "text": " You know, you can imagine what happens to us over the course of many millions of years,"}, {"timestamp": [1509.36, 1515.44], "text": " from here on out, maybe we take over our evolution. Maybe that is the next evolutionary step where we"}, {"timestamp": [1515.44, 1522.32], "text": " can become, where we can deliberately manipulate our own genes, you know, and kind of shape our"}, {"timestamp": [1522.32, 1525.4], "text": " own destiny. We would be the first species that I know of"}, {"timestamp": [1525.4, 1527.32], "text": " that is capable of doing that."}, {"timestamp": [1527.32, 1529.38], "text": " But that is just one topic."}, {"timestamp": [1529.38, 1534.36], "text": " The other is from a political and social perspective,"}, {"timestamp": [1534.36, 1537.42], "text": " because if you think about the fact that, you know,"}, {"timestamp": [1537.42, 1540.22], "text": " 100 years ago, 200 years ago,"}, {"timestamp": [1540.22, 1543.32], "text": " we didn't have the society that we have today"}, {"timestamp": [1543.32, 1545.76], "text": " where equality is something that we value,"}, {"timestamp": [1545.76, 1548.3], "text": " where all people have the vote,"}, {"timestamp": [1548.3, 1552.0], "text": " suffrage, civil rights movements, that sort of thing."}, {"timestamp": [1552.0, 1556.48], "text": " If everyone with those values of 500 years ago was still alive,"}, {"timestamp": [1556.48, 1558.36], "text": " they would be resisting change today,"}, {"timestamp": [1558.36, 1561.12], "text": " just the same that people also resist change."}, {"timestamp": [1561.12, 1568.64], "text": " Now, the idea that death serves a social purpose, an information purpose"}, {"timestamp": [1568.64, 1573.84], "text": " to make room for new ideas is really critical."}, {"timestamp": [1573.84, 1578.84], "text": " Now, on the flip side of that, though, what if we still had more people that had lived"}, {"timestamp": [1578.84, 1584.0], "text": " through the American Civil War, the French Revolution, World War I, World War II?"}, {"timestamp": [1584.0, 1585.9], "text": " What if we had more cultural memory"}, {"timestamp": [1585.9, 1588.4], "text": " of what atrocity really looks like?"}, {"timestamp": [1588.4, 1592.1], "text": " Maybe the world would be more peaceful and more stable as well."}, {"timestamp": [1592.1, 1593.82], "text": " Things might change more slowly,"}, {"timestamp": [1593.82, 1597.5], "text": " but we would have more living memory of terrible things,"}, {"timestamp": [1597.5, 1599.78], "text": " which can provide the role that"}, {"timestamp": [1599.78, 1602.86], "text": " elders provide in families and societies,"}, {"timestamp": [1602.86, 1606.44], "text": " which is you trust dad because dad is older than you and has more"}, {"timestamp": [1606.44, 1609.84], "text": " experience, right? So the value of wisdom and experience is"}, {"timestamp": [1609.84, 1613.44], "text": " also there, but it can be offset from being frozen in time,"}, {"timestamp": [1613.44, 1617.68], "text": " right? So one possibility is that maybe some of the"}, {"timestamp": [1617.68, 1620.48], "text": " rejuvenation therapies will actually keep your mind young"}, {"timestamp": [1620.48, 1623.24], "text": " as well, you know, because imagine if everyone was as"}, {"timestamp": [1623.24, 1627.8], "text": " flexible and dynamic as when they were 18."}, {"timestamp": [1627.8, 1632.4], "text": " Everyone could afford to be idealistic and keep learning and keep growing and keep changing."}, {"timestamp": [1632.4, 1635.44], "text": " I don't know if that's going to be possible, though, just because of the way that your"}, {"timestamp": [1635.44, 1637.92], "text": " brain changes over time."}, {"timestamp": [1637.92, 1642.7], "text": " As you gain more experience, it might be that people just get stuck in their ways."}, {"timestamp": [1642.7, 1646.48], "text": " And if we become a paralyzed society,"}, {"timestamp": [1646.48, 1648.44], "text": " that could be a really bad thing."}, {"timestamp": [1648.44, 1650.64], "text": " So from the evolutionary perspective"}, {"timestamp": [1650.64, 1652.84], "text": " and from the sociological perspective,"}, {"timestamp": [1652.84, 1655.68], "text": " there are good arguments against living forever,"}, {"timestamp": [1655.68, 1658.04], "text": " but we might be able to find solutions for those."}, {"timestamp": [1658.04, 1659.36], "text": " So that's..."}, {"timestamp": [1659.36, 1661.96], "text": " Can you remind me the name of the..."}, {"timestamp": [1663.68, 1666.76], "text": " In your video at the beginning, when you talk about the people"}, {"timestamp": [1666.76, 1676.0], "text": " who are against, they want to, so they will reject any type of unnatural way to live longer."}, {"timestamp": [1676.0, 1679.6], "text": " The term that I use was a mortalitacy."}, {"timestamp": [1679.6, 1685.0], "text": " The people who actually embrace mortality, who want to die eventually."}, {"timestamp": [1685.2, 1687.82], "text": " And there's lots of people in that camp."}, {"timestamp": [1687.82, 1690.46], "text": " And I don't know all the reasons for that."}, {"timestamp": [1690.46, 1695.46], "text": " Like in some cases, I think that there's a desire to die"}, {"timestamp": [1695.82, 1700.82], "text": " just because there's some nihilism, some fatigue, right?"}, {"timestamp": [1700.82, 1704.02], "text": " Like the world is difficult, life is hard,"}, {"timestamp": [1704.02, 1705.46], "text": " death is ultimately an escape. It's engaging actually also. difficult, life is hard, you know, death is ultimately an"}, {"timestamp": [1705.46, 1706.46], "text": " escape."}, {"timestamp": [1706.46, 1707.46], "text": " It's engaging actually also."}, {"timestamp": [1707.46, 1709.46], "text": " Yeah, it can be, right."}, {"timestamp": [1709.46, 1712.5], "text": " It depends on how you embrace life."}, {"timestamp": [1712.5, 1717.06], "text": " And so I think that in the long run, you could end up with these two different camps."}, {"timestamp": [1717.06, 1721.06], "text": " And of course, there's going to be a spectrum of other ideas and dispositions where it's"}, {"timestamp": [1721.06, 1725.64], "text": " like, yes, I'd want to live 200 years or 2000 years or, you know,"}, {"timestamp": [1725.64, 1727.64], "text": " maybe we shouldn't change it at all."}, {"timestamp": [1727.64, 1731.88], "text": " But you know, individual choice is also a thing and some people are going to choose"}, {"timestamp": [1731.88, 1734.24], "text": " to live however long they can."}, {"timestamp": [1734.24, 1740.16], "text": " So we're going to see probably some, I don't want to say fragmentation in society, but"}, {"timestamp": [1740.16, 1744.52], "text": " certainly we're going to see some, some polarization, some different directions that people are"}, {"timestamp": [1744.52, 1745.04], "text": " going in."}, {"timestamp": [1745.04, 1749.04], "text": " Yeah, and I suspect there is also sometimes not really,"}, {"timestamp": [1749.04, 1751.64], "text": " it's not really a question we will ask ourselves,"}, {"timestamp": [1751.64, 1753.6], "text": " like how long do I want to live?"}, {"timestamp": [1753.6, 1756.6], "text": " As long as I want to keep living now because I feel good"}, {"timestamp": [1756.6, 1759.24], "text": " and my life is good and I'm healthy and tomorrow,"}, {"timestamp": [1759.24, 1760.8], "text": " I expect the same,"}, {"timestamp": [1760.8, 1763.52], "text": " then the next day I ask myself the same question"}, {"timestamp": [1763.52, 1766.0], "text": " and maybe a thousand years later,"}, {"timestamp": [1766.0, 1774.16], "text": " I'm still living because why not? It's still going well. Of course, if the conditions on"}, {"timestamp": [1774.16, 1785.68], "text": " Earth are unbearable, maybe it won't be as good and stuff like that. But we also have so much of the trends to kind of, as you said, describe"}, {"timestamp": [1785.68, 1792.96], "text": " in your videos, utopia, and we try to create a better world, better future. So if we live"}, {"timestamp": [1792.96, 1794.44], "text": " longer in a better future."}, {"timestamp": [1794.44, 1800.2], "text": " Yep. Yeah. And hope is an important aspect of that, because if you believe that your"}, {"timestamp": [1800.2, 1806.82], "text": " life is going to continue to get better, if you believe that the world is going to improve, then you're gonna wanna see that future, right?"}, {"timestamp": [1806.82, 1809.46], "text": " But if you believe things are gonna continue getting worse,"}, {"timestamp": [1809.46, 1811.9], "text": " you might not wanna live to see that future, so."}, {"timestamp": [1811.9, 1813.14], "text": " Yeah."}, {"timestamp": [1813.14, 1816.14], "text": " Let's talk a little bit about mind uploading,"}, {"timestamp": [1816.14, 1817.26], "text": " if you don't mind."}, {"timestamp": [1817.26, 1818.62], "text": " Sure."}, {"timestamp": [1818.62, 1820.78], "text": " I've always thought that mind uploading,"}, {"timestamp": [1820.78, 1824.3], "text": " like, whole brain emulation would come first"}, {"timestamp": [1824.3, 1828.54], "text": " before biological immortality. I don't know if you're interested"}, {"timestamp": [1828.54, 1834.54], "text": " in that subject. And what do you think? Do you think 29 2029 is"}, {"timestamp": [1834.54, 1841.14], "text": " going to be like Chris was said? Yeah, the quality and then"}, {"timestamp": [1841.14, 1844.86], "text": " later in the century, well, brain emulation, or maybe you're"}, {"timestamp": [1844.86, 1846.04], "text": " going to get a shortcut."}, {"timestamp": [1846.04, 1849.24], "text": " Because recently, like last year,"}, {"timestamp": [1849.24, 1853.76], "text": " the Flywire experiments gave good results actually"}, {"timestamp": [1853.76, 1858.6], "text": " about the interpretability of brain scans."}, {"timestamp": [1858.6, 1862.72], "text": " Yeah. So there's been some really interesting breakthroughs,"}, {"timestamp": [1862.72, 1863.92], "text": " and not just with flies,"}, {"timestamp": [1863.92, 1866.24], "text": " but there's been some really interesting breakthroughs, and not just with flies, but there's been some cases where"}, {"timestamp": [1866.24, 1869.04], "text": " fMRI images of human brains were able to"}, {"timestamp": [1869.04, 1872.96], "text": " reconstruct thoughts and images that they were seeing."}, {"timestamp": [1872.96, 1875.4], "text": " So with that being said,"}, {"timestamp": [1875.4, 1879.92], "text": " you're getting into more philosophical questions"}, {"timestamp": [1879.92, 1882.92], "text": " that might be difficult to answer or impossible to answer,"}, {"timestamp": [1882.92, 1884.88], "text": " which is what is the nature of consciousness?"}, {"timestamp": [1884.88, 1885.24], "text": " What is the nature of consciousness?"}, {"timestamp": [1885.24, 1888.68], "text": " What is the nature of sentience?"}, {"timestamp": [1888.68, 1895.12], "text": " And so one experiment that I have in mind is going to be a natural experiment once we"}, {"timestamp": [1895.12, 1899.48], "text": " get to brain-computer interfaces."}, {"timestamp": [1899.48, 1905.36], "text": " If Neuralink succeeds and if you a an interface between your brain and a computer"}, {"timestamp": [1906.8, 1911.28], "text": " you know one thing that you might be able to test is can your consciousness reside in a machine"}, {"timestamp": [1912.32, 1916.32], "text": " and i don't personally i don't believe that that's possible i think that it's just going to be"}, {"timestamp": [1916.32, 1921.52], "text": " it's going to feel like a peripheral like you know you're it if if you do have a brain computer"}, {"timestamp": [1921.52, 1926.52], "text": " interface your brain is going to feel like that computer is like an extra hand or something."}, {"timestamp": [1926.52, 1928.7], "text": " It's not going to feel like it's intrinsically part of you."}, {"timestamp": [1928.7, 1931.4], "text": " It's not going to feel like it's part of your mind."}, {"timestamp": [1931.4, 1934.02], "text": " Now, if it's a copy of you,"}, {"timestamp": [1934.02, 1935.8], "text": " sure, you can upload that,"}, {"timestamp": [1935.8, 1937.22], "text": " but I don't believe that you can"}, {"timestamp": [1937.22, 1939.14], "text": " take someone's mind out of their head."}, {"timestamp": [1939.14, 1943.06], "text": " I think that we're going to discover that your mind is"}, {"timestamp": [1943.06, 1947.0], "text": " inextricably locked into your skull."}, {"timestamp": [1947.0, 1956.0], "text": " And so if you plug in virtual reality, you can feel like your mind has gone elsewhere."}, {"timestamp": [1956.0, 1961.0], "text": " But I personally don't think that that's the direction it's going to go."}, {"timestamp": [1961.0, 1967.64], "text": " But that's one of the experiments that I would do is with those brain computer interfaces,"}, {"timestamp": [1967.64, 1970.96], "text": " can you actually occupy a machine?"}, {"timestamp": [1970.96, 1972.96], "text": " And that'll be a critical question."}, {"timestamp": [1972.96, 1979.44], "text": " But another possibility is, let's imagine that you can."}, {"timestamp": [1979.44, 1986.8], "text": " If your mind can subjectively occupy a machine, what does that mean for being human?"}, {"timestamp": [1986.8, 1991.64], "text": " Does that mean that being human matters at all? If your humanity can just leave"}, {"timestamp": [1991.64, 1997.8], "text": " your body, does it matter? Does your body matter anymore? And again, like, that's"}, {"timestamp": [1997.8, 2001.24], "text": " the experiment that I would do, and of course whatever information came from"}, {"timestamp": [2001.24, 2006.9], "text": " that experiment, I would change my mind accordingly. But right now, I have not seen any evidence that"}, {"timestamp": [2006.9, 2013.86], "text": " your mind is able to be detached from your physical form."}, {"timestamp": [2013.86, 2016.12], "text": " Now, that being said,"}, {"timestamp": [2016.12, 2021.68], "text": " if you can have virtual humans or virtual brains or whatever,"}, {"timestamp": [2021.68, 2024.34], "text": " take a copy of your brain and put it in a machine,"}, {"timestamp": [2024.34, 2029.24], "text": " and then you have a virtual version of yourself that's able to help you, or in many cases, it might"}, {"timestamp": [2029.24, 2030.24], "text": " not care about you anymore."}, {"timestamp": [2030.24, 2035.84], "text": " It might say, like, later, Dave, I'm the virtual you, and I'm never going to get tired or sick"}, {"timestamp": [2035.84, 2039.92], "text": " ever again, so peace out."}, {"timestamp": [2039.92, 2044.52], "text": " That could be a problematic result of that kind of experimentation."}, {"timestamp": [2044.52, 2045.6], "text": " But time will tell if"}, {"timestamp": [2045.6, 2051.12], "text": " it's one, if it's even possible, and then two, what will be the impact if it is possible."}, {"timestamp": [2051.12, 2054.8], "text": " Out of curiosity, in terms of consciousness, do you believe in"}, {"timestamp": [2055.92, 2061.52], "text": " substrating the patterns? Do you think AI could be conscious or is it really about"}, {"timestamp": [2061.52, 2066.4], "text": " biology producing a mind and we can't replicate this"}, {"timestamp": [2066.4, 2070.76], "text": " because if we can't put the mind somewhere else,"}, {"timestamp": [2070.76, 2074.12], "text": " does it also mean we can't create a mind virtually?"}, {"timestamp": [2075.6, 2078.6], "text": " Yeah, so there's two primary schools of thought."}, {"timestamp": [2078.6, 2081.32], "text": " There's materialism, which basically says"}, {"timestamp": [2081.32, 2084.64], "text": " that physical matter and energy is all that exists, right?"}, {"timestamp": [2084.64, 2085.98], "text": " And if you make that assumption,"}, {"timestamp": [2085.98, 2088.84], "text": " because you can make that assumption,"}, {"timestamp": [2088.84, 2090.92], "text": " but it is still just an assumption, right?"}, {"timestamp": [2090.92, 2094.14], "text": " If you say that if that which you can measure"}, {"timestamp": [2094.14, 2096.44], "text": " with the thermometer and electricity"}, {"timestamp": [2096.44, 2099.28], "text": " and all the physics in the world,"}, {"timestamp": [2099.28, 2100.36], "text": " if you make the assumption"}, {"timestamp": [2100.36, 2102.78], "text": " that that is all there is to reality,"}, {"timestamp": [2102.78, 2106.5], "text": " then the inevitable conclusion is that"}, {"timestamp": [2106.5, 2108.5], "text": " the brain is just an organic computer."}, {"timestamp": [2108.5, 2111.5], "text": " And that means that consciousness arises from"}, {"timestamp": [2111.5, 2115.5], "text": " the electrical and chemical activity of the brain."}, {"timestamp": [2115.5, 2117.5], "text": " And if that's the case, then there's no reason,"}, {"timestamp": [2117.5, 2119.5], "text": " it's just a matter of information, right?"}, {"timestamp": [2119.5, 2121.5], "text": " It's a matter of information in,"}, {"timestamp": [2121.5, 2123.5], "text": " information processing, and information out."}, {"timestamp": [2123.5, 2125.44], "text": " If materialism is true,"}, {"timestamp": [2126.0, 2131.2], "text": " then there's no reason that, you know, you couldn't have a life form or a conscious entity"}, {"timestamp": [2131.2, 2137.6], "text": " made of anything, really, as long as it had the right information patterns. But then the far end"}, {"timestamp": [2137.6, 2142.32], "text": " of the other end of the spectrum is that maybe consciousness is the fundamental substrate of the"}, {"timestamp": [2142.32, 2150.56], "text": " universe. Maybe consciousness is first and then the rest of reality kind of collapses around that."}, {"timestamp": [2150.56, 2154.64], "text": " And there's evidence of that as well in quantum physics and the measurement problem."}, {"timestamp": [2154.64, 2158.52], "text": " The fact that time doesn't really seem to exist depending on how you measure it from"}, {"timestamp": [2158.52, 2161.46], "text": " the perspective of physics."}, {"timestamp": [2161.46, 2166.24], "text": " And so then the question is, it's kind of a chicken or the egg kind of problem, or,"}, {"timestamp": [2166.24, 2171.56], "text": " you know, putting the cart before the horse, is what is the foundational truth of reality?"}, {"timestamp": [2171.56, 2174.2], "text": " Or is it, are we even thinking about it wrong?"}, {"timestamp": [2174.2, 2179.4], "text": " Is it, is it not materialism as the substrate and then consciousness is above that, or vice"}, {"timestamp": [2179.4, 2183.54], "text": " versa, maybe consciousness is the substrate and physics is above that?"}, {"timestamp": [2183.54, 2185.4], "text": " Maybe it's side by side, right?"}, {"timestamp": [2185.4, 2188.04], "text": " Maybe one cannot exist without the other,"}, {"timestamp": [2188.04, 2191.76], "text": " in which case, like, you're still left with more questions than answers."}, {"timestamp": [2191.76, 2195.84], "text": " And then, of course, another possibility is a simulation hypothesis,"}, {"timestamp": [2195.84, 2198.96], "text": " which is that your mind is actually connected to something"}, {"timestamp": [2198.96, 2201.64], "text": " outside of the simulation or is being simulated."}, {"timestamp": [2201.64, 2205.5], "text": " And in that case, we'll never know one way or the other."}, {"timestamp": [2205.5, 2209.0], "text": " So, unfortunately, I think that this question is"}, {"timestamp": [2209.0, 2211.3], "text": " likely fundamentally unanswerable."}, {"timestamp": [2211.3, 2213.3], "text": " I think that no matter how much science exists,"}, {"timestamp": [2213.3, 2217.1], "text": " we might never know what gives us our subjective experience"}, {"timestamp": [2217.1, 2221.4], "text": " of reality, and we might never know, you know, like,"}, {"timestamp": [2221.4, 2225.24], "text": " if a machine is truly, truly conscious the same way that we are,"}, {"timestamp": [2225.24, 2227.04], "text": " even if it tells us that we are,"}, {"timestamp": [2227.04, 2231.72], "text": " and this is called solipsism in philosophy,"}, {"timestamp": [2231.72, 2235.6], "text": " which is the only experience that I know to be real is my own."}, {"timestamp": [2235.6, 2236.92], "text": " Then I could make the assumption that"}, {"timestamp": [2236.92, 2238.4], "text": " my experience is the only one that's real"}, {"timestamp": [2238.4, 2240.48], "text": " and you are all part of my dream."}, {"timestamp": [2240.48, 2243.0], "text": " But you might feel the same way and so then we talk and it's like,"}, {"timestamp": [2243.0, 2246.14], "text": " well, how can you prove to me that you're not just part"}, {"timestamp": [2246.14, 2252.28], "text": " of my dream because you would tell me that, you know, like, it's a very convincing hallucination."}, {"timestamp": [2252.28, 2257.34], "text": " And so then you kind of very quickly end up in these rabbit holes that basically just"}, {"timestamp": [2257.34, 2260.4], "text": " go to show that it's not provable one way or the other."}, {"timestamp": [2260.4, 2264.4], "text": " And so part of coming to terms with this is the radical acceptance that we just don't"}, {"timestamp": [2264.4, 2265.2], "text": " know, right?"}, {"timestamp": [2265.2, 2267.36], "text": " And that science may never be able to tell us."}, {"timestamp": [2267.36, 2269.52], "text": " And so from there, we just do the best that we can."}, {"timestamp": [2270.4, 2274.24], "text": " We guess and we keep going until information changes."}, {"timestamp": [2274.24, 2276.4], "text": " And, you know, again, it might not ever change."}, {"timestamp": [2277.36, 2283.68], "text": " Yeah, consciousness is a big topic and a very complicated question."}, {"timestamp": [2283.68, 2284.32], "text": " Yep."}, {"timestamp": [2284.32, 2284.88], "text": " Hard problem."}, {"timestamp": [2283.6, 2284.6], "text": " topic and a very complicated question. Yep."}, {"timestamp": [2284.6, 2285.6], "text": " Hard problem."}, {"timestamp": [2285.6, 2289.6], "text": " So, yeah, let's go back to biological immortality."}, {"timestamp": [2289.6, 2295.8], "text": " This time on a personal level, as someone who will live very, very long, you talk about"}, {"timestamp": [2295.8, 2300.68], "text": " two kinds of lifestyle, hedonistic and ascetic."}, {"timestamp": [2300.68, 2305.0], "text": " So maybe you could explain what those two lifestyles are."}, {"timestamp": [2305.0, 2306.0], "text": " Yeah."}, {"timestamp": [2306.0, 2307.0], "text": " Yeah."}, {"timestamp": [2307.0, 2309.04], "text": " So this is and this is just one model."}, {"timestamp": [2309.04, 2314.6], "text": " It's obviously not a not a comprehensive model, but it's a it's a dichotomy to think of."}, {"timestamp": [2314.6, 2320.0], "text": " And so the hedonistic life or the hedonic life is basically living for pleasure, living"}, {"timestamp": [2320.0, 2323.28], "text": " for the things that feel good just because of the way your body works, right?"}, {"timestamp": [2323.28, 2328.8], "text": " You know, good food, sex and fun adventures, you know, going out drinking with people,"}, {"timestamp": [2328.8, 2329.8], "text": " right?"}, {"timestamp": [2329.8, 2334.1], "text": " Because if you have forever and you're going to stay healthy, then why not just have as"}, {"timestamp": [2334.1, 2336.28], "text": " much fun as you can?"}, {"timestamp": [2336.28, 2338.44], "text": " So that's one school of thought."}, {"timestamp": [2338.44, 2343.4], "text": " And there's been plenty of, you know, fiction out there, science fiction and fantasy and"}, {"timestamp": [2343.4, 2345.36], "text": " stuff that kind of imagines this"}, {"timestamp": [2345.36, 2346.36], "text": " possibility."}, {"timestamp": [2346.36, 2349.2], "text": " There's a couple of possible downsides to that, though."}, {"timestamp": [2349.2, 2350.78], "text": " One is just saturation."}, {"timestamp": [2350.78, 2354.4], "text": " If you live and party every day, eventually it's going to get boring, right?"}, {"timestamp": [2354.4, 2358.36], "text": " Because it's going to be the same thing that you experience day after day."}, {"timestamp": [2358.36, 2360.64], "text": " And so variety is the spice of life."}, {"timestamp": [2360.64, 2364.96], "text": " And so then it's like, okay, well, after you party for a couple centuries, maybe you decide"}, {"timestamp": [2364.96, 2368.0], "text": " to grow up and do something more serious and more challenging."}, {"timestamp": [2368.0, 2370.0], "text": " And this is the acetic side of life."}, {"timestamp": [2370.0, 2373.0], "text": " So the acetic side is about discipline and challenge."}, {"timestamp": [2373.0, 2377.0], "text": " And there's plenty of evidence that people thrive on challenge as well."}, {"timestamp": [2377.0, 2381.0], "text": " This is why people play frustrating games like Elden Ring, right?"}, {"timestamp": [2381.0, 2383.0], "text": " It's because it's a difficult game to play."}, {"timestamp": [2383.0, 2386.26], "text": " And then when you finally succeed, you feel good."}, {"timestamp": [2386.26, 2388.94], "text": " And this is why people strive to get Nobel Prizes"}, {"timestamp": [2388.94, 2391.24], "text": " and why people try and get a promotion"}, {"timestamp": [2391.24, 2393.86], "text": " is because overcoming a challenge"}, {"timestamp": [2393.86, 2395.42], "text": " is also deeply rewarding."}, {"timestamp": [2395.42, 2398.36], "text": " And in many cases, you might say it's more rewarding."}, {"timestamp": [2398.36, 2401.74], "text": " Because on the one hand, on the hedonistic side,"}, {"timestamp": [2401.74, 2404.16], "text": " you get an immediate biological reward, right?"}, {"timestamp": [2404.16, 2411.84], "text": " You get dopamine and endorphins and oxytocin. So there's these very short-term biological rewards that you"}, {"timestamp": [2411.84, 2416.32], "text": " get for certain behaviors, right? If you eat cheese, that's more rewarding than just eating"}, {"timestamp": [2416.32, 2420.16], "text": " like vegetables, right? That's why we love pizza. That's why we love burgers is because"}, {"timestamp": [2420.16, 2426.48], "text": " that food is biochemically more rewarding. But on the ascetic side, you look for things that are more"}, {"timestamp": [2426.48, 2430.2], "text": " philosophically rewarding, more transcendentally rewarding,"}, {"timestamp": [2430.2, 2434.8], "text": " and that can be dedicating yourself to things like"}, {"timestamp": [2434.8, 2440.04], "text": " spirituality or service to humanity or intellectual challenges."}, {"timestamp": [2440.04, 2443.92], "text": " That's kind of how I live, which is, can I solve these hard problems?"}, {"timestamp": [2443.92, 2446.56], "text": " And that's where I get some of my satisfaction."}, {"timestamp": [2446.56, 2450.46], "text": " In all cases, it's not going to be 100 percent one or the other."}, {"timestamp": [2450.46, 2452.78], "text": " Everyone has a little bit of both in their life."}, {"timestamp": [2452.78, 2455.34], "text": " It's just a matter of which one they favor."}, {"timestamp": [2455.34, 2459.2], "text": " But it's also helpful to be aware of in terms of,"}, {"timestamp": [2459.2, 2462.08], "text": " everyone knows someone who basically"}, {"timestamp": [2462.08, 2464.34], "text": " just waste all their time playing video games,"}, {"timestamp": [2464.34, 2466.58], "text": " and that's all they do, and they never challenge themselves."}, {"timestamp": [2466.58, 2468.44], "text": " And so they're more like lopsided"}, {"timestamp": [2468.44, 2470.38], "text": " to the hedonistic side of life."}, {"timestamp": [2470.38, 2472.58], "text": " And then you also know people"}, {"timestamp": [2472.58, 2474.7], "text": " who don't know how to have fun, right?"}, {"timestamp": [2474.7, 2476.7], "text": " You have people that all they do is work"}, {"timestamp": [2476.7, 2479.3], "text": " and they never play, and so they're more on the acetic side."}, {"timestamp": [2479.3, 2481.86], "text": " And so it's just a dichotomy to be aware of"}, {"timestamp": [2481.86, 2484.26], "text": " that life is about a balance."}, {"timestamp": [2484.26, 2489.18], "text": " And if you spend too much time on one, maybe spend a little bit of time on the other from"}, {"timestamp": [2489.18, 2490.32], "text": " time to time."}, {"timestamp": [2490.32, 2493.5], "text": " And so that's why I brought that up, brought up that model, because if you're going to"}, {"timestamp": [2493.5, 2498.42], "text": " live forever, if you think about life in those two terms of like, okay, I'm going to live"}, {"timestamp": [2498.42, 2500.46], "text": " forever and I'm going to be young and healthy forever."}, {"timestamp": [2500.46, 2501.58], "text": " So now what do I do?"}, {"timestamp": [2501.58, 2507.0], "text": " So you think about those two goals, you know, the two goals, the enjoying life, but then also challenging yourself."}, {"timestamp": [2507.0, 2509.54], "text": " And if you balance those, I think that people"}, {"timestamp": [2509.54, 2512.3], "text": " can be pretty happy in the long run."}, {"timestamp": [2512.3, 2517.3], "text": " It reminds me what the Buddha said about the middle way,"}, {"timestamp": [2517.42, 2519.78], "text": " because basically he was a prince at the beginning"}, {"timestamp": [2519.78, 2521.82], "text": " of his life and he had everything he wanted,"}, {"timestamp": [2521.82, 2524.66], "text": " all the pleasure of life, as much as he wanted,"}, {"timestamp": [2524.66, 2531.92], "text": " and then he went to be an ascetic and he suffered a lot and he almost died from starvation until"}, {"timestamp": [2531.92, 2535.8], "text": " he realized there was a middle way, which is what you said just before."}, {"timestamp": [2535.8, 2540.52], "text": " So I think it's interesting going back to old philosophy like this."}, {"timestamp": [2540.52, 2541.52], "text": " Yep."}, {"timestamp": [2541.52, 2548.32], "text": " There's something very similar in Greek philosophy as well, the golden mean, right?"}, {"timestamp": [2548.32, 2554.92], "text": " Which is, you pick a middle path between any extreme, and a very similar idea. But yeah,"}, {"timestamp": [2554.92, 2570.4], "text": " I studied both Western and Eastern philosophy to come up with some of these ideas. Just talking about rewards and how we find joy in achieving things or more mundanely"}, {"timestamp": [2570.4, 2573.56], "text": " actually comparing ourselves to others."}, {"timestamp": [2573.56, 2577.52], "text": " What do you think is going to be the impacts of AI?"}, {"timestamp": [2577.52, 2585.0], "text": " We were talking about Nobel Prizes, but what happens if like, AI's find everything"}, {"timestamp": [2591.16, 2591.28], "text": " pretty much resolved everything in a matter of a decade. And so"}, {"timestamp": [2595.2, 2601.68], "text": " it's just to be a little bit more pessimistic about our ability to not to be not to be all head on stick, because of a"}, {"timestamp": [2601.68, 2606.6], "text": " lack of challenge, because of AI. And more generally, what do you think about"}, {"timestamp": [2609.8, 2612.64], "text": " the living together with machines"}, {"timestamp": [2612.64, 2617.64], "text": " and with these machines being conscious or not,"}, {"timestamp": [2618.2, 2623.2], "text": " what is this rise of AI, AGI, gonna mean for us all?"}, {"timestamp": [2625.0, 2627.0], "text": " Yeah, that's a great question."}, {"timestamp": [2627.0, 2630.0], "text": " So to answer both parts of this,"}, {"timestamp": [2630.0, 2633.0], "text": " you have to take a big step back."}, {"timestamp": [2633.0, 2638.0], "text": " And what we're facing is what some people call a meaning crisis."}, {"timestamp": [2638.0, 2644.0], "text": " And this started basically with the invention of the printing press,"}, {"timestamp": [2644.0, 2647.68], "text": " which because, you know, 500 years ago,"}, {"timestamp": [2647.68, 2649.2], "text": " with the invention of the printing press,"}, {"timestamp": [2649.2, 2651.6], "text": " that democratized access to information,"}, {"timestamp": [2651.6, 2655.34], "text": " which then allowed people to read more, right?"}, {"timestamp": [2655.34, 2658.68], "text": " Literacy shot up after the invention of the printing press."}, {"timestamp": [2658.68, 2661.34], "text": " And this ultimately allowed people to write more"}, {"timestamp": [2661.34, 2664.14], "text": " about religion and economics and politics."}, {"timestamp": [2664.14, 2668.4], "text": " And you can trace a direct line between the invention of the printing press"}, {"timestamp": [2668.4, 2672.16], "text": " and the French Revolution, the American Civil War."}, {"timestamp": [2672.16, 2678.0], "text": " The availability of more information allowed more people to participate in thinking"}, {"timestamp": [2678.0, 2683.92], "text": " and discussing the nature of reality and the way that things should be, right?"}, {"timestamp": [2683.92, 2686.84], "text": " Journalism exploded after that."}, {"timestamp": [2686.84, 2689.6], "text": " And what happened is inevitably,"}, {"timestamp": [2689.6, 2693.56], "text": " Western civilization started becoming more secular,"}, {"timestamp": [2693.56, 2696.24], "text": " which is like, you know, the church was the arbiter"}, {"timestamp": [2696.24, 2698.28], "text": " of pretty much all learning in the West"}, {"timestamp": [2698.28, 2699.96], "text": " for a long, long time."}, {"timestamp": [2699.96, 2701.96], "text": " There were a few universities, but again,"}, {"timestamp": [2701.96, 2704.06], "text": " they were very, very tightly gatekept"}, {"timestamp": [2704.06, 2707.96], "text": " and only the aristocracy could go to universities in the West."}, {"timestamp": [2707.96, 2713.3], "text": " But with the democratization of information, that started to change religion, it started"}, {"timestamp": [2713.3, 2718.54], "text": " to change economics, it started to change the education, the academies."}, {"timestamp": [2718.54, 2726.44], "text": " And so then we have been on this like very, very inevitable slide towards nihilism, right?"}, {"timestamp": [2726.44, 2729.6], "text": " And so nihilism is you keep asking questions, right?"}, {"timestamp": [2729.6, 2731.36], "text": " And you say, what does it all mean?"}, {"timestamp": [2731.36, 2737.2], "text": " Well, if we get rid of God, and God is that which confers meaning on us Westerners,"}, {"timestamp": [2737.2, 2739.8], "text": " then maybe nothing has any meaning."}, {"timestamp": [2739.8, 2744.84], "text": " Well, fortunately, Eastern culture has already wrangled with this problem,"}, {"timestamp": [2744.84, 2745.3], "text": " and they figured"}, {"timestamp": [2745.3, 2747.94], "text": " it out 1,200 years ago."}, {"timestamp": [2747.94, 2754.14], "text": " And the answer is that once you get to the bottom of reality, once you realize that there"}, {"timestamp": [2754.14, 2759.74], "text": " is no cosmic power, and you have this radical acceptance of that existential crisis, that"}, {"timestamp": [2759.74, 2765.8], "text": " nihilistic crisis, what you end up with is the preeminence of your experience, the journey"}, {"timestamp": [2765.8, 2767.84], "text": " that you are on."}, {"timestamp": [2767.84, 2775.36], "text": " And so this is deeply, deeply embedded, particularly in Buddhism, Taoism, Shinto, and a few other"}, {"timestamp": [2775.36, 2781.26], "text": " Eastern traditions, where the present moment is the most important thing."}, {"timestamp": [2781.26, 2788.08], "text": " The ephemeral nature of reality is at the forefront, particularly of Japanese culture."}, {"timestamp": [2788.08, 2795.36], "text": " And so the kinds of meaning crisis that we have at a philosophical level, like you zoom out and"}, {"timestamp": [2795.36, 2807.68], "text": " say, what does it all mean? What does it matter? But if you watch Japanese culture, there is an appreciation for, uh, for, for the, the moment that you"}, {"timestamp": [2807.68, 2812.8], "text": " experience that is uniquely yours and your journey is also uniquely yours. And so,"}, {"timestamp": [2813.6, 2818.8], "text": " an example of how this plays out practically, uh, computers can beat you at any game now,"}, {"timestamp": [2818.8, 2825.74], "text": " right? Uh, at chess, at go, but people still play, right? It's not to win, it's to enjoy the experience,"}, {"timestamp": [2825.74, 2829.08], "text": " the journey of getting better and to have that experience"}, {"timestamp": [2829.08, 2831.58], "text": " of competing against another person."}, {"timestamp": [2831.58, 2833.14], "text": " We don't watch computers play chess."}, {"timestamp": [2833.14, 2834.46], "text": " We don't watch computers play go."}, {"timestamp": [2834.46, 2835.86], "text": " We don't care, right?"}, {"timestamp": [2835.86, 2837.42], "text": " It's a completely different experience."}, {"timestamp": [2837.42, 2839.94], "text": " It's not a human experience, and so we don't care."}, {"timestamp": [2839.94, 2842.5], "text": " And so what's gonna happen is one by one,"}, {"timestamp": [2842.5, 2846.08], "text": " those kinds of domains that as a machine takes it over,"}, {"timestamp": [2846.08, 2848.04], "text": " right, we're going to care less, right?"}, {"timestamp": [2848.04, 2850.08], "text": " I'm sure that there could have been a time in the past"}, {"timestamp": [2850.08, 2852.36], "text": " where someone, you know, complained that like,"}, {"timestamp": [2852.36, 2856.56], "text": " oh, well, when you use a machine to harvest corn,"}, {"timestamp": [2856.56, 2858.08], "text": " you lose touch with the land."}, {"timestamp": [2858.08, 2861.24], "text": " And it's like, but you still get corn, right?"}, {"timestamp": [2861.24, 2863.36], "text": " And it allows you to focus on other things."}, {"timestamp": [2863.36, 2868.1], "text": " And that, and also it doesn't mean that,'t mean that on the topic of farming and gardening,"}, {"timestamp": [2868.1, 2869.9], "text": " there are still heirloom varieties."}, {"timestamp": [2869.9, 2872.54], "text": " There are still people that do remain connected to the land."}, {"timestamp": [2872.54, 2876.06], "text": " So even though machines can displace some of that,"}, {"timestamp": [2876.06, 2878.54], "text": " even though machines can take some of it from us,"}, {"timestamp": [2878.54, 2882.38], "text": " there are things that it intrinsically will never be able to take from us."}, {"timestamp": [2882.38, 2888.56], "text": " And so this is what society is going to have to realize and struggle with, is the preeminence"}, {"timestamp": [2888.56, 2895.04], "text": " of your individual journey, the importance of the current moment that you're experiencing"}, {"timestamp": [2895.04, 2898.5], "text": " and the challenge that you have to overcome."}, {"timestamp": [2898.5, 2900.68], "text": " And that is why we're here."}, {"timestamp": [2900.68, 2905.84], "text": " And once you settle into that, like, yes, here's a personal example."}, {"timestamp": [2905.84, 2908.5], "text": " There are lots of things about my life that I don't like."}, {"timestamp": [2908.5, 2911.26], "text": " There are lots of things that I wish I could go back and change."}, {"timestamp": [2911.26, 2912.88], "text": " But this is my journey."}, {"timestamp": [2912.88, 2914.96], "text": " The struggles that I have,"}, {"timestamp": [2914.96, 2917.7], "text": " the problems that I have overcome,"}, {"timestamp": [2917.7, 2919.64], "text": " the things that lay before me,"}, {"timestamp": [2919.64, 2922.02], "text": " I know that most of what I'm struggling with"}, {"timestamp": [2922.02, 2925.84], "text": " will probably be completely trivial in a thousand years."}, {"timestamp": [2925.84, 2926.84], "text": " It won't matter."}, {"timestamp": [2926.84, 2928.16], "text": " That was always true though."}, {"timestamp": [2928.16, 2933.72], "text": " Whether or not someone believes in, you know, Judaism or Christianity or Islam or Buddhism"}, {"timestamp": [2933.72, 2940.84], "text": " or whatever, when you take a big step back, each individual life in the cosmic scale has"}, {"timestamp": [2940.84, 2943.28], "text": " always been mostly meaningless."}, {"timestamp": [2943.28, 2949.12], "text": " But meaning comes from your own subjective experience and your own personal journey."}, {"timestamp": [2949.12, 2956.42], "text": " And then when you find value in that journey, when you find value in that, that is where"}, {"timestamp": [2956.42, 2958.88], "text": " the solution to the meaning crisis comes."}, {"timestamp": [2958.88, 2963.9], "text": " And so artificial intelligence is just another mirror that is going to force us to look in"}, {"timestamp": [2963.9, 2965.0], "text": " deeper. It's just like the Gutenberg printing press, to force us to look in deeper."}, {"timestamp": [2965.0, 2968.5], "text": " It's just like the Gutenberg printing press, which allowed us to talk,"}, {"timestamp": [2968.5, 2970.2], "text": " you know, to spread ideas more."}, {"timestamp": [2970.2, 2974.5], "text": " Artificial intelligence is just the next evolution of human information literacy."}, {"timestamp": [2974.5, 2976.7], "text": " So, it's more of the same."}, {"timestamp": [2976.7, 2979.2], "text": " And of course, like, I make it sound like it's going to be easy."}, {"timestamp": [2979.2, 2980.5], "text": " It's going to be difficult."}, {"timestamp": [2980.5, 2982.0], "text": " People are going to be angry."}, {"timestamp": [2982.0, 2983.1], "text": " People are going to be sad."}, {"timestamp": [2983.1, 2985.0], "text": " People are going to have panic attacks."}, {"timestamp": [2985.0, 2989.0], "text": " There's going to be all kinds of transitions, as has always happened, right?"}, {"timestamp": [2989.0, 2997.0], "text": " Like I said, the Gutenberg printing press, you can trace a direct line between that and revolutions that cost millions of lives."}, {"timestamp": [2997.0, 3005.5], "text": " And hopefully, artificial intelligence doesn't ultimately change humanity and change society in a way that costs millions of lives."}, {"timestamp": [3005.5, 3007.42], "text": " But this is something that people are afraid of"}, {"timestamp": [3007.42, 3009.62], "text": " that very well could happen."}, {"timestamp": [3009.62, 3013.5], "text": " But again, it's a mirror, right?"}, {"timestamp": [3013.5, 3016.92], "text": " That forces us to look into ourselves."}, {"timestamp": [3016.92, 3018.9], "text": " And a lot of people don't like what they see,"}, {"timestamp": [3018.9, 3020.22], "text": " but that is the point."}, {"timestamp": [3020.22, 3022.54], "text": " That is how you learn and how you grow"}, {"timestamp": [3022.54, 3027.28], "text": " is being pushed out of your comfort zone. And in this case, it's like being tackled out of your comfort zone."}, {"timestamp": [3029.44, 3035.28], "text": " Yeah, that's a very fair assessment of the situation, I think."}, {"timestamp": [3035.28, 3044.32], "text": " And I like as well in your video at the end, you kind of foresee or try to look at the next 200"}, {"timestamp": [3044.32, 3046.16], "text": " years and then the next 2000 years. So maybe we"}, {"timestamp": [3046.16, 3053.84], "text": " can start by the next 200 years. What do you expect to happen, not necessarily only in"}, {"timestamp": [3054.64, 3059.6], "text": " biological immortality, but I suppose if we tomorrow become a biological immortal,"}, {"timestamp": [3061.2, 3064.8], "text": " we will see the next 200 years. So what will we see?"}, {"timestamp": [3065.0, 3071.0], "text": " We will see the next 200 years, so what will we see? Well, I hope 200 years from now we can have a follow-up podcast and see how accurate we were."}, {"timestamp": [3071.0, 3073.0], "text": " So that's the first thing."}, {"timestamp": [3073.0, 3082.0], "text": " But, yeah, so again, like I said at the beginning of the episode, some demographic changes are going to be slow."}, {"timestamp": [3082.0, 3086.16], "text": " So if we solve aging and disease"}, {"timestamp": [3086.16, 3087.44], "text": " within the next couple of decades,"}, {"timestamp": [3087.44, 3089.16], "text": " which I suspect we will,"}, {"timestamp": [3089.16, 3090.52], "text": " many of us that are around today"}, {"timestamp": [3090.52, 3094.08], "text": " will probably still be around in 200 years."}, {"timestamp": [3094.08, 3095.16], "text": " And then, you know,"}, {"timestamp": [3095.16, 3098.28], "text": " all the things that we've talked about this episode"}, {"timestamp": [3098.28, 3100.68], "text": " will be things that we're probably still struggling with."}, {"timestamp": [3100.68, 3101.52], "text": " Right?"}, {"timestamp": [3101.52, 3102.6], "text": " There's going to be people that question"}, {"timestamp": [3102.6, 3104.28], "text": " whether or not it's a good idea."}, {"timestamp": [3104.28, 3108.16], "text": " There's going to be questions of equality and how do we find and make meaning in the"}, {"timestamp": [3108.16, 3110.3], "text": " future."}, {"timestamp": [3110.3, 3116.04], "text": " On a global perspective, one thing that I suspect is that we're going to go through"}, {"timestamp": [3116.04, 3123.98], "text": " some big changes culturally because there is, presently, there's mostly Eastern and"}, {"timestamp": [3123.98, 3125.36], "text": " Western culture, and obviously that's mostly Eastern and Western culture,"}, {"timestamp": [3125.36, 3128.64], "text": " and obviously that's kind of an oversimplification,"}, {"timestamp": [3128.64, 3130.6], "text": " but you know, like the culture in China,"}, {"timestamp": [3130.6, 3135.6], "text": " and Japan, and Korea, and most of Asia is very different"}, {"timestamp": [3135.86, 3138.64], "text": " from the culture in America and Europe."}, {"timestamp": [3138.64, 3139.68], "text": " But at the same time,"}, {"timestamp": [3139.68, 3142.4], "text": " the population of Africa is exploding, right?"}, {"timestamp": [3142.4, 3144.24], "text": " And I think there's gonna be something like"}, {"timestamp": [3144.24, 3146.68], "text": " three billion Africans within 50 years"}, {"timestamp": [3146.68, 3148.0], "text": " or something like that."}, {"timestamp": [3148.0, 3151.24], "text": " And so then we're gonna have African culture"}, {"timestamp": [3151.24, 3153.54], "text": " kind of joining the world stage."}, {"timestamp": [3153.54, 3157.9], "text": " And so the biggest thing that I hope that we all live to see"}, {"timestamp": [3157.9, 3161.36], "text": " is this is becoming a truly global species,"}, {"timestamp": [3161.36, 3163.0], "text": " a truly global culture."}, {"timestamp": [3163.0, 3165.2], "text": " And I don't mean becoming one single culture."}, {"timestamp": [3165.2, 3168.68], "text": " What I mean is becoming a multicultural globe"}, {"timestamp": [3168.68, 3172.6], "text": " that is more aware of and tolerant of those differences."}, {"timestamp": [3172.6, 3174.72], "text": " And so from a geopolitical perspective,"}, {"timestamp": [3174.72, 3176.52], "text": " this is called multipolar peace."}, {"timestamp": [3176.52, 3177.64], "text": " It is kind of the goal right now."}, {"timestamp": [3177.64, 3179.72], "text": " So I hope that within 200 years,"}, {"timestamp": [3179.72, 3181.56], "text": " we can figure out multipolar peace,"}, {"timestamp": [3181.56, 3185.44], "text": " which is allowing different nations and different cultures with"}, {"timestamp": [3185.44, 3191.04], "text": " fundamentally different ideologies to coexist without having to say, we need to take over"}, {"timestamp": [3191.04, 3194.2], "text": " the world and enforce our view on everyone else."}, {"timestamp": [3194.2, 3198.92], "text": " I hope that we can figure that out within 200 years."}, {"timestamp": [3198.92, 3203.72], "text": " And then in 2000 years, I don't think we're going to be on earth anymore."}, {"timestamp": [3203.72, 3205.96], "text": " I think that there will be some people on Earth."}, {"timestamp": [3205.96, 3207.28], "text": " I might choose to stay here."}, {"timestamp": [3207.28, 3208.28], "text": " I might not."}, {"timestamp": [3208.28, 3209.6], "text": " You know, 2,000 years is a long time."}, {"timestamp": [3209.6, 3211.88], "text": " I might get bored and say, hey, let's go to Mars."}, {"timestamp": [3211.88, 3214.6], "text": " Let's go somewhere else."}, {"timestamp": [3214.6, 3221.1], "text": " I think that some of the problems in physics will probably be solved, such as if faster"}, {"timestamp": [3221.1, 3225.36], "text": " than light travel is possible, you know, in 2000 years,"}, {"timestamp": [3225.36, 3228.96], "text": " we might be across our entire galaxy if that's true."}, {"timestamp": [3229.8, 3233.24], "text": " And so then, but those are really big questions."}, {"timestamp": [3233.24, 3234.56], "text": " We don't know if that's possible."}, {"timestamp": [3234.56, 3239.32], "text": " And so there's a few major decision points or thresholds"}, {"timestamp": [3239.32, 3243.28], "text": " that we would have to cross in order to even predict."}, {"timestamp": [3243.28, 3246.5], "text": " One thing that I'm, I'm not gonna say that I'm afraid of,"}, {"timestamp": [3246.5, 3248.2], "text": " but one thing that I think might be possible"}, {"timestamp": [3248.2, 3249.82], "text": " is that a lot of humans aren't gonna really"}, {"timestamp": [3249.82, 3252.0], "text": " look human anymore, you know,"}, {"timestamp": [3252.0, 3253.98], "text": " because as we gain more and more abilities"}, {"timestamp": [3253.98, 3255.88], "text": " to change our bodies, to change our brains,"}, {"timestamp": [3255.88, 3258.48], "text": " to change our genetics, it might be that"}, {"timestamp": [3258.48, 3260.82], "text": " rather than evolving through reproduction,"}, {"timestamp": [3260.82, 3263.32], "text": " we just evolve by changing our bodies"}, {"timestamp": [3263.32, 3265.84], "text": " and changing oursel changing ourselves over time."}, {"timestamp": [3265.84, 3270.96], "text": " And, you know, to one point, that's actually going to be necessary to live on another planet."}, {"timestamp": [3270.96, 3276.24], "text": " Because, for instance, the environment on Mars is very, very different from the environment on Earth."}, {"timestamp": [3276.24, 3281.6], "text": " And so then it doesn't make sense to stay in this exact form if you're going to be a Martian."}, {"timestamp": [3282.48, 3286.0], "text": " But if you go to another planet, like let's say you go to a planet that is entirely oceans,"}, {"timestamp": [3286.0, 3288.0], "text": " it might make sense to have gills, right,"}, {"timestamp": [3288.0, 3290.0], "text": " and be more like a fish."}, {"timestamp": [3290.0, 3293.0], "text": " And so then you might change your body more and more over time."}, {"timestamp": [3293.0, 3296.0], "text": " And of course this is the topic of transhumanism or post-humanism."}, {"timestamp": [3296.0, 3300.0], "text": " And so I think that that's going to be a major,"}, {"timestamp": [3300.0, 3304.0], "text": " major point of contention within the next 2,000 years."}, {"timestamp": [3304.0, 3306.68], "text": " Maybe 200 years. It might be something that we're talking"}, {"timestamp": [3306.68, 3309.04], "text": " about sooner rather than later, I'm not sure."}, {"timestamp": [3309.04, 3312.88], "text": " Yeah, I think AI is the wild card here because we,"}, {"timestamp": [3312.88, 3315.36], "text": " especially super artificial intelligence,"}, {"timestamp": [3315.36, 3319.6], "text": " that it can accelerate the future,"}, {"timestamp": [3319.6, 3322.08], "text": " make things that would happen 2000 years"}, {"timestamp": [3322.08, 3324.04], "text": " actually happen at the end of the century."}, {"timestamp": [3324.04, 3329.0], "text": " And sometimes I think to like a change, if it's too fast,"}, {"timestamp": [3329.0, 3333.0], "text": " might be too destabilizing for the world."}, {"timestamp": [3333.0, 3338.0], "text": " For example, maybe nuclear weapons are a technology that we invented too soon."}, {"timestamp": [3338.0, 3343.0], "text": " And now we're kind of struggling on this equilibrium"}, {"timestamp": [3343.0, 3347.36], "text": " and we don't know how long it can last. So yeah."}, {"timestamp": [3349.04, 3355.36], "text": " So you're talking about AI, you and just to conclude this podcast, because your main"}, {"timestamp": [3356.88, 3363.44], "text": " research area is AI and you have this framework called GATO. So I thought we could talk a little"}, {"timestamp": [3363.44, 3367.44], "text": " bit about it. It's regarding the governance of AGI, if I'm not mistaken."}, {"timestamp": [3368.48, 3373.44], "text": " Yeah. Yeah. So GATO, it's a, I know it means a what cake in French,"}, {"timestamp": [3375.28, 3383.36], "text": " but it's a layered approach to solving alignment problems. And so it's an acronym. It means Global"}, {"timestamp": [3383.36, 3385.28], "text": " Alignment Taxonomy Omnibus."}, {"timestamp": [3385.28, 3390.72], "text": " So it's a structured framework that at the very bottom focuses on aligning models."}, {"timestamp": [3390.72, 3397.88], "text": " So this is, you know, if you can get a model, you know, a neural network to behave the way that you want it to behave."}, {"timestamp": [3397.88, 3400.48], "text": " The layer above that is autonomous agents."}, {"timestamp": [3400.48, 3407.5], "text": " And so we're starting to see this with, you know, the announcements announcements of more and more robots and personal assistants."}, {"timestamp": [3407.5, 3413.5], "text": " And so these are pieces of software and hardware that use artificial intelligence,"}, {"timestamp": [3413.5, 3420.5], "text": " but also have other components, like whether it's a robotic body or data or network connections, that sort of thing."}, {"timestamp": [3420.5, 3429.0], "text": " And so alignment is not just about individual models, it's about the software and hardware that goes into the deployment of artificial intelligence systems."}, {"timestamp": [3429.0, 3440.0], "text": " And then layer three of the Gato framework is the networks, because one thing that has been pointed out is that artificial intelligence is going to spend more time talking to each other than to us."}, {"timestamp": [3440.0, 3445.26], "text": " And so we need to make sure that the way that they communicate with each other and the way that they make decisions is"}, {"timestamp": [3445.26, 3448.1], "text": " something that is going to be stable rather than unstable,"}, {"timestamp": [3448.1, 3452.58], "text": " something that's going to trend towards safety and benevolence,"}, {"timestamp": [3452.58, 3455.46], "text": " rather than being destructive or evil."}, {"timestamp": [3455.46, 3458.78], "text": " Layer 4 of the Gato framework is corporate adoption,"}, {"timestamp": [3458.78, 3461.1], "text": " which is basically the messaging,"}, {"timestamp": [3461.1, 3465.32], "text": " the services to ensure that corporations do their part,"}, {"timestamp": [3465.32, 3472.32], "text": " that businesses do their part to adopt good AI practices and to build good AI services"}, {"timestamp": [3472.32, 3477.76], "text": " that move us in the direction of a better future rather than a dystopian future."}, {"timestamp": [3477.76, 3481.28], "text": " Layer 5 of the Gantto framework is national level,"}, {"timestamp": [3481.28, 3483.28], "text": " so national level regulation."}, {"timestamp": [3483.28, 3487.36], "text": " This would be like creating a science firm at the national level, so national level regulation. This would be like creating a science firm"}, {"timestamp": [3487.36, 3491.24], "text": " at the national level or creating regulations,"}, {"timestamp": [3491.24, 3492.32], "text": " rules and policies."}, {"timestamp": [3493.4, 3496.32], "text": " Britain, England has done a lot of work with,"}, {"timestamp": [3496.32, 3498.54], "text": " you know, creating a framework for adopting"}, {"timestamp": [3498.54, 3502.14], "text": " and creating AI responsibly."}, {"timestamp": [3502.14, 3504.52], "text": " And then the next layer above that,"}, {"timestamp": [3504.52, 3506.56], "text": " layer six is the international layer. And so the next layer above that, layer six, is the international layer."}, {"timestamp": [3506.56, 3512.16], "text": " And so the international layer is kind of like the EU AI Act, which is, you know, kind of saying,"}, {"timestamp": [3512.16, 3517.04], "text": " okay, over many nations, we agree that AI is going to look like this, and this is how we're"}, {"timestamp": [3517.04, 3522.08], "text": " going to approach it. But it also has to do with international research, because it's not just"}, {"timestamp": [3522.08, 3525.68], "text": " regulation, it is, you have to put money into doing things correctly."}, {"timestamp": [3525.68, 3526.68], "text": " Right."}, {"timestamp": [3526.68, 3530.92], "text": " So a good example is CERN, which researches particle physics."}, {"timestamp": [3530.92, 3534.88], "text": " It's based in France or physically based in France."}, {"timestamp": [3534.88, 3539.8], "text": " And then the ITER project is an international effort to research nuclear fusion."}, {"timestamp": [3539.8, 3546.52], "text": " So these are two examples of many nations coming together to research fundamentally changing"}, {"timestamp": [3546.52, 3548.98], "text": " technologies that we all need, right?"}, {"timestamp": [3548.98, 3554.94], "text": " Whether it's understanding physics or fusion, I think that we also need to see an international"}, {"timestamp": [3554.94, 3560.2], "text": " agency that researches artificial intelligence because the promise is very high, but the"}, {"timestamp": [3560.2, 3562.38], "text": " danger is also very high."}, {"timestamp": [3562.38, 3568.78], "text": " And then finally, the top layer of Gato or the final layer is global consensus,"}, {"timestamp": [3568.78, 3570.72], "text": " which is spreading the information,"}, {"timestamp": [3570.72, 3574.64], "text": " spreading the message, and bringing everyone to the table."}, {"timestamp": [3574.64, 3579.3], "text": " Because every human is a stakeholder in artificial intelligence,"}, {"timestamp": [3579.3, 3581.4], "text": " whether they know it or not."}, {"timestamp": [3581.68, 3586.84], "text": " Many people are just not even aware of AI yet. And so, what"}, {"timestamp": [3586.84, 3591.0], "text": " we need to do is raise awareness and bring people into the conversation, because as I"}, {"timestamp": [3591.0, 3596.24], "text": " mentioned, everyone is a stakeholder and everyone is going to be impacted by it. And so, therefore,"}, {"timestamp": [3596.24, 3603.64], "text": " everyone's position and everyone's needs must be taken into account. So, this layered approach"}, {"timestamp": [3603.64, 3606.22], "text": " is something that I and a group of people came"}, {"timestamp": [3606.22, 3611.4], "text": " up with. And what we're trying to do, what we're working on doing is creating a decentralized"}, {"timestamp": [3611.4, 3618.6], "text": " grassroots leaderless movement to ensure that AI moves in the correct direction. Because"}, {"timestamp": [3618.6, 3625.76], "text": " from the perspective of game theory and market theory and economics, more often than not,"}, {"timestamp": [3625.76, 3628.64], "text": " what happens with these kinds of technologies"}, {"timestamp": [3628.64, 3632.36], "text": " is that you have what's called a coordination failure."}, {"timestamp": [3632.36, 3634.8], "text": " And so coordination failures"}, {"timestamp": [3634.8, 3638.3], "text": " basically result in outcomes that you don't want, right?"}, {"timestamp": [3638.3, 3641.96], "text": " So climate change is the biggest example"}, {"timestamp": [3641.96, 3643.48], "text": " of a coordination failure."}, {"timestamp": [3643.48, 3645.16], "text": " Nobody wants climate change."}, {"timestamp": [3645.16, 3646.84], "text": " It's not good for anyone."}, {"timestamp": [3646.84, 3650.28], "text": " It harms the ecosystem, it harms food supplies,"}, {"timestamp": [3650.28, 3654.8], "text": " it harms water supplies, it causes climate refugees,"}, {"timestamp": [3654.8, 3656.86], "text": " but we can't get off that track."}, {"timestamp": [3656.86, 3659.04], "text": " And part of the reason that climate change is happening"}, {"timestamp": [3659.04, 3661.9], "text": " is because we don't have an alternative yet, right?"}, {"timestamp": [3661.9, 3665.04], "text": " And so some of that is we could have chosen"}, {"timestamp": [3665.04, 3667.02], "text": " a different path earlier on,"}, {"timestamp": [3667.02, 3670.9], "text": " but we couldn't force people to make the right decisions."}, {"timestamp": [3670.9, 3672.68], "text": " And then of course, there's lots of people"}, {"timestamp": [3672.68, 3674.24], "text": " acting in a self-interested manner,"}, {"timestamp": [3674.24, 3678.44], "text": " namely petroleum companies and petroleum producing nations."}, {"timestamp": [3678.44, 3681.82], "text": " And America is part of that."}, {"timestamp": [3681.82, 3684.22], "text": " I'm not blaming like the Middle East or anyone like,"}, {"timestamp": [3684.22, 3686.48], "text": " the petrodollar was our doing, right?"}, {"timestamp": [3686.48, 3689.44], "text": " But we did that out of self-interest."}, {"timestamp": [3689.44, 3691.32], "text": " And that was short-term thinking."}, {"timestamp": [3691.32, 3696.36], "text": " And so coordination failures are partly based on incentive structures,"}, {"timestamp": [3696.36, 3698.12], "text": " like, do you need to make money?"}, {"timestamp": [3698.12, 3700.8], "text": " And here's the other thing is if we got rid of petroleum today,"}, {"timestamp": [3700.8, 3702.32], "text": " most of us would starve to death."}, {"timestamp": [3702.32, 3705.5], "text": " We are dependent upon the thing that is killing us, right?"}, {"timestamp": [3705.5, 3706.78], "text": " And so because of that,"}, {"timestamp": [3706.78, 3709.06], "text": " that is a major coordination failure."}, {"timestamp": [3709.06, 3710.52], "text": " And what people are afraid of is that"}, {"timestamp": [3710.52, 3713.66], "text": " artificial intelligence is going to do the same thing."}, {"timestamp": [3713.66, 3716.06], "text": " Not, I don't mean make climate change worse,"}, {"timestamp": [3716.06, 3718.74], "text": " but that the incentive structure in place"}, {"timestamp": [3718.74, 3723.14], "text": " to advance artificial intelligence as quickly as possible"}, {"timestamp": [3723.14, 3726.12], "text": " is going to lead to destructive unintended consequences"}, {"timestamp": [3726.12, 3729.16], "text": " that nobody wants, but because of short-term thinking,"}, {"timestamp": [3729.16, 3731.08], "text": " we might end up there anyways."}, {"timestamp": [3731.08, 3733.52], "text": " And so the entire point of the Gato framework"}, {"timestamp": [3733.52, 3736.04], "text": " is to solve that coordination problem"}, {"timestamp": [3736.04, 3737.2], "text": " in a decentralized manner"}, {"timestamp": [3737.2, 3738.56], "text": " because that's the only way you can solve it."}, {"timestamp": [3738.56, 3741.2], "text": " But if everyone has the same roadmap,"}, {"timestamp": [3741.2, 3744.2], "text": " then everyone can do their own part,"}, {"timestamp": [3744.2, 3747.04], "text": " and that is the point of the Gato framework."}, {"timestamp": [3747.04, 3750.0], "text": " So yeah, thanks for asking and good question."}, {"timestamp": [3750.0, 3751.56], "text": " Yeah, that's fascinating."}, {"timestamp": [3751.56, 3755.2], "text": " I think a link to more about it,"}, {"timestamp": [3755.2, 3758.04], "text": " if you have any resources you want to share,"}, {"timestamp": [3758.04, 3759.68], "text": " I can put them in the description"}, {"timestamp": [3759.68, 3761.92], "text": " for anyone who is interested."}, {"timestamp": [3761.92, 3765.92], "text": " It's definitely a framework that makes sense, it's sound."}, {"timestamp": [3766.8, 3773.52], "text": " And you talk a lot about it on your YouTube channel and I invite anyone who's listening to"}, {"timestamp": [3773.52, 3778.72], "text": " check out your work. That's very, very fascinating. And it's not only about AI,"}, {"timestamp": [3778.72, 3783.44], "text": " as we talked about today, also longevity and futurism in general. So that's also"}, {"timestamp": [3783.44, 3786.12], "text": " the topic of this YouTube channel."}, {"timestamp": [3786.12, 3789.36], "text": " So yeah, a lot of common interest."}, {"timestamp": [3789.36, 3790.44], "text": " Yep."}, {"timestamp": [3790.44, 3793.96], "text": " Okay, so I don't know Emmanuel, if you want to add anything."}, {"timestamp": [3793.96, 3798.8], "text": " No, there's a lot of food for thought already,"}, {"timestamp": [3798.8, 3801.56], "text": " but thank you, thank you very much, David."}, {"timestamp": [3801.56, 3803.96], "text": " Yeah, thanks guys, good talk."}, {"timestamp": [3803.96, 3805.0], "text": " And I'm looking forward to seeing it uploaded and posted live, so cheers and. Thanks, guys. Good talk."}, {"timestamp": [3805.0, 3806.72], "text": " And I'm looking forward to seeing it uploaded and posted live."}, {"timestamp": [3806.72, 3817.6], "text": " So cheers and have a good rest of your day."}, {"timestamp": [3818.02, 3820.08], "text": " you"}]}