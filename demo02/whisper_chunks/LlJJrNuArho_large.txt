{"text": " Good morning everybody, David Shapiro here with another video. So, you know me. I am deeply optimistic and one of the things that I've done is I've created GATO, which is the Global Alignment Taxonomy Omnibus, which is a decentralized movement to help achieve alignment of digital superintelligence before it emerges. It has to do with corporate adoption, international regulation, studying models, decentralized networks, that sort of thing. And so of course, I keep my finger on the pulse of the current conversation. I watch pretty much all the congressional hearings on the control of AI and safety, international. I'm connecting with all kinds of people across LinkedIn, across the world, United States, Europe, Britain, pretty much anyone in the space of AI governance and safety, as well as people leading the charge and research. Now there are two papers that just came out that for me signal that obviously it's not a foregone conclusion. It's not over till it's over. But one of the complaints, one of the criticisms that I often have is that it feels like up until this point there were no adults in the room. And the adults were not taking it seriously. Well that has changed in a big way. So the first paper is this one, Frontier AI Regulation Managing Emerging Risks to Public Safety. Now that sounds pretty dry but if you listen to Eliezer Yudkowsky's TED talk, which is a pretty fiery indictment of the failures of the Academy. Pretty much any criticism that I have of the institutions is mild in comparison to his. Now, okay, so taking a step back though, when you look at who wrote this paper, there's a bunch of names, some of them jump right off the page. Miles Brundage from OpenAI and a few other people that I'm just becoming familiar with for the first time. But when you look at the institutions, Center for Governance of AI, Center for New American Security, Google DeepMind, OpenAI, Brookings Institute. So Brookings Institute is an interesting one. Center for Long-Term Resilience, Center for the Study of Existential Risk, the University of Cambridge, University of Washington, Convergence Analysis, Center for International Governance Innovation. Let's see, who else? Harvard, University of Toronto, Vector Institute, Cohere, Microsoft, University, UCLA. So these are big names. These, and this is, this, obviously it's not global, this is pretty much entirely America and Europe, but America and Europe are currently, you know, world leaders in terms of polarity of power. So this paper outlines a few things and they've got it pretty well outlined. So first, they just say, self-regulation is unlikely to provide sufficient protection against the risks from frontier AI models. Government intervention will be needed. So just unequivocally, we need to do this. So we explore how to do this. Mechanisms to create and update safety standards. So that's pretty generic. That's pretty out there. Or like a boilerplate. Mechanisms to give regulators visibility. So visibility, of course, like if you need to Visibility. So visibility, of course, like if you need to see something, there you go. So disclosure regimes, monitoring processes, and whistleblower protections. Yep, these equip regulators to address the appropriate regulatory targets. So basically, establishing that governments and international bodies have the rights and access to see into things. And of course, this is well- well established in many industries, whether you're dealing with toxic waste, nuclear power, and the financial industry of course. The financial industry is one of the most heavily regulated and visible, and of course many people will argue that it's not regulated and visible enough, and others will argue that it is overly transparent and overly regulated. I'm not here to debate about regulatory compliance in other industries, just pointing out that there is a tremendous amount of precedent. Let's see, mechanisms to ensure compliance with safety standards. Self-regulatory efforts such as voluntary certification may go some way. However, this seems likely to be insufficient without government intervention, for example, by empowering a supervisory authority to identify and sanction non-compliance or by licensing the deployment and potentially the development of frontier AI. And so what they mean here is not Joe Schmoe making an open source model. What they're talking about is people working on GPT-5, people working on the next gen stuff, the people that literally have billions of dollars to throw at it. So in terms of the regulatory burden, I'm not really worried about that. So those are three overall categories. Now we describe an initial set of safety standards that if adopted would provide some guardrails. Okay, conducting thorough risk assessments informed by evaluations of dangerous capabilities and controllability. So part of what they're talking about is establishing a battery of tests. You know, whenever the MSDS sheets come out for a substance, it has gone through a rigorous set of tests. And the underwriter's laboratory rigorously tests all new products that go in your home. If you ever see the UL, underwriter laboratory, stamp on anything, and it's pretty much on every product if you look, it's usually on the sticker, on the cord, or whatever. Basically, the products in your home have been tested to failure, and they have been tested to failure in such a way that we know that they won't kill you, or that if they do burst into flames, that it is only under certain circumstances, that sort of thing. So essentially, what they're doing is saying, OK, we need to rigorously test these products for safety and danger and establish a standard, a set of standards tests. Engaging external experts to apply independent scrutiny. Yes, so you don't want the fox guarding the hen house. It's that simple. You know, with the release of GPT-4, OpenAI, they did engage some external red teaming, but they really didn't do enough, and also they were guessing, and it was not transparent. It was entirely private. It was their decision and their dime, and they were not required to do it, and there was no established standard because, well, they were one of the first to do it. So and again like that's not an indictment it's good that they took the initiative to do it, but there is a correct way of going about independent scrutiny And from it from a technology perspective. I have worked with auditors And I'm not saying that you know a Sarbanes-Oxley auditor is the right way to go about it. But that being said, there's lots and lots of models on how to do this. Follow standardized protocols for how frontier AI models can be deployed based on their assessed risk. So this kind of dovetails with the EU AI Act where you kind of categorize AI based into several buckets of risk profiles. And that idea had some backlash. There was an open letter signed by more than 100 executives in Europe saying that the EU AI Safety Act, or the EU AI Act was going to be too onerous on businesses. And you know what, I disagree. or the EU AI Act was going to be too onerous on businesses. And you know what? I disagree. You know, but it's an ongoing conversation and ultimately there is enough attention, there's enough international attention and enough political willpower to solve this problem that, yes, I suspect that there will be some regulatory burden, but as per the Senate hearing where Dr. Ruman Chowdhury spoke, one of the things that they pointed out, and they being the researchers, some of the people on this paper, is that good regulation and good oversight can actually accelerate research because it can de-risk it. And when you de-risk something and you increase the quality of information, you can actually increase investor confidence. And where the money goes, the research goes. And having talked to people in finance, particularly venture capital and big finance, big banks, particularly over in Europe. What I will say is that, yes, there is a tremendous amount of excitement, but there's also a tremendous amount of trepidation. And having this third party level of scrutiny and certification and, and governance and oversight is really going to boost the confidence of venture capital and investors, which of course that means that more money is gonna come in, which means that the companies get built faster and safer. And so by creating a safe container, you're actually going to accelerate business. I know that it's not an intuitive thing, but if you talk to people inside the finance industry, that's generally how it works. Now, that being said, if you are in finance, if you are in VC and you want to talk to me, I'm happy to talk to you, reach out. We can either do a podcast episode or just talk privately so that I better understand the intersection of safety, regulation, compliance, and investment. Then finally, the fourth thing is monitoring and responding to new information on model capabilities. Basically, it's changing fast and so they recognize the need to keep up. Then they, let's see, oh here, I used my thingy-ma-job over here to talk with the model about it. Let's see. This is the International Institutions one. So I asked a few questions. Let's see. This paper focuses on regulation of frontier AI, et cetera, et cetera. Yep, so certainly the paper identifies three key challenges. So basically I had it read the whole paper and kind of summarize very succinctly. So the unexpected capabilities problem, that's what this paper talks about. Unexpected capabilities, this challenge arises from the fact that the capabilities of AI models can improve rapidly and unpredictably. And they also actually have some charts on this too. Unexpected capabilities. Let's see, where is it? Here we go. So they're just talking about, let's see, certain capabilities seem to emerge suddenly. Now, I have talked about how the fact that the sudden emergence is partially due to the measurement problem, which is when GPT-3 came out, nobody knew how to measure these things. And so then if you retroactively go back and test GPT-3 under the correct circumstances for things like theory of mind and planning, it already had that. I demonstrated and documented it in my books going back two years, more than two years now. But so one thing to keep in mind is that some of this is like, it's kind of like how a diagnosis of autism went up after the diagnostic criteria changed. So when you start measuring something and you start finding more of it, that's not necessarily a surprise. Now, that being said, I do agree that training more data on larger models with still fundamentally the same paradigm of large language models just predicting the next token, I will agree that it does seem like some capabilities do spontaneously emerge. And from an information perspective, we don't really know how or why that happens. So, the unexpected capabilities problem. Oh, also, I suspect that this will go up 10x once we have more multimodal models. But anyways, from a regulatory standpoint, this makes it difficult to reliably prevent deployed models from posing severe risks. You don't know what you don't know. You create a black box and you push a button and you don't know what's gonna happen. That's kind of scary from a regulatory standpoint and a safety standpoint. The deployment safety problem. So this is what we talked about already a little bit. The challenge relates to the difficulty of ensuring that deployed AI models do not cause harm. While developers implement measures to prevent misuse, these measures may not always be foolproof and malicious users may find ways to circumvent them. Additionally, the unexpected capabilities problem means that developers may not know all potential harms that need to be guarded against during deployment. And then finally, oh, and that, so this is what I'm, what I was talking about is like, you don't know what you don't know. And so for instance, you know, a small team is not going to be as creative as the rest of the world, as we have seen when people got access to chat GPT and then immediately created chaos GPT. And then there's the people using it to research anthrax and coronavirus and gain of function research, which is synthetic biology, which is AKA the lab leak hypothesis. And then finally, the proliferation problem. And this is something that I've been talking about. This challenge refers to the rapid proliferation of frontier AI models, which can make accountability difficult. Models can be open sourced, reproduced, or stolen, allowing access to their capabilities by unregulated actors. This means the dangerous capabilities could quickly become accessible to criminals or adversarial governments. I actually have a video coming out about this. That specifically, which the TLDR is that we're going to need to fight fire with fire. The only way to keep up in this kind of arms race is to use the AI to defeat other AI. And then I asked the paper like, okay, what are the building blocks? And so it was able to read it and kind of synthesize it down into three primary building blocks. Institutionalized frontier AI safety standards development. So this includes the development of safety standards through multi-stakeholder processes. That's kind of what we're talking about with regulators, compliance, that sort of thing. But it also, they talk about enforceable legal requirements. So basically this is, the governments have the right to shut down a data center if it is going rogue. The more extreme position is that, like Eliezer, Yukowsky said, like we should just straight out bomb data centers if they're non-compliant, which escalating to a shooting war, he's not an international geopolitics military strategist. So if you escalate to a shooting war, I don't know that that's actually gonna be beneficial. And prevention is worth, an ounce of prevention is worth a pound of cure. Put it that way. So a heavy-handed approach like that, probably not good. Now, increasing regulatory visibility, also we kind of talked about that, and ensuring compliance with standards. Okay, cool. So this paper lays out a pretty comprehensive, like, okay, here is a regulatory framework. So this paper will then kind of hand it off to, or begin the conversation with politicians, with think tanks, with all the people that are going to be working on creating these programs and that sort of thing. Now the second paper that I want to go over is this one, which is even more interesting to me. International Instit for advanced AI. Now if you remember I have vociferously advocated for the creation of new research and regulatory institutions. I called one of them GAIA, Global AI Agency, and the other one, this actually came from the YouTube comments, was AGGIS, the, what was it, the Alignment Enforcement for Global Intelligence Systems. Now, obviously, they're not, this paper isn't going to name them, but we collectively came up with this idea, and now this idea has been officially endorsed by many of the same players. So when you look at the names here, Google DeepMind, Blavatnik School of Government, so these are policy people, University of Oxford and the Center for Governance of AI, University of Montreal and Milan, CIFAR, OpenAI, Columbia, Harvard, Toronto, OpenAI again, Stanford, Newfield, and Oxford again. Okay, so what does this paper do? This paper advocates for four kinds of approaches to this. So a commission on frontier AI, Advanced AI Governance Organization, which sounds really like Gaia or Aegis, a frontier AI collaborative. So this is about research and public-private partnerships and then an AI safety project. So they propose these four international entities, these four international bodies. And you see here, they're citing stuff like IPCC, the IPBS, IAEA, the International Atomic Agency, CERN, ITER. So they are citing the existence of plenty of other international agencies, much like I did in my video just a couple days ago. And so then they break it down into this nice table. So the function is broken down into research and enforcement or regulation. So research and regulation. And then they have the four objectives, right? And then they've got some sub behaviors. So under the function, under the research function, there is conduct or support AI safety research, build consensus. So remember, consensus is a big part of the Gato framework. Building global consensus about AI is actually really critical because we all need to be having this conversation because as I tell plenty of other people, whether or not you are involved in AI, you are a stakeholder because it will affect you. Just like how whether or not you know anything about nuclear weapons or bioweapons, you are a stakeholder because guess what? They can kill you. Likewise, artificial intelligence is going to affect the way that you live. It's going to affect your safety. It's going to affect your prosperity. It's going to affect your personal security. So you are a stakeholder in AI, whether or not you care or acknowledge its existence or participate in research. The third thing is develop frontier AI. So this is something that I would have been advocating when I talked about Eater and CERN, which is that the incentives of a for-profit company, when a for-profit company develops something like GPT-4, they have the incentive to keep as much of it secret as possible, which stands in contrast to what is in the best interest of the public good. Now, that being said, I know that Sam Altman has said that he would prefer to democratize access to all AI for everyone. Time will tell. I know that right now open AI has obligations to Microsoft and once they hit that hundred billion dollar mark, we will see. I really hope that Sam Altman and the rest of open AI you know, that they honor their word and that once they earn Microsoft a hundred billion dollars that they honor their word, and that once they earn Microsoft $100 billion, they will open source everything and that they will open source it safely. If they do, great. Sam Altman deserves a frickin Nobel Peace Prize. Now, time will tell because OpenAI has pivoted in the past. OpenAI has said one thing and then done another. So remember, actions speak louder than words. But if a international research organization that is funded by the people and and controlled by the governments of the people is responsible for developing the frontier AI, then maybe their incentive structure is a little bit different. And this is, you saw this with a BRIT GPT, right? The UK has decided that they're going to train their own version of GPT, which is great because that means that then the incentive structure for what they do with that is going to be very different. And also it costs like what? I think one paper estimated that it costs $63 million to train GPT-4. That's a drop in the bucket for a country like Germany and Britain and America. Who cares? That's a trivial amount of money in the grand scheme of things. We waste more, anyways, not gonna rant about government waste here. That's not the point. Distribute and enable access to AI. So again, democratizing access. So these are four kind of behaviors or functions under the research. And you can see that they've kind of got this nice little, it's almost like a racy chart or a racy matrix to say who's going to do what. And then under the regulation, the rulemaking and enforcement, set safety norms and standards, support implementation of standards, monitor compliance, and then control the inputs. So by control inputs, I think what they mean is the data, the hardware and the software. And then they have over here, spreading beneficial technology, harmonizing regulation. So this is solving the coordination problem that Daniel Schmachtenberger talks about. And solving coordination is actually why I created the Gato alignment framework. Because what I realized is that without some sort of global consensus, without that, you know, everyone operating more or less in lockstep, then you're going to have coordination failures and you're going to end up with that terminal race condition that I talked about in a previous video. So by harmonizing regulation across the world, that is how you prevent a race condition. And this is how you keep the world, that is how you prevent a race condition. And this is how you keep the competitive landscape sane, but also predictable. Ensuring safe development and use, and then finally managing geopolitical risk factors. So this is slaughterbots. This is military race conditions and that sort of thing. And so, it's great that they're including all this existing international institutions, and so they have a whole bunch of examples. Now the problem is that there's a few gaps here. So you see there's only a few that exist. So like semiconductor export controls is one of the things that can manage the geopolitical risk factors. Now, then they recommend some new agencies, which is great. So their AI Safety Project, their Commission on Frontier AI, Frontier AI Collaborative, and then the Advanced AI Government Agency. Again, super sounds a lot like Gaia. Yeah, so you know this this paper it's kind of more or less what you know what you see is what you get. They kind of break it down which is great. Now, but really the thing is the thing that is most important to me is that the adults are in the room. That we've got people like Miles Brundage of OpenAI. We've got Ruma and Chowdhury who have heard her speech. She spoke directly to Congress. So these are people that are very serious about this and the fact that they are proposing these things that a bunch of us independently came to tells me that there's a tremendous amount of consensus already. I don't know if my YouTube channel had anything to do with that, I hope it does, but you know I'm here to report the good news. So long story short, when I look at the array of what is happening and who is talking, I strongly believe that the existential risk of AI is, we are well on the way to mitigating those risks. We're also looking at shorter term dangers and harm. So one of the things mentioned in these papers is using, as I mentioned earlier, the using these models to, for instance, create new pandemics. Synthetic biology is one of the biggest thing. Let's see, was it mentioned in this one? Yes, so here we go. Severe risk to public safety. A general purpose personal assistant that is capable of designing and autonomously ordering the manufacture of novel pathogens capable of causing a COVID level pandemic. So this is rapidly percolated up to being one of the greatest risks. And of course, it's fresh in people's mind because we just all survived a pandemic. But the idea that you can create something in a lab that can then hurt the entire world and you can't stop it, that's kind of one of the one of the big things. You know, military's okay deployment to power and weapons and nuclear stuff already exists. But the cost the intellectual and monetary cost of creating pathogens is pretty low and that's getting lower with artificial intelligence going that way that it is. Let's see, was it mentioned here? Yes. So they also cited that paper here. Or no, this is a different paper. What is an advanced market commitment and how could it help it beat COVID? So anyways, these two papers are great news. I think that it will be read the world over, particularly by governments all over the world. And these ideas have some teeth. They've clearly done their homework, they doubted their eyes and crossed their Ts. And yeah, so this really helps me relax with respect to AI. And you know, I did watch Yudkowsky's TED talk which came out yesterday I think, and you know he has a pretty fiery indictment of like, you know, I have been doing this for two decades and nobody listened and I tried to avoid being here where we are and nobody's putting in any effort effort or not enough effort or whatever and I'm like, okay I get it, but like is he not paying attention anymore? Like is he just banging this drum because it's fun to bang the drum? But like what these papers tell me is that one, the adults are in the room and two, they are taking it seriously and three, it's got teeth! Like the money is coming, the legislation is coming, the regulation is coming, the research is coming. And so it's like, I don't know, it seems like he's got an axe to grind and I'm not sure with whom. But, you know, and the thing is, here's the thing, is I don't disagree with Eliezer in terms of when you scientifically look at the risk of super intelligence. But I don't agree that it is not being taken seriously anymore. So anyways, with all that being said, thanks for watching. I hope you got a lot out of this. I hope that you feel like we're moving in the right direction. Again, I'm not going to say it's a foregone conclusion. It's not over till it's over. But this gives me a lot of hope. Cheers. Again, I'm not gonna say it's a foregone conclusion. It's not over till it's over, but this gives me a lot of hope. Cheers.", "chunks": [{"timestamp": [0.0, 3.0], "text": " Good morning everybody, David Shapiro here with another video."}, {"timestamp": [3.0, 14.0], "text": " So, you know me. I am deeply optimistic and one of the things that I've done is I've created GATO, which is the Global Alignment Taxonomy Omnibus,"}, {"timestamp": [14.0, 26.98], "text": " which is a decentralized movement to help achieve alignment of digital superintelligence before it emerges. It has to do with corporate adoption,"}, {"timestamp": [26.98, 30.3], "text": " international regulation, studying models,"}, {"timestamp": [30.3, 32.42], "text": " decentralized networks, that sort of thing."}, {"timestamp": [32.42, 35.18], "text": " And so of course, I keep my finger on the pulse"}, {"timestamp": [35.18, 37.26], "text": " of the current conversation."}, {"timestamp": [37.26, 40.26], "text": " I watch pretty much all the congressional hearings"}, {"timestamp": [40.26, 44.0], "text": " on the control of AI and safety, international."}, {"timestamp": [44.0, 52.2], "text": " I'm connecting with all kinds of people across LinkedIn, across the world, United States, Europe, Britain,"}, {"timestamp": [52.2, 57.84], "text": " pretty much anyone in the space of AI governance and safety, as well as people"}, {"timestamp": [57.84, 70.0], "text": " leading the charge and research. Now there are two papers that just came out that for me signal that obviously it's not a foregone conclusion."}, {"timestamp": [70.0, 73.0], "text": " It's not over till it's over."}, {"timestamp": [73.0, 81.0], "text": " But one of the complaints, one of the criticisms that I often have is that it feels like up until this point there were no adults in the room."}, {"timestamp": [81.0, 86.6], "text": " And the adults were not taking it seriously. Well that has"}, {"timestamp": [86.6, 93.36], "text": " changed in a big way. So the first paper is this one, Frontier AI Regulation"}, {"timestamp": [93.36, 98.84], "text": " Managing Emerging Risks to Public Safety. Now that sounds pretty dry but if you"}, {"timestamp": [98.84, 111.08], "text": " listen to Eliezer Yudkowsky's TED talk, which is a pretty fiery indictment of the failures of the Academy."}, {"timestamp": [111.08, 117.84], "text": " Pretty much any criticism that I have of the institutions is mild in comparison to his."}, {"timestamp": [117.84, 127.28], "text": " Now, okay, so taking a step back though, when you look at who wrote this paper, there's a bunch of names, some"}, {"timestamp": [127.28, 128.78], "text": " of them jump right off the page."}, {"timestamp": [128.78, 135.08], "text": " Miles Brundage from OpenAI and a few other people that I'm just becoming familiar with"}, {"timestamp": [135.08, 137.28], "text": " for the first time."}, {"timestamp": [137.28, 142.32], "text": " But when you look at the institutions, Center for Governance of AI, Center for New American"}, {"timestamp": [142.32, 146.16], "text": " Security, Google DeepMind, OpenAI, Brookings Institute. So"}, {"timestamp": [146.16, 151.36], "text": " Brookings Institute is an interesting one. Center for Long-Term Resilience, Center for the Study"}, {"timestamp": [151.36, 157.6], "text": " of Existential Risk, the University of Cambridge, University of Washington, Convergence Analysis,"}, {"timestamp": [159.04, 166.04], "text": " Center for International Governance Innovation. Let's see, who else? Harvard, University of Toronto,"}, {"timestamp": [166.04, 169.12], "text": " Vector Institute, Cohere, Microsoft,"}, {"timestamp": [170.12, 172.12], "text": " University, UCLA."}, {"timestamp": [172.12, 173.64], "text": " So these are big names."}, {"timestamp": [173.64, 175.48], "text": " These, and this is, this,"}, {"timestamp": [175.48, 176.48], "text": " obviously it's not global,"}, {"timestamp": [176.48, 179.88], "text": " this is pretty much entirely America and Europe,"}, {"timestamp": [179.88, 183.08], "text": " but America and Europe are currently, you know,"}, {"timestamp": [183.08, 187.12], "text": " world leaders in terms of polarity of power."}, {"timestamp": [188.5, 192.46], "text": " So this paper outlines a few things"}, {"timestamp": [193.3, 195.54], "text": " and they've got it pretty well outlined."}, {"timestamp": [195.54, 197.66], "text": " So first, they just say,"}, {"timestamp": [197.66, 200.6], "text": " self-regulation is unlikely to provide sufficient protection"}, {"timestamp": [200.6, 203.5], "text": " against the risks from frontier AI models."}, {"timestamp": [203.5, 205.28], "text": " Government intervention will be needed."}, {"timestamp": [205.28, 210.72], "text": " So just unequivocally, we need to do this. So we explore how to do this. Mechanisms to create and"}, {"timestamp": [210.72, 217.76], "text": " update safety standards. So that's pretty generic. That's pretty out there. Or like a boilerplate."}, {"timestamp": [218.32, 223.68], "text": " Mechanisms to give regulators visibility. So visibility, of course, like if you need to"}, {"timestamp": [221.36, 225.8], "text": " Visibility. So visibility, of course, like if you need to see something,"}, {"timestamp": [225.8, 226.4], "text": " there you go."}, {"timestamp": [226.4, 229.48], "text": " So disclosure regimes, monitoring processes,"}, {"timestamp": [229.48, 232.16], "text": " and whistleblower protections."}, {"timestamp": [232.16, 234.82], "text": " Yep, these equip regulators to address"}, {"timestamp": [234.82, 236.92], "text": " the appropriate regulatory targets."}, {"timestamp": [236.92, 239.96], "text": " So basically, establishing that governments"}, {"timestamp": [239.96, 243.04], "text": " and international bodies have the rights and access"}, {"timestamp": [243.04, 244.52], "text": " to see into things."}, {"timestamp": [244.52, 249.56], "text": " And of course, this is well- well established in many industries, whether you're dealing with"}, {"timestamp": [249.56, 256.6], "text": " toxic waste, nuclear power, and the financial industry of course."}, {"timestamp": [256.6, 260.64], "text": " The financial industry is one of the most heavily regulated and visible, and of course"}, {"timestamp": [260.64, 264.52], "text": " many people will argue that it's not regulated and visible enough, and others will argue"}, {"timestamp": [264.52, 271.44], "text": " that it is overly transparent and overly regulated. I'm not here to debate about"}, {"timestamp": [273.84, 277.6], "text": " regulatory compliance in other industries, just pointing out that there is a tremendous"}, {"timestamp": [277.6, 282.32], "text": " amount of precedent. Let's see, mechanisms to ensure compliance with safety standards."}, {"timestamp": [283.12, 285.74], "text": " Self-regulatory efforts such as voluntary certification"}, {"timestamp": [285.74, 286.86], "text": " may go some way."}, {"timestamp": [288.36, 291.24], "text": " However, this seems likely to be insufficient"}, {"timestamp": [291.24, 292.84], "text": " without government intervention,"}, {"timestamp": [292.84, 295.68], "text": " for example, by empowering a supervisory authority"}, {"timestamp": [295.68, 298.76], "text": " to identify and sanction non-compliance"}, {"timestamp": [298.76, 300.2], "text": " or by licensing the deployment"}, {"timestamp": [300.2, 302.56], "text": " and potentially the development of frontier AI."}, {"timestamp": [303.64, 307.4], "text": " And so what they mean here is not Joe Schmoe"}, {"timestamp": [308.68, 310.36], "text": " making an open source model."}, {"timestamp": [310.36, 313.96], "text": " What they're talking about is people working on GPT-5,"}, {"timestamp": [313.96, 316.0], "text": " people working on the next gen stuff,"}, {"timestamp": [316.0, 318.16], "text": " the people that literally have billions of dollars"}, {"timestamp": [318.16, 319.24], "text": " to throw at it."}, {"timestamp": [319.24, 321.04], "text": " So in terms of the regulatory burden,"}, {"timestamp": [321.04, 323.04], "text": " I'm not really worried about that."}, {"timestamp": [323.04, 330.02], "text": " So those are three overall categories. Now we describe an initial set of safety standards that if"}, {"timestamp": [330.02, 333.86], "text": " adopted would provide some guardrails. Okay, conducting thorough risk"}, {"timestamp": [333.86, 337.22], "text": " assessments informed by evaluations of dangerous capabilities and"}, {"timestamp": [337.22, 341.78], "text": " controllability. So part of what they're talking about is establishing a battery"}, {"timestamp": [341.78, 349.96], "text": " of tests. You know, whenever the MSDS sheets come out for a substance, it has gone through a rigorous"}, {"timestamp": [349.96, 351.16], "text": " set of tests."}, {"timestamp": [351.16, 355.8], "text": " And the underwriter's laboratory rigorously tests all new products that go in your home."}, {"timestamp": [355.8, 361.32], "text": " If you ever see the UL, underwriter laboratory, stamp on anything, and it's pretty much on"}, {"timestamp": [361.32, 365.4], "text": " every product if you look, it's usually on the sticker, on the cord, or whatever."}, {"timestamp": [365.4, 368.08], "text": " Basically, the products in your home"}, {"timestamp": [368.08, 369.96], "text": " have been tested to failure, and they"}, {"timestamp": [369.96, 371.76], "text": " have been tested to failure in such a way"}, {"timestamp": [371.76, 374.32], "text": " that we know that they won't kill you,"}, {"timestamp": [374.32, 376.6], "text": " or that if they do burst into flames,"}, {"timestamp": [376.6, 379.36], "text": " that it is only under certain circumstances,"}, {"timestamp": [379.36, 380.64], "text": " that sort of thing."}, {"timestamp": [380.64, 384.28], "text": " So essentially, what they're doing is saying,"}, {"timestamp": [384.28, 386.92], "text": " OK, we need to rigorously test these products"}, {"timestamp": [386.92, 390.72], "text": " for safety and danger and establish a standard,"}, {"timestamp": [390.72, 392.24], "text": " a set of standards tests."}, {"timestamp": [393.2, 396.92], "text": " Engaging external experts to apply independent scrutiny."}, {"timestamp": [396.92, 400.24], "text": " Yes, so you don't want the fox guarding the hen house."}, {"timestamp": [400.24, 401.28], "text": " It's that simple."}, {"timestamp": [402.2, 405.0], "text": " You know, with the release of GPT-4,"}, {"timestamp": [405.0, 409.0], "text": " OpenAI, they did engage some external red teaming,"}, {"timestamp": [409.0, 412.0], "text": " but they really didn't do enough, and also they were guessing,"}, {"timestamp": [412.0, 414.0], "text": " and it was not transparent."}, {"timestamp": [414.0, 416.0], "text": " It was entirely private."}, {"timestamp": [416.0, 420.0], "text": " It was their decision and their dime,"}, {"timestamp": [420.0, 421.0], "text": " and they were not required to do it,"}, {"timestamp": [421.0, 423.0], "text": " and there was no established standard"}, {"timestamp": [423.0, 428.24], "text": " because, well, they were one of the first to do it. So and again like that's not an indictment"}, {"timestamp": [428.24, 431.72], "text": " it's good that they took the initiative to do it, but there is a"}, {"timestamp": [432.72, 434.72], "text": " correct way of going about"}, {"timestamp": [435.36, 437.2], "text": " independent scrutiny"}, {"timestamp": [437.2, 441.2], "text": " And from it from a technology perspective. I have worked with auditors"}, {"timestamp": [441.8, 447.08], "text": " And I'm not saying that you know a Sarbanes-Oxley auditor is the right way to go about it."}, {"timestamp": [447.08, 451.6], "text": " But that being said, there's lots and lots of models"}, {"timestamp": [451.6, 453.32], "text": " on how to do this."}, {"timestamp": [453.32, 456.32], "text": " Follow standardized protocols for how frontier AI models"}, {"timestamp": [456.32, 458.72], "text": " can be deployed based on their assessed risk."}, {"timestamp": [458.72, 461.2], "text": " So this kind of dovetails with the EU AI Act"}, {"timestamp": [461.2, 468.76], "text": " where you kind of categorize AI based into several buckets of risk profiles."}, {"timestamp": [469.4, 473.7], "text": " And that idea had some backlash."}, {"timestamp": [473.7, 475.28], "text": " There was an open letter signed"}, {"timestamp": [475.28, 477.1], "text": " by more than 100 executives in Europe"}, {"timestamp": [477.1, 480.2], "text": " saying that the EU AI Safety Act,"}, {"timestamp": [480.2, 484.88], "text": " or the EU AI Act was going to be too onerous on businesses."}, {"timestamp": [484.88, 485.2], "text": " And you know what, I disagree. or the EU AI Act was going to be too onerous on businesses."}, {"timestamp": [485.2, 486.2], "text": " And you know what?"}, {"timestamp": [486.2, 488.68], "text": " I disagree."}, {"timestamp": [488.68, 493.56], "text": " You know, but it's an ongoing conversation and ultimately there is enough attention,"}, {"timestamp": [493.56, 497.6], "text": " there's enough international attention and enough political willpower to solve this problem"}, {"timestamp": [497.6, 510.08], "text": " that, yes, I suspect that there will be some regulatory burden, but as per the Senate hearing where Dr. Ruman Chowdhury"}, {"timestamp": [510.08, 516.2], "text": " spoke, one of the things that they pointed out, and they being the researchers, some of the people"}, {"timestamp": [516.2, 522.72], "text": " on this paper, is that good regulation and good oversight can actually accelerate research because"}, {"timestamp": [522.72, 526.08], "text": " it can de-risk it. And when you de-risk something"}, {"timestamp": [526.08, 528.26], "text": " and you increase the quality of information,"}, {"timestamp": [528.26, 531.62], "text": " you can actually increase investor confidence."}, {"timestamp": [531.62, 534.56], "text": " And where the money goes, the research goes."}, {"timestamp": [534.56, 538.32], "text": " And having talked to people in finance,"}, {"timestamp": [538.32, 543.32], "text": " particularly venture capital and big finance,"}, {"timestamp": [544.58, 547.28], "text": " big banks, particularly over in Europe."}, {"timestamp": [547.28, 552.46], "text": " What I will say is that, yes, there is a tremendous amount of excitement, but there's also a tremendous"}, {"timestamp": [552.46, 554.5], "text": " amount of trepidation."}, {"timestamp": [554.5, 562.12], "text": " And having this third party level of scrutiny and certification and, and governance and"}, {"timestamp": [562.12, 568.9], "text": " oversight is really going to boost the confidence of venture capital and investors,"}, {"timestamp": [568.9, 570.52], "text": " which of course that means that more money"}, {"timestamp": [570.52, 572.26], "text": " is gonna come in, which means that the companies"}, {"timestamp": [572.26, 574.86], "text": " get built faster and safer."}, {"timestamp": [574.86, 577.46], "text": " And so by creating a safe container,"}, {"timestamp": [577.46, 579.56], "text": " you're actually going to accelerate business."}, {"timestamp": [579.56, 583.24], "text": " I know that it's not an intuitive thing,"}, {"timestamp": [583.24, 587.36], "text": " but if you talk to people inside the finance industry, that's"}, {"timestamp": [587.36, 588.76], "text": " generally how it works."}, {"timestamp": [588.76, 593.86], "text": " Now, that being said, if you are in finance, if you are in VC and you want to talk to me,"}, {"timestamp": [593.86, 596.78], "text": " I'm happy to talk to you, reach out."}, {"timestamp": [596.78, 601.92], "text": " We can either do a podcast episode or just talk privately so that I better understand"}, {"timestamp": [601.92, 607.32], "text": " the intersection of safety, regulation, compliance, and investment."}, {"timestamp": [607.32, 609.6], "text": " Then finally, the fourth thing is monitoring and"}, {"timestamp": [609.6, 612.16], "text": " responding to new information on model capabilities."}, {"timestamp": [612.16, 614.4], "text": " Basically, it's changing fast and"}, {"timestamp": [614.4, 618.8], "text": " so they recognize the need to keep up."}, {"timestamp": [618.8, 621.72], "text": " Then they, let's see,"}, {"timestamp": [621.72, 625.84], "text": " oh here, I used my thingy-ma-job over here"}, {"timestamp": [625.84, 628.16], "text": " to talk with the model about it."}, {"timestamp": [629.12, 630.16], "text": " Let's see."}, {"timestamp": [630.16, 632.28], "text": " This is the International Institutions one."}, {"timestamp": [632.28, 633.76], "text": " So I asked a few questions."}, {"timestamp": [635.2, 636.12], "text": " Let's see."}, {"timestamp": [636.12, 639.26], "text": " This paper focuses on regulation of frontier AI,"}, {"timestamp": [639.26, 640.92], "text": " et cetera, et cetera."}, {"timestamp": [640.92, 645.0], "text": " Yep, so certainly the paper identifies three key challenges."}, {"timestamp": [645.36, 648.64], "text": " So basically I had it read the whole paper"}, {"timestamp": [648.64, 651.88], "text": " and kind of summarize very succinctly."}, {"timestamp": [651.88, 654.12], "text": " So the unexpected capabilities problem,"}, {"timestamp": [654.12, 655.88], "text": " that's what this paper talks about."}, {"timestamp": [656.76, 658.88], "text": " Unexpected capabilities, this challenge arises"}, {"timestamp": [658.88, 660.96], "text": " from the fact that the capabilities of AI models"}, {"timestamp": [660.96, 662.88], "text": " can improve rapidly and unpredictably."}, {"timestamp": [662.88, 665.52], "text": " And they also actually have some charts on this too."}, {"timestamp": [666.72, 668.64], "text": " Unexpected capabilities."}, {"timestamp": [669.2, 669.92], "text": " Let's see, where is it?"}, {"timestamp": [669.92, 670.48], "text": " Here we go."}, {"timestamp": [671.04, 673.6], "text": " So they're just talking about, let's see,"}, {"timestamp": [673.6, 675.68], "text": " certain capabilities seem to emerge suddenly."}, {"timestamp": [676.96, 680.88], "text": " Now, I have talked about how the fact that the sudden emergence"}, {"timestamp": [680.88, 683.44], "text": " is partially due to the measurement problem,"}, {"timestamp": [683.44, 685.94], "text": " which is when GPT-3 came out,"}, {"timestamp": [685.94, 687.78], "text": " nobody knew how to measure these things."}, {"timestamp": [687.78, 692.26], "text": " And so then if you retroactively go back and test GPT-3"}, {"timestamp": [692.26, 694.8], "text": " under the correct circumstances for things like theory"}, {"timestamp": [694.8, 697.52], "text": " of mind and planning, it already had that."}, {"timestamp": [697.52, 700.24], "text": " I demonstrated and documented it in my books"}, {"timestamp": [700.24, 703.46], "text": " going back two years, more than two years now."}, {"timestamp": [703.46, 705.56], "text": " But so one thing to keep in mind"}, {"timestamp": [705.56, 707.6], "text": " is that some of this is like,"}, {"timestamp": [707.6, 709.96], "text": " it's kind of like how a diagnosis of autism went up"}, {"timestamp": [709.96, 712.86], "text": " after the diagnostic criteria changed."}, {"timestamp": [712.86, 714.6], "text": " So when you start measuring something"}, {"timestamp": [714.6, 716.44], "text": " and you start finding more of it,"}, {"timestamp": [716.44, 718.4], "text": " that's not necessarily a surprise."}, {"timestamp": [718.4, 721.4], "text": " Now, that being said, I do agree"}, {"timestamp": [721.4, 725.0], "text": " that training more data on larger models"}, {"timestamp": [725.36, 727.84], "text": " with still fundamentally the same paradigm"}, {"timestamp": [727.84, 730.9], "text": " of large language models just predicting the next token,"}, {"timestamp": [730.9, 734.48], "text": " I will agree that it does seem like some capabilities"}, {"timestamp": [734.48, 736.32], "text": " do spontaneously emerge."}, {"timestamp": [736.32, 737.8], "text": " And from an information perspective,"}, {"timestamp": [737.8, 740.6], "text": " we don't really know how or why that happens."}, {"timestamp": [740.6, 743.8], "text": " So, the unexpected capabilities problem."}, {"timestamp": [743.8, 747.6], "text": " Oh, also, I suspect that this will go up 10x"}, {"timestamp": [747.6, 749.9], "text": " once we have more multimodal models."}, {"timestamp": [751.46, 754.24], "text": " But anyways, from a regulatory standpoint,"}, {"timestamp": [754.24, 756.72], "text": " this makes it difficult to reliably prevent"}, {"timestamp": [756.72, 759.64], "text": " deployed models from posing severe risks."}, {"timestamp": [759.64, 761.26], "text": " You don't know what you don't know."}, {"timestamp": [761.26, 763.18], "text": " You create a black box and you push a button"}, {"timestamp": [763.18, 767.36], "text": " and you don't know what's gonna happen. That's kind of scary from a regulatory"}, {"timestamp": [767.36, 772.98], "text": " standpoint and a safety standpoint. The deployment safety problem. So this is"}, {"timestamp": [772.98, 776.12], "text": " what we talked about already a little bit. The challenge relates to the"}, {"timestamp": [776.12, 779.86], "text": " difficulty of ensuring that deployed AI models do not cause harm. While"}, {"timestamp": [779.86, 784.24], "text": " developers implement measures to prevent misuse, these measures may not always be"}, {"timestamp": [784.24, 788.28], "text": " foolproof and malicious users may find ways to circumvent them."}, {"timestamp": [788.28, 790.32], "text": " Additionally, the unexpected capabilities problem"}, {"timestamp": [790.32, 792.92], "text": " means that developers may not know all potential harms"}, {"timestamp": [792.92, 795.42], "text": " that need to be guarded against during deployment."}, {"timestamp": [796.48, 798.02], "text": " And then finally, oh, and that,"}, {"timestamp": [798.02, 800.2], "text": " so this is what I'm, what I was talking about is like,"}, {"timestamp": [800.2, 801.48], "text": " you don't know what you don't know."}, {"timestamp": [801.48, 804.72], "text": " And so for instance, you know,"}, {"timestamp": [804.72, 808.4], "text": " a small team is not going to be as creative as the rest of the world,"}, {"timestamp": [808.4, 813.84], "text": " as we have seen when people got access to chat GPT and then immediately created chaos GPT."}, {"timestamp": [814.56, 821.04], "text": " And then there's the people using it to research anthrax and coronavirus and gain of function"}, {"timestamp": [821.68, 826.8], "text": " research, which is synthetic biology, which is AKA the lab leak hypothesis."}, {"timestamp": [827.8, 829.58], "text": " And then finally, the proliferation problem."}, {"timestamp": [829.58, 831.5], "text": " And this is something that I've been talking about."}, {"timestamp": [831.5, 833.58], "text": " This challenge refers to the rapid proliferation"}, {"timestamp": [833.58, 834.78], "text": " of frontier AI models,"}, {"timestamp": [834.78, 837.58], "text": " which can make accountability difficult."}, {"timestamp": [837.58, 839.92], "text": " Models can be open sourced, reproduced, or stolen,"}, {"timestamp": [839.92, 843.32], "text": " allowing access to their capabilities by unregulated actors."}, {"timestamp": [843.32, 844.74], "text": " This means the dangerous capabilities"}, {"timestamp": [844.74, 846.48], "text": " could quickly become accessible to criminals"}, {"timestamp": [846.48, 848.52], "text": " or adversarial governments."}, {"timestamp": [848.52, 851.48], "text": " I actually have a video coming out about this."}, {"timestamp": [851.48, 855.16], "text": " That specifically, which the TLDR is that"}, {"timestamp": [855.16, 858.84], "text": " we're going to need to fight fire with fire."}, {"timestamp": [858.84, 861.4], "text": " The only way to keep up in this kind of arms race"}, {"timestamp": [861.4, 864.56], "text": " is to use the AI to defeat other AI."}, {"timestamp": [864.56, 866.48], "text": " And then I asked the paper like,"}, {"timestamp": [866.48, 871.68], "text": " okay, what are the building blocks? And so it was able to read it and kind of synthesize it down"}, {"timestamp": [871.68, 877.2], "text": " into three primary building blocks. Institutionalized frontier AI safety standards development."}, {"timestamp": [878.16, 881.84], "text": " So this includes the development of safety standards through multi-stakeholder processes."}, {"timestamp": [881.84, 885.26], "text": " That's kind of what we're talking about with regulators,"}, {"timestamp": [885.26, 888.36], "text": " compliance, that sort of thing."}, {"timestamp": [888.36, 893.24], "text": " But it also, they talk about enforceable legal requirements."}, {"timestamp": [893.24, 895.92], "text": " So basically this is, the governments have the right"}, {"timestamp": [895.92, 899.2], "text": " to shut down a data center if it is going rogue."}, {"timestamp": [899.2, 902.24], "text": " The more extreme position is that, like Eliezer,"}, {"timestamp": [902.24, 904.52], "text": " Yukowsky said, like we should just straight out"}, {"timestamp": [904.52, 906.1], "text": " bomb data centers"}, {"timestamp": [906.1, 907.04], "text": " if they're non-compliant,"}, {"timestamp": [907.04, 910.56], "text": " which escalating to a shooting war,"}, {"timestamp": [910.56, 915.32], "text": " he's not an international geopolitics military strategist."}, {"timestamp": [915.32, 917.08], "text": " So if you escalate to a shooting war,"}, {"timestamp": [917.08, 919.78], "text": " I don't know that that's actually gonna be beneficial."}, {"timestamp": [921.24, 922.7], "text": " And prevention is worth,"}, {"timestamp": [923.68, 925.0], "text": " an ounce of prevention is worth a pound of cure."}, {"timestamp": [925.52, 926.84], "text": " Put it that way."}, {"timestamp": [926.84, 930.44], "text": " So a heavy-handed approach like that, probably not good."}, {"timestamp": [930.44, 932.96], "text": " Now, increasing regulatory visibility,"}, {"timestamp": [932.96, 934.96], "text": " also we kind of talked about that,"}, {"timestamp": [934.96, 936.68], "text": " and ensuring compliance with standards."}, {"timestamp": [936.68, 937.8], "text": " Okay, cool."}, {"timestamp": [937.8, 941.44], "text": " So this paper lays out a pretty comprehensive,"}, {"timestamp": [941.44, 945.44], "text": " like, okay, here is a regulatory framework."}, {"timestamp": [945.44, 952.12], "text": " So this paper will then kind of hand it off to, or begin the conversation with politicians,"}, {"timestamp": [952.12, 956.64], "text": " with think tanks, with all the people that are going to be working on creating these"}, {"timestamp": [956.64, 959.08], "text": " programs and that sort of thing."}, {"timestamp": [959.08, 963.52], "text": " Now the second paper that I want to go over is this one, which is even more interesting"}, {"timestamp": [963.52, 970.6], "text": " to me. International Instit for advanced AI. Now if you remember I have"}, {"timestamp": [970.6, 976.76], "text": " vociferously advocated for the creation of new research and regulatory"}, {"timestamp": [976.76, 983.48], "text": " institutions. I called one of them GAIA, Global AI Agency, and the other one, this"}, {"timestamp": [983.48, 989.0], "text": " actually came from the YouTube comments, was AGGIS, the, what was it, the Alignment Enforcement for Global"}, {"timestamp": [989.0, 994.48], "text": " Intelligence Systems. Now, obviously, they're not, this paper isn't going to"}, {"timestamp": [994.48, 1000.52], "text": " name them, but we collectively came up with this idea, and now this idea has"}, {"timestamp": [1000.52, 1005.22], "text": " been officially endorsed by many of the same players."}, {"timestamp": [1005.22, 1010.52], "text": " So when you look at the names here, Google DeepMind, Blavatnik School of Government,"}, {"timestamp": [1010.52, 1015.68], "text": " so these are policy people, University of Oxford and the Center for Governance of AI,"}, {"timestamp": [1015.68, 1023.56], "text": " University of Montreal and Milan, CIFAR, OpenAI, Columbia, Harvard, Toronto, OpenAI again,"}, {"timestamp": [1023.56, 1027.6], "text": " Stanford, Newfield, and Oxford again. Okay, so what"}, {"timestamp": [1027.6, 1037.2], "text": " does this paper do? This paper advocates for four kinds of approaches to this. So"}, {"timestamp": [1037.2, 1043.32], "text": " a commission on frontier AI, Advanced AI Governance Organization, which sounds"}, {"timestamp": [1043.32, 1046.0], "text": " really like Gaia or Aegis,"}, {"timestamp": [1046.0, 1047.88], "text": " a frontier AI collaborative."}, {"timestamp": [1047.88, 1050.72], "text": " So this is about research and public-private partnerships"}, {"timestamp": [1050.72, 1052.36], "text": " and then an AI safety project."}, {"timestamp": [1053.2, 1057.52], "text": " So they propose these four international entities,"}, {"timestamp": [1058.6, 1060.28], "text": " these four international bodies."}, {"timestamp": [1060.28, 1064.4], "text": " And you see here, they're citing stuff like IPCC,"}, {"timestamp": [1064.4, 1068.0], "text": " the IPBS, IAEA, the International"}, {"timestamp": [1068.0, 1071.92], "text": " Atomic Agency, CERN, ITER."}, {"timestamp": [1071.92, 1077.4], "text": " So they are citing the existence of plenty of other international agencies, much like"}, {"timestamp": [1077.4, 1080.6], "text": " I did in my video just a couple days ago."}, {"timestamp": [1080.6, 1088.36], "text": " And so then they break it down into this nice table. So the function is broken down into research"}, {"timestamp": [1088.36, 1090.96], "text": " and enforcement or regulation."}, {"timestamp": [1090.96, 1092.84], "text": " So research and regulation."}, {"timestamp": [1092.84, 1097.84], "text": " And then they have the four objectives, right?"}, {"timestamp": [1098.62, 1100.3], "text": " And then they've got some sub behaviors."}, {"timestamp": [1100.3, 1102.84], "text": " So under the function, under the research function,"}, {"timestamp": [1102.84, 1105.3], "text": " there is conduct or support AI safety"}, {"timestamp": [1105.3, 1107.84], "text": " research, build consensus."}, {"timestamp": [1107.84, 1112.54], "text": " So remember, consensus is a big part of the Gato framework."}, {"timestamp": [1112.54, 1117.6], "text": " Building global consensus about AI is actually really critical because we all need to be"}, {"timestamp": [1117.6, 1121.68], "text": " having this conversation because as I tell plenty of other people, whether or not you"}, {"timestamp": [1121.68, 1125.72], "text": " are involved in AI, you are a stakeholder because it will affect you."}, {"timestamp": [1125.72, 1129.84], "text": " Just like how whether or not you know anything about nuclear weapons or bioweapons, you are"}, {"timestamp": [1129.84, 1131.24], "text": " a stakeholder because guess what?"}, {"timestamp": [1131.24, 1132.76], "text": " They can kill you."}, {"timestamp": [1132.76, 1136.3], "text": " Likewise, artificial intelligence is going to affect the way that you live."}, {"timestamp": [1136.3, 1137.48], "text": " It's going to affect your safety."}, {"timestamp": [1137.48, 1139.76], "text": " It's going to affect your prosperity."}, {"timestamp": [1139.76, 1142.18], "text": " It's going to affect your personal security."}, {"timestamp": [1142.18, 1147.6], "text": " So you are a stakeholder in AI, whether or not you care or acknowledge its existence"}, {"timestamp": [1147.6, 1149.78], "text": " or participate in research."}, {"timestamp": [1151.28, 1153.32], "text": " The third thing is develop frontier AI."}, {"timestamp": [1153.32, 1156.22], "text": " So this is something that I would have been advocating"}, {"timestamp": [1156.22, 1158.4], "text": " when I talked about Eater and CERN,"}, {"timestamp": [1158.4, 1163.4], "text": " which is that the incentives of a for-profit company,"}, {"timestamp": [1164.76, 1168.52], "text": " when a for-profit company develops something like GPT-4,"}, {"timestamp": [1168.52, 1171.2], "text": " they have the incentive to keep as much of it secret"}, {"timestamp": [1171.2, 1174.92], "text": " as possible, which stands in contrast"}, {"timestamp": [1174.92, 1179.34], "text": " to what is in the best interest of the public good."}, {"timestamp": [1179.34, 1181.2], "text": " Now, that being said, I know that Sam Altman has said"}, {"timestamp": [1181.2, 1183.2], "text": " that he would prefer to democratize access"}, {"timestamp": [1183.2, 1188.24], "text": " to all AI for everyone. Time will tell. I know that right now open AI has"}, {"timestamp": [1189.42, 1194.42], "text": " obligations to Microsoft and once they hit that hundred billion dollar mark, we will see. I"}, {"timestamp": [1195.18, 1197.9], "text": " really hope that Sam Altman and the rest of open AI"}, {"timestamp": [1198.78, 1204.52], "text": " you know, that they honor their word and that once they earn Microsoft a hundred billion dollars"}, {"timestamp": [1204.5, 1208.16], "text": " that they honor their word, and that once they earn Microsoft $100 billion, they will open source everything and that they will open source it safely. If"}, {"timestamp": [1208.16, 1214.16], "text": " they do, great. Sam Altman deserves a frickin Nobel Peace Prize. Now, time will"}, {"timestamp": [1214.16, 1219.76], "text": " tell because OpenAI has pivoted in the past. OpenAI has said one thing and"}, {"timestamp": [1219.76, 1226.16], "text": " then done another. So remember, actions speak louder than words. But if a international"}, {"timestamp": [1226.16, 1232.76], "text": " research organization that is funded by the people and and controlled by the"}, {"timestamp": [1232.76, 1237.56], "text": " governments of the people is responsible for developing the frontier AI, then"}, {"timestamp": [1237.56, 1243.2], "text": " maybe their incentive structure is a little bit different. And this is, you saw"}, {"timestamp": [1243.2, 1245.38], "text": " this with a BRIT GPT, right?"}, {"timestamp": [1245.38, 1251.0], "text": " The UK has decided that they're going to train their own version of GPT, which is great because"}, {"timestamp": [1251.0, 1256.28], "text": " that means that then the incentive structure for what they do with that is going to be"}, {"timestamp": [1256.28, 1257.88], "text": " very different."}, {"timestamp": [1257.88, 1259.56], "text": " And also it costs like what?"}, {"timestamp": [1259.56, 1265.0], "text": " I think one paper estimated that it costs $63 million to train GPT-4."}, {"timestamp": [1265.24, 1268.4], "text": " That's a drop in the bucket for a country like Germany"}, {"timestamp": [1268.4, 1269.92], "text": " and Britain and America."}, {"timestamp": [1269.92, 1270.76], "text": " Who cares?"}, {"timestamp": [1270.76, 1271.68], "text": " That's a trivial amount of money"}, {"timestamp": [1271.68, 1273.0], "text": " in the grand scheme of things."}, {"timestamp": [1273.0, 1275.48], "text": " We waste more, anyways,"}, {"timestamp": [1275.48, 1280.24], "text": " not gonna rant about government waste here."}, {"timestamp": [1280.24, 1281.6], "text": " That's not the point."}, {"timestamp": [1281.6, 1284.36], "text": " Distribute and enable access to AI."}, {"timestamp": [1284.36, 1286.96], "text": " So again, democratizing access."}, {"timestamp": [1286.96, 1290.68], "text": " So these are four kind of behaviors or functions"}, {"timestamp": [1290.68, 1291.88], "text": " under the research."}, {"timestamp": [1291.88, 1294.64], "text": " And you can see that they've kind of got this nice little,"}, {"timestamp": [1294.64, 1297.24], "text": " it's almost like a racy chart or a racy matrix"}, {"timestamp": [1297.24, 1299.12], "text": " to say who's going to do what."}, {"timestamp": [1299.12, 1301.4], "text": " And then under the regulation, the rulemaking"}, {"timestamp": [1301.4, 1307.04], "text": " and enforcement, set safety norms and standards, support implementation"}, {"timestamp": [1307.04, 1311.56], "text": " of standards, monitor compliance, and then control the inputs."}, {"timestamp": [1311.56, 1319.76], "text": " So by control inputs, I think what they mean is the data, the hardware and the software."}, {"timestamp": [1319.76, 1329.48], "text": " And then they have over here, spreading beneficial technology, harmonizing regulation. So this is solving the coordination problem that Daniel Schmachtenberger talks about."}, {"timestamp": [1329.48, 1334.72], "text": " And solving coordination is actually why I created the Gato alignment framework."}, {"timestamp": [1334.72, 1344.24], "text": " Because what I realized is that without some sort of global consensus, without"}, {"timestamp": [1344.24, 1347.76], "text": " that, you know, everyone operating more or less"}, {"timestamp": [1347.76, 1352.8], "text": " in lockstep, then you're going to have coordination failures and you're going to end up with that"}, {"timestamp": [1352.8, 1359.44], "text": " terminal race condition that I talked about in a previous video. So by harmonizing regulation across"}, {"timestamp": [1360.16, 1364.64], "text": " the world, that is how you prevent a race condition. And this is how you keep"}, {"timestamp": [1363.4, 1368.28], "text": " the world, that is how you prevent a race condition. And this is how you keep the competitive landscape sane,"}, {"timestamp": [1368.28, 1370.32], "text": " but also predictable."}, {"timestamp": [1370.32, 1371.88], "text": " Ensuring safe development and use,"}, {"timestamp": [1371.88, 1375.04], "text": " and then finally managing geopolitical risk factors."}, {"timestamp": [1375.04, 1376.72], "text": " So this is slaughterbots."}, {"timestamp": [1376.72, 1381.48], "text": " This is military race conditions and that sort of thing."}, {"timestamp": [1381.48, 1388.48], "text": " And so, it's great that they're including all this existing international institutions,"}, {"timestamp": [1388.48, 1396.8], "text": " and so they have a whole bunch of examples. Now the problem is that there's a few gaps here."}, {"timestamp": [1397.6, 1405.52], "text": " So you see there's only a few that exist. So like semiconductor export controls is one of the things that can manage"}, {"timestamp": [1405.52, 1407.98], "text": " the geopolitical risk factors."}, {"timestamp": [1407.98, 1412.98], "text": " Now, then they recommend some new agencies, which is great."}, {"timestamp": [1413.26, 1414.74], "text": " So their AI Safety Project,"}, {"timestamp": [1414.74, 1416.86], "text": " their Commission on Frontier AI,"}, {"timestamp": [1416.86, 1418.74], "text": " Frontier AI Collaborative,"}, {"timestamp": [1418.74, 1422.02], "text": " and then the Advanced AI Government Agency."}, {"timestamp": [1422.02, 1424.4], "text": " Again, super sounds a lot like Gaia."}, {"timestamp": [1424.4, 1429.0], "text": " Yeah, so you know this"}, {"timestamp": [1429.0, 1431.6], "text": " this paper it's kind of more or less what you know what you see is what you"}, {"timestamp": [1431.6, 1438.16], "text": " get. They kind of break it down which is great. Now, but really the thing is the"}, {"timestamp": [1438.16, 1443.36], "text": " thing that is most important to me is that the adults are in the room. That"}, {"timestamp": [1443.36, 1445.72], "text": " we've got people like Miles Brundage of"}, {"timestamp": [1445.72, 1450.36], "text": " OpenAI. We've got Ruma and Chowdhury who have heard her speech. She spoke"}, {"timestamp": [1450.36, 1457.58], "text": " directly to Congress. So these are people that are very serious about this and the"}, {"timestamp": [1457.58, 1462.32], "text": " fact that they are proposing these things that a bunch of us independently"}, {"timestamp": [1462.32, 1465.12], "text": " came to tells me that there's a tremendous"}, {"timestamp": [1465.12, 1470.88], "text": " amount of consensus already. I don't know if my YouTube channel had anything to do"}, {"timestamp": [1470.88, 1475.88], "text": " with that, I hope it does, but you know I'm here to report the good news. So long"}, {"timestamp": [1475.88, 1482.16], "text": " story short, when I look at the array of what is happening and who is talking, I"}, {"timestamp": [1482.16, 1487.76], "text": " strongly believe that the existential risk of AI is,"}, {"timestamp": [1487.76, 1491.4], "text": " we are well on the way to mitigating those risks."}, {"timestamp": [1491.4, 1496.04], "text": " We're also looking at shorter term dangers and harm."}, {"timestamp": [1496.04, 1498.68], "text": " So one of the things mentioned in these papers"}, {"timestamp": [1498.68, 1501.56], "text": " is using, as I mentioned earlier,"}, {"timestamp": [1501.56, 1504.32], "text": " the using these models to, for instance,"}, {"timestamp": [1504.32, 1506.72], "text": " create new pandemics."}, {"timestamp": [1506.72, 1510.02], "text": " Synthetic biology is one of the biggest thing."}, {"timestamp": [1510.02, 1511.68], "text": " Let's see, was it mentioned in this one?"}, {"timestamp": [1511.68, 1513.7], "text": " Yes, so here we go."}, {"timestamp": [1513.7, 1515.0], "text": " Severe risk to public safety."}, {"timestamp": [1515.0, 1516.4], "text": " A general purpose personal assistant"}, {"timestamp": [1516.4, 1518.72], "text": " that is capable of designing and autonomously"}, {"timestamp": [1518.72, 1521.04], "text": " ordering the manufacture of novel pathogens"}, {"timestamp": [1521.04, 1524.0], "text": " capable of causing a COVID level pandemic."}, {"timestamp": [1524.0, 1529.5], "text": " So this is rapidly percolated up to being one of the greatest risks."}, {"timestamp": [1529.5, 1534.4], "text": " And of course, it's fresh in people's mind because we just all survived a pandemic."}, {"timestamp": [1534.4, 1539.04], "text": " But the idea that you can create something in a lab that can then hurt the entire world"}, {"timestamp": [1539.04, 1547.0], "text": " and you can't stop it, that's kind of one of the one of the big things. You know, military's"}, {"timestamp": [1547.0, 1554.66], "text": " okay deployment to power and weapons and nuclear stuff already exists. But the cost the intellectual"}, {"timestamp": [1554.66, 1561.98], "text": " and monetary cost of creating pathogens is pretty low and that's getting lower with artificial"}, {"timestamp": [1561.98, 1565.56], "text": " intelligence going that way that it is. Let's see, was it mentioned here?"}, {"timestamp": [1565.56, 1566.96], "text": " Yes."}, {"timestamp": [1566.96, 1569.04], "text": " So they also cited that paper here."}, {"timestamp": [1571.84, 1573.32], "text": " Or no, this is a different paper."}, {"timestamp": [1573.32, 1574.84], "text": " What is an advanced market commitment"}, {"timestamp": [1574.84, 1577.12], "text": " and how could it help it beat COVID?"}, {"timestamp": [1577.12, 1581.64], "text": " So anyways, these two papers are great news."}, {"timestamp": [1581.64, 1589.32], "text": " I think that it will be read the world over, particularly by governments all"}, {"timestamp": [1589.32, 1595.54], "text": " over the world. And these ideas have some teeth. They've clearly done their homework,"}, {"timestamp": [1595.54, 1607.38], "text": " they doubted their eyes and crossed their Ts. And yeah, so this really helps me relax with respect to AI. And you know, I did watch"}, {"timestamp": [1607.38, 1613.62], "text": " Yudkowsky's TED talk which came out yesterday I think, and you know he has a"}, {"timestamp": [1613.62, 1618.26], "text": " pretty fiery indictment of like, you know, I have been doing this for two decades"}, {"timestamp": [1618.26, 1622.94], "text": " and nobody listened and I tried to avoid being here where we are and"}, {"timestamp": [1622.94, 1626.72], "text": " nobody's putting in any effort effort or not enough effort or whatever"}, {"timestamp": [1626.72, 1630.12], "text": " and I'm like, okay I get it, but like"}, {"timestamp": [1630.12, 1634.72], "text": " is he not paying attention anymore? Like is he just banging this drum because"}, {"timestamp": [1634.72, 1636.08], "text": " it's fun to bang the drum?"}, {"timestamp": [1636.08, 1639.28], "text": " But like what these papers tell me is that"}, {"timestamp": [1639.28, 1642.68], "text": " one, the adults are in the room and two, they are taking it seriously"}, {"timestamp": [1642.68, 1649.2], "text": " and three, it's got teeth! Like the money is coming, the legislation is coming, the regulation is coming, the research"}, {"timestamp": [1649.2, 1651.02], "text": " is coming."}, {"timestamp": [1651.02, 1655.52], "text": " And so it's like, I don't know, it seems like he's got an axe to grind and I'm not sure"}, {"timestamp": [1655.52, 1656.52], "text": " with whom."}, {"timestamp": [1656.52, 1661.92], "text": " But, you know, and the thing is, here's the thing, is I don't disagree with Eliezer in"}, {"timestamp": [1661.92, 1666.64], "text": " terms of when you scientifically look at the risk of super intelligence."}, {"timestamp": [1666.64, 1672.24], "text": " But I don't agree that it is not being taken seriously anymore."}, {"timestamp": [1672.24, 1677.04], "text": " So anyways, with all that being said, thanks for watching. I hope you got a lot out of this."}, {"timestamp": [1677.04, 1680.56], "text": " I hope that you feel like we're moving in the right direction."}, {"timestamp": [1680.56, 1684.16], "text": " Again, I'm not going to say it's a foregone conclusion. It's not over till it's over."}, {"timestamp": [1684.16, 1687.84], "text": " But this gives me a lot of hope. Cheers."}, {"timestamp": [1682.9, 1685.66], "text": " Again, I'm not gonna say it's a foregone conclusion."}, {"timestamp": [1685.66, 1687.34], "text": " It's not over till it's over,"}, {"timestamp": [1687.34, 1690.16], "text": " but this gives me a lot of hope."}, {"timestamp": [1690.16, 1691.0], "text": " Cheers."}]}