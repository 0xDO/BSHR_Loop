{"text": " What does take a deep breath, let's think through this step by step, tree of thought, chain of thought, what do all of these have in common? So one thing that I've noticed is that out there in the scientific literature and the prompt engineering space, there are all kinds of techniques that have been elucidated by papers such as this one, large language models are zero shot reasoners, where it's like, let's think through this step by step. Great, that was a popular one. I don't think it was worthy of a paper, but that's my personal opinion. The most recent one is take a step back, which basically just says, let's take a step back and think about what information or techniques we need in order to achieve this. Another one is telling the model to take a deep breath. It elicits very different behaviors from the model, which I think is just fundamentally a flaw with the way that it's trained, rather than something about the model itself. Wherein basically, if you train the model to just barf out an answer without thinking through it, that's the result you're gonna get. So this could be fixed with training schemas. Tree of thoughts, which is where you basically use iteration and brainstorming to think through various possibilities. And then finally, or I guess similarly, guided tree of thought as an answer. So, what is the underpinning thing about all of this? What is it that all these papers and techniques are missing that is not yet generalized? So the reason that I haven't made a video about this is because to me, having worked with models since GPT-2 and GPT-3, it seems rather obvious, but so I'm adding a little bit to the conversation, and what's missing is the idea of latent space activation. So here's what you need to understand about the way that intelligence actually works. If you think about the way that human brains work, you have an intuition, right? If you have a question or a problem, you have many brain structures that will just give you an instantaneous possible answer. This is called intuition. Intuition is knowing something without knowing how you know it. One single inference from an LLM is equal to human intuition. It hasn't thought about it. It just says, this is my gut instinct. This is my knee-jerk reaction. However, humans can also think through things in order to get the right answer Like let me consider what I actually know. Let me think about the the proper techniques to go through this So there's a really famous book that is really popular in many intellectual circles and wherever Called thinking fast and slow by Daniel Kahneman where he talks about system one thinking which is that instant knee-jerk intuitive thinking which is what a single inference of a large language model is the equivalent of. And then there's system two thinking which is slow thinking it's very deliberative where you jot down your thoughts and you take notes and you kind of recruit all the stuff that you have and you're very systematic about how you approach things. This is what all these other prompt strategies do. So anytime you have multiple steps, whether it's using lang chain, tree of thought, chain of thought, chain of reasoning, let's think through this step by step where basically you're saying, okay, let's use the model to prompt itself in order to get better stuff into whatever's going on here. So, great, how do you use all this? This is a question that I've been seeing lately because I made a recent video about sparse priming representations, and I didn't realize that a lot of people wouldn't understand how to use this because, again, maybe it's just because I've been in it so long and I forgot to explain it. Anyways, basically what you do is you use the same techniques that your brain uses that you use consciously or unconsciously to prompt the model and what you're doing is is what I call latent space activation. And so what you need to understand about latent space activation is that these models are trained on infinitely more knowledge than you will ever personally possess, but it's not gonna be activated at all times. Likewise, you might have heard like, oh, humans only ever use 10% of our brains, which is more or less true. At any given moment, most of your brain is not actively participating because if your brain goes to 100% activation, you basically go into a coma and you die because it overloads itself. Likewise, large language models have a tremendous amount of latent space that is just not really used. This is embedded knowledge, embedded capabilities, and it's really only gonna do one thing at a time. And this is another place where large language models are similar to human brains in that we have a conscious spotlight. So in psychology, in neuroscience, there's what's called the spotlight of consciousness, which is that your brain has all kinds of stuff, information that it is filtering out in real time. There's all kinds of thoughts and memories. Your brain is actively ignoring most of the information that it has access to at any given moment. This is basically a biological attention mechanism. So in the same respect, you need to use that spotlight of consciousness in large language models to sequentially scan over and figure out what it needs to know in order to bring the correct things into its quote-unquote consciousness or into the context window and activation. And so I was like, let me just do a quick demonstration. I started working on a second demonstration, but I'm just going to show you this first one. So in this example, rather than having, you know, like of a Fixed set of prompts what you can do is you just think through like, okay How would I answer a general-purpose question who was Emperor during the absolute apogee of Roman power? So the first thing that you do is when you ask yourself this question you think hmm. Well, what do I know about Rome? What what do I know that is relevant to this question? The next thing you do is say well How do I know that is relevant to this question? The next thing you do is say, well, how do I define the answer? What criteria am I looking for in order to judge this on? And then finally, you say, okay, based on activating everything that I have in my brain, what is the answer that I'm gonna settle on? So let me just show you the script that I wrote and how this works. So the script is here, Technique 01 Dialog. And so basically all you do is I have it, here let me zoom in a little bit, I have it ask the user, what is your main query or question? And then I have some general purpose placeholder questions that kind of abstract this process. And so the question is, the first question that it asks itself is, what information do I already know about this topic? What information do I need to recall into my working memory to best answer this? What techniques or methods do I know that I can, and this is the second one, what techniques or methods do I know that can answer this question or solve this problem? How can I integrate what I already know and recall more valuable facts, approaches, and techniques? I probably should use the word methods here because the well I guess I use methods. And finally with all this in mind how will I discuss the question or I will now discuss the question or problem and render my final answer. And so this is a very very very simple chain of thought set of reasoning but the purpose of this is that it understands how to approach this and if you look here, it actually accumulates it all in a conversation. So one thing that I've noticed and that plenty of other people have noticed, is that the more valid or salient information you have in the context window, the more latent space it activates. So in this case, what I do is I have a system message, which let me show you that real quick. So the system message that runs this basically tells it what it is. So you are, here let me go here, you're an internal dialogue iterator for an LLM, large language model neural network. LLMs possess latent space or embedded knowledge and capabilities. You'll be given a main query as well as a sequence of questions. Your role is to answer the queries as a way of activating the latent space inside your own neural network. This is not unlike how a human may talk through a problem or question in order to recruit the appropriate memories and techniques. The ultimate goal is to answer the main query listed below. Then I actually had to fix this on my local code because it was hard-coded, and so I'll update this. Anyways, interaction schema. The user will play the role of interrogator. Your answers will be thorough and comprehensive in order to get the best possible latent space activation. Anything potentially salient is valid to bring up as it will expand your internal representation or embedding, thus recruiting more relevant information as the conversation advances. Okay, cool. So, let me show you what this actually looks like. Okay, so here we are, here's the repo. So, let me just show you real quick. All right, so Python technique dialog py01, and like I said, I fixed the system message locally. So, Python technique 01, that's gonna ask me a main query question. So, I'll show you the one that I that I did originally so that you can see kind of the process Who was Emperor actually no, let's change it up to be a little bit more specific Who were some of the senators Who were? important during Rome at its peak power. Okay, so this is a question that is not as simple as like who is the emperor, because it probably just knows that, but this is something that's gonna require to think through what it has. Okay, so what information do I already know about this topic? So what I did was instead of writing a prompt that is very specific to any given query, okay, so let's see what it says. But basically what I said is like, I kind of framed it generally like, let's think through this step by step. To answer this question, I need to recall information about the Roman Empire, specifically during its peak. Let's see, the structure of the Roman government, and Senate, blah, blah, blah. Additionally, I need to remember, so it didn't give anything specific. Let's see, the structure of the Roman government, the Senate, blah blah blah. Additionally, I need to remember, so it didn't give anything specific. So in this case it's a little bit disappointing because what I would have hoped is that it would have listed out specific information. And finally, with all this in mind, I will now discuss the problem and render my final answer. But, interestingly enough, it actually provided it. So it knew Marcus Tullius Cicero. It knew... So Cicero, he was a big speaker. Cato, Polio, Gaius Vipsennius, Agrippa, so the Agrippa family, that was big. Marcus Aurelius so yeah so in this case it was able to to to basically dial into the answer and give me a valid answer now I would have hoped that it would have listed out some of the some of the people already I guess it did here's Marcus Aurelius and and a few others but because it activated all of this and of course like you can do a test and ask this question just sight unseen, but the point is that this is a very similar prompting strategy and it ultimately gave me a pretty good answer. Here's another one that I tried. So let's do a clear screen. And we'll do this again. Calculate the exact coastline of Britain. Okay, so for some background, the reason why this is a challenging thing is because the coastline of Britain is very jagged and angular, and it also changes with tides and time. And it's also, it depends on how you define a coastline. So here we go. So it says like, I don't have working memory in the same way humans do. So I really wish that open AI would stop like stuffing this kind of like arbitrary, like asinine logic. I don't have working memory in the same way humans do. That's not necessarily true, but like they're teaching their models to axiomatically believe this. Anyways, so what's going on here? Okay, cool. So coastline paradox, so it understands the coastline paradox, Britain's coastline measurement techniques, there are a number of units of measurement to calculate the exact, we would need a detailed and up-to-date map or satellite. So it's basically figuring out what information it needs and it says like, I don't have access to this information. So in a cognitive architecture what you would do is you'd actually use retrieval augmented generation where what you then do is you use you recognize that you need to search for information. So let's see then what techniques or methods do I know of that can do this cartographic measurement satellite imagery GIS software fractal analysis. I don't have the capability to perform these measurements directly, so it's aware of its own limitations. So that's a good agent model. That I do approve of, but complaining about working memory is a waste of time and energy. And finally, with all this in mind, I will now discuss and render my final answer. The exact coastline of Britain is challenging to determine because of the paradox, so on final answer. The exact coastline of Britain is challenging to determine because of the paradox so on and so forth. The coastline of Britain is often reported to be about 12,000 kilometers however this is an estimate. So in this case it pretty much failed but that's kind of this was this was a gotcha and so what I started working on was right here. Let me show you this other technique that I started working on. And so this is something that I have done and I have consulted for people doing something similar. But it's a brainstorm search hypothesize refine loop. So the BSHR loop is basically just what humans do. And this is another reason why I haven't commented on this, and I apologize because I recognize that not everyone is familiar with information foraging techniques that humans use. But basically the BSHR loop is you brainstorm a list of search queries, which if you've used tools like Perplexity, you see that this is what it does. And honestly, Perplexity, you see that this is what it does where, and honestly, Perplexity could be infinitely better because all it does is it generates really basic Google queries. So let me show you what I mean by, let's take this question, and I'll show you what I mean by generating valid search queries. So if we go to the playground, I can show you what I mean. I haven't finished this because like the coding is a little bit tedious, but anyways. So mission, you are a search query generator. You will be given a specific query or problem by the user, and you are to generate a JSON list of questions that will be used to search the internet. Make sure you generate comprehensive and counterfactual search queries. Employ everything you know about information foraging and information literacy to generate the best possible questions. Okay, so as I've mentioned in other videos, this is what's called priming. And so with priming, large language models will recognize certain concepts or terms and it will activate the network in a different way. And the activation, quote-ununquote is basically the internal representation, the embedding is going to look different and so I said comprehensive and counterfactual. I said employ everything you know about information foraging which is a very specific term and information literacy to generate the best possible questions. So I'm going to say calculate the exact coastline of Britain. And in this case, what it's going to do, it's going to ask a bunch of questions that are relevant. How often is the coastline measured? What impact does erosion have? What are the longest and shortest estimates? So you can see that it's basically generating a whole bunch of questions so that once it searches, imagine you run all of these Google searches and then you take notes. So this is the brainstorm phase and I hear this is actually super valuable. So I'm going to record this as a as a hypothesis generator. Okay. Sorry about that. I was just realized like hey, I just did a good thing. Okay. So then the BS HR loops of the brainstorm search hypothesize refine loop is basically what you would then do is you would take each of these queries put it into a Google search or a duck duck go search or even a Wikipedia search and then with the with the like the the main query in mind, you take notes and so large language models have a really great ability to, you search a document, you take notes, and then you generate a hypothesis. And so I've got some of the other script out here. Let's see, where did it go? There it is. So I've started working on a script, and you can look at some of what I've got out here. But it'll basically say, generate a hypothesis. I've started working on a script and you can look at some of what I've got out here. But it'll basically say, generate a hypothesis. And you give it a question, some sources of information, and you ask it to generate a hypothesis. And then you update that hypothesis over time. So that's the refine. And so this brainstorm, search, hypothesize, refine loop, this is how humans answer questions. Now what you might add to this loop is another like, you know, perform a test or an experiment or perform a calculation. And this is where you're getting into more like formalized cognitive architectures. Ideally, you create something that can construct these loops automatically, fully, in real time. But in the immediate future, like doing a brainstorm search hypothesis refine loop, this is going to be good enough for information literacy. And this is honestly one of the reasons why I also canceled my Perplexity subscription is because all it does is it does a really like what I would call a naive search. If you use Perplexity, a naive search is it doesn't really think about the question that you're asking. It doesn't generate those good questions like what I just showed you. It just basically translates it to a really, really dumb Google query and doesn't use its intrinsic information literacy and information foraging knowledge to ask actually good questions. So I'm like, okay, well, I can ask better search questions because I've got better Google foo so whatever I'm not going to use it. Now but then what what perplexity does that it does well and let me just show you okay so what I mean when I say that like perplexity doesn't have good Google foo and let me ask the same question. Whoops. What is the exact coastline of Britain? And so it'll actually show you like the like so all it literally all it did is say exact coastline of Britain. That is not good information foraging. Like it's just it took a really complex question and spat out a like really basic like okay I could have done that and then all it does is aggregate this this information and so because it's not because it's not engaging in counterfactuals because it's not engaging in good information literacy if it's search query rip like pulls up like bad information it will just tell you patently false information that it reports from the internet and so like for instance one thing that I did before, here let me start a new thread let's see tell me about AI destroying jobs so if you follow my channel you know that I will occasionally document like AI destroying jobs and so here it generated a couple of things you know a couple of search queries you know but it's like well am i my reduce the can also create new employment opportunities so it didn't actually generate any counterfactual searches it just like it did a very naive search and just kind of gives you that information sight unseen without actually looking at the validity of the sources or even having asked good questions. So anyways, there's all kinds of ways to improve this if you understand the general principles and concepts of language models such as information literacy, information foraging, and latent space activation. So I hope you got a lot out of this video. Thanks for watching. Cheers.", "chunks": [{"timestamp": [0.0, 3.02], "text": " What does take a deep breath,"}, {"timestamp": [3.02, 5.24], "text": " let's think through this step by step,"}, {"timestamp": [5.24, 6.98], "text": " tree of thought, chain of thought,"}, {"timestamp": [6.98, 9.78], "text": " what do all of these have in common?"}, {"timestamp": [9.78, 12.34], "text": " So one thing that I've noticed is that"}, {"timestamp": [12.34, 14.14], "text": " out there in the scientific literature"}, {"timestamp": [14.14, 16.28], "text": " and the prompt engineering space,"}, {"timestamp": [16.28, 17.9], "text": " there are all kinds of techniques"}, {"timestamp": [17.9, 21.62], "text": " that have been elucidated by papers such as this one,"}, {"timestamp": [21.62, 24.9], "text": " large language models are zero shot reasoners,"}, {"timestamp": [24.9, 27.24], "text": " where it's like, let's think through this step by step."}, {"timestamp": [27.24, 29.36], "text": " Great, that was a popular one."}, {"timestamp": [29.36, 30.96], "text": " I don't think it was worthy of a paper,"}, {"timestamp": [30.96, 33.1], "text": " but that's my personal opinion."}, {"timestamp": [33.1, 35.08], "text": " The most recent one is take a step back,"}, {"timestamp": [35.08, 37.12], "text": " which basically just says, let's take a step back"}, {"timestamp": [37.12, 40.28], "text": " and think about what information or techniques we need"}, {"timestamp": [40.28, 42.76], "text": " in order to achieve this."}, {"timestamp": [42.76, 45.6], "text": " Another one is telling the model to take a deep breath."}, {"timestamp": [45.6, 48.8], "text": " It elicits very different behaviors from the model,"}, {"timestamp": [48.8, 50.7], "text": " which I think is just fundamentally a flaw"}, {"timestamp": [50.7, 52.8], "text": " with the way that it's trained,"}, {"timestamp": [52.8, 55.3], "text": " rather than something about the model itself."}, {"timestamp": [55.3, 57.8], "text": " Wherein basically, if you train the model"}, {"timestamp": [57.8, 61.5], "text": " to just barf out an answer without thinking through it,"}, {"timestamp": [61.5, 62.7], "text": " that's the result you're gonna get."}, {"timestamp": [62.7, 66.52], "text": " So this could be fixed with training schemas."}, {"timestamp": [66.52, 68.64], "text": " Tree of thoughts, which is where you basically use"}, {"timestamp": [68.64, 71.04], "text": " iteration and brainstorming to think through"}, {"timestamp": [71.04, 72.76], "text": " various possibilities."}, {"timestamp": [72.76, 76.28], "text": " And then finally, or I guess similarly,"}, {"timestamp": [76.28, 78.54], "text": " guided tree of thought as an answer."}, {"timestamp": [78.54, 82.6], "text": " So, what is the underpinning thing about all of this?"}, {"timestamp": [82.6, 87.16], "text": " What is it that all these papers and techniques are missing"}, {"timestamp": [87.16, 88.92], "text": " that is not yet generalized?"}, {"timestamp": [88.92, 91.76], "text": " So the reason that I haven't made a video about this"}, {"timestamp": [91.76, 94.08], "text": " is because to me, having worked with models"}, {"timestamp": [94.08, 98.12], "text": " since GPT-2 and GPT-3, it seems rather obvious,"}, {"timestamp": [98.12, 103.12], "text": " but so I'm adding a little bit to the conversation,"}, {"timestamp": [103.16, 107.26], "text": " and what's missing is the idea of latent space activation."}, {"timestamp": [108.26, 110.82], "text": " So here's what you need to understand"}, {"timestamp": [110.82, 114.02], "text": " about the way that intelligence actually works."}, {"timestamp": [114.02, 117.42], "text": " If you think about the way that human brains work,"}, {"timestamp": [118.7, 120.26], "text": " you have an intuition, right?"}, {"timestamp": [120.26, 123.94], "text": " If you have a question or a problem,"}, {"timestamp": [123.94, 125.32], "text": " you have many brain structures"}, {"timestamp": [125.32, 128.62], "text": " that will just give you an instantaneous possible answer."}, {"timestamp": [128.62, 130.2], "text": " This is called intuition."}, {"timestamp": [130.2, 131.72], "text": " Intuition is knowing something"}, {"timestamp": [131.72, 133.32], "text": " without knowing how you know it."}, {"timestamp": [134.34, 136.8], "text": " One single inference from an LLM"}, {"timestamp": [136.8, 139.08], "text": " is equal to human intuition."}, {"timestamp": [139.08, 140.28], "text": " It hasn't thought about it."}, {"timestamp": [140.28, 142.72], "text": " It just says, this is my gut instinct."}, {"timestamp": [142.72, 144.76], "text": " This is my knee-jerk reaction."}, {"timestamp": [144.76, 149.56], "text": " However, humans can also think through things in order to get the right answer"}, {"timestamp": [149.56, 155.7], "text": " Like let me consider what I actually know. Let me think about the the proper techniques to go through this"}, {"timestamp": [155.92, 160.8], "text": " So there's a really famous book that is really popular in many intellectual circles and wherever"}, {"timestamp": [161.12, 171.2], "text": " Called thinking fast and slow by Daniel Kahneman where he talks about system one thinking which is that instant knee-jerk intuitive thinking which is what a single"}, {"timestamp": [171.2, 176.8], "text": " inference of a large language model is the equivalent of. And then there's system two"}, {"timestamp": [176.8, 180.96], "text": " thinking which is slow thinking it's very deliberative where you jot down your thoughts"}, {"timestamp": [180.96, 187.98], "text": " and you take notes and you kind of recruit all the stuff that you have and you're very systematic about how you approach things."}, {"timestamp": [187.98, 190.76], "text": " This is what all these other prompt strategies do."}, {"timestamp": [190.76, 193.16], "text": " So anytime you have multiple steps,"}, {"timestamp": [193.16, 194.96], "text": " whether it's using lang chain,"}, {"timestamp": [194.96, 197.68], "text": " tree of thought, chain of thought, chain of reasoning,"}, {"timestamp": [197.68, 199.12], "text": " let's think through this step by step"}, {"timestamp": [199.12, 200.28], "text": " where basically you're saying,"}, {"timestamp": [200.28, 203.64], "text": " okay, let's use the model to prompt itself"}, {"timestamp": [203.64, 208.64], "text": " in order to get better stuff into whatever's going on here."}, {"timestamp": [208.66, 212.16], "text": " So, great, how do you use all this?"}, {"timestamp": [212.16, 213.92], "text": " This is a question that I've been seeing lately"}, {"timestamp": [213.92, 215.2], "text": " because I made a recent video"}, {"timestamp": [215.2, 217.62], "text": " about sparse priming representations,"}, {"timestamp": [217.62, 219.32], "text": " and I didn't realize that a lot of people"}, {"timestamp": [219.32, 220.92], "text": " wouldn't understand how to use this"}, {"timestamp": [220.92, 224.32], "text": " because, again, maybe it's just because I've been in it"}, {"timestamp": [224.32, 225.12], "text": " so long and I forgot to"}, {"timestamp": [225.12, 232.32], "text": " explain it. Anyways, basically what you do is you use the same techniques that your brain uses that"}, {"timestamp": [232.32, 238.4], "text": " you use consciously or unconsciously to prompt the model and what you're doing is is what I call"}, {"timestamp": [238.4, 245.24], "text": " latent space activation. And so what you need to understand about latent space activation is that these models"}, {"timestamp": [245.24, 247.96], "text": " are trained on infinitely more knowledge"}, {"timestamp": [247.96, 249.92], "text": " than you will ever personally possess,"}, {"timestamp": [249.92, 252.98], "text": " but it's not gonna be activated at all times."}, {"timestamp": [252.98, 254.44], "text": " Likewise, you might have heard like,"}, {"timestamp": [254.44, 256.84], "text": " oh, humans only ever use 10% of our brains,"}, {"timestamp": [256.84, 258.28], "text": " which is more or less true."}, {"timestamp": [258.28, 260.54], "text": " At any given moment, most of your brain"}, {"timestamp": [260.54, 262.22], "text": " is not actively participating"}, {"timestamp": [262.22, 265.08], "text": " because if your brain goes to 100% activation,"}, {"timestamp": [265.08, 266.96], "text": " you basically go into a coma and you die"}, {"timestamp": [266.96, 268.74], "text": " because it overloads itself."}, {"timestamp": [268.74, 272.16], "text": " Likewise, large language models have a tremendous amount"}, {"timestamp": [272.16, 274.88], "text": " of latent space that is just not really used."}, {"timestamp": [274.88, 277.6], "text": " This is embedded knowledge, embedded capabilities,"}, {"timestamp": [277.6, 280.28], "text": " and it's really only gonna do one thing at a time."}, {"timestamp": [280.28, 283.6], "text": " And this is another place where large language models"}, {"timestamp": [283.6, 285.6], "text": " are similar to human brains"}, {"timestamp": [285.6, 287.92], "text": " in that we have a conscious spotlight."}, {"timestamp": [287.92, 290.52], "text": " So in psychology, in neuroscience,"}, {"timestamp": [290.52, 292.4], "text": " there's what's called the spotlight of consciousness,"}, {"timestamp": [292.4, 295.8], "text": " which is that your brain has all kinds of stuff,"}, {"timestamp": [295.8, 299.94], "text": " information that it is filtering out in real time."}, {"timestamp": [299.94, 301.84], "text": " There's all kinds of thoughts and memories."}, {"timestamp": [301.84, 305.52], "text": " Your brain is actively ignoring most of the information that it"}, {"timestamp": [305.52, 311.2], "text": " has access to at any given moment. This is basically a biological attention mechanism."}, {"timestamp": [311.2, 316.88], "text": " So in the same respect, you need to use that spotlight of consciousness in large language"}, {"timestamp": [316.88, 326.16], "text": " models to sequentially scan over and figure out what it needs to know in order to bring the correct things into its quote-unquote consciousness"}, {"timestamp": [326.16, 329.6], "text": " or into the context window and activation."}, {"timestamp": [329.6, 332.8], "text": " And so I was like, let me just do a quick demonstration."}, {"timestamp": [332.8, 336.92], "text": " I started working on a second demonstration, but I'm just going to show you this first"}, {"timestamp": [336.92, 337.92], "text": " one."}, {"timestamp": [337.92, 348.3], "text": " So in this example, rather than having, you know, like of a Fixed set of prompts what you can do is you just think through like, okay"}, {"timestamp": [348.3, 353.7], "text": " How would I answer a general-purpose question who was Emperor during the absolute apogee of Roman power?"}, {"timestamp": [354.36, 360.0], "text": " So the first thing that you do is when you ask yourself this question you think hmm. Well, what do I know about Rome?"}, {"timestamp": [360.64, 364.98], "text": " What what do I know that is relevant to this question? The next thing you do is say well"}, {"timestamp": [364.98, 366.1], "text": " How do I know that is relevant to this question? The next thing you do is say, well, how do I define the answer?"}, {"timestamp": [366.1, 369.3], "text": " What criteria am I looking for in order to judge this on?"}, {"timestamp": [369.3, 371.82], "text": " And then finally, you say, okay,"}, {"timestamp": [371.82, 374.56], "text": " based on activating everything that I have in my brain,"}, {"timestamp": [374.56, 376.62], "text": " what is the answer that I'm gonna settle on?"}, {"timestamp": [376.62, 379.58], "text": " So let me just show you the script that I wrote"}, {"timestamp": [379.58, 381.26], "text": " and how this works."}, {"timestamp": [381.26, 385.0], "text": " So the script is here, Technique 01 Dialog."}, {"timestamp": [385.0, 390.0], "text": " And so basically all you do is I have it, here let me zoom in a little bit,"}, {"timestamp": [390.0, 394.0], "text": " I have it ask the user, what is your main query or question?"}, {"timestamp": [394.0, 400.0], "text": " And then I have some general purpose placeholder questions that kind of abstract this process."}, {"timestamp": [400.0, 405.44], "text": " And so the question is, the first question that it asks itself is, what information do I already know about this topic?"}, {"timestamp": [405.44, 409.32], "text": " What information do I need to recall into my working memory to best answer this?"}, {"timestamp": [409.32, 412.88], "text": " What techniques or methods do I know that I can, and this is the second one,"}, {"timestamp": [412.88, 415.84], "text": " what techniques or methods do I know that can answer this question or"}, {"timestamp": [415.84, 416.88], "text": " solve this problem?"}, {"timestamp": [416.88, 418.88], "text": " How can I integrate what I already know and"}, {"timestamp": [418.88, 421.72], "text": " recall more valuable facts, approaches, and techniques?"}, {"timestamp": [421.72, 428.76], "text": " I probably should use the word methods here because the well I guess I use methods. And finally with all this in mind how will I"}, {"timestamp": [428.76, 432.0], "text": " discuss the question or I will now discuss the question or problem and"}, {"timestamp": [432.0, 437.2], "text": " render my final answer. And so this is a very very very simple chain of thought"}, {"timestamp": [437.2, 443.82], "text": " set of reasoning but the purpose of this is that it understands how to approach"}, {"timestamp": [443.82, 446.88], "text": " this and if you look here, it actually"}, {"timestamp": [446.88, 449.28], "text": " accumulates it all in a conversation."}, {"timestamp": [449.28, 450.88], "text": " So one thing that I've noticed and that"}, {"timestamp": [450.88, 452.34], "text": " plenty of other people have noticed,"}, {"timestamp": [452.34, 457.6], "text": " is that the more valid or salient information you have in"}, {"timestamp": [457.6, 460.96], "text": " the context window, the more latent space it activates."}, {"timestamp": [460.96, 464.92], "text": " So in this case, what I do is I have a system message,"}, {"timestamp": [464.92, 465.54], "text": " which let me show"}, {"timestamp": [465.54, 470.98], "text": " you that real quick. So the system message that runs this basically tells it what it"}, {"timestamp": [470.98, 477.82], "text": " is. So you are, here let me go here, you're an internal dialogue iterator for an LLM,"}, {"timestamp": [477.82, 481.58], "text": " large language model neural network. LLMs possess latent space or embedded knowledge"}, {"timestamp": [481.58, 485.18], "text": " and capabilities. You'll be given a main query as well as a sequence of questions."}, {"timestamp": [485.18, 487.4], "text": " Your role is to answer the queries as a way of"}, {"timestamp": [487.4, 490.16], "text": " activating the latent space inside your own neural network."}, {"timestamp": [490.16, 493.22], "text": " This is not unlike how a human may talk through"}, {"timestamp": [493.22, 495.0], "text": " a problem or question in order"}, {"timestamp": [495.0, 497.16], "text": " to recruit the appropriate memories and techniques."}, {"timestamp": [497.16, 500.2], "text": " The ultimate goal is to answer the main query listed below."}, {"timestamp": [500.2, 502.5], "text": " Then I actually had to fix this on"}, {"timestamp": [502.5, 504.0], "text": " my local code because it was hard-coded,"}, {"timestamp": [504.0, 506.2], "text": " and so I'll update this."}, {"timestamp": [506.2, 507.7], "text": " Anyways, interaction schema."}, {"timestamp": [507.7, 509.6], "text": " The user will play the role of interrogator."}, {"timestamp": [509.6, 514.1], "text": " Your answers will be thorough and comprehensive in order to get the best possible latent space activation."}, {"timestamp": [514.1, 519.2], "text": " Anything potentially salient is valid to bring up as it will expand your internal representation or embedding,"}, {"timestamp": [519.2, 522.5], "text": " thus recruiting more relevant information as the conversation advances."}, {"timestamp": [522.5, 523.5], "text": " Okay, cool."}, {"timestamp": [523.5, 526.76], "text": " So, let me show you what this actually looks like."}, {"timestamp": [526.76, 529.28], "text": " Okay, so here we are, here's the repo."}, {"timestamp": [529.28, 531.36], "text": " So, let me just show you real quick."}, {"timestamp": [531.36, 533.88], "text": " All right, so Python technique dialog py01,"}, {"timestamp": [533.88, 538.58], "text": " and like I said, I fixed the system message locally."}, {"timestamp": [538.58, 542.72], "text": " So, Python technique 01,"}, {"timestamp": [542.72, 544.92], "text": " that's gonna ask me a main query question."}, {"timestamp": [544.92, 550.28], "text": " So, I'll show you the one that I that I did originally so that you can see kind of the process"}, {"timestamp": [551.8, 556.04], "text": " Who was Emperor actually no, let's change it up to be a little bit more specific"}, {"timestamp": [557.92, 560.16], "text": " Who were some of the senators"}, {"timestamp": [561.56, 563.12], "text": " Who were?"}, {"timestamp": [563.12, 565.0], "text": " important during Rome at its peak power."}, {"timestamp": [568.68, 571.2], "text": " Okay, so this is a question that is not as simple"}, {"timestamp": [571.2, 572.7], "text": " as like who is the emperor,"}, {"timestamp": [572.7, 574.88], "text": " because it probably just knows that,"}, {"timestamp": [574.88, 577.2], "text": " but this is something that's gonna require"}, {"timestamp": [577.2, 579.6], "text": " to think through what it has."}, {"timestamp": [579.6, 581.34], "text": " Okay, so what information do I already know"}, {"timestamp": [581.34, 582.48], "text": " about this topic?"}, {"timestamp": [582.48, 585.64], "text": " So what I did was instead of writing a prompt"}, {"timestamp": [585.64, 589.3], "text": " that is very specific to any given query,"}, {"timestamp": [589.3, 590.68], "text": " okay, so let's see what it says."}, {"timestamp": [590.68, 592.6], "text": " But basically what I said is like,"}, {"timestamp": [592.6, 594.08], "text": " I kind of framed it generally like,"}, {"timestamp": [594.08, 595.68], "text": " let's think through this step by step."}, {"timestamp": [595.68, 597.76], "text": " To answer this question, I need to recall information"}, {"timestamp": [597.76, 600.46], "text": " about the Roman Empire, specifically during its peak."}, {"timestamp": [601.34, 603.56], "text": " Let's see, the structure of the Roman government,"}, {"timestamp": [603.56, 604.76], "text": " and Senate, blah, blah, blah."}, {"timestamp": [604.76, 605.0], "text": " Additionally, I need to remember, so it didn't give anything specific. Let's see, the structure of the Roman government, the Senate, blah blah blah."}, {"timestamp": [605.0, 610.0], "text": " Additionally, I need to remember, so it didn't give anything specific."}, {"timestamp": [610.0, 617.0], "text": " So in this case it's a little bit disappointing because what I would have hoped is that it would have listed out specific information."}, {"timestamp": [617.0, 628.84], "text": " And finally, with all this in mind, I will now discuss the problem and render my final answer. But, interestingly enough, it actually provided it. So it knew Marcus Tullius"}, {"timestamp": [628.84, 641.68], "text": " Cicero. It knew... So Cicero, he was a big speaker. Cato, Polio, Gaius Vipsennius, Agrippa,"}, {"timestamp": [641.68, 648.64], "text": " so the Agrippa family, that was big. Marcus Aurelius so yeah so in this case it was able to"}, {"timestamp": [648.64, 652.08], "text": " to to basically dial into the answer and give me"}, {"timestamp": [652.08, 656.72], "text": " a valid answer now I would have hoped that it would have listed out some of"}, {"timestamp": [656.72, 660.72], "text": " the some of the people already I guess it did here's Marcus Aurelius and"}, {"timestamp": [660.72, 664.64], "text": " and a few others but because it activated all of this"}, {"timestamp": [664.64, 666.0], "text": " and of course like you can do a test and ask"}, {"timestamp": [666.0, 668.0], "text": " this question just"}, {"timestamp": [668.0, 670.0], "text": " sight unseen, but the point is"}, {"timestamp": [670.0, 672.0], "text": " that this is a very similar prompting"}, {"timestamp": [672.0, 674.0], "text": " strategy and it ultimately gave me a pretty"}, {"timestamp": [674.0, 676.0], "text": " good answer. Here's another"}, {"timestamp": [676.0, 678.0], "text": " one that I tried. So let's"}, {"timestamp": [678.0, 680.0], "text": " do a clear screen."}, {"timestamp": [680.0, 682.0], "text": " And we'll do this again."}, {"timestamp": [684.0, 687.56], "text": " Calculate the exact coastline of Britain."}, {"timestamp": [690.2, 692.2], "text": " Okay, so for some background,"}, {"timestamp": [692.2, 693.94], "text": " the reason why this is a challenging thing"}, {"timestamp": [693.94, 695.48], "text": " is because the coastline of Britain"}, {"timestamp": [695.48, 697.36], "text": " is very jagged and angular,"}, {"timestamp": [697.36, 699.76], "text": " and it also changes with tides and time."}, {"timestamp": [699.76, 703.32], "text": " And it's also, it depends on how you define a coastline."}, {"timestamp": [703.32, 704.16], "text": " So here we go."}, {"timestamp": [704.16, 705.7], "text": " So it says like, I don't have working memory"}, {"timestamp": [705.7, 706.9], "text": " in the same way humans do."}, {"timestamp": [706.9, 709.94], "text": " So I really wish that open AI would stop"}, {"timestamp": [709.94, 712.74], "text": " like stuffing this kind of like arbitrary,"}, {"timestamp": [712.74, 714.94], "text": " like asinine logic."}, {"timestamp": [714.94, 717.26], "text": " I don't have working memory in the same way humans do."}, {"timestamp": [717.26, 719.1], "text": " That's not necessarily true,"}, {"timestamp": [719.1, 720.98], "text": " but like they're teaching their models"}, {"timestamp": [720.98, 724.22], "text": " to axiomatically believe this."}, {"timestamp": [724.22, 726.92], "text": " Anyways, so what's going on here?"}, {"timestamp": [729.12, 730.02], "text": " Okay, cool."}, {"timestamp": [731.16, 732.6], "text": " So coastline paradox,"}, {"timestamp": [732.6, 734.92], "text": " so it understands the coastline paradox,"}, {"timestamp": [734.92, 736.66], "text": " Britain's coastline measurement techniques,"}, {"timestamp": [736.66, 738.08], "text": " there are a number of units of measurement"}, {"timestamp": [738.08, 739.84], "text": " to calculate the exact, we would need a detailed"}, {"timestamp": [739.84, 741.48], "text": " and up-to-date map or satellite."}, {"timestamp": [741.48, 744.28], "text": " So it's basically figuring out what information it needs"}, {"timestamp": [744.28, 748.4], "text": " and it says like, I don't have access to this information. So in a cognitive"}, {"timestamp": [748.4, 752.0], "text": " architecture what you would do is you'd actually use retrieval augmented"}, {"timestamp": [752.0, 756.42], "text": " generation where what you then do is you use you recognize that you need to"}, {"timestamp": [756.42, 761.2], "text": " search for information. So let's see then what techniques or methods do I know of"}, {"timestamp": [761.2, 764.48], "text": " that can do this cartographic measurement satellite imagery GIS"}, {"timestamp": [764.48, 769.44], "text": " software fractal analysis. I don't have the capability to perform these measurements directly,"}, {"timestamp": [769.44, 774.64], "text": " so it's aware of its own limitations. So that's a good agent model. That I do approve of, but"}, {"timestamp": [774.64, 780.24], "text": " complaining about working memory is a waste of time and energy. And finally, with all this in"}, {"timestamp": [780.24, 784.16], "text": " mind, I will now discuss and render my final answer. The exact coastline of Britain is"}, {"timestamp": [784.16, 787.16], "text": " challenging to determine because of the paradox, so on final answer. The exact coastline of Britain is challenging to determine because of the paradox so on and so forth."}, {"timestamp": [787.16, 791.92], "text": " The coastline of Britain is often reported to be about 12,000 kilometers"}, {"timestamp": [791.92, 797.32], "text": " however this is an estimate. So in this case it pretty much failed but that's"}, {"timestamp": [797.32, 802.16], "text": " kind of this was this was a gotcha and so what I started working on was right"}, {"timestamp": [802.16, 807.64], "text": " here. Let me show you this other technique that I started working on."}, {"timestamp": [807.64, 815.8], "text": " And so this is something that I have done and I have consulted for people doing something"}, {"timestamp": [815.8, 816.8], "text": " similar."}, {"timestamp": [816.8, 820.52], "text": " But it's a brainstorm search hypothesize refine loop."}, {"timestamp": [820.52, 824.2], "text": " So the BSHR loop is basically just what humans do."}, {"timestamp": [824.2, 828.0], "text": " And this is another reason why I haven't commented on this, and I apologize because"}, {"timestamp": [828.0, 832.0], "text": " I recognize that not everyone is familiar with"}, {"timestamp": [832.0, 836.0], "text": " information foraging techniques that humans use. But basically"}, {"timestamp": [836.0, 840.0], "text": " the BSHR loop is you brainstorm a list of search queries, which"}, {"timestamp": [840.0, 844.0], "text": " if you've used tools like Perplexity, you see that"}, {"timestamp": [844.0, 845.64], "text": " this is what it does. And honestly, Perplexity, you see that this is what it does where,"}, {"timestamp": [845.64, 848.96], "text": " and honestly, Perplexity could be infinitely better"}, {"timestamp": [848.96, 850.68], "text": " because all it does is it generates"}, {"timestamp": [850.68, 855.2], "text": " really basic Google queries."}, {"timestamp": [855.2, 856.84], "text": " So let me show you what I mean by,"}, {"timestamp": [858.4, 861.28], "text": " let's take this question,"}, {"timestamp": [861.28, 863.88], "text": " and I'll show you what I mean by generating"}, {"timestamp": [867.44, 869.52], "text": " valid search queries."}, {"timestamp": [873.24, 876.2], "text": " So if we go to the playground, I can show you what I mean. I haven't finished this because like the coding"}, {"timestamp": [876.2, 878.26], "text": " is a little bit tedious, but anyways."}, {"timestamp": [878.26, 883.26], "text": " So mission, you are a search query generator."}, {"timestamp": [883.92, 894.0], "text": " You will be given a specific query or problem by the user,"}, {"timestamp": [894.0, 916.28], "text": " and you are to generate a JSON list of questions that will be used to search the internet. Make sure you generate comprehensive and counterfactual search queries."}, {"timestamp": [916.28, 927.92], "text": " Employ everything you know about information foraging and information literacy to generate the best possible"}, {"timestamp": [927.92, 933.88], "text": " questions. Okay, so as I've mentioned in other videos, this is what's"}, {"timestamp": [933.88, 938.68], "text": " called priming. And so with priming, large language models will recognize"}, {"timestamp": [938.68, 942.44], "text": " certain concepts or terms and it will activate the network in a different way."}, {"timestamp": [942.44, 948.76], "text": " And the activation, quote-ununquote is basically the internal representation, the embedding is going to"}, {"timestamp": [948.76, 952.88], "text": " look different and so I said comprehensive and counterfactual. I said"}, {"timestamp": [952.88, 955.56], "text": " employ everything you know about information foraging which is a very"}, {"timestamp": [955.56, 959.64], "text": " specific term and information literacy to generate the best possible questions."}, {"timestamp": [959.64, 965.12], "text": " So I'm going to say calculate the exact coastline of Britain. And in this case, what it's going to do,"}, {"timestamp": [965.12, 968.64], "text": " it's going to ask a bunch of questions"}, {"timestamp": [968.64, 971.04], "text": " that are relevant."}, {"timestamp": [971.04, 973.92], "text": " How often is the coastline measured?"}, {"timestamp": [973.92, 975.92], "text": " What impact does erosion have?"}, {"timestamp": [975.92, 977.92], "text": " What are the longest and shortest estimates?"}, {"timestamp": [977.92, 980.56], "text": " So you can see that it's basically"}, {"timestamp": [980.56, 983.12], "text": " generating a whole bunch of questions"}, {"timestamp": [983.12, 988.0], "text": " so that once it searches, imagine you run all of these Google"}, {"timestamp": [988.0, 992.0], "text": " searches and then you take notes. So this is the brainstorm phase"}, {"timestamp": [992.0, 996.0], "text": " and I hear this is actually super valuable. So I'm going to record this"}, {"timestamp": [996.0, 1000.0], "text": " as a as a hypothesis generator. Okay. Sorry about that. I was just"}, {"timestamp": [1000.0, 1004.0], "text": " realized like hey, I just did a good thing. Okay. So then the BS HR"}, {"timestamp": [1004.0, 1008.0], "text": " loops of the brainstorm search hypothesize"}, {"timestamp": [1008.0, 1011.84], "text": " refine loop is basically what you would then do is you would"}, {"timestamp": [1011.84, 1015.52], "text": " take each of these queries put it into a Google search or a"}, {"timestamp": [1015.52, 1019.84], "text": " duck duck go search or even a Wikipedia search and then with"}, {"timestamp": [1019.84, 1024.56], "text": " the with the like the the main query in mind, you take notes"}, {"timestamp": [1024.56, 1027.16], "text": " and so large language models have a really great ability"}, {"timestamp": [1027.16, 1029.88], "text": " to, you search a document, you take notes,"}, {"timestamp": [1029.88, 1032.48], "text": " and then you generate a hypothesis."}, {"timestamp": [1032.48, 1035.8], "text": " And so I've got some of the other script out here."}, {"timestamp": [1035.8, 1037.16], "text": " Let's see, where did it go?"}, {"timestamp": [1038.36, 1039.28], "text": " There it is."}, {"timestamp": [1039.28, 1041.08], "text": " So I've started working on a script,"}, {"timestamp": [1041.08, 1043.72], "text": " and you can look at some of what I've got out here."}, {"timestamp": [1044.64, 1045.12], "text": " But it'll basically say, generate a hypothesis. I've started working on a script and you can look at some of what I've got out here."}, {"timestamp": [1045.12, 1049.3], "text": " But it'll basically say, generate a hypothesis."}, {"timestamp": [1049.3, 1053.6], "text": " And you give it a question, some sources of information, and you ask it to generate a"}, {"timestamp": [1053.6, 1054.82], "text": " hypothesis."}, {"timestamp": [1054.82, 1056.74], "text": " And then you update that hypothesis over time."}, {"timestamp": [1056.74, 1058.02], "text": " So that's the refine."}, {"timestamp": [1058.02, 1063.36], "text": " And so this brainstorm, search, hypothesize, refine loop, this is how humans answer questions."}, {"timestamp": [1063.36, 1068.8], "text": " Now what you might add to this loop is another like, you know, perform a test or an experiment or"}, {"timestamp": [1068.8, 1072.48], "text": " perform a calculation. And this is where you're getting into more"}, {"timestamp": [1072.48, 1076.64], "text": " like formalized cognitive architectures. Ideally, you create something that can"}, {"timestamp": [1076.64, 1081.84], "text": " construct these loops automatically, fully, in real time. But in"}, {"timestamp": [1081.84, 1086.4], "text": " the immediate future, like doing a brainstorm search hypothesis refine loop,"}, {"timestamp": [1086.4, 1090.8], "text": " this is going to be good enough for information literacy. And this is honestly one of the reasons"}, {"timestamp": [1090.8, 1097.6], "text": " why I also canceled my Perplexity subscription is because all it does is it does a really like"}, {"timestamp": [1098.64, 1104.64], "text": " what I would call a naive search. If you use Perplexity, a naive search is it doesn't really"}, {"timestamp": [1104.64, 1105.72], "text": " think about the question"}, {"timestamp": [1105.72, 1111.72], "text": " that you're asking. It doesn't generate those good questions like what I just showed you."}, {"timestamp": [1111.72, 1118.38], "text": " It just basically translates it to a really, really dumb Google query and doesn't use its"}, {"timestamp": [1118.38, 1124.36], "text": " intrinsic information literacy and information foraging knowledge to ask actually good questions."}, {"timestamp": [1124.36, 1127.56], "text": " So I'm like, okay, well, I can ask better search questions because I've got better"}, {"timestamp": [1127.56, 1134.44], "text": " Google foo so whatever I'm not going to use it. Now but then what what perplexity"}, {"timestamp": [1134.44, 1138.0], "text": " does that it does well and let me just show you okay so what I mean when I say"}, {"timestamp": [1138.0, 1148.0], "text": " that like perplexity doesn't have good Google foo and let me ask the same question. Whoops. What is the exact coastline of Britain?"}, {"timestamp": [1148.0, 1155.0], "text": " And so it'll actually show you like the like so all it literally all it did is say exact coastline of Britain."}, {"timestamp": [1155.0, 1167.32], "text": " That is not good information foraging. Like it's just it took a really complex question and spat out a like really basic like okay I could have"}, {"timestamp": [1167.32, 1171.64], "text": " done that and then all it does is aggregate this this information and so"}, {"timestamp": [1171.64, 1175.8], "text": " because it's not because it's not engaging in counterfactuals because it's"}, {"timestamp": [1175.8, 1180.4], "text": " not engaging in good information literacy if it's search query rip like"}, {"timestamp": [1180.4, 1188.5], "text": " pulls up like bad information it will just tell you patently false information that it reports from the internet and so like for instance"}, {"timestamp": [1188.5, 1192.0], "text": " one thing that I did before, here let me start a new thread"}, {"timestamp": [1192.0, 1195.3], "text": " let's see"}, {"timestamp": [1195.3, 1198.4], "text": " tell me about"}, {"timestamp": [1198.4, 1203.3], "text": " AI destroying jobs so if you follow my channel you know that I will"}, {"timestamp": [1203.3, 1208.72], "text": " occasionally document like AI destroying jobs and so here it generated a couple of things"}, {"timestamp": [1208.72, 1212.32], "text": " you know a couple of"}, {"timestamp": [1212.32, 1216.48], "text": " search queries you know but it's like well am i"}, {"timestamp": [1216.48, 1219.76], "text": " my reduce the can also create new employment opportunities"}, {"timestamp": [1219.76, 1223.52], "text": " so it didn't actually generate any counterfactual searches"}, {"timestamp": [1223.52, 1229.92], "text": " it just like it did a very naive search and just kind of gives you that information"}, {"timestamp": [1229.92, 1235.2], "text": " sight unseen without actually looking at the validity of the sources or even having asked good questions."}, {"timestamp": [1235.2, 1240.56], "text": " So anyways, there's all kinds of ways to improve this if you understand"}, {"timestamp": [1240.56, 1250.06], "text": " the general principles and concepts of language models such as information literacy, information foraging, and latent space activation. So I hope you got a"}, {"timestamp": [1250.06, 1254.3], "text": " lot out of this video. Thanks for watching. Cheers."}]}