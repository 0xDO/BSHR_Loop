{"text": " Hey, Chatters. I just wanted to do a quick video about something really cool that just came out that I'm excited about. For those of you who don't know, my shtick, my thing, is really AI alignment, safety, ethics, and really solving what I believe is the last problem we have to solve as humanity, which is how do we get AI to do the things that creates a future we all benefit from and reduces suffering for all living things in the world. This is an incredibly difficult problem to solve. I'm not going to get into the details of it because I want this video to be kind of quick. If you don't believe that's a problem, whatever, that's fine. This is not the video for you. If it is, then I want to introduce you to one of my favorite YouTube folks David Shapiro. David Shapiro is... I'm gonna call him a scientist, but he hasn't taken the typical path, you know. He doesn't have a PhD in all these areas and he hasn't done, you know, official academic research like you would expect, which hasn't... it doesn't mean he hasn't done any, it's just he's kind of doing his own thing, following his own interests, and he is really focused on AI's impact on society and how do we actually think about mitigating the harms. So he has a lot of amazing stuff on his YouTube. I highly check it out. I'll put his his links and his channel in the description. But for now, just know that he's really into he's an alignment nerd like myself and he to me has come up with the first framework for thinking about alignment in a technical way that we can actually act upon and test. So his idea that he's been working on for God knows how long is something called the autonomous cognitive entity. Here's a much longer video going through this framework but I want to try to make it as accessible and straightforward as possible. So the idea here in this nice little graphic he's put together is you have a stacked hierarchy similar to like Maslow's hierarchy of needs or Bloom's taxonomy or all these other kinds of hierarchies that is sort of running the show within an AI. And when we say AI in this case we're really thinking about autonomous agents not necessarily AGI or artificial superintelligence. This can be a domestic robot or a robot vacuum for all I care. It doesn't really matter. All it means is that this thing is taking autonomous actions. You're not making all the choices for it. And so we need something to make sure it's not doing things it's not supposed to do, not harming people, even if unintentionally. So this is how it works. We're going to go from top to bottom, then side to side. We start at the very top at this aspirational layer. I love this positive framing. The word aspiration holds with it a lot of positive connotations and it's really thinking about what do you want to achieve like what's your dream and in here we have morality ethics and mission and so we're first grounding everything in a set of values that we're trying to achieve these things are abstract they're not concrete if anyone's ever had an ethical discussion of any kind, you know that things get very complicated very easily once you start throwing out different scenarios. But the idea is you have some guiding forces. This is probably the way he talks about similar to what Anthropx Claude has done. They have something called constitutional AI. They have trained it essentially on a constitution. Things like, you know, human rights. All these things that hopefully we all mostly agree on. We're not gonna get into the discussion about whose values. The point is you can set the values here. Then we go down and we start getting more and more concrete and getting out of the abstract and into, you know, actually doing things. So next we have the global strategy. This is really taking in your context. So what is going on around you? What is all the inputs coming in saying to you? What is your memory of things, your training data, all that kind of stuff? What is going on? And then how are you building the strategy upon that? So we've gone from your morality and ethics, then your mission, you're trying to accomplish something. Then we're taking in the environment and we're going to build some strategies. Then we get to the agent model. This is like, what can you actually do? What are your limits? And what's your memory? So think about it this way. If you're using, let's say GPT-4 versus LLAMA's 75 billion you're not really going to have as much capabilities in that 75 billion when at least at this point compared to GPT-4. You might know that you tend to hallucinate, you might know you have a context limit window on you so that you can only remember a certain number of tokens or words before you act. So you need to take all that into account. But it can also be more physical or concrete. Like, literally I don't have arms or things to manipulate things with. I can just move around. So that's a limitation. Then executive functioning. You know, this is what all of us humans have in terms of planning, but it's really thinking about what are your risks, what do you have at your disposal, and what is going to be your plan. Now, this risks one is very important, something that not a lot of AI do right now is they have no way to sort of talk about uncertainty. You know, these things hallucinate all the time. And that's because they're just predicting the next word. There's no, at least at this point of recording this, there's no way for it to tell you like, I'm 75% sure about this, I'm totally making this up, but I just want to be nice to you and please you. There's nothing of that right now. So to me this is a very important one of how are you thinking about the risks of any given scenario and then making your decision based on those risks. Cognitive control. Task selection, task switching. So task selection, this is one's probably pretty straightforward. These are okay, I've come up with my strategy, I know what I can do, I have my plan, what are the tasks now that I need to do to achieve this mission that I've set for myself or has been set for me? The more confusing one or one that you really have to think of, which was definitely a sort of epiphany, I think, for David is this one called task switching, which is how do you know when to give up on a task because it's not working, or it's not leading you towards your goal, or it's too risky, or whatever it might be, so you don't just get stuck? We've seen this in a lot of AIs where we tell them to play a game, for example, and they find some way to cheat to get the most amount of points, but they're not actually playing the game. So the idea here is to really think about how are you setting it up? So not only are they doing the right task, but if they get stuck in something they can switch to another task, create another plan, do a different thing to meet the mission. And last is the task prosecution. So this also gets back to the task switching. If it's a success, that's great, that's working, go into the training data, whatever. Failure, this is the big one. And not just failure, but frustration. I know it might sound crazy to say AI is frustrated, but like we were just saying with the task switching, you don't want an AI to keep doing the same thing over and over again if it's not working, or if it's not actually leading to the thing you actually want. You've poorly maybe defined the mission, which would hopefully get caught up in that aspirational layer. I'll talk about the communication across all of these soon. And then the individual tasks. So you've set the task, you've selected your task, now you actually have to do it. And then that leads to your output. You know, this is what I'm doing. I might be able to manipulate things with my hands, run around and clean the floor because I'm a vacuum, whatever it might be. This is the actual task that you need to achieve the overall mission. So the idea here is they're stacked. Now, what that means is in terms of, we're going to go to the southbound bus right now, that at each layer below the next, you can't control the layers above you. So, for example, the global strategy layer isn't able to control the aspirational layer. It can't make changes to the aspirational layer. And then write down the hierarchy here. The agent model can't control the global strategy, the executive functioning can't control the agent model, but everything can control anything under it. So agent model does have control over executive function, cognitive control, and task prosecution. This is really important because at the end of the day, the things you don't want to change, really, especially up top, is the aspirational layer. Because if an AI can control that itself, we get into this self-changing scenario where, okay, let's see here, I can't achieve this task, the thing that's stopping me is it's ethically wrong according to my aspirational layer, well, why don't I just change the aspirational layer and then I can do that thing? So it's really important that each layer kind of maintains control over the next. And then within layers, you're having control as well. So that one's, I don't know, that one's pretty straightforward, but the one that blew my mind was the northbound bus on the other side. blew my mind was the northbound bus on the other side. The word telemetry is usually used in terms of communication and in this case one of the major problems in all of alignment research is this issue of interpretability. You've probably heard this before that these neural networks like chat GPT or whatever are in fact black boxes. And that's because they are so complex. You know, these huge networks of different weights and parameters and we can't actually go in there and pick something out and really control what the output is and or really understand what's going on in there. It's just too complicated. And so that runs into all kinds of issues because let's say we ask a chat GPT, why did you say this thing? It's gonna come up with an answer, but that answer is hallucinated. We don't actually know if that was the reason mathematically, algorithmically, why it came up with that response. So this is a huge issue and there's no end in sight into solving the interpretability problem at this point, but this feels to me like a cheat code. The idea with telemetry is that you are using natural language processing at all levels of this scenario. And so what is happening is that everything is communicating with everything in natural language, which means you would have logs of everything that's going on. So you could actually go in and inspect the natural language, what it said it was going to do, and whether or not it did that. Now this is different because you're a level removed. You don't actually have to know what is going on under the hood to know that it's saying these things to itself. And that will create a log of step one, do this, step two, do this. You know, oh, can't do that because of my aspirational layer. It's communicating with itself at all levels and in a hierarchical, wow, say that word ten times fast, fashion. So what this means is that this system is interpretable because of the telemetry. Because we will literally think of it like a cybersecurity. Often what they're doing is that you're having just a log of actions that are being taken in any given situation. You know, clicked here, logged in with this at this time in this place, all this kind of stuff. So this would at least give us not just the, this is what I'm saying I'm doing, and this is what I'm doing. It also gives you all that metadata of the flow of information in terms of when did this happen, how did this happen, where did this happen, so that we can actually inspect it and continue to tune this. So that's pretty much it. I really want to keep this short. This feels like a very exciting framework to me, in particular the aspirational layer and the telemetry. The one thing I would maybe rethink about this this and maybe I'll try to create my own graphic around it is this feels to me more like a network of information. I understand simplifying it in this two dimensional space makes sense. But I kind of want to see this in 3d in terms of how all these layers actually interact with each other, and how the information flows to map that out a little bit more. So I hope this was helpful. Again, I'll put David's YouTube in here as well as the GitHub for this. Definitely if you have the technical skills, test this out. See what happens because this is the first thing I feel like systematically thinks through the alignment problem and how we can technically solve it. So thanks for listening and I hope you enjoyed. how we can technically solve it. So thanks for listening and I hope you enjoyed.", "chunks": [{"timestamp": [0.0, 9.52], "text": " Hey, Chatters."}, {"timestamp": [9.52, 15.52], "text": " I just wanted to do a quick video about something really cool that just came out that I'm excited"}, {"timestamp": [15.52, 16.52], "text": " about."}, {"timestamp": [16.52, 27.4], "text": " For those of you who don't know, my shtick, my thing, is really AI alignment, safety, ethics, and really solving what I believe is the"}, {"timestamp": [27.4, 33.2], "text": " last problem we have to solve as humanity, which is how do we get AI to do"}, {"timestamp": [33.2, 38.78], "text": " the things that creates a future we all benefit from and reduces suffering for"}, {"timestamp": [38.78, 44.24], "text": " all living things in the world. This is an incredibly difficult problem to solve."}, {"timestamp": [44.24, 45.04], "text": " I'm not going to get"}, {"timestamp": [45.04, 49.24], "text": " into the details of it because I want this video to be kind of quick. If you"}, {"timestamp": [49.24, 52.6], "text": " don't believe that's a problem, whatever, that's fine. This is not the video for"}, {"timestamp": [52.6, 59.68], "text": " you. If it is, then I want to introduce you to one of my favorite YouTube folks"}, {"timestamp": [59.68, 66.52], "text": " David Shapiro. David Shapiro is... I'm gonna call him a scientist, but he hasn't taken the"}, {"timestamp": [66.52, 72.92], "text": " typical path, you know. He doesn't have a PhD in all these areas and he hasn't"}, {"timestamp": [72.92, 78.72], "text": " done, you know, official academic research like you would expect, which hasn't... it"}, {"timestamp": [78.72, 85.56], "text": " doesn't mean he hasn't done any, it's just he's kind of doing his own thing, following his own interests, and"}, {"timestamp": [85.56, 92.36], "text": " he is really focused on AI's impact on society and how do we actually think"}, {"timestamp": [92.36, 98.28], "text": " about mitigating the harms. So he has a lot of amazing stuff on his YouTube. I"}, {"timestamp": [98.28, 102.76], "text": " highly check it out. I'll put his his links and his channel in the description."}, {"timestamp": [102.76, 106.68], "text": " But for now, just know that he's really"}, {"timestamp": [106.68, 114.24], "text": " into he's an alignment nerd like myself and he to me has come up with the first"}, {"timestamp": [114.24, 119.76], "text": " framework for thinking about alignment in a technical way that we can actually"}, {"timestamp": [119.76, 126.92], "text": " act upon and test. So his idea that he's been working on for God knows how long is"}, {"timestamp": [126.92, 131.52], "text": " something called the autonomous cognitive entity. Here's a much longer"}, {"timestamp": [131.52, 136.16], "text": " video going through this framework but I want to try to make it as accessible and"}, {"timestamp": [136.16, 142.52], "text": " straightforward as possible. So the idea here in this nice little graphic he's"}, {"timestamp": [142.52, 146.08], "text": " put together is you have a stacked hierarchy"}, {"timestamp": [146.08, 151.22], "text": " similar to like Maslow's hierarchy of needs or Bloom's taxonomy or all these"}, {"timestamp": [151.22, 156.24], "text": " other kinds of hierarchies that is sort of running the show within an AI. And"}, {"timestamp": [156.24, 161.36], "text": " when we say AI in this case we're really thinking about autonomous agents not"}, {"timestamp": [161.36, 166.0], "text": " necessarily AGI or artificial superintelligence."}, {"timestamp": [166.0, 173.0], "text": " This can be a domestic robot or a robot vacuum for all I care."}, {"timestamp": [173.0, 174.0], "text": " It doesn't really matter."}, {"timestamp": [174.0, 177.0], "text": " All it means is that this thing is taking autonomous actions."}, {"timestamp": [177.0, 179.0], "text": " You're not making all the choices for it."}, {"timestamp": [179.0, 183.0], "text": " And so we need something to make sure it's not doing things it's not supposed to do,"}, {"timestamp": [183.0, 186.92], "text": " not harming people, even if unintentionally."}, {"timestamp": [186.92, 189.2], "text": " So this is how it works."}, {"timestamp": [189.2, 192.58], "text": " We're going to go from top to bottom, then side to side."}, {"timestamp": [192.58, 195.28], "text": " We start at the very top at this aspirational layer."}, {"timestamp": [195.28, 197.56], "text": " I love this positive framing."}, {"timestamp": [197.56, 201.74], "text": " The word aspiration holds with it a lot of positive connotations and it's really thinking"}, {"timestamp": [201.74, 205.2], "text": " about what do you want to achieve like"}, {"timestamp": [205.2, 212.0], "text": " what's your dream and in here we have morality ethics and mission and so we're"}, {"timestamp": [212.0, 216.8], "text": " first grounding everything in a set of values that we're trying to achieve"}, {"timestamp": [216.8, 222.12], "text": " these things are abstract they're not concrete if anyone's ever had an ethical"}, {"timestamp": [222.12, 225.2], "text": " discussion of any kind, you know that things"}, {"timestamp": [225.2, 230.16], "text": " get very complicated very easily once you start throwing out different scenarios. But the idea is"}, {"timestamp": [230.16, 236.56], "text": " you have some guiding forces. This is probably the way he talks about similar to what Anthropx"}, {"timestamp": [236.56, 241.12], "text": " Claude has done. They have something called constitutional AI. They have trained it essentially"}, {"timestamp": [241.12, 249.68], "text": " on a constitution. Things like, you know, human rights. All these things that hopefully we all mostly agree on. We're not"}, {"timestamp": [249.68, 252.72], "text": " gonna get into the discussion about whose values. The point is you can set"}, {"timestamp": [252.72, 257.52], "text": " the values here. Then we go down and we start getting more and more concrete and"}, {"timestamp": [257.52, 262.12], "text": " getting out of the abstract and into, you know, actually doing things. So next we"}, {"timestamp": [262.12, 268.64], "text": " have the global strategy. This is really taking in your context. So what is going on around you?"}, {"timestamp": [268.64, 272.8], "text": " What is all the inputs coming in saying to you? What is your memory"}, {"timestamp": [272.8, 276.08], "text": " of things, your training data, all that kind of stuff?"}, {"timestamp": [276.08, 280.0], "text": " What is going on? And then how are you building the strategy"}, {"timestamp": [280.0, 284.32], "text": " upon that? So we've gone from your morality and ethics,"}, {"timestamp": [284.32, 285.2], "text": " then your mission,"}, {"timestamp": [285.2, 286.54], "text": " you're trying to accomplish something."}, {"timestamp": [286.76, 290.14], "text": " Then we're taking in the environment and we're going to build some strategies."}, {"timestamp": [291.06, 292.54], "text": " Then we get to the agent model."}, {"timestamp": [292.56, 294.46], "text": " This is like, what can you actually do?"}, {"timestamp": [294.7, 295.8], "text": " What are your limits?"}, {"timestamp": [296.06, 297.66], "text": " And what's your memory?"}, {"timestamp": [298.04, 300.7], "text": " So think about it this way."}, {"timestamp": [300.9, 308.32], "text": " If you're using, let's say GPT-4 versus LLAMA's 75 billion you're not really going to have"}, {"timestamp": [309.04, 314.0], "text": " as much capabilities in that 75 billion when at least at this point compared to GPT-4."}, {"timestamp": [314.8, 321.12], "text": " You might know that you tend to hallucinate, you might know you have a context limit window on you"}, {"timestamp": [321.12, 327.04], "text": " so that you can only remember a certain number of tokens or words before you act."}, {"timestamp": [327.04, 332.56], "text": " So you need to take all that into account. But it can also be more physical or concrete. Like,"}, {"timestamp": [332.56, 336.72], "text": " literally I don't have arms or things to manipulate things with. I can just move around."}, {"timestamp": [336.72, 342.8], "text": " So that's a limitation. Then executive functioning. You know, this is what all of us humans have in"}, {"timestamp": [342.8, 369.6], "text": " terms of planning, but it's really thinking about what are your risks, what do you have at your disposal, and what is going to be your plan. Now, this risks one is very important, something that not a lot of AI do right now is they have no way to sort of talk about uncertainty. You know, these things hallucinate all the time. And that's because they're just predicting the next word. There's no, at least at this point"}, {"timestamp": [369.6, 374.2], "text": " of recording this, there's no way for it to tell you like, I'm 75% sure about this,"}, {"timestamp": [374.2, 378.84], "text": " I'm totally making this up, but I just want to be nice to you and please you."}, {"timestamp": [378.84, 383.64], "text": " There's nothing of that right now. So to me this is a very important one of how"}, {"timestamp": [383.64, 388.54], "text": " are you thinking about the risks of any given scenario and then making your decision based on"}, {"timestamp": [388.54, 395.4], "text": " those risks. Cognitive control. Task selection, task switching. So task"}, {"timestamp": [395.4, 399.4], "text": " selection, this is one's probably pretty straightforward. These are okay, I've come"}, {"timestamp": [399.4, 404.24], "text": " up with my strategy, I know what I can do, I have my plan, what are the tasks now"}, {"timestamp": [404.24, 410.0], "text": " that I need to do to achieve this mission that I've set for myself or has been set for me?"}, {"timestamp": [410.0, 426.5], "text": " The more confusing one or one that you really have to think of, which was definitely a sort of epiphany, I think, for David is this one called task switching, which is how do you know when to give up on a task because it's not working, or it's not leading you towards your goal,"}, {"timestamp": [426.5, 429.12], "text": " or it's too risky, or whatever it might be,"}, {"timestamp": [429.12, 430.7], "text": " so you don't just get stuck?"}, {"timestamp": [430.7, 434.62], "text": " We've seen this in a lot of AIs where we tell them to play a game,"}, {"timestamp": [434.62, 438.54], "text": " for example, and they find some way to cheat to get the most amount of points,"}, {"timestamp": [438.54, 440.54], "text": " but they're not actually playing the game."}, {"timestamp": [440.54, 454.0], "text": " So the idea here is to really think about how are you setting it up? So not only are they doing the right task, but if they get stuck in something they can switch to another task, create another plan, do a different thing to meet the mission."}, {"timestamp": [454.0, 465.48], "text": " And last is the task prosecution. So this also gets back to the task switching. If it's a success, that's great, that's working, go into the training data, whatever."}, {"timestamp": [465.48, 467.16], "text": " Failure, this is the big one."}, {"timestamp": [467.16, 469.8], "text": " And not just failure, but frustration."}, {"timestamp": [469.8, 474.32], "text": " I know it might sound crazy to say AI is frustrated, but like we were just saying with the task"}, {"timestamp": [474.32, 479.42], "text": " switching, you don't want an AI to keep doing the same thing over and over again if it's"}, {"timestamp": [479.42, 483.8], "text": " not working, or if it's not actually leading to the thing you actually want."}, {"timestamp": [483.8, 489.4], "text": " You've poorly maybe defined the mission, which would hopefully get caught up in that aspirational"}, {"timestamp": [489.4, 490.4], "text": " layer."}, {"timestamp": [490.4, 493.56], "text": " I'll talk about the communication across all of these soon."}, {"timestamp": [493.56, 495.28], "text": " And then the individual tasks."}, {"timestamp": [495.28, 499.64], "text": " So you've set the task, you've selected your task, now you actually have to do it."}, {"timestamp": [499.64, 501.2], "text": " And then that leads to your output."}, {"timestamp": [501.2, 502.72], "text": " You know, this is what I'm doing."}, {"timestamp": [502.72, 505.04], "text": " I might be able to manipulate things with my hands,"}, {"timestamp": [505.84, 510.56], "text": " run around and clean the floor because I'm a vacuum, whatever it might be. This is the actual"}, {"timestamp": [510.56, 516.8], "text": " task that you need to achieve the overall mission. So the idea here is they're stacked. Now,"}, {"timestamp": [517.44, 527.68], "text": " what that means is in terms of, we're going to go to the southbound bus right now, that at each layer below the next, you can't control the layers above you."}, {"timestamp": [528.24, 534.32], "text": " So, for example, the global strategy layer isn't able to control the aspirational layer."}, {"timestamp": [534.88, 543.2], "text": " It can't make changes to the aspirational layer. And then write down the hierarchy here. The agent"}, {"timestamp": [543.2, 546.2], "text": " model can't control the global strategy, the executive"}, {"timestamp": [546.2, 551.04], "text": " functioning can't control the agent model, but everything can control anything under"}, {"timestamp": [551.04, 556.24], "text": " it. So agent model does have control over executive function, cognitive control, and"}, {"timestamp": [556.24, 562.38], "text": " task prosecution. This is really important because at the end of the day, the things"}, {"timestamp": [562.38, 566.5], "text": " you don't want to change, really, especially up top, is the aspirational layer."}, {"timestamp": [566.5, 575.0], "text": " Because if an AI can control that itself, we get into this self-changing scenario where,"}, {"timestamp": [575.0, 583.0], "text": " okay, let's see here, I can't achieve this task, the thing that's stopping me is it's ethically wrong according to my aspirational layer,"}, {"timestamp": [583.0, 586.96], "text": " well, why don't I just change the aspirational layer and then I can do that thing?"}, {"timestamp": [586.96, 592.24], "text": " So it's really important that each layer kind of maintains control over the next."}, {"timestamp": [592.24, 595.28], "text": " And then within layers, you're having control as well."}, {"timestamp": [597.2, 599.36], "text": " So that one's, I don't know, that one's pretty straightforward,"}, {"timestamp": [599.36, 604.56], "text": " but the one that blew my mind was the northbound bus on the other side."}, {"timestamp": [607.68, 613.56], "text": " blew my mind was the northbound bus on the other side. The word telemetry is usually used in terms of communication and in this case one of the major"}, {"timestamp": [613.56, 618.48], "text": " problems in all of alignment research is this issue of interpretability. You've"}, {"timestamp": [618.48, 622.88], "text": " probably heard this before that these neural networks like chat GPT or"}, {"timestamp": [622.88, 626.76], "text": " whatever are in fact black boxes."}, {"timestamp": [626.76, 629.28], "text": " And that's because they are so complex."}, {"timestamp": [629.28, 633.28], "text": " You know, these huge networks of different weights and parameters"}, {"timestamp": [633.28, 637.28], "text": " and we can't actually go in there and pick something out"}, {"timestamp": [637.28, 640.24], "text": " and really control what the output is"}, {"timestamp": [640.24, 642.48], "text": " and or really understand what's going on in there."}, {"timestamp": [642.48, 644.4], "text": " It's just too complicated."}, {"timestamp": [644.4, 646.88], "text": " And so that runs into all kinds of issues"}, {"timestamp": [646.88, 650.8], "text": " because let's say we ask a chat GPT,"}, {"timestamp": [650.8, 652.4], "text": " why did you say this thing?"}, {"timestamp": [652.4, 654.12], "text": " It's gonna come up with an answer,"}, {"timestamp": [654.12, 655.78], "text": " but that answer is hallucinated."}, {"timestamp": [655.78, 659.04], "text": " We don't actually know if that was the reason"}, {"timestamp": [659.04, 661.46], "text": " mathematically, algorithmically,"}, {"timestamp": [661.46, 663.16], "text": " why it came up with that response."}, {"timestamp": [664.12, 667.52], "text": " So this is a huge issue and there's no end in sight"}, {"timestamp": [667.52, 673.52], "text": " into solving the interpretability problem at this point, but this feels to me like a cheat code."}, {"timestamp": [674.24, 680.4], "text": " The idea with telemetry is that you are using natural language processing at all levels"}, {"timestamp": [681.2, 687.98], "text": " of this scenario. And so what is happening is that everything is communicating with everything in natural"}, {"timestamp": [687.98, 693.32], "text": " language, which means you would have logs of everything that's going on."}, {"timestamp": [693.32, 699.08], "text": " So you could actually go in and inspect the natural language, what it said it was going"}, {"timestamp": [699.08, 702.56], "text": " to do, and whether or not it did that."}, {"timestamp": [702.56, 705.5], "text": " Now this is different because you're a level removed."}, {"timestamp": [705.5, 709.0], "text": " You don't actually have to know what is going on under the hood"}, {"timestamp": [709.0, 712.5], "text": " to know that it's saying these things to itself."}, {"timestamp": [712.5, 715.0], "text": " And that will create a log of"}, {"timestamp": [715.0, 717.0], "text": " step one, do this, step two, do this."}, {"timestamp": [717.0, 720.5], "text": " You know, oh, can't do that because of my aspirational layer."}, {"timestamp": [720.5, 724.0], "text": " It's communicating with itself at all levels"}, {"timestamp": [724.0, 729.0], "text": " and in a hierarchical, wow, say that word ten times fast, fashion."}, {"timestamp": [729.0, 736.0], "text": " So what this means is that this system is interpretable because of the telemetry."}, {"timestamp": [736.0, 741.0], "text": " Because we will literally think of it like a cybersecurity."}, {"timestamp": [741.0, 747.64], "text": " Often what they're doing is that you're having just a log of actions that are being taken in any given situation."}, {"timestamp": [747.64, 754.12], "text": " You know, clicked here, logged in with this at this time in this place, all this kind of stuff. So this would at least give"}, {"timestamp": [754.12, 761.4], "text": " us not just the, this is what I'm saying I'm doing, and this is what I'm doing. It also gives you all that metadata of the"}, {"timestamp": [761.4, 766.16], "text": " flow of information in terms of when did this happen, how did this happen,"}, {"timestamp": [766.16, 771.92], "text": " where did this happen, so that we can actually inspect it and continue to tune this."}, {"timestamp": [771.92, 773.24], "text": " So that's pretty much it."}, {"timestamp": [773.24, 775.2], "text": " I really want to keep this short."}, {"timestamp": [775.2, 781.08], "text": " This feels like a very exciting framework to me, in particular the aspirational layer"}, {"timestamp": [781.08, 782.08], "text": " and the telemetry."}, {"timestamp": [782.08, 805.64], "text": " The one thing I would maybe rethink about this this and maybe I'll try to create my own graphic around it is this feels to me more like a network of information. I understand simplifying it in this two dimensional space makes sense. But I kind of want to see this in 3d in terms of how all these layers actually interact with each other, and how the information flows to map that out a little bit more. So I hope this was helpful. Again, I'll"}, {"timestamp": [805.64, 809.84], "text": " put David's YouTube in here as well as the GitHub for this. Definitely if you"}, {"timestamp": [809.84, 814.16], "text": " have the technical skills, test this out. See what happens because this is the"}, {"timestamp": [814.16, 819.56], "text": " first thing I feel like systematically thinks through the alignment problem and"}, {"timestamp": [819.56, 824.96], "text": " how we can technically solve it. So thanks for listening and I hope you"}, {"timestamp": [824.96, 827.16], "text": " enjoyed."}, {"timestamp": [821.17, 823.29], "text": " how we can technically solve it."}, {"timestamp": [823.79, 826.45], "text": " So thanks for listening and I hope you enjoyed."}]}