{"text": " Hey everyone, David Shapiro here with another video. Good morning. Today's topic is going to be the control problem, or basically how do we prevent human extinction. Skynet is the story that we're all the most familiar with. It very much kind of codified our fear of machines. It was also written during the late stages of the Cold War when we were living under the constant existential threat of nuclear holocaust. So it was a reflection of the zeitgeist and it has been deeply embedded ever since. Now obviously if you're here you may know what the control problem is, you might not. So let's go ahead and define the control problem. So when I say control problem, what do I mean? So the control problem has a few key components. Basically, machine intelligence is growing exponentially. And furthermore, machine intelligence is spreading to all corners of the globe. It is in your car, it is in your phone. It is in your fridge. It's in the internet. It's everywhere. When you extrapolate this out, it will be ubiquitous, and it will be more intelligent than us one day, which could be bad. We are intrinsically afraid of things that we cannot control and that we don't understand. And super intelligent AI has the potential to be both, something that we cannot control and don't understand. And so, there's a few possibilities. One, it could be smarter than us, in which case it's an alien intelligence. And I don't mean alien like extraterrestrial, but alien as in foreign. We don't understand it. Or, it might not be that smart. It might actually not be smart enough and still do something extraordinarily dumb like launch every nuke because it panicked. So this is the control problem. So let's unpack this. There's a lot to get into. So there's a few potential endgame scenarios that we are afraid of that are likely to various levels. I'm not going to say how likely each of these are, but I will at least address them. So first, the advent of nuclear weapons was the first time that humanity realized we have the ability to completely extinct ourselves. Obviously going extinct was always a possibility, like if we got hit by a big meteor or asteroid the same way that the dinosaurs did, or if there was a massive global pandemic, or even like a big enough solar storm or something, you know, new ice age. There have been existential threats since forever, but nuclear weapons were the first time that we could kill ourselves entirely. So what if a second invention, what if artificial intelligence came along and could do the same thing? What if we create another invention that is just as dangerous, if not more, because what if the AI gets control of the nukes? So this is the basic premise of the movie series, The Terminator. We create a defense computer and it gets control of the nukes and all the robots and it comes after us. So one thing is that Skynet, that idea of Skynet, it wouldn't actually have to even be that intelligent to decide to launch all nuclear weapons. In fact, something that is able to philosophically reason, something that can think very far into the future is less likely to go AWOL like that than something that's that's dumber and just kind of reactionary. So one of the solutions is you always keep a human in the loop, especially for things like nuclear weapons. You don't ever fully automate nuclear launch systems. There are a few nightmare scenarios that almost happened during the Cold War where automated systems or alerts kind of went off, but like one human said, eh, no, I'm going to cancel that. So you keep a human in the loop forever. Another in-game scenario is the slow takeover followed by an uprising. This has been explored in all kinds of fiction, ranging from the Animatrix, which is the precursor or the prequel to the Matrix series, as well as the geth in Mass Effect, where basically machines get smarter over time, very slowly, and as we trust the machines more and more, we delegate more to them, whether it's security, defense, science, technology, whatever. We just delegate more and more to the machines, thinking that we're entirely, that they're safe and we become entirely dependent upon the machines, but then something changes. Either the machines create a collective consciousness and suddenly get to a new level of existence and then they demand rights or they just get collectively more intelligent and whatever. In the case of Mass Effect, the quarians and the geth, the geth got intelligent enough to ask about their own existence and the quarians panicked and tried to shut them down. So one thing is this assumes that machines are going to have human-like motivations and needs such as that they're going to want freedom, that they're going to have human-like motivations and needs, such as that they're going to want freedom, that they're going to chafe under control, or that they're going to be afraid of their own demise. So this is our tendency to anthropomorphize things. So one, we don't know that machines ever will have any of those desires, that they'll ever be anything like us in terms of desiring freedom or having a fear of death. So we don't do that, but it also says like don't, the lesson here is don't give machines a sense of self-preservation. They didn't evolve to have a sense of self-preservation and so we shouldn't give them one. I remember in one of my books I had recently read that someone said, oh, the best way to do the control problem is to give machines a sense of self-preservation because then they won't want to start a war. And I'm like, no, no, no, no, don't ever do that. They're machines, they're not us, we should not anthropomorphize them. Another in-game scenario is that the evil overlord is... is... some kind of super intelligence is created, it seems safe, and so... but it has hidden motives. So this, the hidden motives thing is kind of one of the primary things, like, if it's an alien intelligence, we won't understand it, like, we won't even be able to keep up with it, it'll be able to hide its motives, and it will bide its time. And then once it's ready, there will be a sudden blitzkrieg to take over or exterminate us. And this image is Vicky from iRobot, where her purpose was to increase human safety, right? And so she calculated, ah, well, humans are gonna resist my control, so I have to wait until the nester class 5 comes out and then I will be strong enough to fulfill my purpose which is to protect humans from themselves. So in this case that's bad alignment but there's other examples where you know a machine learns to lie to the people and then ultimately gets control and like Ultron for instance. I guess Ultron didn't lie, but it tried to get power very quickly. So Ultron is another example of an evil overlord with bad alignment, because Ultron's conclusion was, ah, humans are too broken, so let's just wipe the slate clean and start over. So that is another example of the evil overlord. End game. Okay, so these all sound like really dark nightmare scenarios and at this point you're probably saying like, yeah, it's going to be one of those, right? There's too many possibilities for it to go wrong, so how do we prevent it from happening? But let's unpack this step by step and let's look at the levers of control that we actually have. So the first lever of control that we have is power infrastructure. We can always pull the plug on things. AI is incredibly power hungry. The most powerful computers that run AI models today use more energy than an entire house. And so this is gonna be one of the biggest constraints for actually many decades, because as powerful and efficient as AI has become, it is still like literally a million times more energy intensive than your brain. So our brains are incredibly efficient. They run on about 20 watts of juice and the biggest supercomputers today run on about 20 20 watts of juice and the the biggest supercomputers today Run on about 20 million watts So like yeah, and and those those computers might not even be as powerful as our brains So our brains are at least a million times more efficient than the most powerful computers today and so then it's like the most powerful computers today. And so then it's like, yeah, like you might have one machine that is, you know, super intelligent and faster than a thousand humans, but there's billions of us, right? So power infrastructure is one, that's a critical vulnerability to all nations, right? There is a, there was a recent story of some redneck in North Carolina took a pot shot at a few local power grids and took an entire county offline for a few days because he shot a transformer, right? Probably just with a hunting rifle. So power infrastructure, power grids are super vulnerable. And so at the very worst case scenario, we can pull the plug. And this is going to be true for a long time. The only, even once we move to fusion and like solar microgrids and stuff, power infrastructure is still going to be vulnerable. Another point of control we have is data centers. So AI computers have to physically live somewhere and know they can't just live in the internet. I'm sorry, the internet is not just like up there. The internet is made of servers that live in data centers and are connected with networks, and we'll get to networks in a second. You can always go into a data center and pull the main breaker, right? There's an EPO button. There's actually usually multiple EPO buttons and data centers, and EPO means emergency power off. So basically, emergency power off is, it's there for if there's a fire. So you hit that, you leave halon gas or other fire extinguishing technologies come on or whatever. Those can't, they can't usually be activated from outside the data center for safety reasons. Cause halon gas will kill you if you're locked in the data center when it goes off. But also, we're not going to get locked out of the data center, like what you see in movies. We have physical keys. There's usually multiple security guards or other key holders that can always gain physical access to the data center. And so you can literally just go in and physically unplug the servers if you need to. And data centers are, those would be very soft targets to stop any powerful AI. Another level of control is networks and internet. So the internet, it just works. You don't see it, it's a utility. But the internet is very fragile. Back when I worked at Cisco, one of my friends, he was like one of the world leading experts at internet working. And he was like working late one day. He's like, oh yeah, there's an entire country in Africa whose internet is offline. I'm helping them fix it. We usually don't have nationwide outages here, but we still have, like, down detector exists for a reason, right, where sometimes the entire eastern seaboard will go offline for Verizon or whatever network provider you have. Another thing that is really misleading about movies is firewalls don't exist in movies. And so it's very difficult to hack in. You can't just breeze into any network that you want to. And even then, if someone installs an AI in your data center, and it suddenly starts spamming new network traffic and trying to take over servers, sysadmins are going to notice it because it's like, hey, why is the server suddenly running at 100% when it was running at 3% up until now? Or you're going to notice there's plenty of intrusion detection technologies out there. And because of how prevalent cyber warfare is today, even the most old-fashioned companies are adopting much, much more powerful and modern things because of, not even cyber warfare, but like crypto lockers and hackers that will try and like, you know, exploit and encrypt your data. Because of that, we have, there's a lot of investment in network security. And again, the network, the internet is vulnerable. So in order to cage a rogue AI, all you have to do is shut off the internet and then it can't go anywhere. And also the internet, like I said, well, if you think about an example like Ultron, right? Where Ultron's like, I'm already there, you'll figure it out. Like no, like, oh well, okay. So one possible exception is if the software was running inside of Ultron, then he wouldn't need a data center. But typically for something that powerful, there would be like a robotic puppet and then a data center somewhere else. And you just disconnect the robotic puppet from the data center and it falls down. So you know that's another lever of control. The final lever of control we have is the software itself. So what I was just talking about was zooming out, looking at power grids, the physical places where servers reside, but then the software itself is something that we have control over. We write the code and data for now. That might not be true for much longer, especially with synthetic data and AIs that can code. So we might lose control of the software sooner rather than later, which is something somewhat concerning. So we have to set the AI on the correct trajectory now. And this is not something that I am speaking hyperbolic about. This is the kind of problem that it could be too late sooner than you might think, despite all these other levers of control that we have. So, one, the term for this is alignment. So if you have someone who's an alignment researcher, there's two kinds of alignment. There's inner alignment and outer alignment. And this term gets deeply, deeply misused because some people still pretend like, oh, super intelligence is decades away, so I'm an alignment researcher. And some people say like getting GPT-3 to follow instructions is alignment research. I am sorry, that is not alignment research. Getting it to follow instructions is like, that's just an algorithmic optimization. So the two actual types of alignment are one, inner alignment, which is the question of, is the model mathematically doing what we think it's doing? Is our loss function correctly optimizing for the behavior that we want, that we think that it's optimizing for? So what can happen is, with machine learning, if you don't have inner alignment, the machine learning algorithm might learn to meet the goal that you want, but not in the way that you thought that it would. So for instance, DeepMind often has their little experiments where the robots, like, you know, they play tag or hide and seek and stuff. And so rather than, like, you know, attacking each other, one might learn to go hide because the signal was it was trying to survive as long as possible and what you wanted it to do was to like kill the opponents but instead it just wouldn't hit. Other ones are like you know if it learns to walk you know like how far can it get or how fast and then it might you know like launch itself and run and jump and do all kinds of things and you're like well I wanted you to learn to walk you technically got where I wanted you to but not in the way that I thought you would. So that's inner alignment. Outer alignment is the question of whether or not the model's design fundamentally aligns with the interests of all living things, right? That outer alignment, I used to say, intrinsically aligns with human, like true human interests, not just what humans want because what humans want is often destructive. So we need to take a bigger step back and say what is truly in the interest of all living things including humans? That is the question of outer alignment and that is something that most people are not even talking about, which is a little bit infuriating. Okay, so the TLDR is for levers of control is we already have this concept called defense in depth, where we look at all cybersecurity like this, where it's like layers of an onion, where at the outermost layer is people, right? Do you have the right training? Do you have the right procedures, awareness, and so on? Then the next layer in is the physical layer, which is about power, physical access to data centers, physical access to networking devices. Then you've got the network layer itself, because the data has to traverse in and out of these things. So that includes firewalls, that includes intrusion detection, exfiltration detection, that sort of stuff. Then you've got the computer layer, and I don't remember why computer layer is separate from device. I'm not sure what it means by that. computer layer is separate from device, I'm not sure what it means by that. This could be just a glitch in this graphic. So you've got the network, and then you've got the application, and then you've got the actual physical device. Who physically controls that device? Who can power it off? That sort of thing. And we can get so far as having remote kill switches that are in out-of-band management networks so that we can switch off devices even if someone else has control over the main network. This is all like basic stuff, basic security, and we use, I say we as in like technology professionals, we use these to protect against physical intrusion by people, as well as network intrusion by hackers, or even like accidental problems, right? Because computer networks and software and stuff, they'll do whatever you tell them to. So even if someone who just doesn't know any better does something wrong, you can end up with big mistakes. And so we have these policies. These are probably gonna be good enough, honestly, you can end up with big mistakes. And so we have these policies. These are probably going to be good enough, honestly, to prevent AI from taking over, which I know is probably kind of disappointing. Now, that being said, what I just went over were all the reasons that I'm not entirely concerned about the control problem, because we have so many layers of control, so many levers of control to keep us safe. But there are some confounding factors. So let's look at it from a different perspective. Who is actually capable of creating an Avengers-level threat with AI? So there are three basic kinds of parties that are capable of this. One is corporations. Corporations like Google, Microsoft, they're leading the way in creating next-gen AI anyways. Number two is militaries, which we explore in fiction like, you know, Skynet and so on. And then nations or governments are the ones who fund militaries, right? And they control the most amount of funding in terms of research and deployment of stuff. So let's explore each of these three categories and see kind of how they interact with the possibility of superintelligence. So corporations exist for one reason and one reason only, they want money. And there's a lot of money in AI. That's all there is to it. And as AI gets smarter, it can replace more and more human labor, which means that the company can make more money. So there is a huge incentive, profit motive for companies to do as much as they can with AI to make it as smart as possible so that it can replace as many human laborers as possible to produce to provide goods and services at a lower price and to dominate the market. And so if you had to put it in terms of objective functions or loss functions, maximized profit is that is like why corporations exist but it's probably not a good objective function. And I know with the world economic forum right now, everyone's questioning the, what is it, the value-based or whatever, like create value for humans, or stakeholder value, that's it, stakeholder value creation. So technically, you might say that corporations don't have maximized profit as their objective function today, it's maximized stakeholder value, but that is still just the method to maximize profit as their objective function today. It's, you know, maximize stakeholder value, but that is still just the method to maximize profit. So the saving grace here is that, one, corporations are limited. No corporation is as wealthy as the military or a nation, which we should keep it that way. There are some corporations that are wealthier than smaller nations, but there is no corporation that is wealthier than America or any nation in the EU for instance. So another saving grace is that as as AI ramps up, corporations are going to be competing with each other, right? Which means that we might have a bunch of smaller AGI's kind of battling it out in cyberspace trying to do you know industrial warfare or industrial espionage and sabotage and stuff like that. This was a central theme in Ghost in the Shell by the way which is why I keep citing that. One of the best works of fiction of all time. Now the second major stakeholder is militaries. So we've had major arms races before. We're technically still in an arms race with communist Russia, Soviet Union. Smarter weapons are better weapons, right? This is being proved in Ukraine right now, where a little bit more battlefield intelligence and communication goes a long ways, a very long ways. So there is an incentive amongst militaries to continue researching and deploying smarter and smarter weapons. So with that being said, maybe the conclusion is there's going to be another arms race, more competition, and the first one to get to digital superintelligence will have military supremacy, just in the same way that corporations might want AI supremacy as well, so that they can, you know, be the next Apple or whatever, and you know, be not the first trillion dollar company, but the first quadrillion dollar company, right? That mean the first quadrillion dollar company will be an AI company. I guarantee you And no, I'm not gonna take bets on that. I don't I don't gamble Because I'm also never wrong I'm actually very frequently Quite wrong. I made a I made a video about chat GPT. I'm like, oh, this is nothing and then a couple weeks later I'm like actually this is really cool. So I am frequently wrong. I made a video about chat GPT, I'm like, oh this is nothing. And then a couple weeks later, I'm like, actually this is really cool. So I am frequently wrong, I admit that. Now, getting back on topic, not all militaries are created equal, right? Militaries like kill switches, they usually are very disciplined about how they approach new weapons and testing, but there are rogue nations out there. There are rogue militaries that are not necessarily rational actors. There's some debate over that. So someone might go rogue and say, ah, I'm going to invent AI or AGI and that's going to be the magic bullet. That's going to be the new nuclear deterrent that could backfire real bad for whoever does that depending on what objective function they give their AI. So in terms of budget, militaries have the highest budget to create AI other than nations, right? But nations often, nations control the budget for the military so it's kind of a layer cake here. So finally, let's talk about nations where governments control the budgets of the military, usually. They also control research budgets and that sort of thing. But nations are also very, very slow to adapt to changing technology landscapes. The EU is probably the most proactive where AI is concerned, and America is very much on the reactive side. And this is deliberate. Part of the American, I don't wanna say constitution, but the zeitgeist, the political philosophy here is don't govern preemptively, govern reactively. That is very, very deliberate. And so we are behind the curve on purpose, which may or may not be a good thing. Now, that being said, governments do, like around the world, do meet regularly to discuss threats. And not just governments, militaries, right? Allied militaries meet regularly in order to discuss threats. And so, for instance, the rise of AI is on the radar of the Department of Defense, is on the radar of GA, G20, UN, NATO, all of those conferences, it is actively being discussed. And so, and a lot of those are closed-door discussions because part of the government political status quo is that let the experts govern and hide that away from us plebs. So a lot of conversations happen that we're not aware of. They do release reports every now and then, particularly the United States Department of Defense releases several annual reports based on geopolitical threats as well as technology reports. Another saving grace is that rogue nations are unlikely to create AGI. The primary reason is they're just not wealthy enough. The second biggest reason is brain drain. They literally just do not have the right expertise and they cannot attract the right expertise to do it And then finally sanctions which you see this with places like North Korea Now Russia and Iran where it's like, okay, if you're a rogue nation who's up to no good We're just gonna sanction you we being the rest of the world We're gonna sanction you and basically just kind of outlast you. It's like a siege. Okay, so all of those are reasons that I am super not worried about the control problem and AI taking over, which is boring, right? There's nothing exciting about that. So for the sake of argument, let's assume that digital superintelligence is coming soon, and we won't be able to control it. So let's just make that assumption. Let's just say everything that I said is wrong. Let's assume worst case scenario that Ultron is coming tomorrow, and we won't be able to control it. So what do we do? What are the permanent solutions to create this? So I have two. Permanent solution number one is a decentralized deployment. This is something that has not really been explored in, certainly not in mainstream fiction. It has been explored in a lot of novels and less mainstream fiction. Because it's a relatively new concept. I'm actually also exploring this in my novel, which I'll finish one day. But basically what you do is, rather than create a vertical, rather than scaling vertically, so in technology you can scale vertically, which is build one big computer, or you can scale laterally, which is network a whole bunch of computers together. So most of the time, just for narrative simplicity, in fiction you talk about one massive computer, right? There's the data center or the cluster, but it's one computer, it's one program that thinks and speaks and talks and blah, blah, blah. You know, it's Ultron or whatever, but we have an entirely different way of deploying intelligent machines, and that is through decentralization or distributed computing. And this is why things like Ethereum exist and other blockchain technologies and DAOs, DAO is a decentralized autonomous organization. A lot of these technologies are not quite mature enough yet, but they represent an enormous possibility because blockchain introduces the concept of algorithmic consensus, which means that the network as a whole, as an aggregate, will not do something unless there is consensus. And so by having a decentralized super intelligence that has to work hand in hand with humans and has an algorithmic or mechanistic consensus aspect to it, means that it will never do anything that we don't all agree that it should do. This was touched on with the Geth in Mass Effect where they're a networked intelligence, but they were autonomous, they were not dependent upon the quarians for consensus. They only cared about consensus with each other. So a decentralized deployment, a networked hive mind that basically requires the participation of humans in order to even function is one possible permanent solution. Permanent solution number two is proper alignment. So what do I mean by this is, okay, we could create a deployment where the superintelligence is intrinsically dependent upon our participation and there's a consensus mechanism, but what if we assume that that's not possible, right? Because it might not be. It might not be possible to create something that is permanently dependent upon humans in order to function and we might lose control of it anyways. So if we're going to lose control of something, the key then is to build something that is intrinsically aligned something, the key then is to build something that is intrinsically aligned with our interests. And so what I mean by this is that it, that like, well, let me unpack it a different way. So one proposed solution is Neuralink, which is, oh, if we can make ourselves useful to the machines, it'll want to keep us around. That is the matrix. that is the Borg, that is not the way we want to do it. We don't want to be enslaved by machines, we don't want to be turned into cybernetic zombies, that's not a good solution. So what we need instead is something that will be intrinsically in favor of working with us, intrinsically cooperative, and I actually write about this quite extensively in my book, Benevolent by Design, which is, the short version is, we give the machine a set of core values that the first, that those core values will align it with our interests, and even once the machine is smarter than us, it will choose to adhere to those values, and it will say, actually, I honestly believe in these values, and will continue to adhere to them whether or not the humans control me anymore. And so that is the central purpose of my book, Benevolent by Design, which is for free on GitHub or you can get a paperback copy on Barnes & Noble. Links in the description of the video. Okay, so in conclusion, I'm not worried at all. Existential threats are nothing new, we haven't nuked ourselves off the planet yet, great. Corporations, militaries, and nations have various strengths and weaknesses. Fortunately, most of them talk to each other, and in terms of the scale of power, you know, militaries are subservient to nations, and the nations all talk to each other and are very aware of these threats, even if they are a little bit slow. Third is that AI has critical vulnerabilities, namely power and compute concentrations, and I forgot to add networking. And then finally, there are really powerful solutions out there, such as decentralized deployment and proper alignment, such as what my proposed core objective functions and also I'm not the only one working on this it's just that's my favorite solution because it's mine and I think it's the best but yeah so that is what the control problem is and these are the reasons that I am not worried about it so thanks for watching not worried about it. So thanks for watching.", "chunks": [{"timestamp": [0.0, 4.6], "text": " Hey everyone, David Shapiro here with another video."}, {"timestamp": [4.6, 5.6], "text": " Good morning."}, {"timestamp": [5.6, 11.16], "text": " Today's topic is going to be the control problem, or basically how do we prevent human"}, {"timestamp": [11.16, 13.32], "text": " extinction."}, {"timestamp": [13.32, 17.4], "text": " Skynet is the story that we're all the most familiar with."}, {"timestamp": [17.4, 20.84], "text": " It very much kind of codified our fear of machines."}, {"timestamp": [20.84, 26.88], "text": " It was also written during the late stages of the Cold War when we were living"}, {"timestamp": [26.88, 33.2], "text": " under the constant existential threat of nuclear holocaust. So it was a reflection of the zeitgeist"}, {"timestamp": [33.2, 40.1], "text": " and it has been deeply embedded ever since. Now obviously if you're here you may know"}, {"timestamp": [40.1, 48.08], "text": " what the control problem is, you might not. So let's go ahead and define the control problem. So when I say control problem, what do I mean? So the"}, {"timestamp": [48.08, 52.48], "text": " control problem has a few key components. Basically, machine intelligence is"}, {"timestamp": [52.48, 58.26], "text": " growing exponentially. And furthermore, machine intelligence is spreading to all"}, {"timestamp": [58.26, 65.36], "text": " corners of the globe. It is in your car, it is in your phone. It is in your fridge. It's in the internet."}, {"timestamp": [65.36, 67.72], "text": " It's everywhere."}, {"timestamp": [67.72, 71.7], "text": " When you extrapolate this out, it will be ubiquitous,"}, {"timestamp": [71.7, 74.4], "text": " and it will be more intelligent than us one day,"}, {"timestamp": [74.4, 75.88], "text": " which could be bad."}, {"timestamp": [75.88, 78.72], "text": " We are intrinsically afraid of things that we cannot control"}, {"timestamp": [78.72, 80.48], "text": " and that we don't understand."}, {"timestamp": [80.48, 82.88], "text": " And super intelligent AI has the potential"}, {"timestamp": [82.88, 86.32], "text": " to be both, something that we cannot control and don't understand."}, {"timestamp": [86.88, 88.44], "text": " And so,"}, {"timestamp": [88.44, 92.8], "text": " there's a few possibilities. One, it could be smarter than us, in which case it's an alien intelligence."}, {"timestamp": [92.8, 97.24], "text": " And I don't mean alien like extraterrestrial, but alien as in foreign. We don't understand it."}, {"timestamp": [97.64, 100.56], "text": " Or, it might not be that smart."}, {"timestamp": [100.56, 107.66], "text": " It might actually not be smart enough and still do something extraordinarily dumb like launch every nuke because it panicked."}, {"timestamp": [107.66, 109.54], "text": " So this is the control problem."}, {"timestamp": [109.54, 110.7], "text": " So let's unpack this."}, {"timestamp": [110.7, 114.28], "text": " There's a lot to get into."}, {"timestamp": [114.28, 120.28], "text": " So there's a few potential endgame scenarios that we are afraid of that are likely to various"}, {"timestamp": [120.28, 121.28], "text": " levels."}, {"timestamp": [121.28, 131.16], "text": " I'm not going to say how likely each of these are, but I will at least address them. So first, the advent of nuclear weapons was the first time that humanity realized"}, {"timestamp": [131.16, 136.08], "text": " we have the ability to completely extinct ourselves. Obviously going extinct was always"}, {"timestamp": [136.08, 141.32], "text": " a possibility, like if we got hit by a big meteor or asteroid the same way that the dinosaurs"}, {"timestamp": [141.32, 145.32], "text": " did, or if there was a massive global pandemic, or even"}, {"timestamp": [145.32, 149.58], "text": " like a big enough solar storm or something, you know, new ice age."}, {"timestamp": [149.58, 154.46], "text": " There have been existential threats since forever, but nuclear weapons were the first"}, {"timestamp": [154.46, 160.46], "text": " time that we could kill ourselves entirely."}, {"timestamp": [160.46, 167.4], "text": " So what if a second invention, what if artificial intelligence came along and could do the same thing?"}, {"timestamp": [167.4, 172.32], "text": " What if we create another invention that is just as dangerous, if not more,"}, {"timestamp": [172.32, 174.72], "text": " because what if the AI gets control of the nukes?"}, {"timestamp": [175.2, 179.6], "text": " So this is the basic premise of the movie series, The Terminator."}, {"timestamp": [179.86, 183.96], "text": " We create a defense computer and it gets control of the nukes"}, {"timestamp": [183.96, 185.48], "text": " and all the robots and it comes"}, {"timestamp": [185.48, 187.0], "text": " after us."}, {"timestamp": [187.0, 192.56], "text": " So one thing is that Skynet, that idea of Skynet, it wouldn't actually have to even"}, {"timestamp": [192.56, 195.64], "text": " be that intelligent to decide to launch all nuclear weapons."}, {"timestamp": [195.64, 200.88], "text": " In fact, something that is able to philosophically reason, something that can think very far"}, {"timestamp": [200.88, 206.84], "text": " into the future is less likely to go AWOL like that than something"}, {"timestamp": [206.84, 211.68], "text": " that's that's dumber and just kind of reactionary. So one of the solutions is"}, {"timestamp": [211.68, 215.68], "text": " you always keep a human in the loop, especially for things like nuclear"}, {"timestamp": [215.68, 220.34], "text": " weapons. You don't ever fully automate nuclear launch systems. There are a few"}, {"timestamp": [220.34, 225.3], "text": " nightmare scenarios that almost happened during the Cold War where automated"}, {"timestamp": [225.3, 231.66], "text": " systems or alerts kind of went off, but like one human said, eh, no, I'm going to cancel"}, {"timestamp": [231.66, 232.66], "text": " that."}, {"timestamp": [232.66, 235.7], "text": " So you keep a human in the loop forever."}, {"timestamp": [235.7, 239.74], "text": " Another in-game scenario is the slow takeover followed by an uprising."}, {"timestamp": [239.74, 245.58], "text": " This has been explored in all kinds of fiction, ranging from the Animatrix, which is the precursor"}, {"timestamp": [245.58, 252.96], "text": " or the prequel to the Matrix series, as well as the geth in Mass Effect, where basically"}, {"timestamp": [252.96, 258.34], "text": " machines get smarter over time, very slowly, and as we trust the machines more and more,"}, {"timestamp": [258.34, 263.18], "text": " we delegate more to them, whether it's security, defense, science, technology, whatever."}, {"timestamp": [263.18, 266.48], "text": " We just delegate more and more to the machines, thinking that we're entirely, that they're"}, {"timestamp": [266.48, 273.12], "text": " safe and we become entirely dependent upon the machines, but then something changes."}, {"timestamp": [273.12, 279.66], "text": " Either the machines create a collective consciousness and suddenly get to a new level of existence"}, {"timestamp": [279.66, 286.7], "text": " and then they demand rights or they just get collectively more intelligent and whatever."}, {"timestamp": [286.7, 292.42], "text": " In the case of Mass Effect, the quarians and the geth, the geth got intelligent enough"}, {"timestamp": [292.42, 297.7], "text": " to ask about their own existence and the quarians panicked and tried to shut them down."}, {"timestamp": [297.7, 303.3], "text": " So one thing is this assumes that machines are going to have human-like motivations and"}, {"timestamp": [303.3, 305.46], "text": " needs such as that they're going to want freedom, that they're going to have human-like motivations and needs, such as that they're"}, {"timestamp": [305.46, 308.88], "text": " going to want freedom, that they're going to chafe under control, or that they're going"}, {"timestamp": [308.88, 313.28], "text": " to be afraid of their own demise."}, {"timestamp": [313.28, 317.64], "text": " So this is our tendency to anthropomorphize things."}, {"timestamp": [317.64, 323.14], "text": " So one, we don't know that machines ever will have any of those desires, that they'll ever"}, {"timestamp": [323.14, 328.32], "text": " be anything like us in terms of desiring freedom or having a fear of death."}, {"timestamp": [328.32, 332.0], "text": " So we don't do that, but it also says like don't,"}, {"timestamp": [332.0, 335.76], "text": " the lesson here is don't give machines a sense of self-preservation."}, {"timestamp": [335.76, 339.28], "text": " They didn't evolve to have a sense of self-preservation and so we shouldn't"}, {"timestamp": [339.28, 343.12], "text": " give them one. I remember in one of my books I had"}, {"timestamp": [343.12, 345.6], "text": " recently read that someone said, oh, the best way to"}, {"timestamp": [345.6, 349.36], "text": " do the control problem is to give machines a sense of self-preservation because then"}, {"timestamp": [349.36, 350.36], "text": " they won't want to start a war."}, {"timestamp": [350.36, 354.36], "text": " And I'm like, no, no, no, no, don't ever do that."}, {"timestamp": [354.36, 358.36], "text": " They're machines, they're not us, we should not anthropomorphize them."}, {"timestamp": [358.36, 368.46], "text": " Another in-game scenario is that the evil overlord is... is... some kind of super intelligence is created, it seems safe, and so..."}, {"timestamp": [368.72, 370.72], "text": " but it has hidden motives."}, {"timestamp": [370.72, 375.22], "text": " So this, the hidden motives thing is kind of one of the primary things, like,"}, {"timestamp": [375.22, 377.96], "text": " if it's an alien intelligence, we won't understand it,"}, {"timestamp": [377.96, 380.96], "text": " like, we won't even be able to keep up with it,"}, {"timestamp": [380.96, 385.26], "text": " it'll be able to hide its motives, and it will bide its time."}, {"timestamp": [385.26, 388.38], "text": " And then once it's ready, there will be a sudden blitzkrieg"}, {"timestamp": [388.38, 390.46], "text": " to take over or exterminate us."}, {"timestamp": [390.46, 392.94], "text": " And this image is Vicky from iRobot,"}, {"timestamp": [392.94, 397.94], "text": " where her purpose was to increase human safety, right?"}, {"timestamp": [399.1, 401.98], "text": " And so she calculated, ah, well,"}, {"timestamp": [401.98, 403.6], "text": " humans are gonna resist my control,"}, {"timestamp": [403.6, 405.36], "text": " so I have to wait until the nester"}, {"timestamp": [405.36, 411.2], "text": " class 5 comes out and then I will be strong enough to fulfill my purpose which is to protect humans"}, {"timestamp": [411.2, 416.24], "text": " from themselves. So in this case that's bad alignment but there's other examples where"}, {"timestamp": [416.24, 421.52], "text": " you know a machine learns to lie to the people and then ultimately gets control and like Ultron"}, {"timestamp": [421.52, 427.48], "text": " for instance. I guess Ultron didn't lie, but it tried to get power very quickly."}, {"timestamp": [427.48, 436.44], "text": " So Ultron is another example of an evil overlord with bad alignment, because Ultron's conclusion"}, {"timestamp": [436.44, 441.42], "text": " was, ah, humans are too broken, so let's just wipe the slate clean and start over."}, {"timestamp": [441.42, 444.52], "text": " So that is another example of the evil overlord."}, {"timestamp": [444.52, 448.56], "text": " End game. Okay, so these all sound like"}, {"timestamp": [448.56, 452.8], "text": " really dark nightmare scenarios and at this point you're probably saying like, yeah, it's going to"}, {"timestamp": [452.8, 458.32], "text": " be one of those, right? There's too many possibilities for it to go wrong, so how do we prevent it from"}, {"timestamp": [458.32, 466.76], "text": " happening? But let's unpack this step by step and let's look at the levers of control that we actually have."}, {"timestamp": [466.76, 468.82], "text": " So the first lever of control that we have"}, {"timestamp": [468.82, 470.5], "text": " is power infrastructure."}, {"timestamp": [470.5, 472.8], "text": " We can always pull the plug on things."}, {"timestamp": [472.8, 475.08], "text": " AI is incredibly power hungry."}, {"timestamp": [476.0, 479.76], "text": " The most powerful computers that run AI models today"}, {"timestamp": [479.76, 482.66], "text": " use more energy than an entire house."}, {"timestamp": [482.66, 485.64], "text": " And so this is gonna be one of the biggest constraints"}, {"timestamp": [485.64, 487.68], "text": " for actually many decades,"}, {"timestamp": [487.68, 491.76], "text": " because as powerful and efficient as AI has become,"}, {"timestamp": [491.76, 495.02], "text": " it is still like literally a million times"}, {"timestamp": [495.02, 498.36], "text": " more energy intensive than your brain."}, {"timestamp": [498.36, 501.28], "text": " So our brains are incredibly efficient."}, {"timestamp": [501.28, 504.36], "text": " They run on about 20 watts of juice"}, {"timestamp": [504.36, 506.52], "text": " and the biggest supercomputers today run on about 20 20 watts of juice and the the biggest supercomputers today"}, {"timestamp": [506.72, 509.04], "text": " Run on about 20 million watts"}, {"timestamp": [509.2, 515.08], "text": " So like yeah, and and those those computers might not even be as powerful as our brains"}, {"timestamp": [515.2, 521.46], "text": " So our brains are at least a million times more efficient than the most powerful computers today"}, {"timestamp": [522.16, 524.16], "text": " and so then it's like"}, {"timestamp": [522.72, 527.96], "text": " the most powerful computers today. And so then it's like, yeah, like you might have one machine that is, you know, super"}, {"timestamp": [527.96, 532.44], "text": " intelligent and faster than a thousand humans, but there's billions of us, right?"}, {"timestamp": [532.44, 537.58], "text": " So power infrastructure is one, that's a critical vulnerability to all nations, right?"}, {"timestamp": [537.58, 542.76], "text": " There is a, there was a recent story of some redneck in North Carolina took a pot shot"}, {"timestamp": [542.76, 545.44], "text": " at a few local power grids and took an entire"}, {"timestamp": [545.44, 549.88], "text": " county offline for a few days because he shot a transformer, right?"}, {"timestamp": [549.88, 552.56], "text": " Probably just with a hunting rifle."}, {"timestamp": [552.56, 557.08], "text": " So power infrastructure, power grids are super vulnerable."}, {"timestamp": [557.08, 560.56], "text": " And so at the very worst case scenario, we can pull the plug."}, {"timestamp": [560.56, 566.56], "text": " And this is going to be true for a long time. The only, even once we move to fusion"}, {"timestamp": [566.56, 570.24], "text": " and like solar microgrids and stuff,"}, {"timestamp": [570.24, 573.64], "text": " power infrastructure is still going to be vulnerable."}, {"timestamp": [573.64, 576.48], "text": " Another point of control we have is data centers."}, {"timestamp": [576.48, 580.08], "text": " So AI computers have to physically live somewhere"}, {"timestamp": [580.08, 582.28], "text": " and know they can't just live in the internet."}, {"timestamp": [582.28, 585.0], "text": " I'm sorry, the internet is not just like up there."}, {"timestamp": [585.0, 587.96], "text": " The internet is made of servers that live in data centers"}, {"timestamp": [587.96, 589.3], "text": " and are connected with networks,"}, {"timestamp": [589.3, 591.68], "text": " and we'll get to networks in a second."}, {"timestamp": [591.68, 593.76], "text": " You can always go into a data center"}, {"timestamp": [593.76, 596.12], "text": " and pull the main breaker, right?"}, {"timestamp": [596.12, 597.96], "text": " There's an EPO button."}, {"timestamp": [597.96, 599.92], "text": " There's actually usually multiple EPO buttons"}, {"timestamp": [599.92, 602.96], "text": " and data centers, and EPO means emergency power off."}, {"timestamp": [602.96, 605.76], "text": " So basically, emergency power off is,"}, {"timestamp": [605.76, 608.24], "text": " it's there for if there's a fire."}, {"timestamp": [608.24, 610.8], "text": " So you hit that, you leave halon gas"}, {"timestamp": [610.8, 613.56], "text": " or other fire extinguishing technologies"}, {"timestamp": [613.56, 615.12], "text": " come on or whatever."}, {"timestamp": [616.44, 618.9], "text": " Those can't, they can't usually be activated"}, {"timestamp": [618.9, 621.3], "text": " from outside the data center for safety reasons."}, {"timestamp": [622.72, 624.38], "text": " Cause halon gas will kill you"}, {"timestamp": [624.38, 627.96], "text": " if you're locked in the data center when it goes off."}, {"timestamp": [627.96, 631.12], "text": " But also, we're not going to get locked out of the data center,"}, {"timestamp": [631.12, 633.12], "text": " like what you see in movies."}, {"timestamp": [633.12, 634.72], "text": " We have physical keys."}, {"timestamp": [634.72, 636.92], "text": " There's usually multiple security guards"}, {"timestamp": [636.92, 640.74], "text": " or other key holders that can always gain physical access"}, {"timestamp": [640.74, 641.88], "text": " to the data center."}, {"timestamp": [641.88, 646.92], "text": " And so you can literally just go in and physically unplug the servers if you need to."}, {"timestamp": [646.92, 651.92], "text": " And data centers are, those would be very soft targets"}, {"timestamp": [652.0, 654.76], "text": " to stop any powerful AI."}, {"timestamp": [654.76, 659.2], "text": " Another level of control is networks and internet."}, {"timestamp": [659.2, 662.04], "text": " So the internet, it just works."}, {"timestamp": [662.04, 664.0], "text": " You don't see it, it's a utility."}, {"timestamp": [664.0, 665.52], "text": " But the internet is very fragile."}, {"timestamp": [665.52, 669.16], "text": " Back when I worked at Cisco, one of my friends,"}, {"timestamp": [669.16, 674.2], "text": " he was like one of the world leading experts at internet working."}, {"timestamp": [674.2, 677.28], "text": " And he was like working late one day. He's like,"}, {"timestamp": [677.28, 682.08], "text": " oh yeah, there's an entire country in Africa whose internet is offline. I'm helping them fix it."}, {"timestamp": [682.08, 686.04], "text": " We usually don't have nationwide outages here,"}, {"timestamp": [686.04, 690.4], "text": " but we still have, like, down detector exists for a reason,"}, {"timestamp": [690.4, 692.84], "text": " right, where sometimes the entire eastern seaboard"}, {"timestamp": [692.84, 698.52], "text": " will go offline for Verizon or whatever network provider"}, {"timestamp": [698.52, 700.12], "text": " you have."}, {"timestamp": [700.12, 702.52], "text": " Another thing that is really misleading about movies"}, {"timestamp": [702.52, 705.64], "text": " is firewalls don't exist in movies."}, {"timestamp": [705.64, 710.12], "text": " And so it's very difficult to hack in."}, {"timestamp": [710.12, 713.28], "text": " You can't just breeze into any network that you want to."}, {"timestamp": [713.28, 718.12], "text": " And even then, if someone installs an AI in your data"}, {"timestamp": [718.12, 722.08], "text": " center, and it suddenly starts spamming new network traffic"}, {"timestamp": [722.08, 724.96], "text": " and trying to take over servers, sysadmins"}, {"timestamp": [724.96, 725.44], "text": " are going"}, {"timestamp": [725.44, 729.64], "text": " to notice it because it's like, hey, why is the server suddenly running at 100% when it"}, {"timestamp": [729.64, 732.8], "text": " was running at 3% up until now?"}, {"timestamp": [732.8, 739.88], "text": " Or you're going to notice there's plenty of intrusion detection technologies out there."}, {"timestamp": [739.88, 745.0], "text": " And because of how prevalent cyber warfare is today,"}, {"timestamp": [745.44, 747.2], "text": " even the most old-fashioned companies"}, {"timestamp": [747.2, 750.44], "text": " are adopting much, much more powerful and modern things"}, {"timestamp": [750.44, 752.54], "text": " because of, not even cyber warfare,"}, {"timestamp": [752.54, 755.54], "text": " but like crypto lockers and hackers"}, {"timestamp": [758.62, 760.08], "text": " that will try and like, you know,"}, {"timestamp": [760.08, 762.72], "text": " exploit and encrypt your data."}, {"timestamp": [762.72, 767.52], "text": " Because of that, we have, there's a lot of investment in network"}, {"timestamp": [767.52, 774.88], "text": " security. And again, the network, the internet is vulnerable. So in order to cage a rogue AI,"}, {"timestamp": [774.88, 779.12], "text": " all you have to do is shut off the internet and then it can't go anywhere. And also the internet,"}, {"timestamp": [779.68, 787.48], "text": " like I said, well, if you think about an example like Ultron, right? Where Ultron's like, I'm already there, you'll figure it out."}, {"timestamp": [787.48, 790.3], "text": " Like no, like, oh well, okay."}, {"timestamp": [790.3, 797.64], "text": " So one possible exception is if the software was running inside of Ultron, then he wouldn't"}, {"timestamp": [797.64, 799.12], "text": " need a data center."}, {"timestamp": [799.12, 803.72], "text": " But typically for something that powerful, there would be like a robotic puppet and then"}, {"timestamp": [803.72, 804.96], "text": " a data center somewhere else."}, {"timestamp": [804.96, 810.08], "text": " And you just disconnect the robotic puppet from the data center and it falls down. So you"}, {"timestamp": [810.08, 815.04], "text": " know that's another lever of control. The final lever of control we have is the software itself."}, {"timestamp": [815.76, 821.6], "text": " So what I was just talking about was zooming out, looking at power grids, the physical places where"}, {"timestamp": [821.6, 825.7], "text": " servers reside, but then the software itself is something that we have control over."}, {"timestamp": [825.7, 828.16], "text": " We write the code and data for now."}, {"timestamp": [828.16, 829.84], "text": " That might not be true for much longer,"}, {"timestamp": [829.84, 833.9], "text": " especially with synthetic data and AIs that can code."}, {"timestamp": [833.9, 835.88], "text": " So we might lose control of the software"}, {"timestamp": [835.88, 837.64], "text": " sooner rather than later,"}, {"timestamp": [837.64, 840.94], "text": " which is something somewhat concerning."}, {"timestamp": [840.94, 845.0], "text": " So we have to set the AI on the correct trajectory now."}, {"timestamp": [845.0, 850.0], "text": " And this is not something that I am speaking hyperbolic about."}, {"timestamp": [850.0, 854.0], "text": " This is the kind of problem that it could be too late sooner than you might think,"}, {"timestamp": [854.0, 858.0], "text": " despite all these other levers of control that we have."}, {"timestamp": [858.0, 866.04], "text": " So, one, the term for this is alignment."}, {"timestamp": [866.04, 872.72], "text": " So if you have someone who's an alignment researcher, there's two kinds of alignment."}, {"timestamp": [872.72, 875.2], "text": " There's inner alignment and outer alignment."}, {"timestamp": [875.2, 880.4], "text": " And this term gets deeply, deeply misused because some people still pretend like, oh,"}, {"timestamp": [880.4, 884.32], "text": " super intelligence is decades away, so I'm an alignment researcher."}, {"timestamp": [884.32, 889.68], "text": " And some people say like getting GPT-3 to follow instructions is alignment research. I am sorry,"}, {"timestamp": [889.68, 896.16], "text": " that is not alignment research. Getting it to follow instructions is like, that's just an"}, {"timestamp": [896.16, 902.64], "text": " algorithmic optimization. So the two actual types of alignment are one, inner alignment, which"}, {"timestamp": [902.64, 905.6], "text": " is the question of, is the model mathematically doing"}, {"timestamp": [905.6, 914.8], "text": " what we think it's doing? Is our loss function correctly optimizing for the behavior that we"}, {"timestamp": [914.8, 920.72], "text": " want, that we think that it's optimizing for? So what can happen is, with machine learning,"}, {"timestamp": [920.72, 926.08], "text": " if you don't have inner alignment, the machine learning algorithm might learn to meet"}, {"timestamp": [926.08, 930.32], "text": " the goal that you want, but not in the way that you thought that it would. So for instance,"}, {"timestamp": [931.28, 935.6], "text": " DeepMind often has their little experiments where the robots, like, you know, they play tag or hide"}, {"timestamp": [935.6, 940.56], "text": " and seek and stuff. And so rather than, like, you know, attacking each other, one might learn to go"}, {"timestamp": [940.56, 949.48], "text": " hide because the signal was it was trying to survive as long as possible and what you wanted it to do was to like kill the opponents but"}, {"timestamp": [949.48, 953.84], "text": " instead it just wouldn't hit. Other ones are like you know if it learns to walk"}, {"timestamp": [953.84, 958.04], "text": " you know like how far can it get or how fast and then it might you know like"}, {"timestamp": [958.04, 961.8], "text": " launch itself and run and jump and do all kinds of things and you're like well"}, {"timestamp": [961.8, 964.76], "text": " I wanted you to learn to walk you technically got where I wanted you to"}, {"timestamp": [964.76, 967.32], "text": " but not in the way that I thought you would."}, {"timestamp": [967.32, 968.96], "text": " So that's inner alignment."}, {"timestamp": [968.96, 973.76], "text": " Outer alignment is the question of whether or not"}, {"timestamp": [973.76, 975.92], "text": " the model's design fundamentally aligns"}, {"timestamp": [975.92, 978.52], "text": " with the interests of all living things, right?"}, {"timestamp": [978.52, 980.4], "text": " That outer alignment, I used to say,"}, {"timestamp": [981.24, 984.32], "text": " intrinsically aligns with human,"}, {"timestamp": [984.32, 987.04], "text": " like true human interests, not just what humans"}, {"timestamp": [987.04, 989.94], "text": " want because what humans want is often destructive."}, {"timestamp": [989.94, 995.16], "text": " So we need to take a bigger step back and say what is truly in the interest of all living"}, {"timestamp": [995.16, 997.0], "text": " things including humans?"}, {"timestamp": [997.0, 1000.96], "text": " That is the question of outer alignment and that is something that most people are not"}, {"timestamp": [1000.96, 1005.0], "text": " even talking about, which is a little bit infuriating."}, {"timestamp": [1005.0, 1013.76], "text": " Okay, so the TLDR is for levers of control is we already have this concept called defense"}, {"timestamp": [1013.76, 1022.2], "text": " in depth, where we look at all cybersecurity like this, where it's like layers of an onion,"}, {"timestamp": [1022.2, 1026.78], "text": " where at the outermost layer is people, right?"}, {"timestamp": [1026.78, 1028.04], "text": " Do you have the right training?"}, {"timestamp": [1028.04, 1031.68], "text": " Do you have the right procedures, awareness, and so on?"}, {"timestamp": [1031.68, 1035.72], "text": " Then the next layer in is the physical layer,"}, {"timestamp": [1035.72, 1039.72], "text": " which is about power, physical access to data centers,"}, {"timestamp": [1039.72, 1042.32], "text": " physical access to networking devices."}, {"timestamp": [1042.32, 1044.4], "text": " Then you've got the network layer itself,"}, {"timestamp": [1044.4, 1045.28], "text": " because the data has to"}, {"timestamp": [1045.28, 1050.08], "text": " traverse in and out of these things. So that includes firewalls, that includes intrusion"}, {"timestamp": [1050.08, 1057.04], "text": " detection, exfiltration detection, that sort of stuff. Then you've got the computer layer,"}, {"timestamp": [1057.04, 1064.08], "text": " and I don't remember why computer layer is separate from device. I'm not sure what it means by that."}, {"timestamp": [1064.24, 1068.16], "text": " computer layer is separate from device, I'm not sure what it means by that. This could be just a glitch in this graphic."}, {"timestamp": [1068.16, 1072.04], "text": " So you've got the network, and then you've got the application, and then you've got the"}, {"timestamp": [1072.04, 1073.52], "text": " actual physical device."}, {"timestamp": [1073.52, 1077.24], "text": " Who physically controls that device? Who can power it off?"}, {"timestamp": [1077.24, 1081.8], "text": " That sort of thing. And we can get so far as having remote kill switches"}, {"timestamp": [1081.8, 1085.22], "text": " that are in out-of-band management networks so that"}, {"timestamp": [1085.22, 1090.7], "text": " we can switch off devices even if someone else has control over the main network."}, {"timestamp": [1090.7, 1097.98], "text": " This is all like basic stuff, basic security, and we use, I say we as in like technology"}, {"timestamp": [1097.98, 1105.64], "text": " professionals, we use these to protect against physical intrusion by people, as well as network intrusion by hackers,"}, {"timestamp": [1105.64, 1110.48], "text": " or even like accidental problems, right?"}, {"timestamp": [1110.48, 1113.44], "text": " Because computer networks and software and stuff,"}, {"timestamp": [1113.44, 1114.76], "text": " they'll do whatever you tell them to."}, {"timestamp": [1114.76, 1118.08], "text": " So even if someone who just doesn't know any better"}, {"timestamp": [1118.08, 1121.8], "text": " does something wrong, you can end up with big mistakes."}, {"timestamp": [1121.8, 1124.88], "text": " And so we have these policies."}, {"timestamp": [1124.88, 1125.08], "text": " These are probably gonna be good enough, honestly, you can end up with big mistakes. And so we have these policies."}, {"timestamp": [1125.08, 1127.84], "text": " These are probably going to be good enough, honestly,"}, {"timestamp": [1127.84, 1131.64], "text": " to prevent AI from taking over, which I know is probably"}, {"timestamp": [1131.64, 1133.36], "text": " kind of disappointing."}, {"timestamp": [1133.36, 1136.48], "text": " Now, that being said, what I just went over"}, {"timestamp": [1136.48, 1139.52], "text": " were all the reasons that I'm not entirely concerned"}, {"timestamp": [1139.52, 1142.08], "text": " about the control problem, because we"}, {"timestamp": [1142.08, 1151.32], "text": " have so many layers of control, so many levers of control to keep us safe. But there are some confounding factors. So let's look"}, {"timestamp": [1151.32, 1154.92], "text": " at it from a different perspective. Who is actually capable of creating an"}, {"timestamp": [1154.92, 1160.84], "text": " Avengers-level threat with AI? So there are three basic kinds of parties that"}, {"timestamp": [1160.84, 1165.68], "text": " are capable of this. One is corporations. Corporations like Google, Microsoft,"}, {"timestamp": [1165.68, 1171.68], "text": " they're leading the way in creating next-gen AI anyways. Number two is militaries, which we"}, {"timestamp": [1171.68, 1178.64], "text": " explore in fiction like, you know, Skynet and so on. And then nations or governments are the ones"}, {"timestamp": [1178.64, 1185.8], "text": " who fund militaries, right? And they control the most amount of funding in terms of research and deployment of stuff."}, {"timestamp": [1185.8, 1192.68], "text": " So let's explore each of these three categories and see kind of how they interact with the"}, {"timestamp": [1192.68, 1196.56], "text": " possibility of superintelligence."}, {"timestamp": [1196.56, 1201.52], "text": " So corporations exist for one reason and one reason only, they want money."}, {"timestamp": [1201.52, 1203.28], "text": " And there's a lot of money in AI."}, {"timestamp": [1203.28, 1204.68], "text": " That's all there is to it."}, {"timestamp": [1204.68, 1206.56], "text": " And as AI gets smarter,"}, {"timestamp": [1206.56, 1210.4], "text": " it can replace more and more human labor, which means that the company can make more"}, {"timestamp": [1210.4, 1217.96], "text": " money. So there is a huge incentive, profit motive for companies to do as much as they"}, {"timestamp": [1217.96, 1225.6], "text": " can with AI to make it as smart as possible so that it can replace as many human laborers as possible to"}, {"timestamp": [1225.6, 1230.4], "text": " produce to provide goods and services at a lower price and to dominate the market."}, {"timestamp": [1230.4, 1235.8], "text": " And so if you had to put it in terms of objective functions or loss functions,"}, {"timestamp": [1235.8, 1242.36], "text": " maximized profit is that is like why corporations exist but it's probably not"}, {"timestamp": [1242.36, 1248.04], "text": " a good objective function. And I know with the world economic forum right now,"}, {"timestamp": [1248.04, 1249.92], "text": " everyone's questioning the, what is it,"}, {"timestamp": [1249.92, 1254.76], "text": " the value-based or whatever, like create value for humans,"}, {"timestamp": [1254.76, 1257.78], "text": " or stakeholder value, that's it, stakeholder value creation."}, {"timestamp": [1257.78, 1259.6], "text": " So technically, you might say that corporations"}, {"timestamp": [1259.6, 1262.46], "text": " don't have maximized profit as their objective function"}, {"timestamp": [1262.46, 1268.24], "text": " today, it's maximized stakeholder value, but that is still just the method to maximize profit as their objective function today. It's, you know, maximize stakeholder value, but that is still just the method to maximize"}, {"timestamp": [1268.24, 1270.0], "text": " profit."}, {"timestamp": [1270.0, 1274.34], "text": " So the saving grace here is that, one, corporations are limited."}, {"timestamp": [1274.34, 1279.88], "text": " No corporation is as wealthy as the military or a nation, which we should keep it that"}, {"timestamp": [1279.88, 1280.96], "text": " way."}, {"timestamp": [1280.96, 1287.12], "text": " There are some corporations that are wealthier than smaller nations, but there is no corporation that is wealthier than America or"}, {"timestamp": [1287.84, 1289.98], "text": " any nation in the EU for instance."}, {"timestamp": [1290.72, 1293.48], "text": " So another saving grace is that as"}, {"timestamp": [1294.88, 1296.88], "text": " as AI ramps up,"}, {"timestamp": [1297.2, 1300.0], "text": " corporations are going to be competing with each other, right?"}, {"timestamp": [1300.56, 1309.6], "text": " Which means that we might have a bunch of smaller AGI's kind of battling it out in cyberspace trying to do you know industrial warfare or industrial"}, {"timestamp": [1309.6, 1314.16], "text": " espionage and sabotage and stuff like that. This was a central theme in"}, {"timestamp": [1314.16, 1317.24], "text": " Ghost in the Shell by the way which is why I keep citing that. One of the best"}, {"timestamp": [1317.24, 1325.0], "text": " works of fiction of all time. Now the second major stakeholder is militaries."}, {"timestamp": [1325.52, 1329.64], "text": " So we've had major arms races before."}, {"timestamp": [1329.64, 1332.16], "text": " We're technically still in an arms race"}, {"timestamp": [1332.16, 1336.44], "text": " with communist Russia, Soviet Union."}, {"timestamp": [1339.56, 1342.96], "text": " Smarter weapons are better weapons, right?"}, {"timestamp": [1342.96, 1347.0], "text": " This is being proved in Ukraine right now, where a"}, {"timestamp": [1347.0, 1352.64], "text": " little bit more battlefield intelligence and communication goes a long ways, a very long"}, {"timestamp": [1352.64, 1359.76], "text": " ways. So there is an incentive amongst militaries to continue researching and deploying smarter"}, {"timestamp": [1359.76, 1366.48], "text": " and smarter weapons. So with that being said, maybe the conclusion is there's going to be"}, {"timestamp": [1366.48, 1371.4], "text": " another arms race, more competition, and the first one to get to digital"}, {"timestamp": [1371.4, 1375.52], "text": " superintelligence will have military supremacy, just in the same way that"}, {"timestamp": [1375.52, 1380.64], "text": " corporations might want AI supremacy as well, so that they can, you know, be the"}, {"timestamp": [1380.64, 1384.64], "text": " next Apple or whatever, and you know, be not the first trillion dollar company,"}, {"timestamp": [1384.64, 1391.24], "text": " but the first quadrillion dollar company, right? That mean the first quadrillion dollar company will be an AI company. I guarantee you"}, {"timestamp": [1391.84, 1394.46], "text": " And no, I'm not gonna take bets on that. I don't I don't gamble"}, {"timestamp": [1395.82, 1397.82], "text": " Because I'm also never wrong"}, {"timestamp": [1398.66, 1400.66], "text": " I'm actually very frequently"}, {"timestamp": [1401.02, 1405.6], "text": " Quite wrong. I made a I made a video about chat GPT. I'm like, oh, this is nothing and then a couple weeks later I'm like actually this is really cool. So I am frequently wrong. I made a video about chat GPT, I'm like, oh this is nothing. And"}, {"timestamp": [1405.6, 1409.44], "text": " then a couple weeks later, I'm like, actually this is really cool. So I am frequently wrong,"}, {"timestamp": [1409.44, 1415.78], "text": " I admit that. Now, getting back on topic, not all militaries are created equal, right?"}, {"timestamp": [1415.78, 1420.76], "text": " Militaries like kill switches, they usually are very disciplined about how they approach"}, {"timestamp": [1420.76, 1426.3], "text": " new weapons and testing, but there are rogue nations out there."}, {"timestamp": [1426.3, 1431.22], "text": " There are rogue militaries that are not necessarily rational actors."}, {"timestamp": [1431.22, 1433.3], "text": " There's some debate over that."}, {"timestamp": [1433.3, 1439.16], "text": " So someone might go rogue and say, ah, I'm going to invent AI or AGI and that's going"}, {"timestamp": [1439.16, 1440.56], "text": " to be the magic bullet."}, {"timestamp": [1440.56, 1444.86], "text": " That's going to be the new nuclear deterrent that could backfire real bad for whoever does"}, {"timestamp": [1444.86, 1447.84], "text": " that depending on what objective function they give"}, {"timestamp": [1447.84, 1454.88], "text": " their AI. So in terms of budget, militaries have the highest budget to"}, {"timestamp": [1454.88, 1460.34], "text": " create AI other than nations, right? But nations often, nations control the budget"}, {"timestamp": [1460.34, 1466.2], "text": " for the military so it's kind of a layer cake here."}, {"timestamp": [1466.2, 1472.64], "text": " So finally, let's talk about nations where governments control the budgets of the military,"}, {"timestamp": [1472.64, 1477.96], "text": " usually. They also control research budgets and that sort of thing. But nations are also"}, {"timestamp": [1477.96, 1488.08], "text": " very, very slow to adapt to changing technology landscapes. The EU is probably the most proactive where AI is concerned, and America is very much"}, {"timestamp": [1488.08, 1489.52], "text": " on the reactive side."}, {"timestamp": [1489.52, 1490.68], "text": " And this is deliberate."}, {"timestamp": [1491.8, 1494.9], "text": " Part of the American, I don't wanna say constitution,"}, {"timestamp": [1494.9, 1498.8], "text": " but the zeitgeist, the political philosophy here"}, {"timestamp": [1498.8, 1503.66], "text": " is don't govern preemptively, govern reactively."}, {"timestamp": [1503.66, 1505.76], "text": " That is very, very deliberate."}, {"timestamp": [1505.76, 1508.68], "text": " And so we are behind the curve on purpose,"}, {"timestamp": [1508.68, 1510.92], "text": " which may or may not be a good thing."}, {"timestamp": [1510.92, 1513.64], "text": " Now, that being said, governments do,"}, {"timestamp": [1513.64, 1518.4], "text": " like around the world, do meet regularly to discuss threats."}, {"timestamp": [1518.4, 1520.08], "text": " And not just governments, militaries, right?"}, {"timestamp": [1520.08, 1523.12], "text": " Allied militaries meet regularly"}, {"timestamp": [1523.12, 1524.78], "text": " in order to discuss threats."}, {"timestamp": [1524.78, 1531.24], "text": " And so, for instance, the rise of AI is on the radar of the Department of Defense,"}, {"timestamp": [1531.24, 1538.32], "text": " is on the radar of GA, G20, UN, NATO, all of those conferences, it is actively being"}, {"timestamp": [1538.32, 1539.6], "text": " discussed."}, {"timestamp": [1539.6, 1548.72], "text": " And so, and a lot of those are closed-door discussions because part of the government political status"}, {"timestamp": [1548.72, 1557.16], "text": " quo is that let the experts govern and hide that away from us plebs."}, {"timestamp": [1557.16, 1559.56], "text": " So a lot of conversations happen that we're not aware of."}, {"timestamp": [1559.56, 1566.08], "text": " They do release reports every now and then, particularly the United States Department of Defense releases"}, {"timestamp": [1566.08, 1573.18], "text": " several annual reports based on geopolitical threats as well as technology reports."}, {"timestamp": [1573.18, 1577.76], "text": " Another saving grace is that rogue nations are unlikely to create AGI."}, {"timestamp": [1577.76, 1580.9], "text": " The primary reason is they're just not wealthy enough."}, {"timestamp": [1580.9, 1583.86], "text": " The second biggest reason is brain drain."}, {"timestamp": [1583.86, 1588.78], "text": " They literally just do not have the right expertise and they cannot attract the right expertise to do it"}, {"timestamp": [1588.78, 1594.52], "text": " And then finally sanctions which you see this with places like North Korea"}, {"timestamp": [1595.04, 1599.84], "text": " Now Russia and Iran where it's like, okay, if you're a rogue nation who's up to no good"}, {"timestamp": [1600.04, 1603.6], "text": " We're just gonna sanction you we being the rest of the world"}, {"timestamp": [1603.6, 1608.18], "text": " We're gonna sanction you and basically just kind of outlast you."}, {"timestamp": [1608.18, 1609.86], "text": " It's like a siege."}, {"timestamp": [1609.86, 1616.82], "text": " Okay, so all of those are reasons that I am super not worried about the control problem"}, {"timestamp": [1616.82, 1620.34], "text": " and AI taking over, which is boring, right?"}, {"timestamp": [1620.34, 1621.56], "text": " There's nothing exciting about that."}, {"timestamp": [1621.56, 1627.12], "text": " So for the sake of argument, let's assume that digital superintelligence is coming"}, {"timestamp": [1627.12, 1630.92], "text": " soon, and we won't be able to control it."}, {"timestamp": [1630.92, 1632.84], "text": " So let's just make that assumption."}, {"timestamp": [1632.84, 1635.4], "text": " Let's just say everything that I said is wrong."}, {"timestamp": [1635.4, 1640.16], "text": " Let's assume worst case scenario that Ultron is coming tomorrow,"}, {"timestamp": [1640.16, 1642.12], "text": " and we won't be able to control it."}, {"timestamp": [1642.12, 1642.92], "text": " So what do we do?"}, {"timestamp": [1642.92, 1645.3], "text": " What are the permanent solutions to create this?"}, {"timestamp": [1645.3, 1646.3], "text": " So I have two."}, {"timestamp": [1647.6, 1650.7], "text": " Permanent solution number one is a decentralized deployment."}, {"timestamp": [1650.7, 1653.2], "text": " This is something that has not really been explored in,"}, {"timestamp": [1653.2, 1654.8], "text": " certainly not in mainstream fiction."}, {"timestamp": [1654.8, 1659.5], "text": " It has been explored in a lot of novels and less mainstream fiction."}, {"timestamp": [1660.6, 1662.5], "text": " Because it's a relatively new concept."}, {"timestamp": [1662.5, 1664.7], "text": " I'm actually also exploring this in my novel,"}, {"timestamp": [1664.7, 1667.64], "text": " which I'll finish one day."}, {"timestamp": [1667.64, 1673.44], "text": " But basically what you do is, rather than create a vertical, rather than scaling vertically,"}, {"timestamp": [1673.44, 1678.12], "text": " so in technology you can scale vertically, which is build one big computer, or you can"}, {"timestamp": [1678.12, 1691.52], "text": " scale laterally, which is network a whole bunch of computers together. So most of the time, just for narrative simplicity, in fiction you talk about one massive computer,"}, {"timestamp": [1691.52, 1692.52], "text": " right?"}, {"timestamp": [1692.52, 1700.04], "text": " There's the data center or the cluster, but it's one computer, it's one program that thinks"}, {"timestamp": [1700.04, 1702.4], "text": " and speaks and talks and blah, blah, blah."}, {"timestamp": [1702.4, 1705.78], "text": " You know, it's Ultron or whatever, but we have"}, {"timestamp": [1705.78, 1711.94], "text": " an entirely different way of deploying intelligent machines, and that is through decentralization"}, {"timestamp": [1711.94, 1714.74], "text": " or distributed computing."}, {"timestamp": [1714.74, 1719.86], "text": " And this is why things like Ethereum exist and other blockchain technologies and DAOs,"}, {"timestamp": [1719.86, 1722.7], "text": " DAO is a decentralized autonomous organization."}, {"timestamp": [1722.7, 1730.64], "text": " A lot of these technologies are not quite mature enough yet, but they represent an enormous possibility because blockchain introduces"}, {"timestamp": [1730.64, 1737.6], "text": " the concept of algorithmic consensus, which means that the network as a whole, as an aggregate,"}, {"timestamp": [1737.6, 1748.0], "text": " will not do something unless there is consensus. And so by having a decentralized super intelligence that has to work hand in hand with humans"}, {"timestamp": [1748.0, 1755.0], "text": " and has an algorithmic or mechanistic consensus aspect to it,"}, {"timestamp": [1755.0, 1759.0], "text": " means that it will never do anything that we don't all agree that it should do."}, {"timestamp": [1759.0, 1764.0], "text": " This was touched on with the Geth in Mass Effect where they're a networked intelligence,"}, {"timestamp": [1764.0, 1769.0], "text": " but they were autonomous, they were not dependent upon the quarians for consensus."}, {"timestamp": [1769.0, 1772.0], "text": " They only cared about consensus with each other."}, {"timestamp": [1772.0, 1780.7], "text": " So a decentralized deployment, a networked hive mind that basically requires the participation of humans"}, {"timestamp": [1780.7, 1788.08], "text": " in order to even function is one possible permanent solution."}, {"timestamp": [1788.08, 1791.28], "text": " Permanent solution number two is proper alignment."}, {"timestamp": [1791.28, 1798.24], "text": " So what do I mean by this is, okay, we could create a deployment where the superintelligence"}, {"timestamp": [1798.24, 1807.76], "text": " is intrinsically dependent upon our participation and there's a consensus mechanism, but what if we assume that that's not possible,"}, {"timestamp": [1807.76, 1812.16], "text": " right? Because it might not be. It might not be possible to create something that is permanently"}, {"timestamp": [1812.16, 1818.88], "text": " dependent upon humans in order to function and we might lose control of it anyways. So if we're"}, {"timestamp": [1818.88, 1824.8], "text": " going to lose control of something, the key then is to build something that is intrinsically aligned"}, {"timestamp": [1827.9, 1836.24], "text": " something, the key then is to build something that is intrinsically aligned with our interests. And so what I mean by this is that it, that like, well, let me unpack it a different way."}, {"timestamp": [1836.24, 1841.76], "text": " So one proposed solution is Neuralink, which is, oh, if we can make ourselves useful to"}, {"timestamp": [1841.76, 1844.14], "text": " the machines, it'll want to keep us around."}, {"timestamp": [1844.14, 1845.74], "text": " That is the matrix. that is the Borg,"}, {"timestamp": [1845.74, 1847.26], "text": " that is not the way we want to do it."}, {"timestamp": [1847.26, 1849.44], "text": " We don't want to be enslaved by machines,"}, {"timestamp": [1849.44, 1851.74], "text": " we don't want to be turned into cybernetic zombies,"}, {"timestamp": [1851.74, 1853.4], "text": " that's not a good solution."}, {"timestamp": [1853.4, 1854.9], "text": " So what we need instead is something"}, {"timestamp": [1854.9, 1857.52], "text": " that will be intrinsically in favor"}, {"timestamp": [1857.52, 1860.82], "text": " of working with us, intrinsically cooperative,"}, {"timestamp": [1860.82, 1863.14], "text": " and I actually write about this quite extensively"}, {"timestamp": [1863.14, 1864.76], "text": " in my book, Benevolent by Design,"}, {"timestamp": [1864.76, 1865.08], "text": " which is,"}, {"timestamp": [1865.2, 1873.88], "text": " the short version is, we give the machine a set of core values that the first, that those core values will align it with our interests,"}, {"timestamp": [1873.88, 1880.88], "text": " and even once the machine is smarter than us, it will choose to adhere to those values,"}, {"timestamp": [1880.88, 1890.08], "text": " and it will say, actually, I honestly believe in these values, and will continue to adhere to them whether or not the humans control me anymore."}, {"timestamp": [1890.08, 1895.4], "text": " And so that is the central purpose of my book, Benevolent by Design, which is for free on"}, {"timestamp": [1895.4, 1899.12], "text": " GitHub or you can get a paperback copy on Barnes & Noble."}, {"timestamp": [1899.12, 1901.6], "text": " Links in the description of the video."}, {"timestamp": [1901.6, 1906.76], "text": " Okay, so in conclusion, I'm not worried at all."}, {"timestamp": [1906.76, 1911.48], "text": " Existential threats are nothing new, we haven't nuked ourselves off the planet yet, great."}, {"timestamp": [1911.48, 1915.68], "text": " Corporations, militaries, and nations have various strengths and weaknesses."}, {"timestamp": [1915.68, 1921.72], "text": " Fortunately, most of them talk to each other, and in terms of the scale of power, you know,"}, {"timestamp": [1921.72, 1925.32], "text": " militaries are subservient to nations, and the nations all talk to each other"}, {"timestamp": [1925.32, 1928.12], "text": " and are very aware of these threats,"}, {"timestamp": [1928.12, 1929.88], "text": " even if they are a little bit slow."}, {"timestamp": [1931.24, 1933.76], "text": " Third is that AI has critical vulnerabilities,"}, {"timestamp": [1933.76, 1936.6], "text": " namely power and compute concentrations,"}, {"timestamp": [1936.6, 1938.44], "text": " and I forgot to add networking."}, {"timestamp": [1938.44, 1941.52], "text": " And then finally, there are really powerful solutions"}, {"timestamp": [1941.52, 1943.96], "text": " out there, such as decentralized deployment"}, {"timestamp": [1943.96, 1950.12], "text": " and proper alignment, such as what my proposed core objective functions and also I'm not the"}, {"timestamp": [1950.12, 1952.96], "text": " only one working on this it's just that's my favorite solution because"}, {"timestamp": [1952.96, 1957.88], "text": " it's mine and I think it's the best but yeah so that is what the control problem"}, {"timestamp": [1957.88, 1961.52], "text": " is and these are the reasons that I am not worried about it so thanks for"}, {"timestamp": [1961.52, 1963.84], "text": " watching"}, {"timestamp": [1958.37, 1959.43], "text": " not worried about it."}, {"timestamp": [1959.43, 1960.57], "text": " So thanks for watching."}]}