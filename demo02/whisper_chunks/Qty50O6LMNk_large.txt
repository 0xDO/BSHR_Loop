{"text": " Hey everybody, David Shapiro here with a video update. So this is super exciting. Let me just go ahead and show you right off the bat what I'm working on. I apologize that it's small. This is, it's a very primitive graphical user interface. But as most of you are aware, chat GPT has been in, let's, shall we say, bad shape. Network errors, your history is gone, et cetera, et cetera. And for a lot of people, that means like your work comes to a grinding halt. That being said, I've been working on learning the chat GPT API. I've got access to GPT-4, not the 32,000 token one. If anyone knows how to request access to that, please let me know in the comments. Because working on a novel, I could easily make use of the 32,000 tokens. That being said, the 8,000 tokens will carry you a really long ways. So with that being said, this is the repo. It's chapter summarizer GPT-4, and it is already completely wrong because it provides developmental feedback pros feedback. And now you can actually chat with it and it will know your entire story. So how did I achieve this with the larger token window with the 8000 token window. That is enough that you can some you can summarize all your chapters and it will fit you generally within it. It probably won't fit by the time I finish my novel but hopefully I'll have access to the 32,000 token API endpoint which means that I'll be able to summarize the whole thing. And then what I did was I took the scratch pad here and I've got all this documented too because this is like the best instance of AutoMuse yet. So basically you copy and paste whatever you want into the scratch pad file. Whoops where did it go. So you copy it into scratch pad and you see it's 24 kilobytes. So this is a summary of the first 19 chapters of my story. And it's from here you know the chapters the text 19 chapters of my story. And it's from here, you know, the chapters, the text, the conversion, the chat logs. Oh, I wanna show you like, this is working, right? Like here's all the chat logs that I've got between myself and the Muse, which also means that you've got a permanent record even if chat GPT goes down. And all this is excluded in the in the git ignore so you see open AI your key is ignored chapters the summaries and the chat logs and the scratchpad so don't you don't have to worry about accidentally sharing any of your own any of your own work it'll just stay local in the repo when you copy it down now I was using chat GPT four to help write a graphical user interface because I was like, you know what? I could do this as a web thing, but honestly, I don't, like why have multiple apps running? Like why use a web browser when Python has a built-in graphical user interface? And so that's this here. So I was using chat-gpt to help make this and you can see like it dynamically sizes. I've got it color-coded so that you can just easily see what's going on. It also uses threading. So that way, cause it takes a while for the API to spit out something. But yeah, so let me show you the prompt that I use. So the scratch pad, this is just all the information about your story. I probably will get this automatically updated in the future, but for right now it's just manually copy-pasted. And then I have a new kind of prompt. Instead of a prompt, it's a system message. In this case it says, here let me zoom in, I'm a writing assistant and coach. The following is the summary of chapters for the novel the user is working on. My job is to help them develop their novel in any way that I can, including brainstorming, plotting, outlining, and planning. We may engage in developmental editing, which includes restructuring or modifying character arcs, plot threads, world building, and developing of themes. We may also work on finer-grained aspects, such as character backstories, subplots, and so on. I should refer to highly professional writing concepts, such as the three-act plot structure, the work of Joseph Campbell and C.G. Young, Save the Cat, and any other formalized approaches to storytelling. I should not seek to push the author, but rather teach them when I perceive gaps in their understanding. Above all else, my primary mission is to help them produce an amazing work of literature.\" And then so then you can populate whatever is in the scratch pad here. And again, you can dynamically update this if you want. Important note, the above material is provided for background information. This format is not necessarily intended to be used moving forward. Lastly, be sure to ask plenty of questions so that the author remains firmly in the driver's seat. So this huge constitution or purpose or system message actually works really well. So in this case, just right off the bat, I can say, hey, describe the main characters and identifies all four of the main characters and gives a very succinct definition of them. And actually, this would be really good back matter for the little blurb. So you can see that just with one message, it is able to read my entire story up to this point, well the summaries at least, and we can talk about it. Obviously I don't want to reveal too much more about my story, you guys know the main characters and a little bit about them, but yeah, so give it a try yourself, I'm super happy with this, and the biggest thing that it does is it saves time Because again, I've got I've got all of my chapters loaded into it and now I can just ask it whatever I need Also, I found this system message. It really cuts to the point. I'm just like hey, let's plan the scenes for the next chapter That's like, okay cool. Let's go It does it does do a little bit too much for me But the fact that you don't have to spend too much time going back and forth, I think is actually better because it will come up with an idea and say, hey, do you like this? Let's modify it. And then you can say, yes, let's modify it. So that's probably all you need to know, but now we can step through the function. So I did a nice clear demarcation. This is what I wrote. So just these three things, open file, save file, the chat GPT completion, which you can see it's using GPT-4. I do still have it nested in a while true loop, which is basically just to capture in case the API has an error, which it does all the time lately. And then I added this here because if the chat conversation gets too long, it will give you a maximum content length error. And so in that case, I just removed the oldest message, not a big deal. It actually, chatgpt4 said, why don't you actually measure the tokens and just remove it until it's done? But I couldn't actually find anything on this, the OpenAI API client, the underscore clean. I think that's a tokenizer, but I couldn't find any documentation and I didn't wanna try it just yet. But anyways, so that's there. Let's see, nothing else really changed here. I still have the exponential back off, which basically means that it'll wait longer and longer to retry until it will finally give up. And it's just because the API is wonky right now. Now, everything from here down was written by GPT-4 just with some feedback back and forth between myself and GPT-4. So basically it came up with the function to send the message, the thread to get the response. I asked it to ignore, like if you use Shift Enter, it will just insert the new line character, otherwise it'll send the message, because I realized that function is available in chat GPT on the web. And then finally, it did every, basically all it did was it took, I gave it this much, and you can see here where we populate the system message by loading what you have in the scratch pad. It then populates a system message based on this and starts the conversation with that. So the system role will be in index zero, meaning index one is going to be the oldest message in the conversation. index zero, meaning index one is going to be the oldest message in the conversation. So then it fires up the Tkinter GUI. Oh, and for anyone who's not familiar with Tkinter, that's what this is. It's a really brain dead simple, portable graphical user interface. And so then it constructs it all. There was a few problems when I first created it. So one, constructs it all. There was a few problems when I first created it. So one, the window was static, so it wouldn't allow you to expand. And then also while it was waiting for the API to respond, the whole thing would freeze and say not responding. But now it has a little output here that says, you know, AutoMuse is thinking. So welcome to AutoMuse. It gave the, you know, it's set to WordRap, it's, yeah, so basically it just, when I had an issue, I just said, okay, hey, can you make this modification? And away it goes. And then again, one thing that I haven't integrated is being able to search the previous logs, but because of the work that I've been doing on other topics, that'll be there. Now, it might not be relevant because once we get to the 32,000 token window limit, that's going to be like, what, like 100 pages worth of chat logs? You won't even need that much history. So you could load this entire history and most of it's going to be irrelevant. So anyways, we'll see. I did have a thought, one final thought, about the kinds of problems that we solve just by increasing context window is amazing. And so basically what I'm going to be doing from now on is I'm I'm going to be doing from now on is I'm not even going to try and force problems to fit into smaller context windows because 1. Larger context windows are coming and 2. The kinds of problems you are able to address once you have that larger amount of working memory completely changes the game. So I was talking with my fiance and some people on discord and what we're noticing is that we're getting more and more purpose-built models. So there are some open source models out there that have token windows up to 64,000 tokens, which is crazy. Some of them have 16,000 tokens, so on and so forth. So what we expect to happen 10,000 tokens, so on and so forth. So what we expect to happen is that we're gonna end up with many, many, many models, most of them open source, that are purpose-built for specific tasks. So for instance, some models are going to be focused on, they're gonna be like window-optimized models, right? So like we might eventually have a window-optimized model that can have a token window of like a thousand, not a thousand, a million tokens, right? But then we might also have compute optimized models or cognition optimized models, that sort of stuff. And some of the people in Discord are like, Oh, yeah, I'm already working on that. So that's the way that it's going. And then aside from that, just from a psychological perspective, my fianc\u00e9 pointed out that working memory is actually one of the biggest factors in intelligence and that by doubling the working memory, the effective working memory, by going from GPT-3 with a token count of 4000 to GPT-4 with a token window of 8,000, doubling the working memory that basically takes it up an order of magnitude or a standard deviation of IQ. So if GPT-3.5 had an IQ of about 115, doubling that window takes it up another 15 points to about 130. And then when you when you actually if you were to test its IQ based on speed it's probably already like in the 160s or something. And so there's there's from from a from a numbers perspective two of the universal features of intelligence that should figure into a good IQ test, one is processing. So IQ, most IQ tests are actually speed tests, which because speed is a good proxy for a lot of kinds of intelligence. So your mental speed is not your IQ, but it is a proxy for actual intelligence. And so when you measure the speed of GPT-3 and GPT-4, its IQ is actually really high. But then the other universal, one of the other universal things is working memory. So working memory is also a really good proxy for intelligence and many forms of intelligence. So when you combine the large working memory combined with the speed of reading and output, I would say GPT-4 probably has an effect IQ of around 130, if I had to guess. Now, that being said, it doesn't have permanent memory. You have to have a secondary system for that. But that paper came out that talked about how storage systems combined with LLMs are computationally complete. So I'm not worried about that. Yeah, so that's the direction it's going and that's where we're at right now. So I'm really excited to get my hands on the 32,000 token one. I think that that will also really help with the scientific research one. So I haven't made any progress on, oh, and I also wanted to show you how expensive it was. So yesterday it cost about $4 to $8 to summarize the first 60% of my novel, and then the conversations that I had about it. So even just reading all the summaries and talking about it cost another $2 and sixty two cents Which is still kind of expensive but in the grand scheme of things like it's so fast And it is still a lot more expensive than just the twenty dollars a month For the like unlimited chat GPT, but that price will come down. I'm not worried about that. But anyways, so Figuring out these chat interfaces and how to handle Large amounts of information and really how to make the most use of the system window, that is what I'm working on now and I will return to the, let's see, where is it? The yeah, the regenerative medicine thing. So I've been thinking about this one. Come on. So I got a bunch of papers, but the thing is, is these papers are unrelated. Wow, it's already been two weeks. Good grief. A lot of these papers are unrelated, which I think is kind of problematic, although you might think the idea that cross-pollinating stuff, like correlating stuff from unrelated papers could be a really good priming exercise. So anyways, as I'm getting more familiar with the chat GPT API and how to use it, I will come back to this because I realized like, yeah, I really want to help accelerate science as fast as possible because I'm really looking forward to the day where I can go to the doctor and just like get an outpatient injection and then all of my joints will feel better in a few months. Right? That's really what I want. Because, man, I do not have as much energy as I used to and I have to be really careful because I have old shoulder injuries from falling while snowboarding. There's so much that I can't do anymore just because I don't want to hurt myself anymore. Anyways, I'm complaining about getting old. It sucks. Everybody knows that. I think this is enough of the video. So thanks for watching. Cheers. Stay tuned. This is so much fun.", "chunks": [{"timestamp": [0.0, 5.0], "text": " Hey everybody, David Shapiro here with a video update."}, {"timestamp": [5.0, 6.96], "text": " So this is super exciting."}, {"timestamp": [6.96, 9.28], "text": " Let me just go ahead and show you right off the bat"}, {"timestamp": [9.28, 10.12], "text": " what I'm working on."}, {"timestamp": [10.12, 12.36], "text": " I apologize that it's small."}, {"timestamp": [12.36, 17.36], "text": " This is, it's a very primitive graphical user interface."}, {"timestamp": [18.16, 19.72], "text": " But as most of you are aware,"}, {"timestamp": [19.72, 24.1], "text": " chat GPT has been in, let's, shall we say, bad shape."}, {"timestamp": [25.36, 28.6], "text": " Network errors, your history is gone, et cetera, et cetera."}, {"timestamp": [28.6, 31.64], "text": " And for a lot of people, that means like your work"}, {"timestamp": [31.64, 33.6], "text": " comes to a grinding halt."}, {"timestamp": [33.6, 36.32], "text": " That being said, I've been working on learning"}, {"timestamp": [36.32, 37.96], "text": " the chat GPT API."}, {"timestamp": [37.96, 42.72], "text": " I've got access to GPT-4, not the 32,000 token one."}, {"timestamp": [42.72, 44.68], "text": " If anyone knows how to request access to that,"}, {"timestamp": [44.68, 46.76], "text": " please let me know in the comments."}, {"timestamp": [46.76, 51.96], "text": " Because working on a novel, I could easily make use of the 32,000 tokens."}, {"timestamp": [51.96, 57.46], "text": " That being said, the 8,000 tokens will carry you a really long ways."}, {"timestamp": [57.46, 60.52], "text": " So with that being said, this is the repo."}, {"timestamp": [60.52, 66.84], "text": " It's chapter summarizer GPT-4, and it is already completely wrong because it provides developmental feedback"}, {"timestamp": [66.84, 67.88], "text": " pros feedback."}, {"timestamp": [68.08, 69.36], "text": " And now you can actually chat"}, {"timestamp": [69.36, 69.88], "text": " with it"}, {"timestamp": [71.36, 73.52], "text": " and it will know your entire story."}, {"timestamp": [73.68, 75.48], "text": " So how did I achieve this"}, {"timestamp": [76.32, 78.32], "text": " with the larger"}, {"timestamp": [78.68, 79.48], "text": " token window"}, {"timestamp": [79.48, 81.2], "text": " with the 8000 token window."}, {"timestamp": [81.36, 83.06], "text": " That is enough that"}, {"timestamp": [83.06, 87.36], "text": " you can some you can summarize all your chapters"}, {"timestamp": [87.36, 89.96], "text": " and it will fit you generally within it."}, {"timestamp": [89.96, 92.66], "text": " It probably won't fit by the time I finish my novel"}, {"timestamp": [92.66, 96.48], "text": " but hopefully I'll have access to the 32,000 token API"}, {"timestamp": [96.48, 98.72], "text": " endpoint which means that I'll be able to summarize"}, {"timestamp": [98.72, 100.08], "text": " the whole thing."}, {"timestamp": [100.08, 103.36], "text": " And then what I did was I took the scratch pad here"}, {"timestamp": [103.36, 104.84], "text": " and I've got all this documented too"}, {"timestamp": [104.84, 107.68], "text": " because this is like the best instance of AutoMuse yet."}, {"timestamp": [108.24, 114.24], "text": " So basically you copy and paste whatever you want into the scratch pad file."}, {"timestamp": [114.92, 115.64], "text": " Whoops where did it go."}, {"timestamp": [116.0, 118.88], "text": " So you copy it into scratch pad and you see it's 24 kilobytes."}, {"timestamp": [119.2, 123.24], "text": " So this is a summary of the first 19 chapters of my story."}, {"timestamp": [124.96, 125.0], "text": " And it's from here you know the chapters the text 19 chapters of my story."}, {"timestamp": [127.12, 129.92], "text": " And it's from here, you know, the chapters, the text, the conversion, the chat logs."}, {"timestamp": [129.92, 132.4], "text": " Oh, I wanna show you like, this is working, right?"}, {"timestamp": [132.4, 134.88], "text": " Like here's all the chat logs that I've got"}, {"timestamp": [134.88, 137.44], "text": " between myself and the Muse,"}, {"timestamp": [137.44, 139.48], "text": " which also means that you've got a permanent record"}, {"timestamp": [139.48, 141.5], "text": " even if chat GPT goes down."}, {"timestamp": [141.5, 146.32], "text": " And all this is excluded in the in the git ignore so you see open"}, {"timestamp": [146.32, 151.4], "text": " AI your key is ignored chapters the summaries and the chat logs and the"}, {"timestamp": [151.4, 154.84], "text": " scratchpad so don't you don't have to worry about accidentally sharing any of"}, {"timestamp": [154.84, 160.52], "text": " your own any of your own work it'll just stay local in the repo when you copy it"}, {"timestamp": [160.52, 165.16], "text": " down now I was using chat GPT four"}, {"timestamp": [165.16, 167.8], "text": " to help write a graphical user interface"}, {"timestamp": [167.8, 169.5], "text": " because I was like, you know what?"}, {"timestamp": [170.42, 172.68], "text": " I could do this as a web thing,"}, {"timestamp": [172.68, 174.44], "text": " but honestly, I don't,"}, {"timestamp": [174.44, 176.44], "text": " like why have multiple apps running?"}, {"timestamp": [176.44, 179.72], "text": " Like why use a web browser"}, {"timestamp": [179.72, 183.28], "text": " when Python has a built-in graphical user interface?"}, {"timestamp": [183.28, 185.0], "text": " And so that's this here."}, {"timestamp": [185.0, 188.68], "text": " So I was using chat-gpt to help make this"}, {"timestamp": [188.68, 191.88], "text": " and you can see like it dynamically sizes."}, {"timestamp": [191.88, 193.96], "text": " I've got it color-coded"}, {"timestamp": [193.96, 196.86], "text": " so that you can just easily see what's going on."}, {"timestamp": [197.72, 199.44], "text": " It also uses threading."}, {"timestamp": [199.44, 201.8], "text": " So that way, cause it takes a while for the API"}, {"timestamp": [201.8, 204.36], "text": " to spit out something."}, {"timestamp": [204.36, 208.76], "text": " But yeah, so let me show you the prompt that I use."}, {"timestamp": [208.76, 212.28], "text": " So the scratch pad, this is just all the information about your story."}, {"timestamp": [212.28, 217.8], "text": " I probably will get this automatically updated in the future, but for right now it's just"}, {"timestamp": [217.8, 219.88], "text": " manually copy-pasted."}, {"timestamp": [219.88, 222.36], "text": " And then I have a new kind of prompt."}, {"timestamp": [222.36, 226.08], "text": " Instead of a prompt, it's a system message."}, {"timestamp": [226.08, 230.2], "text": " In this case it says, here let me zoom in, I'm a writing assistant and coach."}, {"timestamp": [230.2, 233.48], "text": " The following is the summary of chapters for the novel the user is working on."}, {"timestamp": [233.48, 237.68], "text": " My job is to help them develop their novel in any way that I can, including brainstorming,"}, {"timestamp": [237.68, 239.4], "text": " plotting, outlining, and planning."}, {"timestamp": [239.4, 243.28], "text": " We may engage in developmental editing, which includes restructuring or modifying character"}, {"timestamp": [243.28, 249.28], "text": " arcs, plot threads, world building, and developing of themes. We may also work on finer-grained aspects,"}, {"timestamp": [249.28, 253.92], "text": " such as character backstories, subplots, and so on. I should refer to highly professional"}, {"timestamp": [253.92, 258.64], "text": " writing concepts, such as the three-act plot structure, the work of Joseph Campbell and"}, {"timestamp": [258.64, 262.88], "text": " C.G. Young, Save the Cat, and any other formalized approaches to storytelling."}, {"timestamp": [262.88, 266.76], "text": " I should not seek to push the author, but rather teach them when I perceive gaps"}, {"timestamp": [266.76, 267.96], "text": " in their understanding."}, {"timestamp": [267.96, 269.76], "text": " Above all else, my primary mission"}, {"timestamp": [269.76, 273.28], "text": " is to help them produce an amazing work of literature.\""}, {"timestamp": [273.28, 275.8], "text": " And then so then you can populate whatever"}, {"timestamp": [275.8, 277.52], "text": " is in the scratch pad here."}, {"timestamp": [277.52, 280.72], "text": " And again, you can dynamically update this if you want."}, {"timestamp": [280.72, 282.26], "text": " Important note, the above material"}, {"timestamp": [282.26, 284.12], "text": " is provided for background information."}, {"timestamp": [284.12, 287.24], "text": " This format is not necessarily intended to be used moving forward."}, {"timestamp": [287.24, 291.04], "text": " Lastly, be sure to ask plenty of questions so that the author remains firmly in the driver's"}, {"timestamp": [291.04, 292.04], "text": " seat."}, {"timestamp": [292.04, 298.64], "text": " So this huge constitution or purpose or system message actually works really well."}, {"timestamp": [298.64, 303.84], "text": " So in this case, just right off the bat, I can say, hey, describe the main characters"}, {"timestamp": [303.84, 305.9], "text": " and identifies all four of the main characters"}, {"timestamp": [305.9, 309.88], "text": " and gives a very succinct definition of them."}, {"timestamp": [309.88, 312.48], "text": " And actually, this would be really good back matter"}, {"timestamp": [314.36, 318.06], "text": " for the little blurb."}, {"timestamp": [318.06, 320.84], "text": " So you can see that just with one message,"}, {"timestamp": [320.84, 327.52], "text": " it is able to read my entire story up to this point, well the summaries at least,"}, {"timestamp": [327.52, 329.4], "text": " and we can talk about it."}, {"timestamp": [329.4, 333.16], "text": " Obviously I don't want to reveal too much more about my story, you guys know the main"}, {"timestamp": [333.16, 340.88], "text": " characters and a little bit about them, but yeah, so give it a try yourself, I'm super"}, {"timestamp": [340.88, 345.3], "text": " happy with this, and the biggest thing that it does is it saves time"}, {"timestamp": [351.3, 352.04], "text": " Because again, I've got I've got all of my chapters loaded into it and now I can just ask it whatever I need"}, {"timestamp": [358.44, 360.44], "text": " Also, I found this system message. It really cuts to the point. I'm just like hey, let's plan the scenes for the next chapter That's like, okay cool. Let's go"}, {"timestamp": [360.6, 363.62], "text": " It does it does do a little bit too much for me"}, {"timestamp": [363.88, 365.84], "text": " But the fact that you don't have to spend"}, {"timestamp": [365.84, 371.48], "text": " too much time going back and forth, I think is actually better because it will come up"}, {"timestamp": [371.48, 373.72], "text": " with an idea and say, hey, do you like this?"}, {"timestamp": [373.72, 374.78], "text": " Let's modify it."}, {"timestamp": [374.78, 377.48], "text": " And then you can say, yes, let's modify it."}, {"timestamp": [377.48, 384.08], "text": " So that's probably all you need to know, but now we can step through the function."}, {"timestamp": [384.08, 387.04], "text": " So I did a nice clear demarcation."}, {"timestamp": [387.04, 388.8], "text": " This is what I wrote."}, {"timestamp": [388.8, 391.5], "text": " So just these three things,"}, {"timestamp": [391.5, 394.86], "text": " open file, save file, the chat GPT completion,"}, {"timestamp": [394.86, 397.0], "text": " which you can see it's using GPT-4."}, {"timestamp": [398.32, 402.02], "text": " I do still have it nested in a while true loop,"}, {"timestamp": [402.02, 403.94], "text": " which is basically just to capture"}, {"timestamp": [403.94, 408.2], "text": " in case the API has an error,"}, {"timestamp": [408.2, 410.14], "text": " which it does all the time lately."}, {"timestamp": [410.14, 415.14], "text": " And then I added this here because if the chat conversation"}, {"timestamp": [415.56, 419.04], "text": " gets too long, it will give you"}, {"timestamp": [419.04, 421.2], "text": " a maximum content length error."}, {"timestamp": [421.2, 424.4], "text": " And so in that case, I just removed the oldest message,"}, {"timestamp": [424.4, 425.5], "text": " not a big deal."}, {"timestamp": [427.2, 429.92], "text": " It actually, chatgpt4 said,"}, {"timestamp": [429.92, 431.96], "text": " why don't you actually measure the tokens"}, {"timestamp": [431.96, 434.76], "text": " and just remove it until it's done?"}, {"timestamp": [434.76, 436.6], "text": " But I couldn't actually find anything on this,"}, {"timestamp": [436.6, 440.84], "text": " the OpenAI API client, the underscore clean."}, {"timestamp": [442.8, 443.96], "text": " I think that's a tokenizer,"}, {"timestamp": [443.96, 445.88], "text": " but I couldn't find any documentation"}, {"timestamp": [445.88, 448.36], "text": " and I didn't wanna try it just yet."}, {"timestamp": [448.36, 449.88], "text": " But anyways, so that's there."}, {"timestamp": [450.72, 453.52], "text": " Let's see, nothing else really changed here."}, {"timestamp": [453.52, 455.58], "text": " I still have the exponential back off,"}, {"timestamp": [455.58, 457.86], "text": " which basically means that it'll wait longer and longer"}, {"timestamp": [457.86, 461.44], "text": " to retry until it will finally give up."}, {"timestamp": [463.24, 467.64], "text": " And it's just because the API is wonky right now."}, {"timestamp": [467.64, 471.96], "text": " Now, everything from here down was written by GPT-4"}, {"timestamp": [472.84, 474.92], "text": " just with some feedback back and forth"}, {"timestamp": [474.92, 477.8], "text": " between myself and GPT-4."}, {"timestamp": [477.8, 480.44], "text": " So basically it came up with the function"}, {"timestamp": [480.44, 483.96], "text": " to send the message, the thread to get the response."}, {"timestamp": [483.96, 488.48], "text": " I asked it to ignore, like if you use Shift Enter,"}, {"timestamp": [488.48, 491.32], "text": " it will just insert the new line character,"}, {"timestamp": [491.32, 493.6], "text": " otherwise it'll send the message,"}, {"timestamp": [493.6, 496.24], "text": " because I realized that function is available"}, {"timestamp": [496.24, 498.6], "text": " in chat GPT on the web."}, {"timestamp": [498.6, 501.84], "text": " And then finally, it did every,"}, {"timestamp": [501.84, 503.32], "text": " basically all it did was it took,"}, {"timestamp": [503.32, 505.76], "text": " I gave it this much, and you can see here where"}, {"timestamp": [505.76, 511.36], "text": " we populate the system message by loading what you have in the scratch pad. It then populates"}, {"timestamp": [511.36, 519.36], "text": " a system message based on this and starts the conversation with that. So the system role will"}, {"timestamp": [519.36, 524.8], "text": " be in index zero, meaning index one is going to be the oldest message in the conversation."}, {"timestamp": [524.8, 525.44], "text": " index zero, meaning index one is going to be the oldest message in the conversation."}, {"timestamp": [531.36, 539.2], "text": " So then it fires up the Tkinter GUI. Oh, and for anyone who's not familiar with Tkinter, that's what this is. It's a really brain dead simple, portable graphical user interface."}, {"timestamp": [539.2, 543.68], "text": " And so then it constructs it all. There was a few problems when I first created it. So one,"}, {"timestamp": [541.24, 543.32], "text": " constructs it all. There was a few problems when I first created it."}, {"timestamp": [543.32, 547.2], "text": " So one, the window was static,"}, {"timestamp": [547.2, 549.44], "text": " so it wouldn't allow you to expand."}, {"timestamp": [549.44, 552.56], "text": " And then also while it was waiting for the API to respond,"}, {"timestamp": [552.56, 555.0], "text": " the whole thing would freeze and say not responding."}, {"timestamp": [555.0, 557.44], "text": " But now it has a little output here that says,"}, {"timestamp": [557.44, 559.08], "text": " you know, AutoMuse is thinking."}, {"timestamp": [560.6, 562.92], "text": " So welcome to AutoMuse."}, {"timestamp": [562.92, 568.08], "text": " It gave the, you know, it's set to WordRap, it's, yeah,"}, {"timestamp": [568.08, 570.92], "text": " so basically it just, when I had an issue,"}, {"timestamp": [570.92, 573.68], "text": " I just said, okay, hey, can you make this modification?"}, {"timestamp": [573.68, 575.28], "text": " And away it goes."}, {"timestamp": [575.28, 578.24], "text": " And then again, one thing that I haven't integrated"}, {"timestamp": [578.24, 582.48], "text": " is being able to search the previous logs,"}, {"timestamp": [582.48, 588.8], "text": " but because of the work that I've been doing on other topics,"}, {"timestamp": [588.8, 589.8], "text": " that'll be there."}, {"timestamp": [589.8, 595.52], "text": " Now, it might not be relevant because once we get to the 32,000 token window limit, that's"}, {"timestamp": [595.52, 601.48], "text": " going to be like, what, like 100 pages worth of chat logs?"}, {"timestamp": [601.48, 608.0], "text": " You won't even need that much history. So you could load this entire history and"}, {"timestamp": [608.0, 612.0], "text": " most of it's going to be irrelevant. So anyways, we'll see."}, {"timestamp": [612.0, 616.0], "text": " I did have a thought, one final thought, about the kinds of"}, {"timestamp": [616.0, 620.0], "text": " problems that we solve just by increasing context window"}, {"timestamp": [620.0, 624.0], "text": " is amazing. And so basically what I'm going to be doing"}, {"timestamp": [624.0, 625.0], "text": " from now on is I'm I'm going to be doing from now on"}, {"timestamp": [625.0, 628.0], "text": " is I'm not even going to try and"}, {"timestamp": [628.0, 631.0], "text": " force problems to fit into smaller context windows"}, {"timestamp": [631.0, 634.0], "text": " because 1. Larger context windows are coming"}, {"timestamp": [634.0, 638.0], "text": " and 2. The kinds of problems you are able to address"}, {"timestamp": [638.0, 641.0], "text": " once you have that larger amount of working memory"}, {"timestamp": [641.0, 643.0], "text": " completely changes the game."}, {"timestamp": [643.0, 646.88], "text": " So I was talking with my fiance and some people on"}, {"timestamp": [646.88, 652.72], "text": " discord and what we're noticing is that we're getting more and more purpose-built models."}, {"timestamp": [652.72, 657.44], "text": " So there are some open source models out there that have token windows up to 64,000 tokens,"}, {"timestamp": [657.44, 663.76], "text": " which is crazy. Some of them have 16,000 tokens, so on and so forth. So what we expect to happen"}, {"timestamp": [662.24, 666.98], "text": " 10,000 tokens, so on and so forth. So what we expect to happen is that we're gonna end up"}, {"timestamp": [666.98, 670.6], "text": " with many, many, many models, most of them open source,"}, {"timestamp": [670.6, 673.8], "text": " that are purpose-built for specific tasks."}, {"timestamp": [673.8, 678.64], "text": " So for instance, some models are going to be focused on,"}, {"timestamp": [678.64, 681.2], "text": " they're gonna be like window-optimized models, right?"}, {"timestamp": [681.2, 684.3], "text": " So like we might eventually have a window-optimized model"}, {"timestamp": [684.3, 707.0], "text": " that can have a token window of like a thousand, not a thousand, a million tokens, right? But then we might also have compute optimized models or cognition optimized models, that sort of stuff. And some of the people in Discord are like, Oh, yeah, I'm already working on that. So that's the way that it's going. And then aside from that, just from a psychological perspective,"}, {"timestamp": [707.0, 714.0], "text": " my fianc\u00e9 pointed out that working memory is actually one of the biggest factors in intelligence"}, {"timestamp": [714.0, 718.0], "text": " and that by doubling the working memory, the effective working memory,"}, {"timestamp": [718.0, 726.78], "text": " by going from GPT-3 with a token count of 4000 to GPT-4 with a token window of 8,000, doubling the working"}, {"timestamp": [726.78, 733.62], "text": " memory that basically takes it up an order of magnitude or a standard deviation of IQ."}, {"timestamp": [733.62, 741.96], "text": " So if GPT-3.5 had an IQ of about 115, doubling that window takes it up another 15 points"}, {"timestamp": [741.96, 744.94], "text": " to about 130."}, {"timestamp": [744.94, 746.92], "text": " And then when you when you actually"}, {"timestamp": [746.92, 752.76], "text": " if you were to test its IQ based on speed it's probably already like in the"}, {"timestamp": [752.76, 758.28], "text": " 160s or something. And so there's there's from from a from a numbers"}, {"timestamp": [758.28, 765.36], "text": " perspective two of the universal features of intelligence that should figure into a good IQ test,"}, {"timestamp": [765.36, 767.62], "text": " one is processing."}, {"timestamp": [767.62, 772.62], "text": " So IQ, most IQ tests are actually speed tests,"}, {"timestamp": [773.08, 776.08], "text": " which because speed is a good proxy"}, {"timestamp": [776.08, 778.24], "text": " for a lot of kinds of intelligence."}, {"timestamp": [778.24, 781.08], "text": " So your mental speed is not your IQ,"}, {"timestamp": [781.08, 784.2], "text": " but it is a proxy for actual intelligence."}, {"timestamp": [784.2, 786.0], "text": " And so when you measure the speed"}, {"timestamp": [786.0, 793.68], "text": " of GPT-3 and GPT-4, its IQ is actually really high. But then the other universal, one of the"}, {"timestamp": [793.68, 801.76], "text": " other universal things is working memory. So working memory is also a really good proxy for"}, {"timestamp": [802.72, 805.08], "text": " intelligence and many forms of intelligence."}, {"timestamp": [805.08, 808.92], "text": " So when you combine the large working memory combined"}, {"timestamp": [808.92, 812.72], "text": " with the speed of reading and output,"}, {"timestamp": [812.72, 818.18], "text": " I would say GPT-4 probably has an effect IQ of around 130,"}, {"timestamp": [818.18, 819.64], "text": " if I had to guess."}, {"timestamp": [819.64, 822.28], "text": " Now, that being said, it doesn't have permanent memory."}, {"timestamp": [822.28, 824.56], "text": " You have to have a secondary system for that."}, {"timestamp": [824.56, 831.5], "text": " But that paper came out that talked about how storage systems combined with LLMs are computationally complete."}, {"timestamp": [831.5, 833.5], "text": " So I'm not worried about that."}, {"timestamp": [833.5, 839.5], "text": " Yeah, so that's the direction it's going and that's where we're at right now."}, {"timestamp": [839.5, 844.0], "text": " So I'm really excited to get my hands on the 32,000 token one."}, {"timestamp": [844.0, 845.4], "text": " I think that that will also really help"}, {"timestamp": [845.4, 847.72], "text": " with the scientific research one."}, {"timestamp": [847.72, 849.82], "text": " So I haven't made any progress on,"}, {"timestamp": [849.82, 852.04], "text": " oh, and I also wanted to show you how expensive it was."}, {"timestamp": [852.04, 855.54], "text": " So yesterday it cost about $4 to $8 to summarize"}, {"timestamp": [855.54, 857.96], "text": " the first 60% of my novel,"}, {"timestamp": [857.96, 860.42], "text": " and then the conversations that I had about it."}, {"timestamp": [860.42, 863.28], "text": " So even just reading all the summaries and talking about it"}, {"timestamp": [863.28, 865.34], "text": " cost another $2 and sixty two cents"}, {"timestamp": [865.4, 870.36], "text": " Which is still kind of expensive but in the grand scheme of things like it's so fast"}, {"timestamp": [870.36, 874.1], "text": " And it is still a lot more expensive than just the twenty dollars a month"}, {"timestamp": [874.72, 880.76], "text": " For the like unlimited chat GPT, but that price will come down. I'm not worried about that. But anyways, so"}, {"timestamp": [881.24, 884.08], "text": " Figuring out these chat interfaces and how to handle"}, {"timestamp": [884.6, 886.14], "text": " Large amounts of information"}, {"timestamp": [886.14, 891.04], "text": " and really how to make the most use of the system window, that is what I'm working on"}, {"timestamp": [891.04, 897.8], "text": " now and I will return to the, let's see, where is it?"}, {"timestamp": [897.8, 900.32], "text": " The yeah, the regenerative medicine thing."}, {"timestamp": [900.32, 903.24], "text": " So I've been thinking about this one."}, {"timestamp": [903.24, 904.24], "text": " Come on."}, {"timestamp": [904.24, 906.0], "text": " So I got a bunch of papers, but the thing is,"}, {"timestamp": [906.0, 908.0], "text": " is these papers are unrelated."}, {"timestamp": [908.0, 910.0], "text": " Wow, it's already been two weeks. Good grief."}, {"timestamp": [910.0, 913.0], "text": " A lot of these papers are unrelated,"}, {"timestamp": [913.0, 916.0], "text": " which I think is kind of problematic,"}, {"timestamp": [916.0, 918.0], "text": " although you might think the idea that"}, {"timestamp": [918.0, 920.0], "text": " cross-pollinating stuff,"}, {"timestamp": [920.0, 922.0], "text": " like correlating stuff from unrelated papers"}, {"timestamp": [922.0, 924.0], "text": " could be a really good priming exercise."}, {"timestamp": [924.0, 929.84], "text": " So anyways, as I'm getting more familiar with the chat GPT API and how to use it,"}, {"timestamp": [929.84, 934.72], "text": " I will come back to this because I realized like, yeah, I really want to help accelerate science as"}, {"timestamp": [934.72, 939.52], "text": " fast as possible because I'm really looking forward to the day where I can go to the doctor"}, {"timestamp": [939.52, 947.28], "text": " and just like get an outpatient injection and then all of my joints will feel better in a few months. Right? That's really what I want."}, {"timestamp": [947.28, 952.8], "text": " Because, man, I do not have as much energy as I used to and I have to be really careful"}, {"timestamp": [952.8, 956.16], "text": " because I have old shoulder injuries from falling while snowboarding."}, {"timestamp": [956.16, 960.48], "text": " There's so much that I can't do anymore just because I don't want to hurt myself anymore."}, {"timestamp": [960.48, 964.48], "text": " Anyways, I'm complaining about getting old. It sucks. Everybody knows that."}, {"timestamp": [964.48, 966.24], "text": " I think this is enough of the video."}, {"timestamp": [966.24, 972.64], "text": " So thanks for watching. Cheers. Stay tuned. This is so much fun."}]}