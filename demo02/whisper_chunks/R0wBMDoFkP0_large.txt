{"text": " Morning everybody, David Shapiro here with a surprise update. So yesterday, July 5th, this paper dropped. LongNet scaling transformers to 1 billion tokens. Now to put this in context, this is a 3 gigapixel image, which you can make sense of at a glance. image, which you can make sense of at a glance. And I'm not going to dig too deep into the cognitive neuroscience and neurological mechanisms about why you can make sense of so much information so quickly, but if you want to learn more about that, I recommend the forgetting machine. But what I want to point out is that you can take a glance at this image and then you can zoom in and you understand the implications of every little bit of this image. This is clearly an arid mountain range. There's a road going across it. There's some haze. There's a city in the background. You can keep track of all of that information at once just by glancing at this image. And then when you zoom in, you can say, oh, look, there's a nice house on the hillside. And you can keep track of that information in the context of this three gigapixel image. This is fundamentally what sparse attention is. And that is how this paper solves the problem of reading a billion tokens. So let's unpack this a little bit. First, I love this chart. This is a really hilarious flex right at the beginning of this paper, right under the abstract. They're like, okay, you know, 512, 12k, 64k tokens, 262, a million tokens, and then here's us, a billion tokens. So good job. Oh, also I want to point out, this is from Microsoft. This is not just from some podunk, you know, backwater university. This is Microsoft, you know, who's in partnership with OpenAI. And so I saw a post somewhere, I think it was a tweet or something, someone's like, Microsoft seems like they're really just falling down on AI research and I have no idea what rock they're living under. But pay attention to Microsoft. My money is on Microsoft and NVIDIA for the AI race. And then of course there's Google, but I don't understand Google's business model because they invented this stuff and then sat on it for seven years. So I have no idea what Google is doing. Anyway, sorry, I digress. Okay, billion tokens seems kind of out there, kind of hyperbolic, right? The chief innovation here is one, they have a training algorithm, which I don't care about that as much. I mean, distributed training, okay, lots of people have been working on that. But the chief innovation here is what they call dilation. So let me bring that up. So what they do is, let's see, hang on, where did it go? Where did it go? Where's the dilation diagram? All right. So what it allows it to do, dilated attention, sorry. So what dilated attention allows it to do is to zoom out and take in the entire sequence all at once, which controls the amount of memory and computation that it takes to take in that large sequence. Just like you and your brain, zooming in and out and keeping the entire context of this image in mind at the same time. And so the way that it does that is actually relatively similar to the way that human brains do it. Hang on, hang on, where did it go? I'm missing the diagram. Okay. So what they do is they create sparse representations. And those who have been following me for a while, you might remember when I came up with the idea of sparse priming representations. This is something to pay attention to. Because what I realized is that language models only need a few clues, just a few breadcrumbs to remind it to cue in as to what is going on in the message to what's going on in the memories. And this is actually why it's really good. You just give it a tiny chunk of text and it can infer the rest. Why? Because it has read so much that it is able to infer what came before that text and what came after. And so by zooming out and creating these really sparse representations of larger sequences, it can keep track of the entire thing. And what it does is it will take up to a billion token sequence, break it up, slice it up, and then makes layered sparse representations of the entire thing, and it will therefore be able to keep track of it. Now, okay, that sounds really nerdy, but here's what it does for the performance. So with this sparse representation, with this dilation, and doing it in massively in parallel, it solves a few problems. So one, you see that the runtime stayed under a thousand milliseconds, under one second, it's more about half a second, all the way up to a billion tokens. So because of that, it's basically zooming in and out of the text, the representation of the text that it creates, in the same way, a very similar way that your brain keeps track of a three gigapixel image as you zoom in and out. You're like, okay, cool, okay, I see a bunch of cars parked on the side of the road. And you can just remember that fact. Oh, let's do a quick test. What else do you remember about this image? Maybe you remember that the Hollywood sign is in the background over here somewhere. There it is. Oh, no, that's not it. But it's somewhere in here. So it's like, okay, based on the cars and the arid desert and that, I'm based, I'm guessing that this is Los Angeles, right? Anyways, point being is that, oh, there it is, Hollywood. So these are the Hollywood Hills and you can remember, oh yeah, there was a nice mansion over here, there's cars parked over here, that's probably downtown LA, the Hollywood sign is over here. So by keeping, by basically creating a mental map, this treats gigantic pieces of information not unlike a diffusion model. And because I got, I was clued in on that when I looked at the way that it was mapping everything. And I was like, hold on, it's creating a map of the text by just breaking it down algorithmically and saying, okay, let's just make a scatterpl plot of all the text here. Or scatter plot is not the right word, but it's basically making a bitmap of the text, of the representations of what is going on in the sequence. And I'm like, okay, this is a fundamentally different approach to representing text. And this is also really similar to some of the experiments that I've done. If you remember Remo rolling episodic memory organizer which creates layers of abstraction, this does it algorithmically in real time. So this just blows everything that I've done with memory research completely out of the water. It also has the ability to basically kind of summarize as it goes, and that's not necessarily the right word, because summarization means that you take one piece of text and create a smaller piece of text. But this creates a neural summarization, a neural network summarization by creating these layers of abstraction. And this allows it to zoom in and out as it needs to so that it can cast its attention around internally in order to keep track of such a long sequence. Now, okay great, what does this mean? As someone who has been using GPT since GPT-2 where it was basically just a sentence transformer, it couldn't do a whole lot more. You know, like in this model up here the original GPT was 512 tokens and GP GPT2 I think was what, a thousand? I don't remember. Maybe it was 512 as well. And then the initial version of GPT3 was 2,000 tokens. We got upgraded to 4,000 tokens. Then we got GPT3.5 and GPT4, so we're at 8,000 and 16,000 tokens. As these attention mechanisms get bigger, and as the context window gets bigger, one thing that I've noticed is that there are, one, these are step changes in terms of algorithmic efficiencies, but in terms of what they are capable of doing. As I tell a lot of my consultation clients, do not ever try and get around the context window limitation because 1. A new model is coming out within 6 months that's going to completely blow open that window and 2. It's just a limitation of the model. So when you can read a billion tokens, which by the way, humans read about 1-2 billion words in their entire lifetime. When you have a model that can read a billion tokens in a second, that is half a lifetime worth of reading and knowledge that this model can take in in a second. So when you have a model that can ingest that much information, suddenly retraining models doesn't matter. You just give it the log of all news, all events, all papers, whatever task that you're doing, you just give it all of it at once and it can keep track of all of that text in its head, in its virtual head, all at once and it can pay attention to the bits that it needs to with those sparse representations. It is impossible for me to oversell the long-term ramifications of these kinds of algorithmic changes. And so a couple months ago when I said AGI within 18 months, this is the kind of trend that I was paying attention to. There is no limit to the algorithmic breakthroughs we are seeing right now. Now that doesn't mean that there won't eventually be diminishing returns, but at the same time, we are exploring this blue ocean space and we've... Alright, for those of you that have played Skyrim and other RPGs, we unlocked a new map and the grayed out area is this big and we've explored this much of this new map. That is how much potential there is to explore out here. And the other thing is, this research is accelerating. There's a few reasons for that. On one of the live streams, someone asked me, like, how do we know that this isn't an AI winter? And I pulled up a chart that showed an exponential growth of investment. Where the money goes, the research goes. And because the money is flowing into the research, it's happening. What you, and we saw the same thing with solar and literally every other disruptive technology, is once the investment comes, you know that the breakthroughs are gonna follow. It's just that simple. And this is one of those kinds of breakthroughs. So what does this mean? Put it this way. Rather than trying to play Tetris with memory and trying to fit 10 pounds of stuff into a 5 pound bag, now once this becomes commercially ready, which it's coming, it's possible on paper, they did it, so even if we don't get a billion tokens this time next year, it's coming. What this allows you to do is, let's say for instance you are working on a medical research thing, and it's like, okay, well, you know, we've got a literature review of literally 2,000 papers per month to read. Put all the papers in this model and say say tell me exactly which papers are most relevant. So the ability for in-context learning is incredible and it can hold more in its brain, in its mind, than any 10 humans can. And this is, again, this is not the limit. Imagine a year from now or six months from now when LongNet 2 comes out and it's a trillion tokens or 10 trillion tokens. And what they say in this paper is that maybe we're going to see a point very soon where it could have its context window could include basically the entire internet. This is a step towards superintelligence. Make no mistake that the ability to held and use that much information in real time to produce plans, to forecast, to anticipate, to come up with insights, this is a critical step towards digital superintelligence. I am not being hyperbolic here and neither is this paper when they say we could conceivably build a model that can read the entire internet in one go. So with all that being said, I wanted to pivot and talk briefly about OpenAI's announcement also yesterday that they are introducing super alignment. So the TLDR is that OpenAI is creating a dedicated team to aligning superintelligence, which, again, I am super glad that we are living in the timeline where someone is doing this. It's about time. I've got my book out there, Benevolent by Design, where I talked about aligning superintelligence. And my solution is that you really can't. But one thing that I want to point out is that whether or not you can align one model in the lab is, that's part of it, that's a necessary part of the solution. I don't want to disparage the engineers and scientists at OpenAI and Microsoft and other places working on this. But while it is a necessary component of the solution, it is not a complete solution. And this is where researchers like Gary Marcus and Dr. Raman Chowdhury have testified to Congress saying, look, they expect that open source models will reach parity with closed source models and then overtake them. And so when open source models who anyone can deploy are aligned any which way that you want, you lose total control. So that while I definitely appreciate and value, because we need to know how to align super intelligent models. The good guys, the aligned models, need to be as powerful as all the unaligned models, because in the AI arms race it's going to be AI versus AI. In the example of cybersecurity where we already have adaptive intelligence in in firewalls and other security appliances basically, you're going to have you know an AI agent running in your firewall versus an AI based DDoS attack. Just as one example, you're going to have AI based infiltration programs versus AI hunter DDoS attack. Just as one example, you're gonna have AI based infiltration programs versus AI hunter programs on the inside. So it's gonna be spy versus spy. And so we need to make sure that the models that we build that do remain aligned, that we do remain control over, are as smart as possible and also trustworthy. Absolutely needs to happen. Basically you fight fire with fire. And I know that that sounds like mutually assured destruction and it kind of is, which is another reason that the nuclear arms race metaphor is very apt for the AI arms race. So this research absolutely needs to happen but what I want to drive home is that it is a necessary but not sufficient set of solutions. That there also needs to be the adoption, the implementation, and deployment of aligned systems. And we also need to make sure that those aligned systems can communicate and collaborate together. So with all that being said, big steps in the right direction, but it is coming faster than anyone realizes. And I stand by my assertion, AGI by the end of 2024. Actually, by the mid, basically, let's see, September or October 2024, any definition that you have of AGI will be satisfied. And then from there, it's a very, very, very short period of time to superintelligence. Now, fortunately for us right now, the only computers capable of running these models and researching them are like the NVIDIA supercomputers that they're building. So that but that barrier that threshold is going to start going down because remember NVIDIA at their keynote speech and and for several months they've been saying hey you know our machines are literally a million times more powerful in the last decade and we're going to do it again in the next decade. Well, when your desktop computer is as powerful as today's supercomputers in 10 years, you're going to be able to run all of these and then when you combine that with the ongoing algorithmic efficiencies, everyone is going to be running their own AGI within 5 to 10 years. efficiencies, everyone is going to be running their own AGI within five to ten years. Mark my words. So time is of the essence. We do need a sense of urgency and I am really glad that OpenAI is doing this. Again, you know, I'm not... I would like to see more governmental participation, more universities. I would like to see something like a GAIA agency, a global AI agency, or an Aegis agency, an alignment enforcement for global intelligence systems. Because the thing is, is corporations and governments are not ready for this. And that to me is the biggest risk because from a purely scientific standpoint, I 100% believe that we can align superintelligence. I wrote a book about it. I demonstrated how you can take unaligned models and align them to universal principles very, very easily. I've done it plenty of times. The data sets are out there for free. Just search for heuristic imperatives and core objective functions on my GitHub. But again, aligning a single model is not the entire solution. You also need the deployment. You need the security models. We need to update things like the OSI model and defense in depth. We need to look at the entire technology stack, but we also need to look at the entire economic and governmental stack to make sure that companies are aware of this and that companies start deploying these systems, whether it's security checkpoints, whether it's internal policies, that sort of thing. Because when you've got a really powerful cannon, you have to aim that cannon really, really well. Otherwise, it's going to kill everybody. And again, you all know me, I am a very, very, very optimistic person when it comes to alignment and the future that we can build. But at the same time, when you're playing with fire, you need to make sure that you wear the proper safety gear. Because the more energy something has, the more dangerous it is. And the level of energy or intelligence or however you want to look at it, whatever metaphor you want to pick, is going up very quickly. So thanks for watching. I hope you got a lot out of this. It's the long net paper and then of course introducing super alignment. But yeah, thanks for watching. Cheers.", "chunks": [{"timestamp": [0.0, 7.92], "text": " Morning everybody, David Shapiro here with a surprise update. So yesterday, July 5th,"}, {"timestamp": [7.92, 17.68], "text": " this paper dropped. LongNet scaling transformers to 1 billion tokens. Now to put this in context,"}, {"timestamp": [18.56, 24.16], "text": " this is a 3 gigapixel image, which you can make sense of at a glance."}, {"timestamp": [28.88, 30.08], "text": " image, which you can make sense of at a glance. And I'm not going to dig too deep into the"}, {"timestamp": [34.72, 35.36], "text": " cognitive neuroscience and neurological mechanisms about why you can make sense"}, {"timestamp": [38.64, 47.02], "text": " of so much information so quickly, but if you want to learn more about that, I recommend the forgetting machine. But what I want to point out is that you can take a glance at this image and then you can"}, {"timestamp": [47.02, 53.24], "text": " zoom in and you understand the implications of every little bit of this image."}, {"timestamp": [53.24, 55.38], "text": " This is clearly an arid mountain range."}, {"timestamp": [55.38, 56.98], "text": " There's a road going across it."}, {"timestamp": [56.98, 58.24], "text": " There's some haze."}, {"timestamp": [58.24, 59.9], "text": " There's a city in the background."}, {"timestamp": [59.9, 70.4], "text": " You can keep track of all of that information at once just by glancing at this image. And then when you zoom in, you can say, oh, look, there's a nice house on the hillside."}, {"timestamp": [70.4, 76.46], "text": " And you can keep track of that information in the context of this three gigapixel image."}, {"timestamp": [76.46, 81.56], "text": " This is fundamentally what sparse attention is."}, {"timestamp": [81.56, 86.72], "text": " And that is how this paper solves the problem of reading a billion tokens."}, {"timestamp": [87.68, 94.8], "text": " So let's unpack this a little bit. First, I love this chart. This is a really hilarious flex right"}, {"timestamp": [94.8, 101.36], "text": " at the beginning of this paper, right under the abstract. They're like, okay, you know, 512, 12k,"}, {"timestamp": [101.36, 107.76], "text": " 64k tokens, 262, a million tokens, and then here's us, a billion tokens."}, {"timestamp": [107.76, 108.76], "text": " So good job."}, {"timestamp": [108.76, 111.92], "text": " Oh, also I want to point out, this is from Microsoft."}, {"timestamp": [111.92, 115.88], "text": " This is not just from some podunk, you know, backwater university."}, {"timestamp": [115.88, 121.02], "text": " This is Microsoft, you know, who's in partnership with OpenAI."}, {"timestamp": [121.02, 124.36], "text": " And so I saw a post somewhere, I think it was a tweet or something, someone's like,"}, {"timestamp": [124.36, 128.5], "text": " Microsoft seems like they're really just falling down on AI research and I have no idea what"}, {"timestamp": [128.5, 129.58], "text": " rock they're living under."}, {"timestamp": [129.58, 132.54], "text": " But pay attention to Microsoft."}, {"timestamp": [132.54, 136.48], "text": " My money is on Microsoft and NVIDIA for the AI race."}, {"timestamp": [136.48, 141.0], "text": " And then of course there's Google, but I don't understand Google's business model because"}, {"timestamp": [141.0, 143.72], "text": " they invented this stuff and then sat on it for seven years."}, {"timestamp": [143.72, 145.28], "text": " So I have no idea what Google is doing."}, {"timestamp": [145.28, 147.68], "text": " Anyway, sorry, I digress."}, {"timestamp": [147.68, 156.44], "text": " Okay, billion tokens seems kind of out there, kind of hyperbolic, right?"}, {"timestamp": [156.44, 162.2], "text": " The chief innovation here is one, they have a training algorithm, which I don't care about"}, {"timestamp": [162.2, 163.2], "text": " that as much."}, {"timestamp": [163.2, 166.0], "text": " I mean, distributed training, okay, lots of people have been working on that."}, {"timestamp": [166.0, 169.16], "text": " But the chief innovation here is what they call dilation."}, {"timestamp": [169.16, 172.52], "text": " So let me bring that up."}, {"timestamp": [172.52, 175.56], "text": " So what they do is, let's see, hang on, where did it go?"}, {"timestamp": [175.56, 176.56], "text": " Where did it go?"}, {"timestamp": [176.56, 177.56], "text": " Where's the dilation diagram?"}, {"timestamp": [177.56, 178.56], "text": " All right."}, {"timestamp": [178.56, 182.08], "text": " So what it allows it to do, dilated attention, sorry."}, {"timestamp": [182.08, 186.32], "text": " So what dilated attention allows it to do is to zoom out"}, {"timestamp": [186.32, 189.64], "text": " and take in the entire sequence all at once,"}, {"timestamp": [189.64, 194.36], "text": " which controls the amount of memory and computation"}, {"timestamp": [194.36, 197.12], "text": " that it takes to take in that large sequence."}, {"timestamp": [197.12, 201.28], "text": " Just like you and your brain, zooming in and out"}, {"timestamp": [201.28, 210.3], "text": " and keeping the entire context of this image in mind at the same time."}, {"timestamp": [210.3, 217.46], "text": " And so the way that it does that is actually relatively similar to the way that human brains"}, {"timestamp": [217.46, 218.46], "text": " do it."}, {"timestamp": [218.46, 219.46], "text": " Hang on, hang on, where did it go?"}, {"timestamp": [219.46, 221.5], "text": " I'm missing the diagram."}, {"timestamp": [221.5, 222.5], "text": " Okay."}, {"timestamp": [222.5, 226.36], "text": " So what they do is they create sparse representations."}, {"timestamp": [226.36, 229.72], "text": " And those who have been following me for a while, you might remember when I came up with"}, {"timestamp": [229.72, 231.8], "text": " the idea of sparse priming representations."}, {"timestamp": [231.8, 234.76], "text": " This is something to pay attention to."}, {"timestamp": [234.76, 239.88], "text": " Because what I realized is that language models only need a few clues, just a few breadcrumbs"}, {"timestamp": [239.88, 246.0], "text": " to remind it to cue in as to what is going on in the message to what's going on in the memories."}, {"timestamp": [246.0, 248.0], "text": " And this is actually why it's really good."}, {"timestamp": [248.0, 251.88], "text": " You just give it a tiny chunk of text and it can infer the rest."}, {"timestamp": [251.88, 252.88], "text": " Why?"}, {"timestamp": [252.88, 257.32], "text": " Because it has read so much that it is able to infer what came before that text and what"}, {"timestamp": [257.32, 258.78], "text": " came after."}, {"timestamp": [258.78, 265.0], "text": " And so by zooming out and creating these really sparse representations of larger sequences,"}, {"timestamp": [265.0, 267.0], "text": " it can keep track of the entire thing."}, {"timestamp": [267.0, 274.0], "text": " And what it does is it will take up to a billion token sequence,"}, {"timestamp": [274.0, 276.0], "text": " break it up, slice it up,"}, {"timestamp": [276.0, 281.0], "text": " and then makes layered sparse representations of the entire thing,"}, {"timestamp": [281.0, 283.0], "text": " and it will therefore be able to keep track of it."}, {"timestamp": [283.0, 285.68], "text": " Now, okay, that sounds really nerdy, but"}, {"timestamp": [286.88, 292.32], "text": " here's what it does for the performance. So with this sparse representation, with this dilation,"}, {"timestamp": [293.12, 300.4], "text": " and doing it in massively in parallel, it solves a few problems. So one, you see that the runtime"}, {"timestamp": [300.4, 305.88], "text": " stayed under a thousand milliseconds, under one second, it's more about half a"}, {"timestamp": [305.88, 311.4], "text": " second, all the way up to a billion tokens. So because of that, it's basically"}, {"timestamp": [311.4, 314.68], "text": " zooming in and out of the text, the representation of the text that it"}, {"timestamp": [314.68, 319.24], "text": " creates, in the same way, a very similar way that your brain keeps track of a"}, {"timestamp": [319.24, 323.04], "text": " three gigapixel image as you zoom in and out. You're like, okay, cool, okay, I see a"}, {"timestamp": [323.04, 329.52], "text": " bunch of cars parked on the side of the road. And you can just remember that fact. Oh, let's do a quick test."}, {"timestamp": [329.52, 333.28], "text": " What else do you remember about this image? Maybe you remember that the Hollywood sign"}, {"timestamp": [333.28, 338.04], "text": " is in the background over here somewhere. There it is. Oh, no, that's not it. But it's"}, {"timestamp": [338.04, 343.04], "text": " somewhere in here. So it's like, okay, based on the cars and the arid desert and that,"}, {"timestamp": [343.04, 346.36], "text": " I'm based, I'm guessing that this is Los Angeles,"}, {"timestamp": [346.36, 347.36], "text": " right?"}, {"timestamp": [347.36, 350.84], "text": " Anyways, point being is that, oh, there it is, Hollywood."}, {"timestamp": [350.84, 355.12], "text": " So these are the Hollywood Hills and you can remember, oh yeah, there was a nice mansion"}, {"timestamp": [355.12, 359.56], "text": " over here, there's cars parked over here, that's probably downtown LA, the Hollywood"}, {"timestamp": [359.56, 360.56], "text": " sign is over here."}, {"timestamp": [360.56, 365.78], "text": " So by keeping, by basically creating a mental map, this treats gigantic pieces"}, {"timestamp": [365.78, 372.3], "text": " of information not unlike a diffusion model. And because I got, I was clued in on that"}, {"timestamp": [372.3, 377.34], "text": " when I looked at the way that it was mapping everything. And I was like, hold on, it's"}, {"timestamp": [377.34, 382.62], "text": " creating a map of the text by just breaking it down algorithmically and saying, okay,"}, {"timestamp": [382.62, 386.24], "text": " let's just make a scatterpl plot of all the text here."}, {"timestamp": [391.2, 395.6], "text": " Or scatter plot is not the right word, but it's basically making a bitmap of the text, of the representations of what is going on in the sequence. And I'm like, okay,"}, {"timestamp": [396.24, 400.16], "text": " this is a fundamentally different approach to representing text. And this is also really"}, {"timestamp": [400.16, 405.68], "text": " similar to some of the experiments that I've done. If you remember Remo rolling episodic memory organizer which"}, {"timestamp": [405.68, 409.6], "text": " creates layers of abstraction, this does it algorithmically in"}, {"timestamp": [409.6, 413.68], "text": " real time. So this just blows everything that I've done with"}, {"timestamp": [413.68, 418.48], "text": " memory research completely out of the water. It also has the"}, {"timestamp": [418.48, 424.0], "text": " ability to basically kind of summarize as it goes, and"}, {"timestamp": [424.0, 426.6], "text": " that's not necessarily the right word,"}, {"timestamp": [426.6, 429.04], "text": " because summarization means that you take one piece of text"}, {"timestamp": [429.04, 431.18], "text": " and create a smaller piece of text."}, {"timestamp": [431.18, 433.78], "text": " But this creates a neural summarization,"}, {"timestamp": [433.78, 435.58], "text": " a neural network summarization"}, {"timestamp": [435.58, 438.46], "text": " by creating these layers of abstraction."}, {"timestamp": [438.46, 441.74], "text": " And this allows it to zoom in and out as it needs to"}, {"timestamp": [441.74, 444.78], "text": " so that it can cast its attention around internally"}, {"timestamp": [444.78, 445.28], "text": " in order to"}, {"timestamp": [445.28, 451.04], "text": " keep track of such a long sequence. Now, okay great, what does this mean? As someone"}, {"timestamp": [451.04, 456.08], "text": " who has been using GPT since GPT-2 where it was basically just a sentence"}, {"timestamp": [456.08, 461.44], "text": " transformer, it couldn't do a whole lot more. You know, like in this model up here"}, {"timestamp": [461.44, 468.16], "text": " the original GPT was 512 tokens and GP GPT2 I think was what, a thousand?"}, {"timestamp": [468.16, 476.48], "text": " I don't remember. Maybe it was 512 as well. And then the initial version of GPT3 was 2,000 tokens."}, {"timestamp": [476.48, 483.92], "text": " We got upgraded to 4,000 tokens. Then we got GPT3.5 and GPT4, so we're at 8,000 and 16,000 tokens."}, {"timestamp": [488.0, 492.52], "text": " As these attention mechanisms get bigger, and as the context window gets bigger, one thing that I've noticed is that there are, one, these are step"}, {"timestamp": [492.52, 501.36], "text": " changes in terms of algorithmic efficiencies, but in terms of what they are capable of doing. As I"}, {"timestamp": [501.36, 509.0], "text": " tell a lot of my consultation clients, do not ever try and get around the context window limitation"}, {"timestamp": [509.0, 515.0], "text": " because 1. A new model is coming out within 6 months that's going to completely blow open that window"}, {"timestamp": [515.0, 518.0], "text": " and 2. It's just a limitation of the model."}, {"timestamp": [518.0, 527.58], "text": " So when you can read a billion tokens, which by the way, humans read about 1-2 billion words in their entire lifetime."}, {"timestamp": [527.58, 533.8], "text": " When you have a model that can read a billion tokens in a second, that is half a lifetime"}, {"timestamp": [533.8, 538.66], "text": " worth of reading and knowledge that this model can take in in a second."}, {"timestamp": [538.66, 543.72], "text": " So when you have a model that can ingest that much information, suddenly retraining models"}, {"timestamp": [543.72, 544.72], "text": " doesn't matter."}, {"timestamp": [544.72, 545.36], "text": " You just give it the"}, {"timestamp": [545.36, 550.4], "text": " log of all news, all events, all papers, whatever task that you're doing, you just give it all"}, {"timestamp": [550.4, 557.12], "text": " of it at once and it can keep track of all of that text in its head, in its virtual head,"}, {"timestamp": [557.12, 568.64], "text": " all at once and it can pay attention to the bits that it needs to with those sparse representations. It is impossible for me to oversell the long-term"}, {"timestamp": [568.64, 575.28], "text": " ramifications of these kinds of algorithmic changes. And so a couple months ago when I said"}, {"timestamp": [575.28, 581.8], "text": " AGI within 18 months, this is the kind of trend that I was paying attention to. There is no limit"}, {"timestamp": [581.8, 587.8], "text": " to the algorithmic breakthroughs we are seeing right now. Now that doesn't mean that there won't eventually be diminishing returns,"}, {"timestamp": [587.8, 593.0], "text": " but at the same time, we are exploring this blue ocean space and we've..."}, {"timestamp": [593.0, 597.4], "text": " Alright, for those of you that have played Skyrim and other RPGs,"}, {"timestamp": [597.4, 604.2], "text": " we unlocked a new map and the grayed out area is this big and we've explored this much of this new map."}, {"timestamp": [604.2, 608.44], "text": " That is how much potential there is to explore out here."}, {"timestamp": [608.44, 611.8], "text": " And the other thing is, this research is accelerating."}, {"timestamp": [611.8, 613.1], "text": " There's a few reasons for that."}, {"timestamp": [613.1, 616.28], "text": " On one of the live streams, someone asked me, like, how do we know that this isn't an"}, {"timestamp": [616.28, 618.12], "text": " AI winter?"}, {"timestamp": [618.12, 622.74], "text": " And I pulled up a chart that showed an exponential growth of investment."}, {"timestamp": [622.74, 625.52], "text": " Where the money goes, the research goes. And because the"}, {"timestamp": [625.52, 630.18], "text": " money is flowing into the research, it's happening. What you, and we saw the same"}, {"timestamp": [630.18, 633.42], "text": " thing with solar and literally every other disruptive technology, is once the"}, {"timestamp": [633.42, 637.58], "text": " investment comes, you know that the breakthroughs are gonna follow. It's just"}, {"timestamp": [637.58, 641.96], "text": " that simple. And this is one of those kinds of breakthroughs. So what does this"}, {"timestamp": [641.96, 645.98], "text": " mean? Put it this way."}, {"timestamp": [645.98, 652.3], "text": " Rather than trying to play Tetris with memory and trying to fit 10 pounds of stuff into"}, {"timestamp": [652.3, 659.5], "text": " a 5 pound bag, now once this becomes commercially ready, which it's coming, it's possible on"}, {"timestamp": [659.5, 664.46], "text": " paper, they did it, so even if we don't get a billion tokens this time next year, it's"}, {"timestamp": [664.46, 665.76], "text": " coming."}, {"timestamp": [665.76, 673.2], "text": " What this allows you to do is, let's say for instance you are working on a medical research"}, {"timestamp": [673.2, 678.04], "text": " thing, and it's like, okay, well, you know, we've got a literature review of literally"}, {"timestamp": [678.04, 681.14], "text": " 2,000 papers per month to read."}, {"timestamp": [681.14, 685.12], "text": " Put all the papers in this model and say say tell me exactly which papers are most relevant."}, {"timestamp": [686.8, 693.92], "text": " So the ability for in-context learning is incredible and it can hold more in its brain,"}, {"timestamp": [693.92, 701.44], "text": " in its mind, than any 10 humans can. And this is, again, this is not the limit. Imagine a year from"}, {"timestamp": [701.44, 705.28], "text": " now or six months from now when LongNet 2 comes out and it's a"}, {"timestamp": [705.28, 711.52], "text": " trillion tokens or 10 trillion tokens. And what they say in this paper is that maybe we're going"}, {"timestamp": [711.52, 719.2], "text": " to see a point very soon where it could have its context window could include basically the entire"}, {"timestamp": [719.2, 726.62], "text": " internet. This is a step towards superintelligence. Make no mistake that the ability to held"}, {"timestamp": [726.62, 733.34], "text": " and use that much information in real time to produce plans, to forecast, to"}, {"timestamp": [733.34, 738.7], "text": " anticipate, to come up with insights, this is a critical step towards digital"}, {"timestamp": [738.7, 743.28], "text": " superintelligence. I am not being hyperbolic here and neither is this paper"}, {"timestamp": [743.28, 749.04], "text": " when they say we could conceivably build a model that can read the entire internet in one go."}, {"timestamp": [749.04, 754.84], "text": " So with all that being said, I wanted to pivot and talk briefly about OpenAI's announcement"}, {"timestamp": [754.84, 759.2], "text": " also yesterday that they are introducing super alignment."}, {"timestamp": [759.2, 768.36], "text": " So the TLDR is that OpenAI is creating a dedicated team to aligning superintelligence, which, again, I"}, {"timestamp": [768.36, 770.88], "text": " am super glad that we are living in the timeline"}, {"timestamp": [770.88, 772.04], "text": " where someone is doing this."}, {"timestamp": [772.04, 774.08], "text": " It's about time."}, {"timestamp": [774.08, 776.24], "text": " I've got my book out there, Benevolent by Design,"}, {"timestamp": [776.24, 778.24], "text": " where I talked about aligning superintelligence."}, {"timestamp": [778.24, 780.32], "text": " And my solution is that you really can't."}, {"timestamp": [780.32, 782.44], "text": " But one thing that I want to point out"}, {"timestamp": [782.44, 786.16], "text": " is that whether or not you can align one model"}, {"timestamp": [786.16, 791.2], "text": " in the lab is, that's part of it, that's a necessary part of the solution."}, {"timestamp": [791.2, 794.82], "text": " I don't want to disparage the engineers and scientists at OpenAI and Microsoft and other"}, {"timestamp": [794.82, 797.44], "text": " places working on this."}, {"timestamp": [797.44, 803.14], "text": " But while it is a necessary component of the solution, it is not a complete solution."}, {"timestamp": [803.14, 806.44], "text": " And this is where researchers like Gary Marcus"}, {"timestamp": [806.44, 814.94], "text": " and Dr. Raman Chowdhury have testified to Congress saying, look, they expect that open"}, {"timestamp": [814.94, 820.28], "text": " source models will reach parity with closed source models and then overtake them. And"}, {"timestamp": [820.28, 826.96], "text": " so when open source models who anyone can deploy are aligned any which way that you want,"}, {"timestamp": [826.96, 828.84], "text": " you lose total control."}, {"timestamp": [828.84, 832.04], "text": " So that while I definitely appreciate and value,"}, {"timestamp": [832.04, 833.68], "text": " because we need to know how to align"}, {"timestamp": [833.68, 836.0], "text": " super intelligent models."}, {"timestamp": [836.0, 838.8], "text": " The good guys, the aligned models,"}, {"timestamp": [838.8, 843.48], "text": " need to be as powerful as all the unaligned models,"}, {"timestamp": [843.48, 845.82], "text": " because in the AI arms race"}, {"timestamp": [845.82, 852.02], "text": " it's going to be AI versus AI. In the example of cybersecurity where we already have adaptive intelligence in"}, {"timestamp": [852.6, 855.12], "text": " in firewalls and other security appliances"}, {"timestamp": [855.88, 862.42], "text": " basically, you're going to have you know an AI agent running in your firewall versus an AI based DDoS attack."}, {"timestamp": [862.62, 865.6], "text": " Just as one example, you're going to have AI based infiltration programs versus AI hunter DDoS attack. Just as one example, you're gonna have AI based"}, {"timestamp": [865.6, 870.8], "text": " infiltration programs versus AI hunter programs on the inside. So it's gonna be"}, {"timestamp": [870.8, 876.16], "text": " spy versus spy. And so we need to make sure that the models that we"}, {"timestamp": [876.16, 881.2], "text": " build that do remain aligned, that we do remain control over, are as smart as"}, {"timestamp": [881.2, 887.4], "text": " possible and also trustworthy. Absolutely needs to happen. Basically you fight fire with fire."}, {"timestamp": [887.4, 890.8], "text": " And I know that that sounds like mutually assured destruction and it kind"}, {"timestamp": [890.8, 891.32], "text": " of is,"}, {"timestamp": [891.32, 895.6], "text": " which is another reason that the nuclear arms race metaphor is very apt for the"}, {"timestamp": [895.6, 896.8], "text": " AI arms race."}, {"timestamp": [896.8, 901.4], "text": " So this research absolutely needs to happen but what I want to drive home"}, {"timestamp": [901.4, 906.12], "text": " is that it is a necessary but not sufficient set of solutions."}, {"timestamp": [906.12, 910.52], "text": " That there also needs to be the adoption, the implementation, and deployment of aligned"}, {"timestamp": [910.52, 911.52], "text": " systems."}, {"timestamp": [911.52, 916.36], "text": " And we also need to make sure that those aligned systems can communicate and collaborate together."}, {"timestamp": [916.36, 920.94], "text": " So with all that being said, big steps in the right direction, but it is coming faster"}, {"timestamp": [920.94, 926.92], "text": " than anyone realizes. And I stand by my assertion, AGI by the end of 2024."}, {"timestamp": [926.92, 930.52], "text": " Actually, by the mid, basically, let's see,"}, {"timestamp": [930.52, 935.48], "text": " September or October 2024, any definition that you have of AGI"}, {"timestamp": [935.48, 937.28], "text": " will be satisfied."}, {"timestamp": [937.28, 940.06], "text": " And then from there, it's a very, very, very short period"}, {"timestamp": [940.06, 942.28], "text": " of time to superintelligence."}, {"timestamp": [942.28, 944.76], "text": " Now, fortunately for us right now,"}, {"timestamp": [944.76, 945.0], "text": " the only"}, {"timestamp": [945.0, 948.72], "text": " computers capable of running these models and researching them"}, {"timestamp": [948.72, 953.76], "text": " are like the NVIDIA supercomputers that they're building. So that but that"}, {"timestamp": [953.76, 958.72], "text": " barrier that threshold is going to start going down because remember NVIDIA at"}, {"timestamp": [958.72, 963.2], "text": " their keynote speech and and for several months they've been saying hey"}, {"timestamp": [963.2, 965.68], "text": " you know our machines are literally a"}, {"timestamp": [965.68, 970.0], "text": " million times more powerful in the last decade and we're going to do it again in the next decade."}, {"timestamp": [970.0, 975.28], "text": " Well, when your desktop computer is as powerful as today's supercomputers in 10 years,"}, {"timestamp": [975.84, 979.52], "text": " you're going to be able to run all of these and then when you combine that with the ongoing"}, {"timestamp": [979.52, 984.88], "text": " algorithmic efficiencies, everyone is going to be running their own AGI within 5 to 10 years."}, {"timestamp": [984.64, 991.48], "text": " efficiencies, everyone is going to be running their own AGI within five to ten years. Mark my words. So time is of the essence. We do need a sense of urgency"}, {"timestamp": [991.48, 998.64], "text": " and I am really glad that OpenAI is doing this. Again, you know, I'm not... I"}, {"timestamp": [998.64, 1003.76], "text": " would like to see more governmental participation, more universities. I would"}, {"timestamp": [1003.76, 1009.28], "text": " like to see something like a GAIA agency, a global AI agency, or an Aegis agency, an"}, {"timestamp": [1009.28, 1013.76], "text": " alignment enforcement for global intelligence systems."}, {"timestamp": [1013.76, 1020.04], "text": " Because the thing is, is corporations and governments are not ready for this. And"}, {"timestamp": [1020.04, 1027.84], "text": " that to me is the biggest risk because from a purely scientific standpoint, I 100%"}, {"timestamp": [1027.84, 1030.04], "text": " believe that we can align superintelligence."}, {"timestamp": [1030.04, 1031.36], "text": " I wrote a book about it."}, {"timestamp": [1031.36, 1035.88], "text": " I demonstrated how you can take unaligned models and align them to universal principles"}, {"timestamp": [1035.88, 1036.88], "text": " very, very easily."}, {"timestamp": [1036.88, 1038.52], "text": " I've done it plenty of times."}, {"timestamp": [1038.52, 1040.2], "text": " The data sets are out there for free."}, {"timestamp": [1040.2, 1044.64], "text": " Just search for heuristic imperatives and core objective functions on my GitHub."}, {"timestamp": [1044.64, 1049.2], "text": " But again, aligning a single model is not the entire solution."}, {"timestamp": [1049.2, 1051.28], "text": " You also need the deployment."}, {"timestamp": [1051.28, 1052.92], "text": " You need the security models."}, {"timestamp": [1052.92, 1056.18], "text": " We need to update things like the OSI model and defense in depth."}, {"timestamp": [1056.18, 1061.04], "text": " We need to look at the entire technology stack, but we also need to look at the entire economic"}, {"timestamp": [1061.04, 1065.12], "text": " and governmental stack to make sure that companies are aware of this and that"}, {"timestamp": [1065.12, 1072.88], "text": " companies start deploying these systems, whether it's security checkpoints, whether it's internal"}, {"timestamp": [1072.88, 1078.64], "text": " policies, that sort of thing. Because when you've got a really powerful cannon, you have to aim that"}, {"timestamp": [1078.64, 1089.16], "text": " cannon really, really well. Otherwise, it's going to kill everybody. And again, you all know me, I am a very, very, very optimistic person when it comes to alignment"}, {"timestamp": [1089.16, 1091.2], "text": " and the future that we can build."}, {"timestamp": [1091.2, 1095.4], "text": " But at the same time, when you're playing with fire, you need to make sure that you"}, {"timestamp": [1095.4, 1097.76], "text": " wear the proper safety gear."}, {"timestamp": [1097.76, 1101.36], "text": " Because the more energy something has, the more dangerous it is."}, {"timestamp": [1101.36, 1109.6], "text": " And the level of energy or intelligence or however you want to look at it, whatever metaphor you want to pick, is going up very quickly. So thanks for watching. I hope you"}, {"timestamp": [1109.6, 1115.44], "text": " got a lot out of this. It's the long net paper and then of course introducing super alignment."}, {"timestamp": [1116.16, 1118.48], "text": " But yeah, thanks for watching. Cheers."}]}