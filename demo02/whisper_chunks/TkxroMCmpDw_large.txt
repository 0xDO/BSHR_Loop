{"text": " Good morning everybody, David Shapiro here with another spicy video. Things are getting real interesting real fast. So my GPT-4 predictions video was pretty popular, so let's do the same thing. But first, let's do a quick recap for GPT-4 before jumping into GPT-5. So perhaps the spiciest thing that happened after the release of Chat GPT-4, which is not the foundation model of current GPT-4, but Microsoft Research, this is Microsoft, this is not some podunk shop, this is Microsoft says that GPT-4 represents the first sparks of AGI, and that it performs strikingly close to human level performance on many, many tasks. So given the breadth and depth of its capabilities, it could be reasonably viewed as an early yet incomplete AGI. So that was the kind of the thing that the shot heard around the world so to speak. There's been a little bit other news or numbers and features, sorry, about GPT-4. So the base model of chat GPT-4 has an 8,000 token window, which that alone has been a game changer. Doubling from 4,000 to 8 8000 tokens unlocks a lot of capabilities. They already have an officially announced 32000 token window. So that is eight times larger than GPT-3 which a 32000 token window is going to be even larger game changer. There's going to be so many things that you can unlock with that. Now in terms of parameter count we don't really know but if I had to give you a best guess looking at the scale of speed because you look at Curie versus DaVinci which are you know GPT-3 models the distance the difference was about a 10x difference right. And so then you look at the relative speed of chat GPT-4 versus chat GPT-3 and it's like OK maybe it's about 10 times again. So if I had to guess maybe chat GPT-4 is about you know in the one trillion parameter range. Who knows. But it is definitely slower. And the fact that it is slower indicates that it's doing more processing which means more parameters or more layers a deeper larger model. Now one other thing about GPT-4 that most of us haven't seen yet but they did demonstrate it is that it is multimodal. It's not just text anymore it supports images. Another thing about JAT GPT-4 is it passed the bar exam in the 90th percentile. It has passed some other tests in the 99th percentile, so it's pretty smart. On some benchmarks, it outperforms most humans already. And then from using chat GPT-4, it is qualitatively better at pretty much everything. It is a step improvement above everything that GPT-3 and Chat GPT-3 can do. And then of course MIT released a study showing that even just Chat GPT-3.5 increased white-collar productivity by 40% and GPT-4 is going to do the same thing again. So these models are coming, they're already having a huge impact, and people are just beginning to learn how to use them. So that's all GPT 3.5 and 4. Just as a recap, before we jump into GPT 5. Now I believe this is the last slide of kind of recapping the way that things are right now. So as many of you have heard, there has been an open letter signed by a whole bunch of people calling that's being dubbed the Great Pause. People are calling for the Great Pause, which is a six-month moratorium on building anything more powerful than GPT-4. The reasons are safety, ethics, regulations, so on and so forth. There's also been a call for a public version, basically the CERN of AI, which when you look at how much money goes into CERN, it's billions and billions of dollars a year. Funding AI research at just 1% of that is a drop in the bucket and could probably produce public versions, commonly owned or fully open source versions, just kind of like how the internet was developed actually at CERN, or at least the World Wide Web, HTML and so on. There have been no major regulatory movements yet, which is really interesting. So no governments, as far as I know, even in Europe, have gone so far as to say, hey, let's put the kibosh on this for a little while. Which usually the European Union and European nations are a little bit more kind of ahead of the curve because America is very reactionary. I can't remember the name of this paradigm, but American politics and legislation is very deliberately only going to react to things once they happen rather than preemptively legislate. Whereas Germany and the EU and France and other places are much more likely to proactively legislate things just on the anticipation of a problem. But even Europe, as far as I know, has not put any restrictions on language models and deep learning, so that's very interesting. According to some rumors, this ricocheted around Reddit a while ago, GPT-5 is already being trained on 25,000 NVIDIA GPUs. The estimate was over $200 million worth of NVIDIA hardware is being used to train GPT-5. Again, that's a rumor. Another big piece of news was Sam Altman was recently on the Lex Friedman podcast. And what he said and this this to me was from a technical perspective the most interesting thing. He said that GPT-4 did not come about from any paradigm shifts. It was not a new architecture or anything but that it came about from hundreds and hundreds of small incremental improvements that had a multiplicative effect across the whole thing which resulted in you know new new ways of processing and preparing data better algorithms so on and so forth. And so if GPT 4 came about from incremental improvements and nothing major maybe we can expect more of the same for GPT 5 can expect more of the same for GPT-5, that it's going to be ongoing improvements of data, pre-processing, training patterns, so on and so forth. So that's in the news. So now let's skip ahead to GPT-5 predictions and some rumors. All right, so first, top of mind, when is it going to come out? Obviously the internet is rife with rumors. Some of it has more validity than others. According to one website, and I found some of this with the help of Bing actually, ironically enough. One website said that they expect GPT 4.5 to come out this September. So that would be a little bit quicker of a turnaround. Another blog said that we should expect GPT-5 by the end of 2024 or early 2025. Just given the historical pattern, that seems pretty reasonable. When you consider that the testing cycle for GPT-4 was six to nine months, so they had it, like rumor has that that they had GPT-4 like last summer or last fall. So maybe our predictions about when GPT-4 was completed was correct, but they delayed it the release due to testing. Who knows? One Twitter user said that I don't know if this was in response to the leaked Morgan Stanley document, but basically, you know, and of course it's on Twitter, so take it with a grain of salt, but basically that he said that GPT-5 is scheduled to be finished with its training this December. So that kind of lines up with late 2023, early 2024, and then you add the training cycle of six to nine months, that puts it at mid 2024. So another thing that was interesting is in the documentation, OpenAI has a few snapshots of the current models that are set to expire in June, which is really interesting because they've never done that before. So my interpretation is that they're going to say, okay, we're going to expire these models but you can use them because they're probably testing new ideas and then they're going to recycle those models or replace them or upgrade them or something. So either way, all of these rumors and some of the facts that we're gleaning really kind of point to a shorter testing and release cycle which considering OpenAI's close partnership with Microsoft, Microsoft is very familiar with a regular cadence right you've got patch Tuesday with Microsoft Server and Microsoft Desktop they regularly release new, major and minor versions of Windows and other software. So they're probably being pushed to be more like a conventional software vendor. And of course, that's the direction it's all going. Right now, large language models and AI are new and shiny, but before long, it's going to be a commodity just like anything else. Just like your smartphone, just like your laptop, whatever. So I think that we probably can expect some more traction by the end of this year, even if it's an incremental update. But certainly GPT-5, I think that probably mid 2024 at the earliest if I had to guess, but I think that the end of 2024, that seems to be where the consensus is right now. I wouldn't put money on it, you never know, but that seems to be the consensus. Window size. So one of the biggest functional changes of the jump from GPT-3 to 4 was going from a 4000 token window up to an 8000 token window with being teased with a 32,000 token window. The amount of problems that I have been able to solve and address just by doubling the token window, incredible. So if that pattern continues where it either, you know, it goes up 2x or 8x or whatever, if you extrapolate that pattern out, then GPT-5 could have anywhere from 64,000 tokens to 256,000 tokens. So that is roughly 42,000 words up to 170,000 words. To put that into perspective, I think that Dune, the original Dune, was 180,000 words. So it could read all of Dune in one go. Couldn't write it, though. But when you consider that most novels are 50 to 70,000 words, that is more than enough token window to read an entire novel and write another draft of it. So just digest that for a minute and think about how much information that is. The number of scientific papers that could be, so on and so forth. Now, when we talk about window size, if we assume that they overcome any diminishing returns on memory, performance, and compute, because it's going to be a trade-off, right? The larger those internal vectors are, the more memory it's going to take. And one thing that I didn't include in this because it looked a little too dry, but people are basically predicting that GPT-4 takes 10 to 40 times as much compute as GPT-3. And then if you extrapolate that out again, GPT-5 will take another 10 to 40 times as much compute. So the amount of compute is ramping up exponentially. Possibly, we don't know. But what if there's going to be diminishing returns on an algorithmic level? So for instance, maybe when you get the vectors that large, you might get a dilution, which for RNNs and other things, basically dilution, I'm probably using the wrong word, but it kind of forgets what it was talking about at the end of it. So do we need new attention mechanisms? Are we going to need a new architecture? Or just hundreds of more kinds of algorithmic and incremental optimizations? We don't know. One other thing that we need to be asking ourselves is, how many tokens do we actually need? Because Chad GPT with 8,000 tokens is able to serve 90% of our needs right now. Only with very long conversations does it forget the original, like at the beginning. And also I think there's some evidence that they have other memory stuff going on, because I've had some pretty long conversations with chatgpt now and I ask it like, okay what was the first thing that we talked about, and it remembers. So I don't know if they've got some search and retrieval going on or some good summarization, not sure. But the point is, there's probably a diminishing returns in terms of utility value, in terms of functional value to us, the end user, and that includes ordinary citizens and civilians like us, as well as corporations and business and enterprise use cases. More is not always better. So there might be a trade-off in terms of speed, cost, and intelligence, right? Because what if they find out that, like, okay, 8,000 tokens actually satisfies 95% of all use cases, so let's just make that 8,000 token model, make it faster, cheaper, and smarter. And then, you know, maybe we have models that are optimized for much larger windows for specific kinds of tasks, like summarizing, you know, half a million scientific papers. Not really sure, but it is interesting because honestly, if they came out with a 256,000 token model tomorrow, I think that 99% of people are never gonna use that many tokens. I could be wrong. You know, I probably sound like some of the people who said like, oh, nobody's ever gonna use a desktop computer, so maybe I'm completely wrong. You know, I'm the first to admit I frequently am wrong when I make some of these predictions and sometimes I'm hilariously wrong. Okay, so moving on. Modality. For me, the biggest shock of GPT-4 was that it was multimodal. I didn't think they were going to go there yet, but GPT-4, they demonstrated it. You can give it pictures, it can spit out pictures. Most people don't have access to that yet. It probably requires some work on the API because if you're just sending text over a JSON, you know, a REST API, that's one thing. Sending images, it's a little bit different. So I suspect that they're probably working on the integrations with that, which that's a lot to figure out. I don't envy them that problem. It sounds very tedious. But when you look at the fact that that OpenAI has DALI, they have Whisper, GPT-4 has images, you do the math. I suspect, oh and then you look at at how how much like text-to-video and video-to-text is coming out, I suspect that GPT-5 will be audio, video, images, and text, if not more. But even still that would be a great start. So I was talking with some people about this. And what does that mean for vectors? Because if you can represent an image, or audio, or video, or text in vectors, those vectors are going to have a lot more nuance to them. And so the vector is the embedding, right? That is the mathematical representation of the input, which is then used to generate the output of these models. So if you have these multimodal vectors, it's entirely possible that these vectors are going to be more abstract and human-like thoughts inside the model, which that has all kinds of potential implications. And I'm not saying that it's going to magically become sentient or self-aware or anything like that. Just that if you have a more nuanced way of representing information about reality, it's entirely possible that that will unlock entirely new capabilities within GPT-5. So one other big question is where are they getting the data? One of the rumors was that they actually ran out of high quality Internet text data that they actually downloaded the entire Internet and after they filtered out the garbage they realized there's not any more text data out there. We need other modalities and that's why they worked on Whisper. That's why they worked on on Dolly. And so if that's the case then maybe they're working on downloading all of YouTube all the podcasts all of every you know was a was it Dailymotion or whatever, you know, like basically every content provider out there that they can get their hands on and legally and ethically get that data if it's under Creative Commons or other open licensing. So anyways, this is, it's really difficult to anticipate, but just the fact that GPT-3 was single modal and GPT-4 is multimodal, I think we should at least assume that that trend is going to continue. Again, there might be diminishing returns. They might find that most people don't need multimodal models, and so then we might end up with a branching kind of schema. NVIDIA does this, by the way. NVIDIA publishes hundreds and hundreds of different models that have different specializations. And NVIDIA is really good at cranking out very specific models for specific tasks, whereas, at least right now, OpenAI seems to be focusing on one flagship model. That business model might change over time. Not sure. Okay, so intelligence and capabilities. This is where I kind of really dive off into sci-fi land. So if we look at the relative performance of GPT-3 versus GPT-4, it was a huge jump in intelligence where it went from, you know, I think GPT 3.5 was able to pass the bar in the 10th percentile and then 4 was able to pass in the 90th percentile. So that's a quantum leap forward. So if we extrapolate that out, then we could probably assume that GPT 5 is going to pass all tests and all benchmarks in the 99th percentile or greater. If it's that smart, then with the correct integrations, which are already working on integrations, chat GPT plugins, right? With the correct integrations, GPT-5 could then outperform humans at 99% of all other tasks that includes STEM jobs, science, technology, engineering, and math. And so the idea that I had was basically given the right integrations and enough time, you could ask GPT-5 to design a spaceship and it will do it. And then if you give it the right robotics, it could build the thing too. So when I I wrote that down I was like, this is absurd. Then I'm like, you know, if we take out the quantum leap from three to four and do that again this is actually within the realm of possibility I think. And then another probably even more controversial prediction is that it will be able to surpass humans in most artistic endeavors as well, such as writing symphonies, composing stories, and even acting on stage given the correct rigging and framework. So like maybe it can control a virtual actor like in the Unreal Engine or a robotic actor. Because you look at Disney, Disney is making very, very, very lifelike animatronics. So I suspect that one way or another human actors are going the way of the dinosaurs just full stop. Why? Because human actors are expensive. And most actors have signed away rights to their likenesses by now anyways, many of them unwittingly. This came up in conversation where voice actors and even some actors that are getting older have very deliberately signed away their likeness so that they can be immortalized in AI. So if any of this is remotely what happens with GPT-5, I can understand why people are calling for a moratorium, but it's gonna happen because competition is there, right? If open AI doesn't do it, someone else is going to, and if America doesn't do it, some other country is going to do it. And nobody wants to fall behind. So I really don't think a moratorium is going to happen. But that begs the question, what does happen? Is this AGI? Is this singularity? Is this, you know, are we going to get regulation? Are we going to get competition? And of course, if you're familiar with my channel, you saw that I predicted AGI within 18 months. If GPT-5 qualifies, we could have GPT-5 mid-2024, so the timing is there. So all that being said, buckle up. As a commenter said in a previous video, it's about to get silly. Yes, that's pretty much all we can really guarantee right now is that it's about to get real silly real fast. Thanks for watching. It's about to get silly. Yes, that's pretty much all we can really guarantee right now is that it's about to get real silly real fast Thanks for watching", "chunks": [{"timestamp": [0.0, 6.52], "text": " Good morning everybody, David Shapiro here with another spicy video."}, {"timestamp": [6.52, 9.56], "text": " Things are getting real interesting real fast."}, {"timestamp": [9.56, 16.44], "text": " So my GPT-4 predictions video was pretty popular, so let's do the same thing."}, {"timestamp": [16.44, 25.24], "text": " But first, let's do a quick recap for GPT-4 before jumping into GPT-5."}, {"timestamp": [27.88, 30.08], "text": " So perhaps the spiciest thing that happened after the release of Chat GPT-4,"}, {"timestamp": [30.08, 34.76], "text": " which is not the foundation model of current GPT-4,"}, {"timestamp": [34.76, 37.68], "text": " but Microsoft Research, this is Microsoft,"}, {"timestamp": [37.68, 39.28], "text": " this is not some podunk shop,"}, {"timestamp": [39.28, 42.96], "text": " this is Microsoft says that GPT-4 represents"}, {"timestamp": [42.96, 49.44], "text": " the first sparks of AGI, and that it performs strikingly close"}, {"timestamp": [49.44, 53.7], "text": " to human level performance on many, many tasks."}, {"timestamp": [53.7, 58.76], "text": " So given the breadth and depth of its capabilities, it could be reasonably viewed as an early"}, {"timestamp": [58.76, 61.48], "text": " yet incomplete AGI."}, {"timestamp": [61.48, 68.2], "text": " So that was the kind of the thing that the shot heard around the world so to speak."}, {"timestamp": [68.2, 76.1], "text": " There's been a little bit other news or numbers and features, sorry, about GPT-4. So the base"}, {"timestamp": [76.1, 82.24], "text": " model of chat GPT-4 has an 8,000 token window, which that alone has been a game changer."}, {"timestamp": [82.24, 85.52], "text": " Doubling from 4,000 to 8 8000 tokens unlocks a lot of"}, {"timestamp": [85.52, 87.26], "text": " capabilities."}, {"timestamp": [87.26, 90.86], "text": " They already have an officially announced 32000 token window."}, {"timestamp": [90.86, 99.84], "text": " So that is eight times larger than GPT-3 which a 32000 token window is going to be even larger"}, {"timestamp": [99.84, 100.84], "text": " game changer."}, {"timestamp": [100.84, 104.1], "text": " There's going to be so many things that you can unlock with that."}, {"timestamp": [104.1, 110.0], "text": " Now in terms of parameter count we don't really know but if I had to give you a best guess"}, {"timestamp": [110.0, 114.76], "text": " looking at the scale of speed because you look at Curie versus DaVinci which are you"}, {"timestamp": [114.76, 121.1], "text": " know GPT-3 models the distance the difference was about a 10x difference right."}, {"timestamp": [121.1, 126.0], "text": " And so then you look at the relative speed of chat GPT-4 versus chat GPT-3"}, {"timestamp": [126.32, 134.0], "text": " and it's like OK maybe it's about 10 times again. So if I had to guess maybe chat GPT-4 is about"}, {"timestamp": [134.04, 141.52], "text": " you know in the one trillion parameter range. Who knows. But it is definitely slower. And the"}, {"timestamp": [141.52, 149.56], "text": " fact that it is slower indicates that it's doing more processing which means more parameters or more layers a deeper larger model."}, {"timestamp": [149.56, 155.6], "text": " Now one other thing about GPT-4 that most of us haven't seen yet but they did demonstrate"}, {"timestamp": [155.6, 158.2], "text": " it is that it is multimodal."}, {"timestamp": [158.2, 162.0], "text": " It's not just text anymore it supports images."}, {"timestamp": [162.0, 166.48], "text": " Another thing about JAT GPT-4 is it passed the bar exam in the 90th percentile."}, {"timestamp": [166.48, 170.8], "text": " It has passed some other tests in the 99th percentile, so it's pretty smart."}, {"timestamp": [170.8, 175.28], "text": " On some benchmarks, it outperforms most humans already."}, {"timestamp": [175.28, 181.0], "text": " And then from using chat GPT-4, it is qualitatively better at pretty much everything."}, {"timestamp": [181.0, 185.0], "text": " It is a step improvement above everything that"}, {"timestamp": [185.0, 190.72], "text": " GPT-3 and Chat GPT-3 can do. And then of course MIT released a study"}, {"timestamp": [190.72, 196.16], "text": " showing that even just Chat GPT-3.5 increased white-collar productivity by"}, {"timestamp": [196.16, 202.48], "text": " 40% and GPT-4 is going to do the same thing again. So these models are coming,"}, {"timestamp": [202.48, 205.28], "text": " they're already having a huge impact, and people"}, {"timestamp": [205.28, 213.2], "text": " are just beginning to learn how to use them. So that's all GPT 3.5 and 4. Just as a recap,"}, {"timestamp": [213.2, 220.88], "text": " before we jump into GPT 5. Now I believe this is the last slide of kind of recapping the way that"}, {"timestamp": [220.88, 225.92], "text": " things are right now. So as many of you have heard, there has been an"}, {"timestamp": [225.92, 229.42], "text": " open letter signed by a whole bunch of people calling that's being dubbed the"}, {"timestamp": [229.42, 234.32], "text": " Great Pause. People are calling for the Great Pause, which is a six-month"}, {"timestamp": [234.32, 241.52], "text": " moratorium on building anything more powerful than GPT-4. The reasons are"}, {"timestamp": [241.52, 245.04], "text": " safety, ethics, regulations, so on and so forth."}, {"timestamp": [245.04, 253.4], "text": " There's also been a call for a public version, basically the CERN of AI, which when you look"}, {"timestamp": [253.4, 260.72], "text": " at how much money goes into CERN, it's billions and billions of dollars a year."}, {"timestamp": [260.72, 266.96], "text": " Funding AI research at just 1% of that is a drop in the bucket and could probably produce"}, {"timestamp": [266.96, 275.76], "text": " public versions, commonly owned or fully open source versions, just kind of like how the"}, {"timestamp": [275.76, 285.72], "text": " internet was developed actually at CERN, or at least the World Wide Web, HTML and so on."}, {"timestamp": [285.72, 290.78], "text": " There have been no major regulatory movements yet, which is really interesting."}, {"timestamp": [290.78, 297.56], "text": " So no governments, as far as I know, even in Europe, have gone so far as to say, hey,"}, {"timestamp": [297.56, 300.18], "text": " let's put the kibosh on this for a little while."}, {"timestamp": [300.18, 306.24], "text": " Which usually the European Union and European nations are a little bit more"}, {"timestamp": [306.24, 309.52], "text": " kind of ahead of the curve because America is very reactionary."}, {"timestamp": [309.52, 314.88], "text": " I can't remember the name of this paradigm, but American politics and legislation"}, {"timestamp": [314.88, 319.52], "text": " is very deliberately only going to react to things once they happen"}, {"timestamp": [319.52, 321.84], "text": " rather than preemptively legislate."}, {"timestamp": [321.84, 325.36], "text": " Whereas Germany and the EU and France and other places"}, {"timestamp": [325.36, 329.2], "text": " are much more likely to proactively legislate things just on the"}, {"timestamp": [329.2, 334.12], "text": " anticipation of a problem. But even Europe, as far as I know, has not put any"}, {"timestamp": [334.12, 339.2], "text": " restrictions on language models and deep learning, so that's very interesting."}, {"timestamp": [339.2, 346.4], "text": " According to some rumors, this ricocheted around Reddit a while ago, GPT-5 is already"}, {"timestamp": [346.4, 350.92], "text": " being trained on 25,000 NVIDIA GPUs."}, {"timestamp": [350.92, 357.16], "text": " The estimate was over $200 million worth of NVIDIA hardware is being used to train GPT-5."}, {"timestamp": [357.16, 359.9], "text": " Again, that's a rumor."}, {"timestamp": [359.9, 365.0], "text": " Another big piece of news was Sam Altman was recently on the Lex Friedman podcast."}, {"timestamp": [365.0, 370.0], "text": " And what he said and this this to me was from a technical perspective the most"}, {"timestamp": [370.0, 375.0], "text": " interesting thing. He said that GPT-4 did not come about from any paradigm shifts."}, {"timestamp": [375.0, 380.0], "text": " It was not a new architecture or anything but that it came about from hundreds and"}, {"timestamp": [380.0, 387.6], "text": " hundreds of small incremental improvements that had a multiplicative effect across the whole thing which resulted in"}, {"timestamp": [387.68, 394.32], "text": " you know new new ways of processing and preparing data better algorithms so on and so forth."}, {"timestamp": [394.4, 402.48], "text": " And so if GPT 4 came about from incremental improvements and nothing major maybe we can expect more of the same for GPT 5"}, {"timestamp": [405.88, 410.28], "text": " can expect more of the same for GPT-5, that it's going to be ongoing improvements of data, pre-processing, training patterns, so on and so forth."}, {"timestamp": [410.28, 412.6], "text": " So that's in the news."}, {"timestamp": [412.6, 418.12], "text": " So now let's skip ahead to GPT-5 predictions and some rumors."}, {"timestamp": [418.12, 423.56], "text": " All right, so first, top of mind, when is it going to come out?"}, {"timestamp": [423.56, 425.84], "text": " Obviously the internet is rife with rumors."}, {"timestamp": [425.84, 429.24], "text": " Some of it has more validity than others."}, {"timestamp": [429.24, 433.92], "text": " According to one website, and I found some of this with the help of Bing actually, ironically"}, {"timestamp": [433.92, 435.86], "text": " enough."}, {"timestamp": [435.86, 440.92], "text": " One website said that they expect GPT 4.5 to come out this September."}, {"timestamp": [440.92, 446.26], "text": " So that would be a little bit quicker of a turnaround. Another blog said that we"}, {"timestamp": [446.26, 453.82], "text": " should expect GPT-5 by the end of 2024 or early 2025. Just given the historical pattern,"}, {"timestamp": [453.82, 459.88], "text": " that seems pretty reasonable. When you consider that the testing cycle for GPT-4 was six to"}, {"timestamp": [459.88, 465.76], "text": " nine months, so they had it, like rumor has that that they had GPT-4 like last summer"}, {"timestamp": [465.76, 472.08], "text": " or last fall. So maybe our predictions about when GPT-4 was completed was correct, but"}, {"timestamp": [472.08, 480.6], "text": " they delayed it the release due to testing. Who knows? One Twitter user said that I don't"}, {"timestamp": [480.6, 485.8], "text": " know if this was in response to the leaked Morgan Stanley document, but basically,"}, {"timestamp": [485.8, 490.4], "text": " you know, and of course it's on Twitter, so take it with a grain of salt, but basically"}, {"timestamp": [490.4, 498.26], "text": " that he said that GPT-5 is scheduled to be finished with its training this December."}, {"timestamp": [498.26, 504.36], "text": " So that kind of lines up with late 2023, early 2024, and then you add the training cycle"}, {"timestamp": [504.36, 506.48], "text": " of six to nine months, that puts it"}, {"timestamp": [506.48, 509.34], "text": " at mid 2024."}, {"timestamp": [509.34, 515.2], "text": " So another thing that was interesting is in the documentation, OpenAI has a few snapshots"}, {"timestamp": [515.2, 520.68], "text": " of the current models that are set to expire in June, which is really interesting because"}, {"timestamp": [520.68, 522.48], "text": " they've never done that before."}, {"timestamp": [522.48, 526.24], "text": " So my interpretation is that they're going to say, okay, we're going to expire these"}, {"timestamp": [526.24, 532.36], "text": " models but you can use them because they're probably testing new ideas and then they're"}, {"timestamp": [532.36, 538.16], "text": " going to recycle those models or replace them or upgrade them or something."}, {"timestamp": [538.16, 545.92], "text": " So either way, all of these rumors and some of the facts that we're gleaning really"}, {"timestamp": [545.92, 551.28], "text": " kind of point to a shorter testing and release cycle which considering OpenAI's"}, {"timestamp": [551.28, 555.32], "text": " close partnership with Microsoft, Microsoft is very familiar with a"}, {"timestamp": [555.32, 560.32], "text": " regular cadence right you've got patch Tuesday with Microsoft Server and"}, {"timestamp": [560.32, 568.68], "text": " Microsoft Desktop they regularly release new, major and minor versions of Windows and other software."}, {"timestamp": [568.68, 573.78], "text": " So they're probably being pushed to be more like a conventional software vendor."}, {"timestamp": [573.78, 575.68], "text": " And of course, that's the direction it's all going."}, {"timestamp": [575.68, 580.66], "text": " Right now, large language models and AI are new and shiny, but before long, it's going"}, {"timestamp": [580.66, 582.62], "text": " to be a commodity just like anything else."}, {"timestamp": [582.62, 590.2], "text": " Just like your smartphone, just like your laptop, whatever. So I think that we probably can expect"}, {"timestamp": [590.2, 594.48], "text": " some more traction by the end of this year, even if it's an incremental update."}, {"timestamp": [594.48, 601.9], "text": " But certainly GPT-5, I think that probably mid 2024 at the earliest if I"}, {"timestamp": [601.9, 606.0], "text": " had to guess, but I think that the end of 2024, that seems to be where the"}, {"timestamp": [606.0, 611.36], "text": " consensus is right now. I wouldn't put money on it, you never know, but that seems to be the consensus."}, {"timestamp": [612.48, 619.92], "text": " Window size. So one of the biggest functional changes of the jump from GPT-3 to 4 was going"}, {"timestamp": [619.92, 627.2], "text": " from a 4000 token window up to an 8000 token window with being teased with a 32,000 token"}, {"timestamp": [627.2, 628.2], "text": " window."}, {"timestamp": [628.2, 632.26], "text": " The amount of problems that I have been able to solve and address just by doubling the"}, {"timestamp": [632.26, 634.74], "text": " token window, incredible."}, {"timestamp": [634.74, 640.32], "text": " So if that pattern continues where it either, you know, it goes up 2x or 8x or whatever,"}, {"timestamp": [640.32, 648.5], "text": " if you extrapolate that pattern out, then GPT-5 could have anywhere from 64,000 tokens to 256,000 tokens."}, {"timestamp": [648.5, 653.3], "text": " So that is roughly 42,000 words up to 170,000 words."}, {"timestamp": [653.3, 658.4], "text": " To put that into perspective, I think that Dune, the original Dune, was 180,000 words."}, {"timestamp": [658.4, 661.0], "text": " So it could read all of Dune in one go."}, {"timestamp": [661.0, 663.0], "text": " Couldn't write it, though."}, {"timestamp": [663.0, 667.18], "text": " But when you consider that most novels are 50 to 70,000 words, that is"}, {"timestamp": [667.18, 674.38], "text": " more than enough token window to read an entire novel and write another draft of it."}, {"timestamp": [674.38, 680.74], "text": " So just digest that for a minute and think about how much information that is."}, {"timestamp": [680.74, 685.6], "text": " The number of scientific papers that could be, so on and so forth."}, {"timestamp": [685.6, 694.4], "text": " Now, when we talk about window size, if we assume that they overcome any diminishing returns on memory, performance, and compute,"}, {"timestamp": [694.4, 695.8], "text": " because it's going to be a trade-off, right?"}, {"timestamp": [695.8, 699.8], "text": " The larger those internal vectors are, the more memory it's going to take."}, {"timestamp": [699.8, 711.68], "text": " And one thing that I didn't include in this because it looked a little too dry, but people are basically predicting that GPT-4 takes 10 to 40 times as much compute as GPT-3."}, {"timestamp": [711.68, 717.04], "text": " And then if you extrapolate that out again, GPT-5 will take another 10 to 40 times as much compute."}, {"timestamp": [717.04, 721.68], "text": " So the amount of compute is ramping up exponentially. Possibly, we don't know."}, {"timestamp": [722.8, 727.52], "text": " But what if there's going to be diminishing returns on an algorithmic level?"}, {"timestamp": [727.52, 735.92], "text": " So for instance, maybe when you get the vectors that large, you might get a dilution, which"}, {"timestamp": [735.92, 740.6], "text": " for RNNs and other things, basically dilution, I'm probably using the wrong word, but it"}, {"timestamp": [740.6, 743.22], "text": " kind of forgets what it was talking about at the end of it."}, {"timestamp": [743.22, 745.0], "text": " So do we need new attention mechanisms?"}, {"timestamp": [745.0, 747.0], "text": " Are we going to need a new architecture?"}, {"timestamp": [747.0, 752.0], "text": " Or just hundreds of more kinds of algorithmic and incremental optimizations?"}, {"timestamp": [752.0, 754.0], "text": " We don't know."}, {"timestamp": [754.0, 756.0], "text": " One other thing that we need to be asking ourselves is,"}, {"timestamp": [756.0, 759.0], "text": " how many tokens do we actually need?"}, {"timestamp": [759.0, 766.8], "text": " Because Chad GPT with 8,000 tokens is able to serve 90% of our needs right now."}, {"timestamp": [767.36, 772.16], "text": " Only with very long conversations does it forget the original, like at the beginning."}, {"timestamp": [772.16, 776.16], "text": " And also I think there's some evidence that they have other memory stuff going on,"}, {"timestamp": [776.16, 780.32], "text": " because I've had some pretty long conversations with chatgpt now and I ask it like,"}, {"timestamp": [780.32, 782.96], "text": " okay what was the first thing that we talked about, and it remembers."}, {"timestamp": [783.52, 787.82], "text": " So I don't know if they've got some search and retrieval going on or some good summarization,"}, {"timestamp": [787.82, 789.32], "text": " not sure."}, {"timestamp": [789.32, 794.98], "text": " But the point is, there's probably a diminishing returns in terms of utility value, in terms"}, {"timestamp": [794.98, 800.96], "text": " of functional value to us, the end user, and that includes ordinary citizens and civilians"}, {"timestamp": [800.96, 805.68], "text": " like us, as well as corporations and business and enterprise use cases."}, {"timestamp": [805.68, 807.18], "text": " More is not always better."}, {"timestamp": [807.18, 812.4], "text": " So there might be a trade-off in terms of speed, cost, and intelligence, right?"}, {"timestamp": [812.4, 819.04], "text": " Because what if they find out that, like, okay, 8,000 tokens actually satisfies 95%"}, {"timestamp": [819.04, 827.04], "text": " of all use cases, so let's just make that 8,000 token model, make it faster, cheaper, and smarter."}, {"timestamp": [827.04, 831.0], "text": " And then, you know, maybe we have models"}, {"timestamp": [831.0, 834.08], "text": " that are optimized for much larger windows"}, {"timestamp": [834.08, 835.52], "text": " for specific kinds of tasks,"}, {"timestamp": [835.52, 837.4], "text": " like summarizing, you know,"}, {"timestamp": [837.4, 839.98], "text": " half a million scientific papers."}, {"timestamp": [839.98, 842.64], "text": " Not really sure, but it is interesting"}, {"timestamp": [842.64, 844.64], "text": " because honestly, if they came out"}, {"timestamp": [844.64, 847.6], "text": " with a 256,000 token model tomorrow,"}, {"timestamp": [847.6, 849.32], "text": " I think that 99% of people"}, {"timestamp": [849.32, 851.72], "text": " are never gonna use that many tokens."}, {"timestamp": [851.72, 852.72], "text": " I could be wrong."}, {"timestamp": [852.72, 854.64], "text": " You know, I probably sound like some of the people"}, {"timestamp": [854.64, 856.84], "text": " who said like, oh, nobody's ever gonna use"}, {"timestamp": [856.84, 859.34], "text": " a desktop computer, so maybe I'm completely wrong."}, {"timestamp": [859.34, 862.54], "text": " You know, I'm the first to admit I frequently am wrong"}, {"timestamp": [862.54, 864.24], "text": " when I make some of these predictions"}, {"timestamp": [864.24, 866.88], "text": " and sometimes I'm hilariously wrong."}, {"timestamp": [866.88, 869.26], "text": " Okay, so moving on."}, {"timestamp": [869.26, 870.26], "text": " Modality."}, {"timestamp": [870.26, 873.88], "text": " For me, the biggest shock of GPT-4 was that it was multimodal."}, {"timestamp": [873.88, 878.88], "text": " I didn't think they were going to go there yet, but GPT-4, they demonstrated it."}, {"timestamp": [878.88, 881.72], "text": " You can give it pictures, it can spit out pictures."}, {"timestamp": [881.72, 883.28], "text": " Most people don't have access to that yet."}, {"timestamp": [883.28, 888.34], "text": " It probably requires some work on the API because if you're just sending text over a JSON, you"}, {"timestamp": [888.34, 891.92], "text": " know, a REST API, that's one thing. Sending images, it's a little bit"}, {"timestamp": [891.92, 895.24], "text": " different. So I suspect that they're probably working on the integrations"}, {"timestamp": [895.24, 899.84], "text": " with that, which that's a lot to figure out. I don't envy them that problem. It"}, {"timestamp": [899.84, 905.04], "text": " sounds very tedious. But when you look at the fact that that OpenAI has DALI, they have"}, {"timestamp": [905.04, 911.56], "text": " Whisper, GPT-4 has images, you do the math. I suspect, oh and then you look at at how"}, {"timestamp": [911.56, 916.2], "text": " how much like text-to-video and video-to-text is coming out, I suspect"}, {"timestamp": [916.2, 923.56], "text": " that GPT-5 will be audio, video, images, and text, if not more. But even still that"}, {"timestamp": [923.56, 925.08], "text": " would be a great start."}, {"timestamp": [925.08, 927.52], "text": " So I was talking with some people about this."}, {"timestamp": [927.52, 930.24], "text": " And what does that mean for vectors?"}, {"timestamp": [930.24, 935.04], "text": " Because if you can represent an image, or audio, or video,"}, {"timestamp": [935.04, 937.88], "text": " or text in vectors, those vectors"}, {"timestamp": [937.88, 940.12], "text": " are going to have a lot more nuance to them."}, {"timestamp": [940.12, 941.88], "text": " And so the vector is the embedding, right?"}, {"timestamp": [941.88, 944.16], "text": " That is the mathematical representation"}, {"timestamp": [944.16, 947.8], "text": " of the input, which is then used to generate the output of these models."}, {"timestamp": [947.8, 950.3], "text": " So if you have these multimodal vectors,"}, {"timestamp": [950.3, 954.5], "text": " it's entirely possible that these vectors are going to be more abstract"}, {"timestamp": [954.5, 956.8], "text": " and human-like thoughts inside the model,"}, {"timestamp": [956.8, 960.0], "text": " which that has all kinds of potential implications."}, {"timestamp": [960.0, 962.7], "text": " And I'm not saying that it's going to magically become sentient"}, {"timestamp": [962.7, 965.72], "text": " or self-aware or anything like that."}, {"timestamp": [965.72, 972.76], "text": " Just that if you have a more nuanced way of representing information about reality, it's"}, {"timestamp": [972.76, 979.6], "text": " entirely possible that that will unlock entirely new capabilities within GPT-5."}, {"timestamp": [979.6, 983.56], "text": " So one other big question is where are they getting the data?"}, {"timestamp": [983.56, 985.44], "text": " One of the rumors was that they actually ran out"}, {"timestamp": [985.44, 987.36], "text": " of high quality Internet text data that they"}, {"timestamp": [987.36, 989.48], "text": " actually downloaded the entire Internet"}, {"timestamp": [989.76, 991.72], "text": " and after they filtered out the garbage"}, {"timestamp": [992.44, 994.4], "text": " they realized there's not any more text"}, {"timestamp": [994.4, 996.08], "text": " data out there. We need other modalities"}, {"timestamp": [996.08, 997.76], "text": " and that's why they worked on Whisper."}, {"timestamp": [998.0, 999.84], "text": " That's why they worked on on Dolly."}, {"timestamp": [1000.2, 1002.04], "text": " And so if that's the case then maybe they're"}, {"timestamp": [1002.04, 1003.96], "text": " working on downloading all of YouTube all"}, {"timestamp": [1003.96, 1008.36], "text": " the podcasts all of every you know was a was it Dailymotion or whatever, you know, like"}, {"timestamp": [1008.36, 1014.22], "text": " basically every content provider out there that they can get their hands on and legally"}, {"timestamp": [1014.22, 1021.22], "text": " and ethically get that data if it's under Creative Commons or other open licensing."}, {"timestamp": [1021.22, 1028.08], "text": " So anyways, this is, it's really difficult to anticipate, but just the fact that GPT-3"}, {"timestamp": [1028.08, 1033.48], "text": " was single modal and GPT-4 is multimodal, I think we should at least assume that that"}, {"timestamp": [1033.48, 1034.84], "text": " trend is going to continue."}, {"timestamp": [1034.84, 1037.2], "text": " Again, there might be diminishing returns."}, {"timestamp": [1037.2, 1041.28], "text": " They might find that most people don't need multimodal models, and so then we might end"}, {"timestamp": [1041.28, 1046.0], "text": " up with a branching kind of schema. NVIDIA does this, by the way."}, {"timestamp": [1046.0, 1049.3], "text": " NVIDIA publishes hundreds and hundreds of different models"}, {"timestamp": [1049.3, 1052.3], "text": " that have different specializations."}, {"timestamp": [1052.3, 1054.4], "text": " And NVIDIA is really good at cranking out"}, {"timestamp": [1054.4, 1057.5], "text": " very specific models for specific tasks,"}, {"timestamp": [1057.5, 1059.1], "text": " whereas, at least right now,"}, {"timestamp": [1059.1, 1062.7], "text": " OpenAI seems to be focusing on one flagship model."}, {"timestamp": [1062.7, 1066.84], "text": " That business model might change over time."}, {"timestamp": [1066.84, 1067.84], "text": " Not sure."}, {"timestamp": [1067.84, 1070.82], "text": " Okay, so intelligence and capabilities."}, {"timestamp": [1070.82, 1074.56], "text": " This is where I kind of really dive off into sci-fi land."}, {"timestamp": [1074.56, 1084.0], "text": " So if we look at the relative performance of GPT-3 versus GPT-4, it was a huge jump"}, {"timestamp": [1084.0, 1088.08], "text": " in intelligence where it went from, you know, I think GPT 3.5"}, {"timestamp": [1088.08, 1093.76], "text": " was able to pass the bar in the 10th percentile and then 4 was able to pass in the 90th percentile."}, {"timestamp": [1093.76, 1099.76], "text": " So that's a quantum leap forward. So if we extrapolate that out, then we could probably"}, {"timestamp": [1099.76, 1106.28], "text": " assume that GPT 5 is going to pass all tests and all benchmarks in the 99th percentile"}, {"timestamp": [1106.28, 1109.44], "text": " or greater."}, {"timestamp": [1109.44, 1114.88], "text": " If it's that smart, then with the correct integrations, which are already working on"}, {"timestamp": [1114.88, 1117.34], "text": " integrations, chat GPT plugins, right?"}, {"timestamp": [1117.34, 1122.76], "text": " With the correct integrations, GPT-5 could then outperform humans at 99% of all other"}, {"timestamp": [1122.76, 1125.84], "text": " tasks that includes STEM jobs, science,"}, {"timestamp": [1125.84, 1131.76], "text": " technology, engineering, and math. And so the idea that I had was basically given the right"}, {"timestamp": [1131.76, 1138.0], "text": " integrations and enough time, you could ask GPT-5 to design a spaceship and it will do it. And then"}, {"timestamp": [1138.0, 1147.04], "text": " if you give it the right robotics, it could build the thing too. So when I I wrote that down I was like, this is absurd."}, {"timestamp": [1147.04, 1151.68], "text": " Then I'm like, you know, if we take out the quantum leap from three to four and do that"}, {"timestamp": [1151.68, 1155.92], "text": " again this is actually within the realm of possibility I think."}, {"timestamp": [1155.92, 1161.26], "text": " And then another probably even more controversial prediction is that it will be able to surpass"}, {"timestamp": [1161.26, 1165.76], "text": " humans in most artistic endeavors as well, such as writing symphonies,"}, {"timestamp": [1165.76, 1172.72], "text": " composing stories, and even acting on stage given the correct rigging and framework. So like maybe"}, {"timestamp": [1172.72, 1177.6], "text": " it can control a virtual actor like in the Unreal Engine or a robotic actor. Because you look at"}, {"timestamp": [1177.6, 1188.0], "text": " Disney, Disney is making very, very, very lifelike animatronics. So I suspect that one way or another human actors are going the way of the dinosaurs"}, {"timestamp": [1188.0, 1189.52], "text": " just full stop."}, {"timestamp": [1189.52, 1190.52], "text": " Why?"}, {"timestamp": [1190.52, 1193.36], "text": " Because human actors are expensive."}, {"timestamp": [1193.36, 1198.04], "text": " And most actors have signed away rights to their likenesses by now anyways, many of them"}, {"timestamp": [1198.04, 1199.04], "text": " unwittingly."}, {"timestamp": [1199.04, 1204.0], "text": " This came up in conversation where voice actors and even some actors that are getting older"}, {"timestamp": [1204.0, 1208.92], "text": " have very deliberately signed away their likeness so that they can be immortalized in AI."}, {"timestamp": [1208.92, 1216.76], "text": " So if any of this is remotely what happens with GPT-5, I can understand why"}, {"timestamp": [1216.76, 1221.56], "text": " people are calling for a moratorium, but it's gonna happen because competition is"}, {"timestamp": [1221.56, 1228.4], "text": " there, right? If open AI doesn't do it, someone else is going to, and if America doesn't do it, some other country is going to do it. And nobody wants"}, {"timestamp": [1228.4, 1232.16], "text": " to fall behind. So I really don't think a moratorium is going to happen. But that begs"}, {"timestamp": [1232.16, 1237.52], "text": " the question, what does happen? Is this AGI? Is this singularity? Is this, you know, are"}, {"timestamp": [1237.52, 1240.96], "text": " we going to get regulation? Are we going to get competition? And of course, if you're"}, {"timestamp": [1240.96, 1245.02], "text": " familiar with my channel, you saw that I predicted AGI within 18 months."}, {"timestamp": [1245.02, 1254.82], "text": " If GPT-5 qualifies, we could have GPT-5 mid-2024, so the timing is there."}, {"timestamp": [1254.82, 1257.54], "text": " So all that being said, buckle up."}, {"timestamp": [1257.54, 1262.08], "text": " As a commenter said in a previous video, it's about to get silly."}, {"timestamp": [1262.08, 1266.08], "text": " Yes, that's pretty much all we can really guarantee right now is that it's about to"}, {"timestamp": [1266.08, 1267.76], "text": " get real silly real fast."}, {"timestamp": [1267.76, 1268.44], "text": " Thanks for watching."}, {"timestamp": [1264.14, 1271.4], "text": " It's about to get silly. Yes, that's pretty much all we can really guarantee right now is that it's about to get real silly real fast"}, {"timestamp": [1271.62, 1273.62], "text": " Thanks for watching"}]}