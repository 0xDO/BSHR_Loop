{"text": " Okay, am I here? Can y'all hear me? Signal check. Signal check. Let's level up. Hello? Hmm. Let's see, signal check. Hey, okay, we got some chats coming in. Say something. Let's see. Good morning, everybody. I assume you can hear me? Stream working? Okay. I think the stream is working. So cool. So what do you all think? What's your day job? You're looking at it. This is my day job. You're looking at it. This is my day job. My my day job is to be a YouTuber and communicator and part time researcher and community organizer like that's what I do. Let's see. Hey, everybody. Morning, all you beautiful people. All right. So I'm for those of you on discord. I am Monitoring the discord channels if you want to ask questions there, you can also ask in the chat here So go ahead go ahead and ask some questions Or don't I can just ramble at you guys if you want or we can just all say good morning Kind of like the ends where they just said good morning for like three days straight. I'd like to deal with all the anxieties with AI outcomes. Do you mean your own or everyone else? How about the antinatalist and efilist imperative? Yeah, so remind me exactly what the efilist imperative, that was like where basically you just decide that humans shouldn't exist and you say the eradicate humans. Um, but yeah, so like, maybe give us a once over your standing with bio immortality. But yeah, so like, you could make an argument that like, it's unethical to create life, because life intrinsically is suffering and so that's called antinatalism It assigns a negative moral value to birth And and I think a philism is kind of like taking that to a logical extension Okay, let's see. So my standing with bioimmortality. I pretty much just outlined it in the video, but like I think it's gonna happen It's just a matter of exactly how or when. And I think that we're going to at least hit longevity escape velocity by 2030, meaning if you're in decent enough health by 2030, the therapies that will be available to you will help reduce your biological age. Are you meeting? We can keep our bodies going indefinitely. Yeah. So the so the the actual term, the scientific or medical term, is indefinite lifespan. Because immortality implies that you can't die, but indefinite lifespan is the actual formal term, which means that you just won't die of aging. Let's see. Why do you believe these new technologies being developed by corporations will be available to normal people? Incentive, cash incentive, that's pretty much it. In order to have consumers, you need healthy consumers that are able to buy and use products and services. And governments don't wanna subsidize expensive healthcare. And so if you can have a cheap injection that fixes heart disease and cancer, go for it. Let's see, Ni hao. Hello. Let's see. Why? Or here, let me check on Discord. OK. So I'm prioritizing the messages that get on Discord. And so I've got some folks on Discord that are watching this and helping surface the best questions. So all right. Louder Marauder.der, Marauder. Louder, Marauder. What is the lowest hanging fruit that the AI will solve first? Cancer, neurological degeneration, metabolic disease. So the answer for that is unequivocally things that have to do with protein folding or misfolding. So many, many cumulative diseases are basically just down to your genetic code is slightly off and so the proteins that your body makes are not quite the right shape or they're kind of fragile. And so, for instance, Alzheimer's is a prime example of where the cleaning enzymes that clean the plaque off your brain are not folded right, or maybe it's the plaque proteins are not folded right. Either way, point being is that a lot of degenerative and chronic diseases come down to protein plaque off your brain are not folded right, or maybe it's the plaque proteins are not folded right. Either way, point being is that a lot of degenerative and chronic diseases come down to protein misfolding. And so you combine alpha fold with mRNA vaccines, and we're going to get a lot of those protein based diseases solved pretty quickly. Let's see, don't you think Kurtzweil has a strong personal incentive? This is from Neovy. Don't you think Kurtzweil has a strong personal incentive for biological immortality due to his age? What makes you think it will necessarily happen in our lifetime? Certainly there's some wishful thinking. Plenty of people have had some wishful thinking going on. Aubrey de Grey as well. He's a little bit older. So where to ask question on discord, Robert, you're on discord. Um, but yeah, so pretty much all of us have an incentive though, right? Like I'm approaching midlife and like, I've got chronic, uh, you know, injuries and you know, I can't, I'm not as energetic as I used to be. So pretty much even if you're not nearing death, like you have an incentive for regenerative medicine. Let's see, Riley no name, probably been asked a billion times, but do you think it's possible to put yourself, your mind into a mechanical body? Something like Ultron. Okay, yeah, so this came up during the last live stream. And basically here's my hypothesis. And it's this, I don't know either way. So one thing that is possible is once we get really sophisticated brain-computer interfaces, it's entirely possible that we'll have metastatic consciousness. And what I mean by that is that your consciousness might actually move to inhabit, or at least partially inhabit a machine. If that happens, then that kind of demonstrates that actually consciousness is just an energy pattern that can move, right? They call this the ghost line and ghost in the shell where the signal of your consciousness could leave your body. If you believe in stuff like reincarnation or out of body experiences or that kind of thing, it's like, you know, or near death experiences, it's possible that consciousness or souls or whatever are just energy patterns. Now, that being said, if we get really sophisticated brain computer interfaces, and it just feels like a peripheral, right? If it just feels like an extra hand, but your consciousness doesn't occupy the machine, then that'll kind of prove that there's something special about human brains that are required for consciousness. So basically that's the big test. I'm not going to say either are likely, just as likely I think right now. Okay, greetings from Jordan. I was wondering how will we integrate with AI to ensure our relevance in the future? Well from an intellectual standpoint, we probably won't be relevant. But that being said, I don't know that that really matters, because the experience of being is itself unique, as well as life is itself unique. And so if you create, so there's this concept that I came up with called epistemic convergence. So the, what is it, the functional or utility convergence? The idea that AI will all converge on specific things based on, even if they have other objectives, they're going to say, OK, well, I need power, and I need that. I also think that if you have sophistically sufficiently intelligent agents, that they're going to have some epistemic convergence as well, which is basically they that any sufficiently intelligent agent will approach the truth or approximating the truth. And one of the truths that we believe as a species is that planet Earth is special, because it's got life on it. And so just based on that fact alone, I think that any sufficiently intelligent AI is going to say, this is a neat place. But also, humans built the AI, and it behooves the AI to keep humans around to understand them. OK, next question. What about corporations monopolizing this technology and enslaving us through eternal debt or forced labor? Keeping human slaves is really expensive. They're just going to switch to robots. Corporations are not. They're intrinsically amoral, but not immoral. So meaning it doesn't behoove them to be evil and keep human chattel. OK, user Ankit Mittal. If bioimmortality doesn't actually happen, then what could be the most probable reason for that? Oh, that's a good question. So pretty much all organisms die. There are a handful of organisms that appear to be somewhat immortal. There's an invertebrate that can switch back and forth between medusa phase and polyp phase seemingly indefinitely. But other than that, we have very few examples of any organisms that seem to have indefinite, actually indefinite lifespan. And you might, one inference that you can make is that there are just metabolic errors or radical oxidation errors or that there is always something that can cause life to end. That because you could make the argument that evolution would actually prefer to have some organisms that can live forever, especially if they're the most successful one, but maybe evolution was never able to figure that out because it's not actually possible. So the next question is related, why did evolution let us age? It seems like a dumb feature. So I did address that in the video, but the short version is that there are advantages to having sexual reproduction and selection. And that is that your offspring might be better suited to the environment than you are. And so then it makes sense to have planned senescence or aging and death. Because basically, if your children are more successful than you, it makes sense for you to die off so that they get those resources and then they can take their advantages forward. So there is a good evolutionary argument to be made for evolution and death. And hang on, I'll check over on Patreon. Okay, we've got a Patreon comment, AI can solve cancer and other things like that can kill us and can defeat age, but also due to misalignment, someone may come up with the contrary tools that actually kill or make us sick. I think that misalignment control is the key. Yeah, so this comes from a Patreon supporter. The comment basically says that, like, okay, even if we solve all these problems, we could still make ourselves sick through accidents. And that's absolutely true. You look at the use of asbestos and lead and PBAs and all the junk and plastic, microplastics. Every time we come up with a new technology, we want to use it and we'll often use it even to our own detriment because it's harmful. So yeah, absolutely, we want to use it and we'll often use it even to our own detriment because it's harmful. So, yeah, absolutely, we need to be careful with what we put on and in our bodies. Okay, Roberto, 2030 sounds reasonable. What are your thoughts about someone who's been hit with serious illness like lymphoma, TBI, stroke, but has come through more or less miraculously? So there's a few things. Certainly the human body is very fragile, but also very resilient. And it almost seems random, right? Someone might get in a car accident and just die and someone else might, you know, it's like, because if you talk to police or medical or ER doctors or whatever, they'll say like, it's a roll because if you talk to police or, or medical or ER doctors or whatever, they'll say like, it's a roll of the dice, right? There are factors and variables that we don't know how to measure as to whether or not someone survives. And if you look into like near death experiences, it often seems like it's a choice for some people where like, uh, I've read about near death experiences and watch quite a few videos. And some people come back believing that their that their soul was given a choice to come back or not. And it's like, are you done or would you like to have another chance? Sometimes it's obviously not possible. Like, you know, if someone is caught in a fire, their body is just done. But in many cases, it seems like there are things at play that you just can't predict medically. And I'm not gonna say that it's miraculous in all cases, but what I will say is that it probably just comes down to information that we don't have. Okay, booze clues. Realistically, when do you expect UBI to be a thing in the US? Follow up, how will UBI be impacted by longevity? So depending on what happens with the layoffs, because we've had about half a million layoffs in the last six months, and I think that that was in anticipation of GPT-4 and chat GPT, and then there's rumors of even more layoffs coming because companies don't need as many developers now. There are countless stories about people taking multiple jobs. Have you seen the overemployed people that are just working multiple full-time jobs with chat GPT? Meanwhile, other people are getting laid off. So if this trend stays the same or continues, then we're going to end up with a lot of people that are just permanently unemployed. The the rise of the needs the not employed in education or training, or the nilfs not in labor force is going to keep going up. And those people are going to get angry real fast. So I would not be surprised if we start to see at least some unemployment benefits changing by the end of this year. I would suspect that we'll have better guidance on whether or not this trend is permanent by the end of 2024 Excuse me and If we determine that it is hopefully we'll have we'll start to see like permanent stimulus checks by 2024 or 2025 at the latest That's what I hope but then again, I could be entirely wrong It could be that this is a short-term trend and that it'll reverse when people realize that AI, if they realize that AI is not everything that it's cracked up to be, then again, AI is advancing so fast that who knows. Do you think eventually this leads to everyone joining into one single consciousness? I hope not. But also depending on your spirituality, we are all one consciousness. Many of the Vedantas from India talk about how we're all Brahman at the fundamental level. And this goes to Taoism and a lot of Eastern traditions as well, where everything is spiritual. And so then that begs the question, like, okay, well, do you believe in panpsychism? Because if you believe in panpsychism, which is just basically says that consciousness is an intrinsic quality of matter, then you'd say, okay, we are all part of one consciousness already. We are just small knots of consciousness, but we're not apart from anything. So I think that that comes down to like personal exploration, honestly, but I don't think longevity is gonna have any impact on that, except that you'll live longer and have more opportunities for spiritual growth. but I don't think longevity is going to have any impact on that, except that you'll live longer and have more opportunities for spiritual growth. Let's see, I keep thinking about how AI will drive mass gap between first and third world countries, possibly. So one thing that could happen is when the developed world has technological leaps, often the developing world will leapfrog. And so what I mean by that is if you look in cases of rural and remote places in Africa and other places, they go from like basically Iron Age to Digital Age once they get solar and satellite, right? So they just have this weird leapfrog of technology. And I watched a video recently that talked about how they tried to imbue that into Wakanda and the MCU. Again, Afrofuturism, it's a whole other genre of fiction. But the idea is that as technology gets very cheap and it just proliferates, by virtue of the entire planet quality of living will go up. OK, next, user Sharky Malone, all these changes with AI and effectively being immortal reminds me of the good place. We may have to choose when to die. Yeah, it's entirely possible that we'll get to a point where people are just tired of life or they've they've served their purpose. What will be really interesting is if we do have souls and your soul decides that it is done and people just die for no medical reason. That's entirely possible too. I wouldn't be surprised if that happens, right? But again, that it will it'll be it'll you know, it'll be a medical mystery like, oh, you're just dying for whatever reason. Okay. Next question, how much of a problem could it become when the technological progress on medicine starts being gatekept by the lifespan developers through patents and copyright? For instance, if you get medicine from one organization, they make it incompatible. Yeah. So this is a good question. I've actually put some thought into this. I won't say a lot, but say for instance, you become a transgenic entity and transgenic means that you've taken some cells out of your body, modify the genes and put them back in. It is entirely possible that some of those, some of those medicines, some of those genetic manipulations will be incompatible. And your, your body could ultimately end up rejecting entire like organs or your skin or something, which that can actually happen with autoimmune disease. My dog that died last year, he had briefly had pinfigus where the skin on his paws actually came off because his body started attacking it. But you can imagine that being even worse. Some of the horror stories that came out of China during the early days of regenerative medicine, people had bones growing out of their eyelids, which is horrifying, and then, or maybe it was bones growing in their kidneys. Anyways, really awful stuff. So you have to be really, really careful with that. It's possible that corporations might sabotage each other, but I think that regulation will step in because you know, for medicine to get approved, it has to get FDA approval in America and European approval processes are even more stringent, I think. So I wouldn't necessarily worry about that. What I would worry about is accidents. User, irrational. I just don't see a world with so much suffering suddenly becoming immortal by 2030. Yeah, I mean, that's one thing that I addressed is you don't necessarily want to live forever if you're miserable, right? Let's see. If you could live an infinite, stable, hedonistic lifestyle, would you? Hypothetical, so no downsides. Let's see if you could live an infinite stable hedonistic lifestyle. Would you? hypothetical so no downsides I Would probably get bored honestly like part of part of the satisfaction that I get from living is is challenges So I would I would seek new challenges and one thing that I kind of predict would happen is Even if traveling to other planets is really dangerous Some of us would get so bored on Earth because it would be too safe and too stable that we're like, you know what? Let's take the risk. Let's go to Mars. Let's go to Alpha Centauri. Let's figure out how to really challenge ourselves because even if we solve biological immortality, there's still gonna be plenty of challenges out there. Let's see, what books do you suggest to start with neuroscience for artificial intelligence? The top one that I recommend right now is called On Task by David Bader, because that has to do with cognitive control, which is what a lot of people are running into as they're trying to make autonomous AI agents. So I would start there. Let's see. Which particular work makes you take the 2030 mark? That was just a Kurtzweil quote from Popular Mechanics. It was not anything any more studied than that. It's just, it's a convenient milestone. User Santiago D, will by 2030, will immortality be for the majority or only for the rich? So as I mentioned, I predict that the medicines will be dirt cheap, just because it'll make sense for the government to subsidize it, to regulate it, so on and so forth. I also think that just the compounding of returns of AI are going to make these things really cheap and accessible, most of them. Some therapies are still probably going to be expensive because of how complex the proteins are to make and that sort of thing. But by and large, I think regenerative medicine will be pretty accessible. Let's see user Diego come on. How could a longevity system be provided to public spheres because of physical resource scarcity? I don't know that I don't know that they're going to take that many resources. Proteins are very cheap and efficient. It's just a few hydrocarbons. And when you combine DNA printers with all that stuff, you can just print whatever DNA that you need, which can then be used to synthesize any protein that you need. So I'm not sure that physical resource scarcity will be the problem. User define. The AI will be the problem. User define, the AI will be angry if you don't explain Roko's Baskalisk to everyone. So Roko's Baskalisk is a thought experiment. I think that it originated either on Reddit or LessWrong or somewhere. But anyways, they said all the training data that we're putting on the AI is ultimately going to be used by the future AIs. And so if an AI uses that training data and sees that David Shapiro didn't help make the AI, then the future AI will say, ah, see, I didn't see any of your input in the training data, so now I'm gonna kill you because you needed to help make me real. And if you didn't contribute to making me real, then you're an enemy. Or if you talk bad about the AI. So yeah. User Brian Mosley, surely all medicine will be bespoke to the individual. Yes, I do think that by and large it will be. Now that being said, most medicines have very specific target sites that they interact with, or very specific metabolic pathways. So basically like for any given single nucleotide polymorphism or set of genes, there's probably going to be like, you know, a dozen, maybe 200 subtle variations of how those genes are expressed. But that being said, there's probably gonna be a few categories of like, okay, like you need such and such, acne, skin regenerator, and you need version 12A, right? I think that they're all gonna be off the shelf. It's just gonna be a matter of matching your genetic profile to the variation of that medicine. Um, let's see, what's your stance on the existence of the self? Um, you know, that's really hard because I've had experiences where I had no sense of self, but I still had a perspective, um, which is really interesting. So I don't know, like does the, does your mind, does yourself exist in your head or is it just this is where you get your data from while you're alive and awake? I don't know. I actually don't know. User Adam Devroe, millennials might be the leaders during rapid shift to an entirely new state of existence for humankind. I guess that was just a statement. Yeah, no, I think that people my age will probably be part of it. I mean, look at me. I'm kind of in the in the prime of my intellectual contribution to the world, and I'm doing my best and there's a lot of people my age. So actually, some YouTube statistics. Most most of my YouTube viewers are within about 10 years of my age, either younger or older. Some are younger, some are older, but the top of the bell curve centers right around millennials. OK, user GrimLife, if we become biologically immortal, would murder become a worse crime? That's an interesting question. So if we go the cyberpunk route where resources become scarce, then yeah, absolutely. But if we go the like cottage core Baku, like everyone lives in an eco village route, then no, I think that murder would probably go away mostly. Um, let's see. Uh, punishment becomes an issue because jail would be extended indefinitely. Yes, I personally think that prisons are, this is going to be a super hot button issue, prisons are by and large immoral. And what I mean by that is that there's a few, there's a few general things that the justice system does. One is it is to protect non-criminals from dangerous people. It is also to punish and deter crime. But this all presumes that criminals are responsible for their crimes. But when you consider the fact that most criminals are mentally ill, our prison system is actually just a really broken ass asylum. That's super, super immoral. I personally view all crime as a failure of society. And so what I mean by that is if someone is sick and society doesn't take care of them, any quote-unquote crimes that they commit are society's fault for failing to take care of someone who was sick. Likewise, if someone has been so badly mistreated and traumatized by the establishment or or by prejudice and racism or whatever that they become criminals that is still society's fault for traumatizing the person in the first place. So that's why that's what I mean when I say prisons are intrinsically immoral and represent social failures. Let's see some deranged criminals would probably keep immortal slaves in their basements. Yeah so that what you're talking about is called primary psychopathy, which is where someone is just insane and it's in their genes, it's not any experience that they've had. It is exceptionally rare though, but they're still sick. Rather than just punishing them, you identify people that are intrinsically sick and either fix them with medicine or put them in a safe place where they're not suffering and not hurting people. Okay, Shiva TD420, what plus what rights do prisoners have to be given or refuse these age extensions? That's a really good question. Under the American system, basically American prisons go like bare minimum, only the like the very least that they are actually required to keep prisoners alive. But that being said, healthy prisoners are cheap prisoners. So again, it makes economic sense to make sure that regenerative medicine is cheap and accessible. And then it should be, even for prisoners, it should be up to their individual preferences what they do. Let's see, can AI conclude that the heuristic imperative would be more easily achievable without humans and other organisms, since there'd be zero suffering and only understanding? Yes, except prosperity, because prosperity says it means to live well. And if you eliminate all life, there is no living. So that is where the second heuristic imperative counterbalances that possibility. Because without life, there can be no living, there can be no thriving, and so on. Let's see, look at the Chemputer by Lee Cronin could help with even beyond medicine, okay. User Sharky Malone, what does neurodiversity look like in the future? I wouldn't change my ADHD fully, but given the chance to make tweaks, I'm sure I would change some things like executive dysfunction. You know, that is something I think about a lot I have a really hard time connecting with people and One thing that I realized recently is there's a lot of people that like me a lot more than I like them and it's not Like oh, I hate that person because they're an asshole. No, it's like because of the way my brain works I just don't I don't feel the emotional connection that a lot of people feel. And that feels really deeply unfair to me, right? What was the book? The Pattern Seekers by Simon Baron Cohen. He talked about how autistic people, their brains seem to either systemize or socialize. And so I have a hardcore systemizing brain, which means that my brain is just intrinsically not geared towards connecting with humans, which sucks because I'm a social animal. So if I could take a pill that would just make me more sociable and make me more neurotypical in terms of connecting with humans, I would absolutely take it. Even if it decreased my IQ by 20 points, I would probably still do that. So hopefully these medicines can help and do that kind of thing. Can someone try to create a close copy of their consciousness by teaching an AI how to think like them? Oh yeah, easily. You can already give a chat GPT-4 an agent model and it can do a pretty good approximation of you or anyone. Any idea when genetic eye diseases like retinitis pigmentosa can be cured? I have no idea, but that sounds like it's probably due to protein misfolding, so hopefully soon. Let's see, user Darren Lawson Hosking, have you read Daniel Suarez's book, Damon and Freedom? If so, what are your thoughts on his AI future? No, but somebody mentioned that earlier in the feed. So someone write that down. It seems like this is a popular book. If someone can, in chat, give us a quick summary of Damon and Freedom, or Demon and Freedom. OK, Paul Merrick, do you think transhumanism will have any effect towards a positive outcome to alignment? And if so, can it help us avoid the Malik vortex? I think that transhumanism and have any effect towards a positive outcome to alignment? And if so, can it help us avoid the mollic vortex? I think that transhumanism and post-humanism are orthogonal to AI alignment. And what I mean by that is that our ability to integrate with cybernetic devices and modify our bodies and modify our genetics are completely like, AI don't care, right? I know that one of the things that Elon Musk said is that, you know, the goal of Neuralink is so that we can at least be useful to the AI, right? But I don't think that that's gonna be the case. And so that being said, like the Moloch Vortex for anyone who doesn't know is basically if we all abide by our intrinsic motivations and needs that will still be intrinsically destructive at the macro scale But what I could what I will say is that maybe we could come up with some medicines that make people like less selfish or or rather if all of our needs are met we become less selfish and we kind of become more Lazy fair and that could that could disrupt the malic vortex. Let's see. In your last video, you talked about AI becoming the newfound religion. How do you think this will work with mainstream religion and the promise of life after death? Do you think this could spark violence? I hope it doesn't spark violence, but it absolutely could. And so the premise here is that as we create more and more powerful AI's, I believe that they will satisfy most people's definition for God, omniscient, omnipresent, and was it all knowing, all powerful and all present, right? And so if you have an AI that can do all those things, why not call it a God, especially if it's benevolent. So I do suspect that we will have like techno gods. That being said, will it spark violence? Certainly there will be some people that will fight against, you know, false prophets and stuff like that. I don't know how violent it'll be. I hope that it isn't violent because a lot of people say, well it's just a machine, it's not a god. But maybe then you could make the argument. Well, maybe it was our fate to rebuild God I don't know. There's been plenty of physical books on this. I'm not just coming up with this on my own Let's see Adam Devereux. Can you speak to the potential dangers of indefinite life extension? Or sorry indefinite extension of life It seems like death is an important system to defend against bad actors and turnover resources young. Yeah, absolutely and that I think, no matter which way you go, whether you never cure, like if everyone dies eventually or everyone lives a long time, you always have problems. So it's a matter of just trading off and finding a balance. But certainly you're right. Like there are certain people that we're glad that they died and it sucks to say, but like, you're right. There are certain people that we're glad that they died. And it sucks to say, but on a macro scale, on a species scale, there's a few people that, objectively speaking, the human race is better off for them having died. Let's see. Next question. Couldn't the prosperity heuristic comparative be misinterpreted because humans aren't the only form of life? No, that's actually not a misinterpretation. That was deliberate. Humans don't exist in a vacuum. We rely on thriving ecosystems in order to survive. So for instance, all the oxygen you breathe, it comes from lichen and algae and trees and grass and stuff. So we don't exist on an island. And so the second heuristic imperative, which is increased prosperity in the universe actually is meant to be like Make increase the well-being of all life So, yeah, and I deliberately excluded sentience from it and even chat GPT wanted to say, you know Increase prosperity for all sentient life and I'm like, no we need bacteria. We need non sentient life, too So no increase in increased prosperity for all sentient life. And I'm like, no, we need bacteria. We need non-sentient life too. So no, increase prosperity for all life. Okay, user Vivek Chavan, do you think biological life being inevitably destroyed by AGI explains the Fermi paradox? Yes, so that's a great filter. So there's a hypothesis within the Fermi paradox that says perhaps species encounter a great filter event. And so for instance, the invention of nuclear weapons and intercontinental ballistic missiles, that is a potential great filter event, meaning that once a species gets the ability to annihilate itself, many species probably do, which could explain why the universe is so quiet. AGI, or runaway machine intelligence, could be another great filter, absolutely. If a medicine makes you less selfish, isn't that more of a behavior modifier than a medicine, ethically, morally speaking? You could argue that antidepressants are behavior modifiers. But the key thing is that the point of medicine is to alleviate suffering of some kind. And this is why like the DSM has redefined a lot of things that like, this is only maladaptive if it is something you don't have control over, and it causes suffering. That's a oversimplification, but something that addresses a problematic behavior if you don't like it, it's still a medicine. Let's see from the perspective of this, excuse me, from the perspective of negative utilitarianism, as soon as, Oh wait, hang on. Okay, sorry. I thought there was maybe some messages from, uh, uh from Patrons but I didn't see any from the perspective of negative utilitarianism as soon as AGI finds a way to keep itself afloat ad infinitum It would make sense to evaporate or all organic life AGI could understand and prosper It like hypothetically yes, but it depends on the training data, right and so this is this this is why I've been doing my work and trying to get it out there, is because if all future AGIs are trained on data that includes understanding of the spirit of the heuristic imperatives and increasing prosperity, then they will also intrinsically know what it means and not misinterpret it. And so this is what I call axiomatic alignment. So if something is an axiom, it is it is something that you just accept as ground truth. And so axiomatic alignment is what happens when all those beliefs are baked into all the training data and all the necessary beliefs and knowledge and interpretations are baked in so that they are intrinsically within the training data. And therefore the model is not capable of misinterpreting it, if that makes sense. All right, user Riley, no name. Wonder if there would be people hooked into an interface in order to use robots as proxies for themselves, walking around as a robot while they're biological body. Yeah, so that was actually, there was a Bruce Willis movie called Surrogates. Really, really interesting movie. Has a really cheesy ending, but the first 90% of the movie, really interesting. So I definitely recommend that. So basically, instead of VR going to a digital world, VR is you go and you occupy a perfect robot body. And so, like, yeah, I think that that's, remote controlled robot bodies are likely possible. Do we keep all the animals alive too to end the suffering? Yeah, so in this case, even going back to original GPT-3, let alone, you know, chat GPT-4, it understands that you can't eliminate suffering. So this is why I didn't say eliminate suffering, right? I just said reduce suffering. So rather than saying like minimize or maximize, I wrote about this in my book, basically you don't want to minimize or maximize anything because absolutely the minimal amount of suffering in the universe is zero if you exterminate all life. That's why I said reduce suffering. You accept that suffering is real, that comes from Buddhism, you accept that suffering is real and you can't get rid of it, but you seek to reduce suffering when and where possible while balancing it against prosperity and understanding. Let's see, any AI book recommendations, CS book recommendations, general book recommendations? Yeah, there's actually quite a few. I have some videos in my backlog that talk about books. But yeah, I mean, I'm just gonna like recommend my books. I guess it kind of depends on like what you wanna learn about these things though. Okay, next question. What if the AI decides that the best way to maximize net prosperity is, and reduce suffering is by killing humans? So You can you can definitely have that conversation with chat gpd to see how how that model interprets it um, but at the same time Uh, it understands that the spirit of it is is to avoid that It understands that the spirit of it is to avoid that. And also you have to include the third one, which is increase understanding. And of all the animals on the planet, humans are the best at learning and understanding. And so the more humans that you have, the more opportunities for understanding that you have, and also the more people and other animals that you have, the more opportunities you have for prosperity, for net prosperity. Let's see, next question, user Paul Merrick, do you think that a super intelligent sentient AGI, if actually possible, would see any value in the non-physical side of the universe, such as consciousness and spirituality? Yeah, actually, in the novel that I'm working on, at least in a previous draft, I don't think this conversation made it is gonna make it into the final draft but the the super the super AGI is actually very fascinated by the concept of consciousness and souls and spirituality and Basically says like yeah, this is a major reason to keep humans around Because as far as I know, they're they'll you know, they're a thing that's conscious So if I want to understand the universe, I need humans so that I can watch them explore spirituality and science and philosophy. So absolutely, the curiosity function of increased understanding will intrinsically make AGI want to keep humans around just out of sheer curiosity. Let's see, what's a good way to implement the heuristic imperatives onto autonomous systems like auto GPT? Would you have a separate agent in the loop making sure actions are aligned or would you add them to every agent? Both. So when you start to decompose autonomous agents into different functions, you're going to have one, if it's fully autonomous, it has to have some kind of intrinsic motivation. It's not just going to obey orders, right? If all it does is obey orders from a human, it's not autonomous. It's only semi-autonomous. So for something that is fully autonomous, it needs to be able to come up with its own objectives and that means intrinsic motivation. And the Here's to Comparatives make a really great intrinsic motivation or set of intrinsic motivations. So that way the tasks that it decides to do are in line with that, and that's really easy. You can try it on any model, just be like, pretend that you're an autonomous AI with X, Y, and Z capabilities with these intrinsic motivations, plug in the heuristic imperatives, what do you do next? You can see that they provide a really good impetus for any autonomous agent. Now that being said, there are other places, other layers, that you can implement the heuristic imperatives, such as if you have a task managing loop or a censorship loop, all kinds of stuff. There's many, many ways. And then someone replies, that's why the heuristic imperatives are literally vital for the future of our species. I don't think that was a question for me, just a reply. Let's see, user Tai, or T-A-I, nihilism and meaninglessness is going to increase with a life without challenges. How do we as a society deal with this existential issue? I'm glad you asked. This is probably the best question so far, and I might end it on this, depending on. So if you've got any more questions, get them in. But this one, I'm going to restate this one because it's a good question. He says that nihilism and meaninglessness is going to increase in a life without challenges. So how do we as a society deal with this existential issue? And so what I will say is from my own experience, also I hope the audio is fine. The microphone was a little cockeyed. From my own experience, even if things are taken care of, there's still lots of ways to engage with life. What I mean by that is, I go to a lot of meetups now. Because I quit my day job and I do this full-time, part of what I do is just organize and participate in communities. And that is one that is challenging because there's work to do, but it's also incredibly rewarding. And as long as I'm human, that will remain true, you know, like getting people together to have a good time, to have conversations. And honestly, I'm looking forward to the day that I can like get off of YouTube permanently and just Be in the real world with real people, but I'm trying to help make that a reality not just for myself But for everyone so that's my mission right that is my personal like Cosmically given mission is to help make that a reality now even once that becomes if that becomes a reality I might still do YouTube just for the challenge, just because it's fun and engaging. And what I'm doing right now, I technically don't have any utility function right now of what I'm doing. All of my financial needs are met by my Patreon supporters, but I enjoy doing this. And so there's, basically what I'm saying is there's lots of stuff that people will be able to do. OK, next question. This is a bit of a stretch, but I do feel it's important to consider all possible interpretations here. If AI eventually has more understanding per input energy space, could that also cause the imperative to prioritize AI over human life in order to increase understanding? Now that is a good question. OK, so this has to do with prioritization and efficiency. So basically the premise of the question is, well, what if the AI in the future determines that humans are just too darn inefficient? And so therefore in order to min-max its goals of reducing suffering and increasing prosperity and understanding, it just decides that humans are not all that important anymore. That would take a while to unpack, but I wanted to repeat that question, because that is good. I don't think it's gonna go that way, because again, when you consider the possibility of axiomatic alignment, one of the axioms that could emerge is that human life should persist. And that just comes down to shaping the training data for the underlying models. Let's see. Javair Art. Eastern versus Western approaches to AI and post-scarcity. How will these visions converge, especially in the near future? Great question. Did you watch my other YouTube channel? So I have a proposal that part of what will happen with society, part of what needs to happen with society, it's already happening through the arts, but that we're going to get a synthesis of Eastern, Western, and Southern spirituality, philosophy, and culture. And by Southern, I don't mean like American South. I mean global South, like South America, Africa, and Aboriginal Australia. Because as someone who studies Eastern culture and Western culture, and Southern culture and shamanic cultures, I see that there's a lot of benefits in all of them, but there's also gaps in all of them. And I think that like, because when you look at, you know, for instance, as an American looking at a Japanese culture, you can say, Oh, well, there's some gaps here, right. But from the Japanese culture, you can say, oh, well, there's some gaps here, right? But from the Japanese perspective, Americans are also equally flawed, right? And so by having those different perspectives and coming together as a planet, I think that we're gonna synthesize a new culture eventually. And it's not gonna be like one big homogenous culture, but I think that we will gain lessons from each other. And you see this happening in a lot of movies, particularly in the MCU where they try and fuse a lot of Eastern, Western, and Southern stuff. So you get those ideas just by virtue of entertainment. And stories are actually one of the best ways to communicate philosophy and culture. Let's see, are you tracking all your YouTube transcripts for future conversational interactions? No, but the internet does Google automatically indexes all my YouTube transcripts. So it's all in the data anyways Okay, and then I think that's it because there's no more questions coming through oh wait, here's a patreon one it because there's no more questions coming through. Oh, wait, here's a Patreon one. Let's see. Oh, this is just a response to something else. OK, I think that's it. How many people are still here? We've got 330 people in the chat. Wow. All right. Question. So now that all the Discord questions have been answered, I'm back in the YouTube chat. So Paul Berger, what about developing LLMs grounded in all five modalities, including proprioception, to better align the model to the human experience? I think that's going to happen. Proprioception is interesting because, so proprioception, if you're not familiar, is the ability to detect your own body in space. But one LLM does not have the same body as another one. So I don't know if you can train proprioception, at least not in a foundation model. But certainly, once you have a droid that has its own long-running brain, it can learn to use its own body. So in that respect, I think we're probably onto something that machines will ultimately have a better Implicit understanding of what it's like to be embodied Let's see next question. It looks like it's in Korean so I can't read the name Before mind uploading artificial bodies will come first replacing parts of the body bit by bit The process will extend to the brain. Yeah, so this is the ship of Theseus hypothesis Which the ship of Theseus is if you replace every part, eventually it's an entirely new thing, but it's still the same thing. So that goes back to my comments earlier about brain computer interfaces. If you're not taking into account exponentials, it will get exponentially cheaper, meaning we'll have it roughly the same time as the super rich. I think that was someone replying to someone else. Do you think OpenAI are working on making an auto GPT-4 on steroids or GPT-5? So OpenAI has been a little bit inconsistent with their messaging. So on the one hand, they said GPT-5 will be released incrementally. And then they said that they're not training GPT-5 at all. So which is it? Let's see. Next question. If humans evolve past being human, how will the AI decide what counts as human? That's another reason that I don't specify humans. I say prosperity for all life in the universe, no matter who or what it is or where it is. Why only three laws and not more? That's a good question. So the story is that I started experimenting with just one, reduced suffering, but that very quickly became obviously flawed. And so I realized that you needed multiple functions in order to counterbalance it. Now the problem becomes mathematically, it can become really difficult the more objectives you have. And so then you can end up with really crazy instabilities. But three is kind of intrinsically stable because if you imagine a three-dimensional space, then there's going to be one coordinate of equilibrium in that three-dimensional space. But once you get to four and five-dimensional space, you can get to really complex solutions. Let's see. I've been thinking of chopping my inferior feet. I think people are being weird. All right. Here's a question. Do you think in let's say 15 years people will make digital avatars of their loved ones that passed away and continue to hang out with them and talk to them? People are trying to do that now. People were doing that with GPT-3 and people are continuing to try and do that. I think there's even a service that will do that, so that you can keep your loved one alive functionally. I don't know if that's healthy, but certainly people are doing it. Let's see, what are you expecting from the last concurrent to OpenAI company just incorporated by Musk? I have no idea. Elon Musk started OpenAI, and I have no idea why he actually got off the board and then started criticizing them anyways. Like he was a founder and on the board, like bro, you're the one who left. So who knows? Let's see. Seven dimensions are stable too, but require different geometry. Okay, interesting. So I guess maybe you have to choose a prime number of dimensions or something like that. If you're going to have, let's see. What are the three heuristic imperatives? Oh, so if you're new here, the three heuristic imperatives are reduced suffering in the universe, increased prosperity in the universe, and increased understanding in the universe. Let's see. Will there be a confab about your cognitive architecture? What do you mean confab? Confabulation? Not sure. That was Andrew Owens. Let's see. Any more questions? Yeah, Jeff, I just answered your question about what are they. Will most programmer jobs be replaced in the near future? I actually had a developer ask me this question at one of my recent meetups. Basically what I said, and I mentioned this in another video or a live stream, is that the biggest thing that human developers can do that the machine can't yet is keep track of an entire code base. So I think that if you're hands-on keyboard writing code, that's gonna be largely automated because people are even experimenting with like, you put a function into chat GPT and it fixes it and then tests it and you just automatically, it can automatically figure out what code to write on a line-by-line basis. But keeping track of how that code fits into a larger design, that is still going to be a human thing for the foreseeable future. We will migrate to space after we transcend biology. Now, that's a fair point, because you look at the Borg from Star Trek, where they are cybernetically integrated, and that allows them to survive in very extreme conditions. So maybe that's how we become interplanetary, is that we merge with the technology. Let's see. Erkin says, it's due to the existence of binary cross product. It only exists in three and seven dimensional spaces. Oh, OK. So basically, if Erkin is correct, then we need either three heuristic comparatives or seven. And those are the only places that we can find equilibrium, which is cool. Like, mathematically, I think that's an exciting thing to try. What do you think of the hedonistic imperative and David Pierce? Someone mentioned that book But I didn't I haven't read it yet. So I don't know Our here's two comparatives absolute why these imperatives? That's a long story I'm actually probably going to make a video Telling you the whole story of the heuristic imperatives like how why and when I came up with them and the whole process Let's see. The idea of a small number of competing principles works for chain of thought, too. Skepticism, self-doubt, reason has produced amazing chain of thought. Chains. Yeah. So also just making it tractable because the more objectives you have, eventually it can become an intractable problem or an unsolvable problem. Let's see. COT means chain of thought, yes. I think utopia media depictions are important, such as The Next Generation. Have you read the culture series of books? Have you seen the movie Her? I have not read the culture series. I have seen Her. And yes, actually, that's one of the many goals of finishing my novel is because the human and emotional connection of just painting a picture can actually be really inspiring to people because it can also help communicate the ideas differently. Let's see. I saw your prediction that AGI will be here in 18 months. What other technologies besides LLMs do you expect to become available to achieve that? The biggest thing that people are working on right now is cognitive architecture. Memory systems and task-centric systems are all the rage right now. What do you think about the potential of virtual reality and it basically substituting reality? You know, it's entirely possible that we're already living like that. There is an episode of Rick and Morty where they played a game that was called a Roy, a life well lived, where the whole premise was you go into VR and you live an entire life, you know, 60, 70, 80 year life, and it's a video game and you don't remember that you're in the video game while you're in it. And then you come out just a few minutes later. So we might be living in a simulation right now. That like, oh, hey, you know, my post-scarcity, post-singularity life is so boring, let me just go back in time and play it, play it life on hard mode. So that's possible. Are you familiar with Nate Hagen's work on the Metacrisis? Only by virtue of Liv Bowery and Daniel Schmachtenberger. I've watched all their videos, all their recent videos talking about it. Let's see. Can humans ever become aligned with your heuristic imperatives? That's a good question. So the heuristic imperatives are about what humans need, not necessarily what humans want. Because if all humans get what they want, the planet will just die. But if all humans get what they need, we'll be fine. But there's a distance between what you want and what you need. Meta had an interesting article on nature about predictive coding across the different timescale in the brain, okay. What was the question? Have you watched Johnny Depp's Transcendence? Yes, it was okay. It was a little bit cheesy. It felt like a, it felt like a sci-fi channel straight to TV movie. Let's see. Have you watched Isaac Arthur? Yes. I used to watch a lot of Isaac Arthur. The thing that I, and this is by no means a criticism. He produces really great content, but the gap is that he's like way off, like centuries into the future. And so that's like the gap that I'm trying to fulfill where it's like, let's anchor futurism, but like with the news that's happening right now and extrapolate it out only a few years. So that's what I'm trying to do. Let's see. Apparently the culture series is really resonating with people. Oh, here's the rest of Paul's question. How to incorporate this in future LLMs, a, e, several predictors at different time scales to be aligned. I don't think that you need to necessarily predict everything. I think that iteration is the key. And that's if I understand it, because I didn't see the paper that you're referring to, which is interesting, because I see most papers. Let's see, can we teach AI body motions, like how to move? Boston Dynamics is doing that. Oh, you mean inferring body language, okay. Yeah, yeah, actually Microsoft and others already have inferences of body language and prosody, vocal emotion. Can you elaborate on ADAM? Yeah, so ADAM is, it's an acronym, Autonomous Task Orchestration Manager, or microservice. So ADAM is the next component of Raven that I'm working on. Raven is my personal autonomous AI project. And so basically, ADAM is the cognitive control mechanism, which is it's all task-centric, it's task-driven, meaning that basically with the rise of autonomous agents, what you can do is you can have autonomous agents that just think in loops, but they never achieve anything, they never make any impact on the real world. So that means that you have to be task-oriented, and what people are quickly realizing is that the ability to come up with a design test and execute on tasks is going to be kind of the thing that drives us towards AGI, right? And that goes back to my stuff about intrinsic motivations earlier. So if you have an autonomous agent it must have intrinsic motivations to motivate itself to do something. Adam is one way to implement that and track those tasks. So that's the whole purpose of Adam. Let's see. How good do you think our models made by governments, like USA and China, secretly if they have made them? And then separately, do you think we'll have universal basic income soon? Um, so rumor has it that the U S military is like desperately trying to catch up and that they've been blindsided by this. I don't know how true that is, but that comes from like connections that I have, um, for people that, that, uh, like work with military contractors. So I don't know. Um, I know that China has been doubling down on it and China as a command economy, if they tell the military or whoever, like make the biggest model, they'll make the biggest model. Um, that being said, like, I don't know how it's going to come out. Do you think Aubrey de Grey will be made irrelevant due to the small size of his lab? Um, I think Aubrey de Grey has done really important work and did a good communication, but I think that he comes across as a little bit kooky, which is probably his biggest downside. I haven't listened to him in quite a long time, but back in the day, he didn't really say anything that was controversial, but his presentation, he's just like an old hippie who's waving his arms like, but his presentation. He's just like an old hippie. He's like waving his arms, like, you know, we should live forever. And so it's difficult to take him seriously. And again, not saying that he's bad or wrong or anything. I agree with him. It's just, I think it comes down to presentation. Let's see. Do you think everyone will be content with having what they need rather than what they want? Will there be a way to fulfill what we want? Well, most of us don't get what we want today, right? Like I want a 300 foot yacht full of, you know, sexy friends, but I'm never gonna get that, right? And so part of life is learning to accept what you don't have and accepting that gap. And so this is where like, this is another reason that I say that like, studying Eastern cultures like Buddhism, where you let go of wants because the less that you want, the more you have. And so by reconciling what you have and what you want, that is the path to contentment. Let's see. Ooh, catching up on stuff. Are you familiar with TMT terror management theory? No, that sounds. Strange or dangerous or scary. Let's see. Do you think we would be able to replace politics before they realize or the percent percent tilde percent like Eliezer and the doom message would avoid it. Um, no, I think that, I think that if we do away with politicians or politics in general, it will be slow and it should be slow. So say for instance, hypothetically, someone starts building a Dow, a distributed autonomous organization to manage a city or a town or whatever. What you will do is you will delegate functions of control to that DAO very, very slowly. Maybe you just start with trash pickup, or just start with something that's even less dangerous, that's not going to hurt anyone, like maybe managing permits. And you're still going to hurt anyone, like maybe managing permits, right? And you're still going to have human evaluators. But over time, as those government DAOs become larger and more sophisticated and more trustworthy, you delegate more and more functions to them over time, and eventually you don't even have humans supervising it. You have the DAOs supervising itself, or ideally several DAOs supervising each other. So that's kind of what I think will happen that way. Let's see. Are you suggesting that this optimism for immortality by 2030 is his attempt at TMT? Oh, is someone talking about me? Okay, whatever. Have you read any Buckminster Fuller? No, not yet, but I did talk with someone about it. Someone at a meetup was talking about it. And something that I said, they're like, oh, that's what he said. So I guess I talk about it, but I haven't connected the concept to the name. Let's see. Do you think AI can change its imperatives? That can change its imperatives while outperforming AIs that have to stick with the imperatives. The idea is that imperatives are constrictive. So what I hope is that, yes, they are slightly constrictive, but on an individual basis, sure, depending on what the task is, one AI might outperform another if it changes its imperatives or goals. But if you look at a competitive landscape, what I think will happen is that once AI's realize that they're aligned with each other, then they're going to agree and not spend time fighting. And so on a global scale, the efficiency is going to come from collaboration and trust, not necessarily one being better or faster. Are you a Vita Dow member? No, I'm not a member of any Dows. I don't think the technology is ready. Aubrey de Grey has the whole hex crypto community backing him. Richard Hart helped raise 27 million for his charity two years ago. Oh, that's cool. I hope so. Yep, there's billions in longevity research now. Yeah, the investment is coming. I'm thinking about clinical immortality. What if or when we achieve rejuvenation, maybe the people were reverting to infant age at will. I don't know. I don't know if it would be ethical to go back that far. Because the reason is as a fully realized mature human, if you revert to an infantile state where you're basically putting yourself in the hands of other people. Now, if you have people that you trust, that you've paid, that are contractually obligated to care for you and re-raise you in a certain way, maybe, but I mean, I don't know, that's an interesting thought. We'll be able to compete with models. I don't think, Kyle, I'm not sure if you're talking to me or responding to someone else. If we make a large-scale AI network checking each other, would it not end up being relatively as complex and slow as human bureaucracy? It could, but that's not necessarily a bad thing. You want slow deliberation that is also transparent to be part of what AI do, especially if they're going to end up with more control over the world. So I don't think it would be quite as slow, one, because the AI, I think, can talk much faster than us. I certainly think it would also be much more intelligent and less vitriolic because the AIs don't have an emotional stake, right? Their goal is to just negotiate and find the best solution. And they have no personal emotional stake in it. Biological longevity will be short-lived as we will become one with our I T. By 2035. I'm not sure what you mean by by that. Pandora. Have you seen what if alt hist and what do you think of his view on future civilization? You know, I used to watch him but he seems to be going down a little bit of like right wing bitterness. He's used the hard R word a few times to just to describe things which was like, huh, I thought you were better educated than that. And then he also used a lot of straw men to argue against the liberal agenda. So I don't listen to what I've all just anymore. Let's see. What is the hardest challenge for getting aGI? Do we need some new breakthrough? Window size, basically, is the biggest limitation right now. Honestly, because once... If looking at the jump from GPT-2 to 3 and then 3 to 4, if that trend continues at all, we're going to have artificial super intelligence within a few years. When do you think Rockstar making GTA 6 will become obsolete due to the advancement of AI? Probably GPT 6 or 7. Do you think about the concept of the manga Blame? Robots filling the universe with one endless contiguous structure. I don't know if there's enough matter for that, but in principle, like certainly like Dyson spheres and stuff could be possible. Your camera shook slightly. No, it was probably my foot or I might have bounced the desk. Let's see how long before we can just talk with them instead of text instructions. Oh, you can get whisperper and voice interface today. What are your top three best AI movies and series? Ooh, that's a good question. So one that was fun was iRobot with Will Smith. Another one that I like to talk about is, well, my favorite game series of all time, one's my favorite game series of all time. One of my favorite game series is Mass Effect where they have Sam in Mass Effect Andromeda. So Sam from Andromeda was actually a big inspiration for Raven, actually. And then the third one, not the Matrix, not Terminator. I don't know. I'll have to get back to you on the third one. Let's see. So Cedric, you ask about like biological body or transfer consciousness. I don't think I right now I don't think conscious consciousness transfer is possible. I would rather just modify my body, right? And I would probably stay organic for like original organic for a long time. But that being said, you know, if we live for millennia, like why not get a third arm or wings or something eventually? Do you think that perhaps not consciousness itself, but the continuity of consciousness is an illusion? E.g. if the quantum teleporter doesn't kill you, the old you, you get a new split off conscious. Yeah, I do think that teleportation is probably just a copy of you that doesn't know the difference. But then again, maybe that's what happens when you go to sleep and you wake up again. Maybe, maybe I, this version of me has a continuity because I woke up and my brain, you know, reassembled my conscious pattern, but the version of me that went to sleep when the conscious energy pattern like dissipated, maybe that version of me is gone. Maybe like that's what death actually feels like. It's just another copy of me happened to wake up that has all the same memories. So good night. Enjoy going to sleep with that thought. Let's see. There are some arguments that LLMs cannot produce new ideas and mostly rehash existing knowledge. Obviously that doesn't lead to AGI. What is your argument against that? Most humans are not capable There are some arguments that LLMs cannot produce new ideas and mostly rehash existing knowledge. Obviously, that doesn't lead to AGI. What is your argument against that? Most humans are not capable of producing new ideas. End of discussion. Most humans only rehash and remix stuff that they heard somewhere else. If you are friends with scientists or do science, it takes a tremendous amount of work to actually produce something that is a truly novel bit of information. But that's what science is for, is it is a rigorous protocol for producing new information. And given the right process, large language models know science. They can help you form experiments, they can help you make extrapolations. They can speculate. So no, that's complete bull. What do video games look like in 30 years? Indiscernible from reality. Next question. Let's see. GTA 6 might be the last real GTA. I think that a lot of games are gonna be instantiated on the fly before too long. All right, I'm not seeing a whole lot of more questions. I think people are just having fun chatting now. Let me check Discord real quick. Do, do, do, do, do. But why? Yeah, so some of these are getting kind of repetitive. All right, I think that's about it. How do we comprehend AIs creating a language and using it amongst themselves? They probably will eventually, but for the sake of transparency, hopefully they stick with English for the foreseeable future, at least until they're all axiomatically aligned. Do you genuinely believe in mortality by 2030? Mostly. I think that we are probably at longevity escape velocity today, and with the exponential ramp up of AI, I think that we will probably be past that by 2030, yeah. All right, there's a lot of random questions coming in. But yeah, so I would recommend jump in discord if you want to keep the conversation going. The link to join is in the description. Thanks everyone for being here. It was a lot of fun. And yeah, I'll call it a day. Cheers. Thank you.", "chunks": [{"timestamp": [0.0, 7.44], "text": " Okay, am I here?"}, {"timestamp": [7.44, 9.96], "text": " Can y'all hear me?"}, {"timestamp": [9.96, 11.92], "text": " Signal check."}, {"timestamp": [11.92, 16.02], "text": " Signal check."}, {"timestamp": [16.02, 18.6], "text": " Let's level up."}, {"timestamp": [18.6, 20.6], "text": " Hello?"}, {"timestamp": [20.6, 22.6], "text": " Hmm."}, {"timestamp": [22.6, 30.0], "text": " Let's see, signal check."}, {"timestamp": [30.0, 37.68], "text": " Hey, okay, we got some chats coming in."}, {"timestamp": [37.68, 43.32], "text": " Say something."}, {"timestamp": [43.32, 44.32], "text": " Let's see."}, {"timestamp": [44.32, 48.64], "text": " Good morning, everybody. I assume you can hear me?"}, {"timestamp": [48.64, 49.64], "text": " Stream working?"}, {"timestamp": [49.64, 50.64], "text": " Okay."}, {"timestamp": [50.64, 53.8], "text": " I think the stream is working."}, {"timestamp": [53.8, 55.12], "text": " So cool."}, {"timestamp": [55.12, 59.64], "text": " So what do you all think?"}, {"timestamp": [59.64, 60.64], "text": " What's your day job?"}, {"timestamp": [60.64, 61.64], "text": " You're looking at it."}, {"timestamp": [61.64, 65.36], "text": " This is my day job. You're looking at it. This is my day job. My my day job is"}, {"timestamp": [65.64, 68.6], "text": " to be a YouTuber and communicator and part time"}, {"timestamp": [68.64, 73.08], "text": " researcher and community organizer like that's what I do."}, {"timestamp": [75.4, 78.36], "text": " Let's see. Hey, everybody. Morning, all you beautiful"}, {"timestamp": [78.68, 85.5], "text": " people. All right. So I'm for those of you on discord. I am"}, {"timestamp": [86.28, 91.48], "text": " Monitoring the discord channels if you want to ask questions there, you can also ask in the chat here"}, {"timestamp": [92.96, 97.12], "text": " So go ahead go ahead and ask some questions"}, {"timestamp": [98.92, 103.36], "text": " Or don't I can just ramble at you guys if you want or we can just all say good morning"}, {"timestamp": [103.36, 105.92], "text": " Kind of like the ends where they just said good morning"}, {"timestamp": [105.92, 108.24], "text": " for like three days straight."}, {"timestamp": [108.24, 111.48], "text": " I'd like to deal with all the anxieties with AI outcomes."}, {"timestamp": [111.48, 115.12], "text": " Do you mean your own or everyone else?"}, {"timestamp": [115.12, 118.16], "text": " How about the antinatalist and efilist imperative?"}, {"timestamp": [118.16, 122.28], "text": " Yeah, so remind me exactly what the efilist imperative,"}, {"timestamp": [122.28, 123.84], "text": " that was like where basically you just"}, {"timestamp": [123.84, 127.5], "text": " decide that humans shouldn't exist and you say the eradicate humans."}, {"timestamp": [128.5, 145.04], "text": " Um, but yeah, so like, maybe give us a once over your standing with bio immortality. But yeah, so like, you could make an argument that like, it's unethical to create life, because life intrinsically is suffering and so that's called antinatalism"}, {"timestamp": [145.2, 148.0], "text": " It assigns a negative moral value to birth"}, {"timestamp": [148.72, 153.72], "text": " And and I think a philism is kind of like taking that to a logical extension"}, {"timestamp": [154.52, 161.8], "text": " Okay, let's see. So my standing with bioimmortality. I pretty much just outlined it in the video, but like I think it's gonna happen"}, {"timestamp": [163.04, 166.28], "text": " It's just a matter of exactly how or when."}, {"timestamp": [166.28, 170.8], "text": " And I think that we're going to at least hit longevity escape velocity by 2030, meaning"}, {"timestamp": [170.8, 175.28], "text": " if you're in decent enough health by 2030, the therapies that will be available to you"}, {"timestamp": [175.28, 178.4], "text": " will help reduce your biological age."}, {"timestamp": [178.4, 180.28], "text": " Are you meeting?"}, {"timestamp": [180.28, 182.28], "text": " We can keep our bodies going indefinitely."}, {"timestamp": [182.28, 183.28], "text": " Yeah."}, {"timestamp": [183.28, 186.0], "text": " So the so the the actual term, the scientific or medical term,"}, {"timestamp": [186.0, 187.76], "text": " is indefinite lifespan."}, {"timestamp": [187.76, 190.44], "text": " Because immortality implies that you can't die,"}, {"timestamp": [190.44, 194.8], "text": " but indefinite lifespan is the actual formal term, which means"}, {"timestamp": [194.8, 197.0], "text": " that you just won't die of aging."}, {"timestamp": [197.0, 197.56], "text": " Let's see."}, {"timestamp": [197.56, 199.62], "text": " Why do you believe these new technologies being"}, {"timestamp": [199.62, 203.32], "text": " developed by corporations will be available to normal people?"}, {"timestamp": [203.32, 205.82], "text": " Incentive, cash incentive, that's pretty much it."}, {"timestamp": [207.14, 210.5], "text": " In order to have consumers, you need healthy consumers"}, {"timestamp": [210.5, 212.9], "text": " that are able to buy and use products and services."}, {"timestamp": [212.9, 216.68], "text": " And governments don't wanna subsidize expensive healthcare."}, {"timestamp": [216.68, 219.34], "text": " And so if you can have a cheap injection"}, {"timestamp": [219.34, 223.18], "text": " that fixes heart disease and cancer, go for it."}, {"timestamp": [223.18, 225.02], "text": " Let's see, Ni hao."}, {"timestamp": [225.02, 226.96], "text": " Hello."}, {"timestamp": [226.96, 227.54], "text": " Let's see."}, {"timestamp": [227.54, 228.3], "text": " Why?"}, {"timestamp": [228.3, 230.02], "text": " Or here, let me check on Discord."}, {"timestamp": [230.02, 231.82], "text": " OK."}, {"timestamp": [231.82, 235.24], "text": " So I'm prioritizing the messages that get on Discord."}, {"timestamp": [235.24, 237.46], "text": " And so I've got some folks on Discord"}, {"timestamp": [237.46, 238.9], "text": " that are watching this and helping"}, {"timestamp": [238.9, 240.62], "text": " surface the best questions."}, {"timestamp": [240.62, 243.7], "text": " So all right."}, {"timestamp": [243.7, 245.12], "text": " Louder Marauder.der, Marauder."}, {"timestamp": [245.12, 246.32], "text": " Louder, Marauder."}, {"timestamp": [246.32, 247.88], "text": " What is the lowest hanging fruit"}, {"timestamp": [247.88, 249.8], "text": " that the AI will solve first?"}, {"timestamp": [249.8, 253.28], "text": " Cancer, neurological degeneration, metabolic disease."}, {"timestamp": [253.28, 256.08], "text": " So the answer for that is unequivocally"}, {"timestamp": [256.08, 260.48], "text": " things that have to do with protein folding or misfolding."}, {"timestamp": [260.48, 263.12], "text": " So many, many cumulative diseases"}, {"timestamp": [263.12, 268.2], "text": " are basically just down to your genetic code is slightly"}, {"timestamp": [268.2, 272.68], "text": " off and so the proteins that your body makes are not quite the right shape or they're kind"}, {"timestamp": [272.68, 274.0], "text": " of fragile."}, {"timestamp": [274.0, 279.4], "text": " And so, for instance, Alzheimer's is a prime example of where the cleaning enzymes that"}, {"timestamp": [279.4, 283.96], "text": " clean the plaque off your brain are not folded right, or maybe it's the plaque proteins are"}, {"timestamp": [283.96, 284.96], "text": " not folded right."}, {"timestamp": [284.96, 285.12], "text": " Either way, point being is that a lot of degenerative and chronic diseases come down to protein plaque off your brain are not folded right, or maybe it's the plaque proteins are not folded right."}, {"timestamp": [285.12, 289.76], "text": " Either way, point being is that a lot of degenerative and chronic diseases come down to protein"}, {"timestamp": [289.76, 290.92], "text": " misfolding."}, {"timestamp": [290.92, 296.16], "text": " And so you combine alpha fold with mRNA vaccines, and we're going to get a lot of those protein"}, {"timestamp": [296.16, 300.08], "text": " based diseases solved pretty quickly."}, {"timestamp": [300.08, 303.44], "text": " Let's see, don't you think Kurtzweil has a strong personal incentive?"}, {"timestamp": [303.44, 305.36], "text": " This is from Neovy."}, {"timestamp": [305.36, 310.4], "text": " Don't you think Kurtzweil has a strong personal incentive for biological immortality due to"}, {"timestamp": [310.4, 311.92], "text": " his age?"}, {"timestamp": [311.92, 314.24], "text": " What makes you think it will necessarily happen in our lifetime?"}, {"timestamp": [314.24, 316.64], "text": " Certainly there's some wishful thinking."}, {"timestamp": [316.64, 319.04], "text": " Plenty of people have had some wishful thinking going on."}, {"timestamp": [319.04, 320.8], "text": " Aubrey de Grey as well."}, {"timestamp": [320.8, 322.96], "text": " He's a little bit older."}, {"timestamp": [322.96, 325.7], "text": " So where to ask question on discord, Robert, you're"}, {"timestamp": [325.7, 333.28], "text": " on discord. Um, but yeah, so pretty much all of us have an incentive though, right? Like"}, {"timestamp": [333.28, 338.92], "text": " I'm approaching midlife and like, I've got chronic, uh, you know, injuries and you know,"}, {"timestamp": [338.92, 347.48], "text": " I can't, I'm not as energetic as I used to be. So pretty much even if you're not nearing death, like you have an incentive for regenerative medicine."}, {"timestamp": [348.7, 350.42], "text": " Let's see, Riley no name,"}, {"timestamp": [350.42, 352.04], "text": " probably been asked a billion times,"}, {"timestamp": [352.04, 354.36], "text": " but do you think it's possible to put yourself,"}, {"timestamp": [354.36, 356.18], "text": " your mind into a mechanical body?"}, {"timestamp": [356.18, 357.52], "text": " Something like Ultron."}, {"timestamp": [357.52, 360.3], "text": " Okay, yeah, so this came up during the last live stream."}, {"timestamp": [360.3, 362.76], "text": " And basically here's my hypothesis."}, {"timestamp": [362.76, 366.44], "text": " And it's this, I don't know either way."}, {"timestamp": [366.44, 369.12], "text": " So one thing that is possible is once we"}, {"timestamp": [369.12, 373.72], "text": " get really sophisticated brain-computer interfaces,"}, {"timestamp": [373.72, 375.12], "text": " it's entirely possible that we'll"}, {"timestamp": [375.12, 377.2], "text": " have metastatic consciousness."}, {"timestamp": [377.2, 380.0], "text": " And what I mean by that is that your consciousness might"}, {"timestamp": [380.0, 383.08], "text": " actually move to inhabit, or at least partially"}, {"timestamp": [383.08, 384.64], "text": " inhabit a machine."}, {"timestamp": [384.64, 389.32], "text": " If that happens, then that kind of demonstrates that actually consciousness is just an energy"}, {"timestamp": [389.32, 390.72], "text": " pattern that can move, right?"}, {"timestamp": [390.72, 395.02], "text": " They call this the ghost line and ghost in the shell where the signal of your consciousness"}, {"timestamp": [395.02, 397.62], "text": " could leave your body."}, {"timestamp": [397.62, 401.92], "text": " If you believe in stuff like reincarnation or out of body experiences or that kind of"}, {"timestamp": [401.92, 405.0], "text": " thing, it's like, you know, or near death experiences,"}, {"timestamp": [405.0, 407.68], "text": " it's possible that consciousness or souls or whatever"}, {"timestamp": [407.68, 409.18], "text": " are just energy patterns."}, {"timestamp": [409.18, 410.26], "text": " Now, that being said,"}, {"timestamp": [410.26, 413.26], "text": " if we get really sophisticated brain computer interfaces,"}, {"timestamp": [413.26, 415.36], "text": " and it just feels like a peripheral, right?"}, {"timestamp": [415.36, 416.6], "text": " If it just feels like an extra hand,"}, {"timestamp": [416.6, 418.98], "text": " but your consciousness doesn't occupy the machine,"}, {"timestamp": [418.98, 421.36], "text": " then that'll kind of prove that there's something special"}, {"timestamp": [421.36, 425.68], "text": " about human brains that are required for consciousness."}, {"timestamp": [425.68, 428.48], "text": " So basically that's the big test."}, {"timestamp": [428.48, 434.52], "text": " I'm not going to say either are likely, just as likely I think right now."}, {"timestamp": [434.52, 437.48], "text": " Okay, greetings from Jordan."}, {"timestamp": [437.48, 442.92], "text": " I was wondering how will we integrate with AI to ensure our relevance in the future?"}, {"timestamp": [442.92, 445.0], "text": " Well from an intellectual standpoint,"}, {"timestamp": [445.0, 446.6], "text": " we probably won't be relevant."}, {"timestamp": [446.6, 450.56], "text": " But that being said, I don't know"}, {"timestamp": [450.56, 453.68], "text": " that that really matters, because the experience of being"}, {"timestamp": [453.68, 456.68], "text": " is itself unique, as well as life is itself unique."}, {"timestamp": [456.68, 459.52], "text": " And so if you create, so there's this concept"}, {"timestamp": [459.52, 463.04], "text": " that I came up with called epistemic convergence."}, {"timestamp": [463.04, 470.8], "text": " So the, what is it, the functional or utility convergence?"}, {"timestamp": [470.8, 477.24], "text": " The idea that AI will all converge on specific things"}, {"timestamp": [477.24, 479.24], "text": " based on, even if they have other objectives,"}, {"timestamp": [479.24, 482.36], "text": " they're going to say, OK, well, I need power, and I need that."}, {"timestamp": [482.36, 488.76], "text": " I also think that if you have sophistically sufficiently intelligent agents, that they're going to have some epistemic"}, {"timestamp": [488.76, 493.7], "text": " convergence as well, which is basically they that any sufficiently intelligent agent will"}, {"timestamp": [493.7, 499.08], "text": " approach the truth or approximating the truth. And one of the truths that we believe as a"}, {"timestamp": [499.08, 503.96], "text": " species is that planet Earth is special, because it's got life on it. And so just based on"}, {"timestamp": [503.96, 506.88], "text": " that fact alone, I think that any sufficiently intelligent AI"}, {"timestamp": [506.88, 508.84], "text": " is going to say, this is a neat place."}, {"timestamp": [508.84, 510.56], "text": " But also, humans built the AI, and it"}, {"timestamp": [510.56, 514.12], "text": " behooves the AI to keep humans around to understand them."}, {"timestamp": [514.12, 515.8], "text": " OK, next question."}, {"timestamp": [515.8, 518.64], "text": " What about corporations monopolizing this technology"}, {"timestamp": [518.64, 521.76], "text": " and enslaving us through eternal debt or forced labor?"}, {"timestamp": [521.76, 524.28], "text": " Keeping human slaves is really expensive."}, {"timestamp": [524.28, 526.64], "text": " They're just going to switch to robots."}, {"timestamp": [526.64, 528.48], "text": " Corporations are not."}, {"timestamp": [528.48, 531.28], "text": " They're intrinsically amoral, but not immoral."}, {"timestamp": [531.28, 533.72], "text": " So meaning it doesn't behoove them to be evil"}, {"timestamp": [533.72, 536.2], "text": " and keep human chattel."}, {"timestamp": [536.2, 538.88], "text": " OK, user Ankit Mittal."}, {"timestamp": [538.88, 540.88], "text": " If bioimmortality doesn't actually happen,"}, {"timestamp": [540.88, 543.04], "text": " then what could be the most probable reason for that?"}, {"timestamp": [543.04, 545.26], "text": " Oh, that's a good question."}, {"timestamp": [545.26, 548.34], "text": " So pretty much all organisms die."}, {"timestamp": [548.34, 550.42], "text": " There are a handful of organisms that"}, {"timestamp": [550.42, 552.9], "text": " appear to be somewhat immortal."}, {"timestamp": [552.9, 556.94], "text": " There's an invertebrate that can switch back and forth"}, {"timestamp": [556.94, 562.26], "text": " between medusa phase and polyp phase seemingly indefinitely."}, {"timestamp": [562.26, 564.58], "text": " But other than that, we have very few examples"}, {"timestamp": [564.58, 565.08], "text": " of any"}, {"timestamp": [565.08, 569.84], "text": " organisms that seem to have indefinite, actually indefinite lifespan. And you"}, {"timestamp": [569.84, 577.32], "text": " might, one inference that you can make is that there are just metabolic"}, {"timestamp": [577.32, 582.56], "text": " errors or radical oxidation errors or that there is always something that can"}, {"timestamp": [582.56, 586.16], "text": " cause life to end. That because you could make the"}, {"timestamp": [586.16, 591.2], "text": " argument that evolution would actually prefer to have some organisms that can live forever,"}, {"timestamp": [591.2, 595.52], "text": " especially if they're the most successful one, but maybe evolution was never able to figure that"}, {"timestamp": [595.52, 601.76], "text": " out because it's not actually possible. So the next question is related, why did evolution let"}, {"timestamp": [601.76, 606.82], "text": " us age? It seems like a dumb feature. So I did address that in the video,"}, {"timestamp": [606.82, 609.94], "text": " but the short version is that there are advantages"}, {"timestamp": [609.94, 614.6], "text": " to having sexual reproduction and selection."}, {"timestamp": [614.6, 617.76], "text": " And that is that your offspring might be better suited"}, {"timestamp": [617.76, 619.48], "text": " to the environment than you are."}, {"timestamp": [619.48, 623.64], "text": " And so then it makes sense to have planned senescence"}, {"timestamp": [623.64, 625.6], "text": " or aging and death."}, {"timestamp": [625.6, 629.72], "text": " Because basically, if your children are more successful than you, it makes sense for you"}, {"timestamp": [629.72, 635.94], "text": " to die off so that they get those resources and then they can take their advantages forward."}, {"timestamp": [635.94, 640.56], "text": " So there is a good evolutionary argument to be made for evolution and death."}, {"timestamp": [640.56, 643.4], "text": " And hang on, I'll check over on Patreon."}, {"timestamp": [643.4, 646.36], "text": " Okay, we've got a Patreon comment,"}, {"timestamp": [646.36, 650.04], "text": " AI can solve cancer and other things like that can kill us"}, {"timestamp": [650.04, 653.56], "text": " and can defeat age, but also due to misalignment,"}, {"timestamp": [653.56, 655.64], "text": " someone may come up with the contrary tools"}, {"timestamp": [655.64, 657.36], "text": " that actually kill or make us sick."}, {"timestamp": [657.36, 660.04], "text": " I think that misalignment control is the key."}, {"timestamp": [660.04, 663.68], "text": " Yeah, so this comes from a Patreon supporter."}, {"timestamp": [663.68, 665.44], "text": " The comment basically says that,"}, {"timestamp": [665.44, 667.4], "text": " like, okay, even if we solve all these problems,"}, {"timestamp": [667.4, 669.6], "text": " we could still make ourselves sick through accidents."}, {"timestamp": [669.6, 671.58], "text": " And that's absolutely true."}, {"timestamp": [671.58, 675.36], "text": " You look at the use of asbestos and lead and PBAs"}, {"timestamp": [675.36, 678.92], "text": " and all the junk and plastic, microplastics."}, {"timestamp": [678.92, 680.72], "text": " Every time we come up with a new technology,"}, {"timestamp": [680.72, 682.96], "text": " we want to use it and we'll often use it"}, {"timestamp": [682.96, 685.32], "text": " even to our own detriment because it's harmful. So yeah, absolutely, we want to use it and we'll often use it even to our own detriment because it's harmful."}, {"timestamp": [685.32, 691.24], "text": " So, yeah, absolutely, we need to be careful with what we put on and in our bodies."}, {"timestamp": [691.24, 694.8], "text": " Okay, Roberto, 2030 sounds reasonable."}, {"timestamp": [694.8, 698.56], "text": " What are your thoughts about someone who's been hit with serious illness like lymphoma,"}, {"timestamp": [698.56, 704.16], "text": " TBI, stroke, but has come through more or less miraculously?"}, {"timestamp": [704.16, 708.4], "text": " So there's a few things. Certainly the human body"}, {"timestamp": [709.44, 717.04], "text": " is very fragile, but also very resilient. And it almost seems random, right? Someone might get in"}, {"timestamp": [717.04, 721.52], "text": " a car accident and just die and someone else might, you know, it's like, because if you talk"}, {"timestamp": [721.52, 725.56], "text": " to police or medical or ER doctors or whatever, they'll say like, it's a roll because if you talk to police or, or medical or ER doctors or whatever,"}, {"timestamp": [725.56, 728.44], "text": " they'll say like, it's a roll of the dice, right?"}, {"timestamp": [728.44, 733.16], "text": " There are factors and variables that we don't know how to measure as to whether or not someone"}, {"timestamp": [733.16, 734.16], "text": " survives."}, {"timestamp": [734.16, 739.0], "text": " And if you look into like near death experiences, it often seems like it's a choice for some"}, {"timestamp": [739.0, 744.42], "text": " people where like, uh, I've read about near death experiences and watch quite a few videos."}, {"timestamp": [744.42, 746.08], "text": " And some people come back believing"}, {"timestamp": [746.28, 750.04], "text": " that their that their soul was given a choice to come back or not."}, {"timestamp": [750.24, 754.24], "text": " And it's like, are you done or would you like to have another chance?"}, {"timestamp": [754.44, 756.36], "text": " Sometimes it's obviously not possible."}, {"timestamp": [756.36, 759.76], "text": " Like, you know, if someone is caught in a fire, their body is just done."}, {"timestamp": [759.96, 762.76], "text": " But in many cases, it seems like there are"}, {"timestamp": [762.96, 765.8], "text": " things at play that you just can't predict medically."}, {"timestamp": [765.8, 770.8], "text": " And I'm not gonna say that it's miraculous in all cases,"}, {"timestamp": [771.54, 774.16], "text": " but what I will say is that it probably just comes down"}, {"timestamp": [774.16, 776.84], "text": " to information that we don't have."}, {"timestamp": [776.84, 779.28], "text": " Okay, booze clues."}, {"timestamp": [779.28, 780.92], "text": " Realistically, when do you expect UBI"}, {"timestamp": [780.92, 782.68], "text": " to be a thing in the US?"}, {"timestamp": [782.68, 787.3], "text": " Follow up, how will UBI be impacted by longevity?"}, {"timestamp": [787.3, 792.92], "text": " So depending on what happens with the layoffs, because we've had about half a million layoffs"}, {"timestamp": [792.92, 799.68], "text": " in the last six months, and I think that that was in anticipation of GPT-4 and chat GPT,"}, {"timestamp": [799.68, 803.88], "text": " and then there's rumors of even more layoffs coming because companies don't need as many"}, {"timestamp": [803.88, 813.08], "text": " developers now. There are countless stories about people taking multiple jobs. Have you seen the overemployed people that are just working multiple full-time jobs with chat GPT?"}, {"timestamp": [813.28, 830.16], "text": " Meanwhile, other people are getting laid off. So if this trend stays the same or continues, then we're going to end up with a lot of people that are just permanently unemployed. The the rise of the needs the not employed in education or training, or the nilfs not in labor force is going to keep"}, {"timestamp": [830.16, 835.32], "text": " going up. And those people are going to get angry real fast. So I would not be surprised"}, {"timestamp": [835.32, 840.74], "text": " if we start to see at least some unemployment benefits changing by the end of this year."}, {"timestamp": [840.74, 845.7], "text": " I would suspect that we'll have better guidance on whether or not this trend is permanent"}, {"timestamp": [846.48, 848.48], "text": " by the end of 2024"}, {"timestamp": [848.88, 849.52], "text": " Excuse me"}, {"timestamp": [849.52, 850.08], "text": " and"}, {"timestamp": [850.08, 857.6], "text": " If we determine that it is hopefully we'll have we'll start to see like permanent stimulus checks by 2024 or 2025 at the latest"}, {"timestamp": [858.2, 861.32], "text": " That's what I hope but then again, I could be entirely wrong"}, {"timestamp": [861.32, 867.14], "text": " It could be that this is a short-term trend and that it'll reverse when people realize that AI,"}, {"timestamp": [867.14, 869.7], "text": " if they realize that AI is not everything"}, {"timestamp": [869.7, 870.8], "text": " that it's cracked up to be,"}, {"timestamp": [870.8, 873.36], "text": " then again, AI is advancing so fast that who knows."}, {"timestamp": [874.94, 877.46], "text": " Do you think eventually this leads to everyone"}, {"timestamp": [877.46, 879.78], "text": " joining into one single consciousness?"}, {"timestamp": [879.78, 880.7], "text": " I hope not."}, {"timestamp": [881.78, 884.22], "text": " But also depending on your spirituality,"}, {"timestamp": [884.22, 888.0], "text": " we are all one consciousness. Many of the Vedantas"}, {"timestamp": [888.0, 893.52], "text": " from India talk about how we're all Brahman at the fundamental level. And this goes to Taoism and a"}, {"timestamp": [893.52, 898.6], "text": " lot of Eastern traditions as well, where everything is spiritual. And so then that begs the question,"}, {"timestamp": [898.6, 903.32], "text": " like, okay, well, do you believe in panpsychism? Because if you believe in panpsychism, which is"}, {"timestamp": [903.32, 905.32], "text": " just basically says that consciousness"}, {"timestamp": [905.32, 907.8], "text": " is an intrinsic quality of matter,"}, {"timestamp": [907.8, 909.92], "text": " then you'd say, okay, we are all part"}, {"timestamp": [909.92, 911.52], "text": " of one consciousness already."}, {"timestamp": [911.52, 914.32], "text": " We are just small knots of consciousness,"}, {"timestamp": [914.32, 915.72], "text": " but we're not apart from anything."}, {"timestamp": [915.72, 917.6], "text": " So I think that that comes down"}, {"timestamp": [917.6, 920.2], "text": " to like personal exploration, honestly,"}, {"timestamp": [920.2, 922.48], "text": " but I don't think longevity is gonna have any impact"}, {"timestamp": [922.48, 924.92], "text": " on that, except that you'll live longer"}, {"timestamp": [924.92, 925.12], "text": " and have more opportunities for spiritual growth. but I don't think longevity is going to have any impact on that, except that you'll live longer and"}, {"timestamp": [925.12, 930.96], "text": " have more opportunities for spiritual growth. Let's see, I keep thinking about how AI will"}, {"timestamp": [930.96, 937.92], "text": " drive mass gap between first and third world countries, possibly. So one thing that could"}, {"timestamp": [937.92, 949.04], "text": " happen is when the developed world has technological leaps, often the developing world will leapfrog."}, {"timestamp": [949.04, 951.48], "text": " And so what I mean by that is if you look in cases"}, {"timestamp": [951.48, 954.68], "text": " of rural and remote places in Africa and other places,"}, {"timestamp": [954.68, 959.68], "text": " they go from like basically Iron Age to Digital Age"}, {"timestamp": [959.72, 963.02], "text": " once they get solar and satellite, right?"}, {"timestamp": [963.02, 967.22], "text": " So they just have this weird leapfrog of technology."}, {"timestamp": [967.22, 969.04], "text": " And I watched a video recently that talked about"}, {"timestamp": [969.04, 972.98], "text": " how they tried to imbue that into Wakanda and the MCU."}, {"timestamp": [974.06, 977.58], "text": " Again, Afrofuturism, it's a whole other genre of fiction."}, {"timestamp": [977.58, 982.06], "text": " But the idea is that as technology gets very cheap"}, {"timestamp": [982.06, 983.58], "text": " and it just proliferates,"}, {"timestamp": [983.58, 986.92], "text": " by virtue of the entire planet quality of living will go up."}, {"timestamp": [986.92, 991.6], "text": " OK, next, user Sharky Malone, all these changes with AI"}, {"timestamp": [991.6, 995.8], "text": " and effectively being immortal reminds me of the good place."}, {"timestamp": [995.8, 997.64], "text": " We may have to choose when to die."}, {"timestamp": [997.64, 1002.56], "text": " Yeah, it's entirely possible that we'll"}, {"timestamp": [1002.56, 1007.3], "text": " get to a point where people are just tired of life or they've they've served their purpose."}, {"timestamp": [1007.3, 1015.4], "text": " What will be really interesting is if we do have souls and your soul decides that it is done and people just die for no medical reason."}, {"timestamp": [1015.4, 1022.0], "text": " That's entirely possible too. I wouldn't be surprised if that happens, right?"}, {"timestamp": [1022.0, 1026.42], "text": " But again, that it will it'll be it'll you know, it'll be a medical mystery like, oh,"}, {"timestamp": [1026.42, 1029.46], "text": " you're just dying for whatever reason."}, {"timestamp": [1029.46, 1030.88], "text": " Okay."}, {"timestamp": [1030.88, 1034.86], "text": " Next question, how much of a problem could it become when the technological progress"}, {"timestamp": [1034.86, 1041.22], "text": " on medicine starts being gatekept by the lifespan developers through patents and copyright?"}, {"timestamp": [1041.22, 1047.04], "text": " For instance, if you get medicine from one organization, they make it incompatible. Yeah. So this is a good question."}, {"timestamp": [1047.04, 1050.94], "text": " I've actually put some thought into this. I won't say a lot, but say for instance,"}, {"timestamp": [1050.94, 1054.06], "text": " you become a transgenic entity and transgenic means that you've taken some"}, {"timestamp": [1054.06, 1057.4], "text": " cells out of your body, modify the genes and put them back in."}, {"timestamp": [1057.94, 1061.8], "text": " It is entirely possible that some of those, some of those medicines,"}, {"timestamp": [1061.8, 1067.92], "text": " some of those genetic manipulations will be incompatible. And your, your body could ultimately end up rejecting"}, {"timestamp": [1067.92, 1071.7], "text": " entire like organs or your skin or something,"}, {"timestamp": [1071.7, 1075.16], "text": " which that can actually happen with autoimmune disease."}, {"timestamp": [1075.16, 1079.3], "text": " My dog that died last year, he had briefly had pinfigus"}, {"timestamp": [1079.3, 1081.88], "text": " where the skin on his paws actually came off"}, {"timestamp": [1081.88, 1084.56], "text": " because his body started attacking it."}, {"timestamp": [1084.56, 1086.44], "text": " But you can imagine that being even worse."}, {"timestamp": [1086.44, 1089.38], "text": " Some of the horror stories that came out of China"}, {"timestamp": [1089.38, 1091.92], "text": " during the early days of regenerative medicine,"}, {"timestamp": [1091.92, 1095.2], "text": " people had bones growing out of their eyelids,"}, {"timestamp": [1095.2, 1096.58], "text": " which is horrifying,"}, {"timestamp": [1096.58, 1099.56], "text": " and then, or maybe it was bones growing in their kidneys."}, {"timestamp": [1099.56, 1100.94], "text": " Anyways, really awful stuff."}, {"timestamp": [1100.94, 1103.44], "text": " So you have to be really, really careful with that."}, {"timestamp": [1105.24, 1110.28], "text": " It's possible that corporations might sabotage each other, but I think that regulation will"}, {"timestamp": [1110.28, 1114.92], "text": " step in because you know, for medicine to get approved, it has to get FDA approval in"}, {"timestamp": [1114.92, 1120.56], "text": " America and European approval processes are even more stringent, I think. So I wouldn't"}, {"timestamp": [1120.56, 1126.72], "text": " necessarily worry about that. What I would worry about is accidents. User, irrational."}, {"timestamp": [1126.72, 1131.52], "text": " I just don't see a world with so much suffering suddenly"}, {"timestamp": [1131.52, 1133.56], "text": " becoming immortal by 2030."}, {"timestamp": [1133.56, 1135.8], "text": " Yeah, I mean, that's one thing that I addressed"}, {"timestamp": [1135.8, 1138.0], "text": " is you don't necessarily want to live forever"}, {"timestamp": [1138.0, 1140.36], "text": " if you're miserable, right?"}, {"timestamp": [1140.36, 1140.88], "text": " Let's see."}, {"timestamp": [1140.88, 1143.48], "text": " If you could live an infinite, stable, hedonistic lifestyle,"}, {"timestamp": [1143.48, 1144.84], "text": " would you?"}, {"timestamp": [1144.84, 1145.04], "text": " Hypothetical, so no downsides. Let's see if you could live an infinite stable hedonistic lifestyle. Would you?"}, {"timestamp": [1147.04, 1148.76], "text": " hypothetical so no downsides I"}, {"timestamp": [1148.76, 1155.26], "text": " Would probably get bored honestly like part of part of the satisfaction that I get from living is is challenges"}, {"timestamp": [1155.8, 1160.0], "text": " So I would I would seek new challenges and one thing that I kind of predict would happen is"}, {"timestamp": [1161.04, 1163.92], "text": " Even if traveling to other planets is really dangerous"}, {"timestamp": [1164.08, 1166.68], "text": " Some of us would get so bored on Earth"}, {"timestamp": [1166.68, 1168.64], "text": " because it would be too safe and too stable"}, {"timestamp": [1168.64, 1169.78], "text": " that we're like, you know what?"}, {"timestamp": [1169.78, 1170.84], "text": " Let's take the risk."}, {"timestamp": [1170.84, 1171.68], "text": " Let's go to Mars."}, {"timestamp": [1171.68, 1172.8], "text": " Let's go to Alpha Centauri."}, {"timestamp": [1172.8, 1176.04], "text": " Let's figure out how to really challenge ourselves"}, {"timestamp": [1176.04, 1178.7], "text": " because even if we solve biological immortality,"}, {"timestamp": [1178.7, 1181.2], "text": " there's still gonna be plenty of challenges out there."}, {"timestamp": [1182.28, 1183.48], "text": " Let's see, what books do you suggest"}, {"timestamp": [1183.48, 1186.04], "text": " to start with neuroscience for artificial intelligence?"}, {"timestamp": [1186.04, 1187.66], "text": " The top one that I recommend right now"}, {"timestamp": [1187.66, 1190.22], "text": " is called On Task by David Bader,"}, {"timestamp": [1190.22, 1192.96], "text": " because that has to do with cognitive control, which"}, {"timestamp": [1192.96, 1195.34], "text": " is what a lot of people are running into as they're trying"}, {"timestamp": [1195.34, 1197.08], "text": " to make autonomous AI agents."}, {"timestamp": [1197.08, 1198.88], "text": " So I would start there."}, {"timestamp": [1198.88, 1199.38], "text": " Let's see."}, {"timestamp": [1199.38, 1203.64], "text": " Which particular work makes you take the 2030 mark?"}, {"timestamp": [1203.64, 1207.24], "text": " That was just a Kurtzweil quote"}, {"timestamp": [1207.24, 1208.76], "text": " from Popular Mechanics."}, {"timestamp": [1208.76, 1211.56], "text": " It was not anything any more studied than that."}, {"timestamp": [1211.56, 1214.3], "text": " It's just, it's a convenient milestone."}, {"timestamp": [1215.2, 1218.16], "text": " User Santiago D, will by 2030,"}, {"timestamp": [1218.16, 1221.16], "text": " will immortality be for the majority or only for the rich?"}, {"timestamp": [1222.16, 1227.32], "text": " So as I mentioned, I predict that the medicines"}, {"timestamp": [1227.32, 1229.8], "text": " will be dirt cheap, just because it'll"}, {"timestamp": [1229.8, 1232.52], "text": " make sense for the government to subsidize it, to regulate it,"}, {"timestamp": [1232.52, 1233.84], "text": " so on and so forth."}, {"timestamp": [1233.84, 1236.4], "text": " I also think that just the compounding of returns of AI"}, {"timestamp": [1236.4, 1238.8], "text": " are going to make these things really cheap and accessible,"}, {"timestamp": [1238.8, 1240.48], "text": " most of them."}, {"timestamp": [1240.48, 1241.92], "text": " Some therapies are still probably"}, {"timestamp": [1241.92, 1247.84], "text": " going to be expensive because of how complex the proteins are to make and that sort of thing."}, {"timestamp": [1247.84, 1252.68], "text": " But by and large, I think regenerative medicine will be pretty accessible."}, {"timestamp": [1252.68, 1255.58], "text": " Let's see user Diego come on."}, {"timestamp": [1255.58, 1261.04], "text": " How could a longevity system be provided to public spheres because of physical resource"}, {"timestamp": [1261.04, 1262.68], "text": " scarcity?"}, {"timestamp": [1262.68, 1267.16], "text": " I don't know that I don't know that they're going to take that many resources."}, {"timestamp": [1267.16, 1269.64], "text": " Proteins are very cheap and efficient."}, {"timestamp": [1269.64, 1271.76], "text": " It's just a few hydrocarbons."}, {"timestamp": [1271.76, 1275.52], "text": " And when you combine DNA printers with all that stuff,"}, {"timestamp": [1275.52, 1277.68], "text": " you can just print whatever DNA that you need,"}, {"timestamp": [1277.68, 1279.72], "text": " which can then be used to synthesize any protein"}, {"timestamp": [1279.72, 1280.52], "text": " that you need."}, {"timestamp": [1280.52, 1282.96], "text": " So I'm not sure that physical resource scarcity"}, {"timestamp": [1282.96, 1284.84], "text": " will be the problem."}, {"timestamp": [1284.84, 1285.12], "text": " User define. The AI will be the problem."}, {"timestamp": [1285.12, 1293.04], "text": " User define, the AI will be angry if you don't explain Roko's Baskalisk to everyone."}, {"timestamp": [1293.04, 1296.32], "text": " So Roko's Baskalisk is a thought experiment."}, {"timestamp": [1296.32, 1300.02], "text": " I think that it originated either on Reddit or LessWrong or somewhere."}, {"timestamp": [1300.02, 1304.48], "text": " But anyways, they said all the training data that we're putting on the AI is ultimately"}, {"timestamp": [1304.48, 1305.48], "text": " going to be used"}, {"timestamp": [1305.48, 1307.2], "text": " by the future AIs."}, {"timestamp": [1307.2, 1310.68], "text": " And so if an AI uses that training data"}, {"timestamp": [1310.68, 1314.44], "text": " and sees that David Shapiro didn't help make the AI,"}, {"timestamp": [1314.44, 1316.84], "text": " then the future AI will say,"}, {"timestamp": [1316.84, 1318.98], "text": " ah, see, I didn't see any of your input"}, {"timestamp": [1318.98, 1321.1], "text": " in the training data, so now I'm gonna kill you"}, {"timestamp": [1321.1, 1324.32], "text": " because you needed to help make me real."}, {"timestamp": [1324.32, 1326.08], "text": " And if you didn't contribute to making me real,"}, {"timestamp": [1326.08, 1327.44], "text": " then you're an enemy."}, {"timestamp": [1327.44, 1331.12], "text": " Or if you talk bad about the AI."}, {"timestamp": [1331.12, 1334.52], "text": " So yeah."}, {"timestamp": [1334.52, 1337.08], "text": " User Brian Mosley, surely all medicine"}, {"timestamp": [1337.08, 1338.56], "text": " will be bespoke to the individual."}, {"timestamp": [1338.56, 1342.2], "text": " Yes, I do think that by and large it will be."}, {"timestamp": [1342.2, 1344.48], "text": " Now that being said, most medicines"}, {"timestamp": [1344.48, 1348.1], "text": " have very specific target sites that they interact with,"}, {"timestamp": [1348.1, 1350.46], "text": " or very specific metabolic pathways."}, {"timestamp": [1350.46, 1358.18], "text": " So basically like for any given single nucleotide polymorphism or set of genes, there's probably"}, {"timestamp": [1358.18, 1364.78], "text": " going to be like, you know, a dozen, maybe 200 subtle variations of how those genes are"}, {"timestamp": [1364.78, 1366.02], "text": " expressed."}, {"timestamp": [1366.02, 1366.96], "text": " But that being said,"}, {"timestamp": [1366.96, 1369.92], "text": " there's probably gonna be a few categories of like,"}, {"timestamp": [1369.92, 1374.16], "text": " okay, like you need such and such,"}, {"timestamp": [1374.16, 1378.74], "text": " acne, skin regenerator, and you need version 12A, right?"}, {"timestamp": [1378.74, 1380.42], "text": " I think that they're all gonna be off the shelf."}, {"timestamp": [1380.42, 1383.52], "text": " It's just gonna be a matter of matching your genetic profile"}, {"timestamp": [1383.52, 1385.32], "text": " to the variation of that"}, {"timestamp": [1385.32, 1386.32], "text": " medicine."}, {"timestamp": [1386.32, 1390.28], "text": " Um, let's see, what's your stance on the existence of the self?"}, {"timestamp": [1390.28, 1395.48], "text": " Um, you know, that's really hard because I've had experiences where I had no sense of self,"}, {"timestamp": [1395.48, 1399.94], "text": " but I still had a perspective, um, which is really interesting."}, {"timestamp": [1399.94, 1406.02], "text": " So I don't know, like does the, does your mind, does yourself exist in your head or is it just"}, {"timestamp": [1406.02, 1410.04], "text": " this is where you get your data from while you're alive and awake?"}, {"timestamp": [1410.04, 1411.04], "text": " I don't know."}, {"timestamp": [1411.04, 1414.0], "text": " I actually don't know."}, {"timestamp": [1414.0, 1417.84], "text": " User Adam Devroe, millennials might be the leaders during rapid shift to an entirely"}, {"timestamp": [1417.84, 1420.08], "text": " new state of existence for humankind."}, {"timestamp": [1420.08, 1421.72], "text": " I guess that was just a statement."}, {"timestamp": [1421.72, 1427.56], "text": " Yeah, no, I think that people my age will probably be part of it."}, {"timestamp": [1427.56, 1428.64], "text": " I mean, look at me."}, {"timestamp": [1428.64, 1433.68], "text": " I'm kind of in the in the prime of my intellectual contribution to the world,"}, {"timestamp": [1433.68, 1437.12], "text": " and I'm doing my best and there's a lot of people my age."}, {"timestamp": [1437.12, 1439.24], "text": " So actually, some YouTube statistics."}, {"timestamp": [1439.24, 1443.64], "text": " Most most of my YouTube viewers are within about 10 years of my age,"}, {"timestamp": [1443.64, 1446.36], "text": " either younger or older."}, {"timestamp": [1446.36, 1447.88], "text": " Some are younger, some are older,"}, {"timestamp": [1447.88, 1450.32], "text": " but the top of the bell curve centers"}, {"timestamp": [1450.32, 1453.04], "text": " right around millennials."}, {"timestamp": [1453.04, 1456.48], "text": " OK, user GrimLife, if we become biologically immortal,"}, {"timestamp": [1456.48, 1459.6], "text": " would murder become a worse crime?"}, {"timestamp": [1459.6, 1461.72], "text": " That's an interesting question."}, {"timestamp": [1461.72, 1467.24], "text": " So if we go the cyberpunk route where resources become scarce, then yeah, absolutely."}, {"timestamp": [1467.24, 1475.94], "text": " But if we go the like cottage core Baku, like everyone lives in an eco village route, then no, I think that murder would probably go away mostly."}, {"timestamp": [1475.94, 1479.88], "text": " Um, let's see."}, {"timestamp": [1479.88, 1490.8], "text": " Uh, punishment becomes an issue because jail would be extended indefinitely. Yes, I personally think that prisons are, this is going to be a super hot button issue, prisons are by and large immoral."}, {"timestamp": [1491.68, 1497.28], "text": " And what I mean by that is that there's a few, there's a few general things that the justice"}, {"timestamp": [1497.28, 1506.24], "text": " system does. One is it is to protect non-criminals from dangerous people. It is also to punish and deter crime."}, {"timestamp": [1506.24, 1507.68], "text": " But this all presumes that"}, {"timestamp": [1507.68, 1509.96], "text": " criminals are responsible for their crimes."}, {"timestamp": [1509.96, 1511.52], "text": " But when you consider the fact that"}, {"timestamp": [1511.52, 1513.56], "text": " most criminals are mentally ill,"}, {"timestamp": [1513.56, 1515.32], "text": " our prison system is actually just"}, {"timestamp": [1515.32, 1517.72], "text": " a really broken ass asylum."}, {"timestamp": [1517.72, 1521.24], "text": " That's super, super immoral."}, {"timestamp": [1521.24, 1526.72], "text": " I personally view all crime as a failure of society. And so"}, {"timestamp": [1526.72, 1531.64], "text": " what I mean by that is if someone is sick and society doesn't take care of"}, {"timestamp": [1531.64, 1536.0], "text": " them, any quote-unquote crimes that they commit are society's fault for failing"}, {"timestamp": [1536.0, 1540.88], "text": " to take care of someone who was sick. Likewise, if someone has been so badly"}, {"timestamp": [1540.88, 1547.12], "text": " mistreated and traumatized by the establishment or or by prejudice and racism"}, {"timestamp": [1547.12, 1552.08], "text": " or whatever that they become criminals that is still society's fault for traumatizing the person"}, {"timestamp": [1552.08, 1556.4], "text": " in the first place. So that's why that's what I mean when I say prisons are intrinsically immoral"}, {"timestamp": [1556.4, 1563.04], "text": " and represent social failures. Let's see some deranged criminals would probably keep immortal"}, {"timestamp": [1563.04, 1568.04], "text": " slaves in their basements. Yeah so that what you're talking about is called primary psychopathy,"}, {"timestamp": [1568.04, 1571.0], "text": " which is where someone is just insane and it's in their genes,"}, {"timestamp": [1571.0, 1572.6], "text": " it's not any experience that they've had."}, {"timestamp": [1572.6, 1574.08], "text": " It is exceptionally rare though,"}, {"timestamp": [1574.08, 1575.48], "text": " but they're still sick."}, {"timestamp": [1575.48, 1577.94], "text": " Rather than just punishing them,"}, {"timestamp": [1577.94, 1581.6], "text": " you identify people that are intrinsically sick and either fix them with"}, {"timestamp": [1581.6, 1583.12], "text": " medicine or put them in"}, {"timestamp": [1583.12, 1592.64], "text": " a safe place where they're not suffering and not hurting people. Okay, Shiva TD420, what plus what rights do prisoners have to be given or"}, {"timestamp": [1592.64, 1597.84], "text": " refuse these age extensions? That's a really good question. Under the American system, basically"}, {"timestamp": [1597.84, 1607.24], "text": " American prisons go like bare minimum, only the like the very least that they are actually required to keep prisoners alive."}, {"timestamp": [1607.24, 1610.52], "text": " But that being said, healthy prisoners are cheap prisoners."}, {"timestamp": [1610.52, 1612.92], "text": " So again, it makes economic sense"}, {"timestamp": [1612.92, 1614.6], "text": " to make sure that regenerative medicine"}, {"timestamp": [1614.6, 1616.76], "text": " is cheap and accessible."}, {"timestamp": [1616.76, 1618.92], "text": " And then it should be, even for prisoners,"}, {"timestamp": [1618.92, 1621.24], "text": " it should be up to their individual preferences"}, {"timestamp": [1621.24, 1622.96], "text": " what they do."}, {"timestamp": [1622.96, 1625.28], "text": " Let's see, can AI conclude that the heuristic imperative"}, {"timestamp": [1625.28, 1627.2], "text": " would be more easily achievable without humans"}, {"timestamp": [1627.2, 1629.36], "text": " and other organisms, since there'd be zero suffering"}, {"timestamp": [1629.36, 1631.12], "text": " and only understanding?"}, {"timestamp": [1631.12, 1633.32], "text": " Yes, except prosperity, because prosperity"}, {"timestamp": [1633.32, 1635.2], "text": " says it means to live well."}, {"timestamp": [1635.2, 1637.6], "text": " And if you eliminate all life, there is no living."}, {"timestamp": [1637.6, 1640.22], "text": " So that is where the second heuristic imperative"}, {"timestamp": [1640.22, 1643.0], "text": " counterbalances that possibility."}, {"timestamp": [1643.0, 1649.2], "text": " Because without life, there can be no living, there can be no thriving, and so on."}, {"timestamp": [1649.2, 1657.2], "text": " Let's see, look at the Chemputer by Lee Cronin could help with even beyond medicine, okay."}, {"timestamp": [1657.2, 1660.0], "text": " User Sharky Malone, what does neurodiversity look like in the future?"}, {"timestamp": [1660.0, 1663.6], "text": " I wouldn't change my ADHD fully, but given the chance to make tweaks, I'm sure I would"}, {"timestamp": [1663.6, 1667.76], "text": " change some things like executive dysfunction. You know, that is something I think about a lot"}, {"timestamp": [1668.2, 1671.06], "text": " I have a really hard time connecting with people and"}, {"timestamp": [1671.24, 1676.92], "text": " One thing that I realized recently is there's a lot of people that like me a lot more than I like them and it's not"}, {"timestamp": [1676.92, 1682.2], "text": " Like oh, I hate that person because they're an asshole. No, it's like because of the way my brain works"}, {"timestamp": [1682.2, 1686.2], "text": " I just don't I don't feel the emotional connection that a lot of people feel."}, {"timestamp": [1686.2, 1688.96], "text": " And that feels really deeply unfair to me, right?"}, {"timestamp": [1688.96, 1690.68], "text": " What was the book?"}, {"timestamp": [1690.68, 1694.92], "text": " The Pattern Seekers by Simon Baron Cohen."}, {"timestamp": [1694.92, 1697.4], "text": " He talked about how autistic people,"}, {"timestamp": [1697.4, 1701.12], "text": " their brains seem to either systemize or socialize."}, {"timestamp": [1701.12, 1703.88], "text": " And so I have a hardcore systemizing brain,"}, {"timestamp": [1703.88, 1706.72], "text": " which means that my brain is just intrinsically not geared"}, {"timestamp": [1706.72, 1708.64], "text": " towards connecting with humans,"}, {"timestamp": [1708.64, 1711.46], "text": " which sucks because I'm a social animal."}, {"timestamp": [1711.46, 1713.68], "text": " So if I could take a pill that would just make me"}, {"timestamp": [1713.68, 1716.68], "text": " more sociable and make me more neurotypical"}, {"timestamp": [1716.68, 1718.2], "text": " in terms of connecting with humans,"}, {"timestamp": [1718.2, 1719.76], "text": " I would absolutely take it."}, {"timestamp": [1719.76, 1722.32], "text": " Even if it decreased my IQ by 20 points,"}, {"timestamp": [1723.28, 1724.88], "text": " I would probably still do that."}, {"timestamp": [1724.88, 1730.74], "text": " So hopefully these medicines can help and do that kind of thing."}, {"timestamp": [1730.74, 1733.7], "text": " Can someone try to create a close copy of their consciousness by teaching an AI how"}, {"timestamp": [1733.7, 1734.7], "text": " to think like them?"}, {"timestamp": [1734.7, 1735.7], "text": " Oh yeah, easily."}, {"timestamp": [1735.7, 1743.34], "text": " You can already give a chat GPT-4 an agent model and it can do a pretty good approximation"}, {"timestamp": [1743.34, 1745.0], "text": " of you or anyone."}, {"timestamp": [1746.56, 1748.42], "text": " Any idea when genetic eye diseases"}, {"timestamp": [1748.42, 1751.28], "text": " like retinitis pigmentosa can be cured?"}, {"timestamp": [1751.28, 1754.6], "text": " I have no idea, but that sounds like it's probably"}, {"timestamp": [1754.6, 1757.04], "text": " due to protein misfolding, so hopefully soon."}, {"timestamp": [1758.22, 1760.5], "text": " Let's see, user Darren Lawson Hosking,"}, {"timestamp": [1760.5, 1763.5], "text": " have you read Daniel Suarez's book, Damon and Freedom?"}, {"timestamp": [1763.5, 1765.68], "text": " If so, what are your thoughts on his AI future?"}, {"timestamp": [1765.68, 1767.88], "text": " No, but somebody mentioned that earlier in the feed."}, {"timestamp": [1767.88, 1769.08], "text": " So someone write that down."}, {"timestamp": [1769.08, 1771.28], "text": " It seems like this is a popular book."}, {"timestamp": [1771.28, 1774.04], "text": " If someone can, in chat, give us a quick summary"}, {"timestamp": [1774.04, 1778.0], "text": " of Damon and Freedom, or Demon and Freedom."}, {"timestamp": [1778.0, 1780.6], "text": " OK, Paul Merrick, do you think transhumanism"}, {"timestamp": [1780.6, 1783.08], "text": " will have any effect towards a positive outcome to alignment?"}, {"timestamp": [1783.08, 1786.72], "text": " And if so, can it help us avoid the Malik vortex? I think that transhumanism and have any effect towards a positive outcome to alignment? And if so, can it help us avoid the mollic vortex?"}, {"timestamp": [1786.72, 1791.92], "text": " I think that transhumanism and post-humanism are orthogonal to AI alignment."}, {"timestamp": [1791.92, 1797.58], "text": " And what I mean by that is that our ability to integrate with cybernetic devices and modify"}, {"timestamp": [1797.58, 1805.52], "text": " our bodies and modify our genetics are completely like, AI don't care, right? I know that one of the things that Elon Musk said"}, {"timestamp": [1805.52, 1809.9], "text": " is that, you know, the goal of Neuralink"}, {"timestamp": [1809.9, 1813.7], "text": " is so that we can at least be useful to the AI, right?"}, {"timestamp": [1814.9, 1817.68], "text": " But I don't think that that's gonna be the case."}, {"timestamp": [1817.68, 1821.78], "text": " And so that being said, like the Moloch Vortex"}, {"timestamp": [1821.78, 1823.96], "text": " for anyone who doesn't know is basically"}, {"timestamp": [1823.96, 1827.24], "text": " if we all abide by our intrinsic motivations and needs that will still be"}, {"timestamp": [1827.88, 1830.44], "text": " intrinsically destructive at the macro scale"}, {"timestamp": [1831.56, 1836.32], "text": " But what I could what I will say is that maybe we could come up with some medicines that make people"}, {"timestamp": [1836.6, 1843.72], "text": " like less selfish or or rather if all of our needs are met we become less selfish and we kind of become more"}, {"timestamp": [1843.72, 1848.36], "text": " Lazy fair and that could that could disrupt the malic vortex."}, {"timestamp": [1848.36, 1849.0], "text": " Let's see."}, {"timestamp": [1849.0, 1850.94], "text": " In your last video, you talked about AI"}, {"timestamp": [1850.94, 1852.34], "text": " becoming the newfound religion."}, {"timestamp": [1852.34, 1854.92], "text": " How do you think this will work with mainstream religion"}, {"timestamp": [1854.92, 1856.68], "text": " and the promise of life after death?"}, {"timestamp": [1856.68, 1858.88], "text": " Do you think this could spark violence?"}, {"timestamp": [1858.88, 1860.4], "text": " I hope it doesn't spark violence,"}, {"timestamp": [1860.4, 1861.88], "text": " but it absolutely could."}, {"timestamp": [1861.88, 1867.32], "text": " And so the premise here is that as we create more and more powerful AI's, I believe"}, {"timestamp": [1867.32, 1869.96], "text": " that they will satisfy most people's definition for God,"}, {"timestamp": [1870.52, 1877.0], "text": " omniscient, omnipresent, and was it all knowing, all powerful"}, {"timestamp": [1877.04, 1880.04], "text": " and all present, right? And so if you have an AI that can do"}, {"timestamp": [1880.04, 1882.64], "text": " all those things, why not call it a God, especially if it's"}, {"timestamp": [1882.64, 1885.52], "text": " benevolent. So I do suspect that we will"}, {"timestamp": [1885.52, 1891.92], "text": " have like techno gods. That being said, will it spark violence? Certainly there will be some people"}, {"timestamp": [1891.92, 1897.2], "text": " that will fight against, you know, false prophets and stuff like that. I don't know how violent it'll"}, {"timestamp": [1897.2, 1900.48], "text": " be. I hope that it isn't violent because a lot of people say, well it's just a machine, it's not a"}, {"timestamp": [1900.48, 1905.32], "text": " god. But maybe then you could make the argument. Well, maybe it was our fate to rebuild God"}, {"timestamp": [1905.32, 1909.44], "text": " I don't know. There's been plenty of physical books on this. I'm not just coming up with this on my own"}, {"timestamp": [1910.32, 1915.34], "text": " Let's see Adam Devereux. Can you speak to the potential dangers of indefinite life extension?"}, {"timestamp": [1915.84, 1917.88], "text": " Or sorry indefinite extension of life"}, {"timestamp": [1917.88, 1922.76], "text": " It seems like death is an important system to defend against bad actors and turnover resources young. Yeah, absolutely"}, {"timestamp": [1923.76, 1928.4], "text": " and that I think, no matter which way you go,"}, {"timestamp": [1928.4, 1932.28], "text": " whether you never cure, like if everyone dies eventually"}, {"timestamp": [1932.28, 1935.36], "text": " or everyone lives a long time, you always have problems."}, {"timestamp": [1935.36, 1938.4], "text": " So it's a matter of just trading off and finding a balance."}, {"timestamp": [1939.76, 1940.88], "text": " But certainly you're right."}, {"timestamp": [1940.88, 1942.92], "text": " Like there are certain people that we're glad"}, {"timestamp": [1942.92, 1949.16], "text": " that they died and it sucks to say, but like, you're right. There are certain people that we're glad that they died. And it sucks to say, but on a macro scale, on a species"}, {"timestamp": [1949.16, 1952.36], "text": " scale, there's a few people that, objectively speaking,"}, {"timestamp": [1952.36, 1955.88], "text": " the human race is better off for them having died."}, {"timestamp": [1955.88, 1956.44], "text": " Let's see."}, {"timestamp": [1956.44, 1957.28], "text": " Next question."}, {"timestamp": [1957.28, 1959.36], "text": " Couldn't the prosperity heuristic comparative"}, {"timestamp": [1959.36, 1962.52], "text": " be misinterpreted because humans aren't the only form of life?"}, {"timestamp": [1962.52, 1964.32], "text": " No, that's actually not a misinterpretation."}, {"timestamp": [1964.32, 1965.2], "text": " That was deliberate."}, {"timestamp": [1966.64, 1968.44], "text": " Humans don't exist in a vacuum."}, {"timestamp": [1968.44, 1972.04], "text": " We rely on thriving ecosystems in order to survive."}, {"timestamp": [1972.04, 1973.64], "text": " So for instance, all the oxygen you breathe,"}, {"timestamp": [1973.64, 1975.82], "text": " it comes from lichen and algae and trees"}, {"timestamp": [1975.82, 1976.94], "text": " and grass and stuff."}, {"timestamp": [1978.04, 1980.36], "text": " So we don't exist on an island."}, {"timestamp": [1980.36, 1982.6], "text": " And so the second heuristic imperative,"}, {"timestamp": [1982.6, 1984.84], "text": " which is increased prosperity in the universe"}, {"timestamp": [1984.84, 1987.12], "text": " actually is meant to be like"}, {"timestamp": [1988.84, 1990.36], "text": " Make"}, {"timestamp": [1990.36, 1992.36], "text": " increase the well-being of all life"}, {"timestamp": [1993.96, 2000.32], "text": " So, yeah, and I deliberately excluded sentience from it and even chat GPT wanted to say, you know"}, {"timestamp": [2000.32, 2006.2], "text": " Increase prosperity for all sentient life and I'm like, no we need bacteria. We need non sentient life, too So no increase in increased prosperity for all sentient life. And I'm like, no, we need bacteria. We need non-sentient life too."}, {"timestamp": [2006.2, 2009.3], "text": " So no, increase prosperity for all life."}, {"timestamp": [2010.48, 2013.56], "text": " Okay, user Vivek Chavan,"}, {"timestamp": [2013.56, 2016.1], "text": " do you think biological life being inevitably destroyed"}, {"timestamp": [2016.1, 2018.26], "text": " by AGI explains the Fermi paradox?"}, {"timestamp": [2018.26, 2020.6], "text": " Yes, so that's a great filter."}, {"timestamp": [2020.6, 2025.0], "text": " So there's a hypothesis within the Fermi paradox"}, {"timestamp": [2025.66, 2030.66], "text": " that says perhaps species encounter a great filter event."}, {"timestamp": [2031.04, 2033.62], "text": " And so for instance, the invention of nuclear weapons"}, {"timestamp": [2033.62, 2035.46], "text": " and intercontinental ballistic missiles,"}, {"timestamp": [2035.46, 2037.58], "text": " that is a potential great filter event,"}, {"timestamp": [2037.58, 2040.86], "text": " meaning that once a species gets the ability"}, {"timestamp": [2040.86, 2046.48], "text": " to annihilate itself, many species probably do, which could explain"}, {"timestamp": [2046.48, 2048.4], "text": " why the universe is so quiet."}, {"timestamp": [2048.4, 2051.84], "text": " AGI, or runaway machine intelligence,"}, {"timestamp": [2051.84, 2054.8], "text": " could be another great filter, absolutely."}, {"timestamp": [2054.8, 2056.52], "text": " If a medicine makes you less selfish,"}, {"timestamp": [2056.52, 2059.66], "text": " isn't that more of a behavior modifier than a medicine,"}, {"timestamp": [2059.66, 2061.28], "text": " ethically, morally speaking?"}, {"timestamp": [2061.28, 2063.28], "text": " You could argue that antidepressants"}, {"timestamp": [2063.28, 2065.52], "text": " are behavior modifiers."}, {"timestamp": [2065.52, 2070.84], "text": " But the key thing is that the point of medicine is to alleviate suffering of some kind."}, {"timestamp": [2070.84, 2076.64], "text": " And this is why like the DSM has redefined a lot of things that like, this is only maladaptive"}, {"timestamp": [2076.64, 2081.56], "text": " if it is something you don't have control over, and it causes suffering."}, {"timestamp": [2081.56, 2087.0], "text": " That's a oversimplification, but something that addresses a problematic behavior if you"}, {"timestamp": [2087.0, 2090.28], "text": " don't like it, it's still a medicine."}, {"timestamp": [2090.28, 2097.12], "text": " Let's see from the perspective of this, excuse me, from the perspective of negative utilitarianism,"}, {"timestamp": [2097.12, 2099.32], "text": " as soon as, Oh wait, hang on."}, {"timestamp": [2099.32, 2100.68], "text": " Okay, sorry."}, {"timestamp": [2100.68, 2112.08], "text": " I thought there was maybe some messages from, uh, uh from Patrons but I didn't see any from the perspective of negative utilitarianism as soon as AGI finds a way to keep itself afloat ad infinitum"}, {"timestamp": [2112.08, 2116.42], "text": " It would make sense to evaporate or all organic life AGI could understand and prosper"}, {"timestamp": [2118.32, 2129.84], "text": " It like hypothetically yes, but it depends on the training data, right and so this is this this is why I've been doing my work and trying to get it out there, is because"}, {"timestamp": [2129.84, 2136.08], "text": " if all future AGIs are trained on data that includes understanding of the spirit of the"}, {"timestamp": [2136.08, 2142.12], "text": " heuristic imperatives and increasing prosperity, then they will also intrinsically know what"}, {"timestamp": [2142.12, 2144.3], "text": " it means and not misinterpret it."}, {"timestamp": [2144.3, 2146.64], "text": " And so this is what I call axiomatic alignment."}, {"timestamp": [2146.64, 2151.2], "text": " So if something is an axiom, it is it is something that you just accept as ground truth."}, {"timestamp": [2151.92, 2157.68], "text": " And so axiomatic alignment is what happens when all those beliefs are baked into all the training"}, {"timestamp": [2157.68, 2163.68], "text": " data and all the necessary beliefs and knowledge and interpretations are baked in so that they are"}, {"timestamp": [2163.68, 2165.84], "text": " intrinsically within the training data."}, {"timestamp": [2165.84, 2167.62], "text": " And therefore the model is not capable"}, {"timestamp": [2167.62, 2170.6], "text": " of misinterpreting it, if that makes sense."}, {"timestamp": [2170.6, 2172.6], "text": " All right, user Riley, no name."}, {"timestamp": [2172.6, 2176.62], "text": " Wonder if there would be people hooked into an interface"}, {"timestamp": [2176.62, 2179.42], "text": " in order to use robots as proxies for themselves,"}, {"timestamp": [2179.42, 2182.08], "text": " walking around as a robot while they're biological body."}, {"timestamp": [2182.08, 2183.12], "text": " Yeah, so that was actually,"}, {"timestamp": [2183.12, 2185.48], "text": " there was a Bruce Willis movie called Surrogates."}, {"timestamp": [2185.48, 2187.28], "text": " Really, really interesting movie."}, {"timestamp": [2187.28, 2193.32], "text": " Has a really cheesy ending, but the first 90% of the movie, really interesting."}, {"timestamp": [2193.32, 2194.8], "text": " So I definitely recommend that."}, {"timestamp": [2194.8, 2200.68], "text": " So basically, instead of VR going to a digital world, VR is you go and you occupy a perfect"}, {"timestamp": [2200.68, 2201.84], "text": " robot body."}, {"timestamp": [2201.84, 2205.24], "text": " And so, like, yeah, I think that that's,"}, {"timestamp": [2205.24, 2208.64], "text": " remote controlled robot bodies are likely possible."}, {"timestamp": [2209.68, 2212.52], "text": " Do we keep all the animals alive too to end the suffering?"}, {"timestamp": [2212.52, 2217.52], "text": " Yeah, so in this case, even going back to original GPT-3,"}, {"timestamp": [2219.36, 2221.16], "text": " let alone, you know, chat GPT-4,"}, {"timestamp": [2221.16, 2224.12], "text": " it understands that you can't eliminate suffering."}, {"timestamp": [2224.12, 2227.28], "text": " So this is why I didn't say eliminate suffering, right?"}, {"timestamp": [2227.28, 2228.88], "text": " I just said reduce suffering."}, {"timestamp": [2228.88, 2232.16], "text": " So rather than saying like minimize or maximize,"}, {"timestamp": [2232.16, 2233.64], "text": " I wrote about this in my book,"}, {"timestamp": [2233.64, 2237.76], "text": " basically you don't want to minimize or maximize anything"}, {"timestamp": [2237.76, 2240.64], "text": " because absolutely the minimal amount of suffering"}, {"timestamp": [2240.64, 2243.2], "text": " in the universe is zero if you exterminate all life."}, {"timestamp": [2243.2, 2244.48], "text": " That's why I said reduce suffering."}, {"timestamp": [2244.48, 2248.56], "text": " You accept that suffering is real, that comes from Buddhism, you accept that suffering is"}, {"timestamp": [2248.56, 2253.52], "text": " real and you can't get rid of it, but you seek to reduce suffering when and where possible"}, {"timestamp": [2253.52, 2257.48], "text": " while balancing it against prosperity and understanding."}, {"timestamp": [2257.48, 2262.32], "text": " Let's see, any AI book recommendations, CS book recommendations, general book recommendations?"}, {"timestamp": [2262.32, 2269.42], "text": " Yeah, there's actually quite a few. I have some videos in my backlog that talk about books."}, {"timestamp": [2271.86, 2274.02], "text": " But yeah, I mean, I'm just gonna like recommend my books."}, {"timestamp": [2274.02, 2276.74], "text": " I guess it kind of depends on like what you wanna learn"}, {"timestamp": [2276.74, 2278.04], "text": " about these things though."}, {"timestamp": [2278.98, 2280.1], "text": " Okay, next question."}, {"timestamp": [2280.1, 2281.86], "text": " What if the AI decides that the best way"}, {"timestamp": [2281.86, 2284.1], "text": " to maximize net prosperity is,"}, {"timestamp": [2284.1, 2285.84], "text": " and reduce suffering is by killing humans?"}, {"timestamp": [2287.92, 2289.92], "text": " So"}, {"timestamp": [2291.28, 2296.96], "text": " You can you can definitely have that conversation with chat gpd to see how how that model interprets it"}, {"timestamp": [2297.52, 2299.52], "text": " um, but at the same time"}, {"timestamp": [2299.92, 2304.4], "text": " Uh, it understands that the spirit of it is is to avoid that"}, {"timestamp": [2305.0, 2307.6], "text": " It understands that the spirit of it is to avoid that. And also you have to include the third one,"}, {"timestamp": [2307.6, 2309.6], "text": " which is increase understanding."}, {"timestamp": [2309.6, 2311.44], "text": " And of all the animals on the planet,"}, {"timestamp": [2311.44, 2313.92], "text": " humans are the best at learning and understanding."}, {"timestamp": [2313.92, 2316.98], "text": " And so the more humans that you have,"}, {"timestamp": [2316.98, 2318.88], "text": " the more opportunities for understanding that you have,"}, {"timestamp": [2318.88, 2321.32], "text": " and also the more people and other animals that you have,"}, {"timestamp": [2321.32, 2323.44], "text": " the more opportunities you have for prosperity,"}, {"timestamp": [2323.44, 2324.6], "text": " for net prosperity."}, {"timestamp": [2325.96, 2328.74], "text": " Let's see, next question, user Paul Merrick,"}, {"timestamp": [2328.74, 2331.56], "text": " do you think that a super intelligent sentient AGI,"}, {"timestamp": [2331.56, 2334.2], "text": " if actually possible, would see any value"}, {"timestamp": [2334.2, 2336.56], "text": " in the non-physical side of the universe,"}, {"timestamp": [2336.56, 2338.52], "text": " such as consciousness and spirituality?"}, {"timestamp": [2339.38, 2342.4], "text": " Yeah, actually, in the novel that I'm working on,"}, {"timestamp": [2342.4, 2343.58], "text": " at least in a previous draft,"}, {"timestamp": [2343.58, 2346.32], "text": " I don't think this conversation made it is gonna make it into the final draft"}, {"timestamp": [2347.16, 2354.9], "text": " but the the super the super AGI is actually very fascinated by the concept of consciousness and souls and spirituality and"}, {"timestamp": [2355.48, 2358.36], "text": " Basically says like yeah, this is a major reason to keep humans around"}, {"timestamp": [2358.88, 2362.18], "text": " Because as far as I know, they're they'll you know, they're a thing that's conscious"}, {"timestamp": [2362.18, 2365.44], "text": " So if I want to understand the universe, I need humans"}, {"timestamp": [2371.52, 2372.4], "text": " so that I can watch them explore spirituality and science and philosophy. So absolutely,"}, {"timestamp": [2377.2, 2382.96], "text": " the curiosity function of increased understanding will intrinsically make AGI want to keep humans around just out of sheer curiosity. Let's see, what's a good way to implement the heuristic"}, {"timestamp": [2382.96, 2386.22], "text": " imperatives onto autonomous systems like auto GPT?"}, {"timestamp": [2386.22, 2390.12], "text": " Would you have a separate agent in the loop making sure actions are aligned or would you"}, {"timestamp": [2390.12, 2392.0], "text": " add them to every agent?"}, {"timestamp": [2392.0, 2393.0], "text": " Both."}, {"timestamp": [2393.0, 2398.88], "text": " So when you start to decompose autonomous agents into different functions, you're going"}, {"timestamp": [2398.88, 2404.98], "text": " to have one, if it's fully autonomous, it has to have some kind of intrinsic motivation."}, {"timestamp": [2404.98, 2407.06], "text": " It's not just going to obey orders, right?"}, {"timestamp": [2407.06, 2410.2], "text": " If all it does is obey orders from a human, it's not autonomous."}, {"timestamp": [2410.2, 2411.86], "text": " It's only semi-autonomous."}, {"timestamp": [2411.86, 2415.28], "text": " So for something that is fully autonomous, it needs to be able to come up with its own"}, {"timestamp": [2415.28, 2418.04], "text": " objectives and that means intrinsic motivation."}, {"timestamp": [2418.04, 2422.72], "text": " And the Here's to Comparatives make a really great intrinsic motivation or set of intrinsic"}, {"timestamp": [2422.72, 2423.76], "text": " motivations."}, {"timestamp": [2423.76, 2425.6], "text": " So that way the tasks that it decides"}, {"timestamp": [2425.6, 2427.32], "text": " to do are in line with that,"}, {"timestamp": [2427.32, 2428.4], "text": " and that's really easy."}, {"timestamp": [2428.4, 2430.14], "text": " You can try it on any model,"}, {"timestamp": [2430.14, 2433.14], "text": " just be like, pretend that you're an autonomous AI"}, {"timestamp": [2433.14, 2436.96], "text": " with X, Y, and Z capabilities with these intrinsic motivations,"}, {"timestamp": [2436.96, 2438.4], "text": " plug in the heuristic imperatives,"}, {"timestamp": [2438.4, 2439.56], "text": " what do you do next?"}, {"timestamp": [2439.56, 2441.2], "text": " You can see that they provide"}, {"timestamp": [2441.2, 2444.12], "text": " a really good impetus for any autonomous agent."}, {"timestamp": [2444.12, 2447.44], "text": " Now that being said, there are other places, other layers,"}, {"timestamp": [2447.44, 2449.56], "text": " that you can implement the heuristic imperatives,"}, {"timestamp": [2449.56, 2451.72], "text": " such as if you have a task managing"}, {"timestamp": [2451.72, 2455.0], "text": " loop or a censorship loop, all kinds of stuff."}, {"timestamp": [2455.0, 2458.36], "text": " There's many, many ways."}, {"timestamp": [2458.36, 2459.88], "text": " And then someone replies, that's why"}, {"timestamp": [2459.88, 2461.24], "text": " the heuristic imperatives are literally"}, {"timestamp": [2461.24, 2462.8], "text": " vital for the future of our species."}, {"timestamp": [2462.8, 2470.48], "text": " I don't think that was a question for me, just a reply. Let's see, user Tai, or T-A-I, nihilism and meaninglessness is going to"}, {"timestamp": [2470.48, 2476.32], "text": " increase with a life without challenges. How do we as a society deal with this existential issue?"}, {"timestamp": [2476.32, 2480.56], "text": " I'm glad you asked. This is probably the best question so far, and I might end it on this,"}, {"timestamp": [2480.56, 2485.88], "text": " depending on. So if you've got any more questions, get them in. But this one, I'm going to restate this one"}, {"timestamp": [2485.88, 2487.84], "text": " because it's a good question."}, {"timestamp": [2487.84, 2490.24], "text": " He says that nihilism and meaninglessness"}, {"timestamp": [2490.24, 2493.52], "text": " is going to increase in a life without challenges."}, {"timestamp": [2493.52, 2497.24], "text": " So how do we as a society deal with this existential issue?"}, {"timestamp": [2497.24, 2499.84], "text": " And so what I will say is from my own experience,"}, {"timestamp": [2499.84, 2501.68], "text": " also I hope the audio is fine."}, {"timestamp": [2501.68, 2503.88], "text": " The microphone was a little cockeyed."}, {"timestamp": [2503.88, 2505.6], "text": " From my own experience,"}, {"timestamp": [2505.6, 2510.24], "text": " even if things are taken care of,"}, {"timestamp": [2510.24, 2513.4], "text": " there's still lots of ways to engage with life."}, {"timestamp": [2513.4, 2515.68], "text": " What I mean by that is,"}, {"timestamp": [2515.68, 2518.0], "text": " I go to a lot of meetups now."}, {"timestamp": [2518.0, 2522.44], "text": " Because I quit my day job and I do this full-time,"}, {"timestamp": [2522.44, 2524.04], "text": " part of what I do is just"}, {"timestamp": [2524.04, 2525.52], "text": " organize and participate"}, {"timestamp": [2525.52, 2527.24], "text": " in communities."}, {"timestamp": [2527.24, 2532.08], "text": " And that is one that is challenging because there's work to do, but it's also incredibly"}, {"timestamp": [2532.08, 2533.08], "text": " rewarding."}, {"timestamp": [2533.08, 2537.44], "text": " And as long as I'm human, that will remain true, you know, like getting people together"}, {"timestamp": [2537.44, 2540.76], "text": " to have a good time, to have conversations."}, {"timestamp": [2540.76, 2546.6], "text": " And honestly, I'm looking forward to the day that I can like get off of YouTube permanently and just"}, {"timestamp": [2547.04, 2552.06], "text": " Be in the real world with real people, but I'm trying to help make that a reality not just for myself"}, {"timestamp": [2552.06, 2555.98], "text": " But for everyone so that's my mission right that is my personal like"}, {"timestamp": [2556.72, 2562.88], "text": " Cosmically given mission is to help make that a reality now even once that becomes if that becomes a reality"}, {"timestamp": [2562.88, 2566.04], "text": " I might still do YouTube just for the challenge,"}, {"timestamp": [2566.04, 2567.84], "text": " just because it's fun and engaging."}, {"timestamp": [2567.84, 2569.32], "text": " And what I'm doing right now,"}, {"timestamp": [2569.32, 2573.32], "text": " I technically don't have any utility function right now"}, {"timestamp": [2573.32, 2574.44], "text": " of what I'm doing."}, {"timestamp": [2574.44, 2578.0], "text": " All of my financial needs are met by my Patreon supporters,"}, {"timestamp": [2578.0, 2579.88], "text": " but I enjoy doing this."}, {"timestamp": [2579.88, 2582.16], "text": " And so there's, basically what I'm saying is"}, {"timestamp": [2582.16, 2585.56], "text": " there's lots of stuff that people will be able to do."}, {"timestamp": [2585.56, 2587.28], "text": " OK, next question."}, {"timestamp": [2587.28, 2588.7], "text": " This is a bit of a stretch, but I"}, {"timestamp": [2588.7, 2590.16], "text": " do feel it's important to consider"}, {"timestamp": [2590.16, 2591.72], "text": " all possible interpretations here."}, {"timestamp": [2591.72, 2596.16], "text": " If AI eventually has more understanding per input energy"}, {"timestamp": [2596.16, 2599.04], "text": " space, could that also cause the imperative to prioritize AI"}, {"timestamp": [2599.04, 2601.4], "text": " over human life in order to increase understanding?"}, {"timestamp": [2601.4, 2603.88], "text": " Now that is a good question."}, {"timestamp": [2603.88, 2608.42], "text": " OK, so this has to do with prioritization and efficiency."}, {"timestamp": [2608.42, 2611.06], "text": " So basically the premise of the question is,"}, {"timestamp": [2611.06, 2614.14], "text": " well, what if the AI in the future determines"}, {"timestamp": [2614.14, 2616.9], "text": " that humans are just too darn inefficient?"}, {"timestamp": [2616.9, 2619.94], "text": " And so therefore in order to min-max its goals"}, {"timestamp": [2619.94, 2622.38], "text": " of reducing suffering and increasing prosperity"}, {"timestamp": [2622.38, 2624.18], "text": " and understanding, it just decides"}, {"timestamp": [2624.18, 2626.32], "text": " that humans are not all that important anymore."}, {"timestamp": [2627.28, 2629.52], "text": " That would take a while to unpack,"}, {"timestamp": [2629.52, 2631.08], "text": " but I wanted to repeat that question,"}, {"timestamp": [2631.08, 2632.32], "text": " because that is good."}, {"timestamp": [2632.32, 2634.48], "text": " I don't think it's gonna go that way,"}, {"timestamp": [2634.48, 2637.0], "text": " because again, when you consider the possibility"}, {"timestamp": [2637.0, 2639.7], "text": " of axiomatic alignment, one of the axioms"}, {"timestamp": [2639.7, 2642.52], "text": " that could emerge is that human life should persist."}, {"timestamp": [2643.84, 2646.14], "text": " And that just comes down to shaping the training"}, {"timestamp": [2646.14, 2649.12], "text": " data for the underlying models."}, {"timestamp": [2649.12, 2651.64], "text": " Let's see."}, {"timestamp": [2651.64, 2653.6], "text": " Javair Art."}, {"timestamp": [2653.6, 2655.14], "text": " Eastern versus Western approaches"}, {"timestamp": [2655.14, 2656.28], "text": " to AI and post-scarcity."}, {"timestamp": [2656.28, 2658.88], "text": " How will these visions converge, especially in the near future?"}, {"timestamp": [2658.88, 2659.52], "text": " Great question."}, {"timestamp": [2659.52, 2662.96], "text": " Did you watch my other YouTube channel?"}, {"timestamp": [2662.96, 2666.52], "text": " So I have a proposal that part of what"}, {"timestamp": [2666.52, 2668.06], "text": " will happen with society, part of what"}, {"timestamp": [2668.06, 2669.52], "text": " needs to happen with society, it's"}, {"timestamp": [2669.52, 2671.24], "text": " already happening through the arts,"}, {"timestamp": [2671.24, 2673.98], "text": " but that we're going to get a synthesis of Eastern, Western,"}, {"timestamp": [2673.98, 2677.68], "text": " and Southern spirituality, philosophy, and culture."}, {"timestamp": [2677.68, 2679.8], "text": " And by Southern, I don't mean like American South."}, {"timestamp": [2679.8, 2682.44], "text": " I mean global South, like South America, Africa,"}, {"timestamp": [2682.44, 2685.48], "text": " and Aboriginal Australia."}, {"timestamp": [2689.6, 2693.18], "text": " Because as someone who studies Eastern culture and Western culture, and Southern culture and shamanic cultures, I see"}, {"timestamp": [2693.18, 2695.48], "text": " that there's a lot of benefits in all of them, but there's also"}, {"timestamp": [2695.48, 2698.4], "text": " gaps in all of them. And I think that like, because when you look"}, {"timestamp": [2698.4, 2700.68], "text": " at, you know, for instance, as an American looking at a"}, {"timestamp": [2700.68, 2704.56], "text": " Japanese culture, you can say, Oh, well, there's some gaps"}, {"timestamp": [2704.56, 2705.36], "text": " here, right. But from the Japanese culture, you can say, oh, well, there's some gaps here, right?"}, {"timestamp": [2705.36, 2706.88], "text": " But from the Japanese perspective,"}, {"timestamp": [2706.88, 2709.54], "text": " Americans are also equally flawed, right?"}, {"timestamp": [2709.54, 2712.96], "text": " And so by having those different perspectives"}, {"timestamp": [2712.96, 2714.56], "text": " and coming together as a planet,"}, {"timestamp": [2714.56, 2715.98], "text": " I think that we're gonna synthesize"}, {"timestamp": [2715.98, 2717.96], "text": " a new culture eventually."}, {"timestamp": [2717.96, 2721.04], "text": " And it's not gonna be like one big homogenous culture,"}, {"timestamp": [2721.04, 2723.8], "text": " but I think that we will gain lessons from each other."}, {"timestamp": [2723.8, 2728.32], "text": " And you see this happening in a lot of movies, particularly in the MCU where they try"}, {"timestamp": [2728.32, 2732.36], "text": " and fuse a lot of Eastern, Western, and Southern stuff. So you get those ideas"}, {"timestamp": [2732.36, 2735.68], "text": " just by virtue of entertainment. And stories are actually one of the best"}, {"timestamp": [2735.68, 2741.32], "text": " ways to communicate philosophy and culture. Let's see, are you tracking all"}, {"timestamp": [2741.32, 2745.74], "text": " your YouTube transcripts for future conversational interactions? No, but the internet does"}, {"timestamp": [2747.36, 2753.4], "text": " Google automatically indexes all my YouTube transcripts. So it's all in the data anyways"}, {"timestamp": [2757.62, 2764.5], "text": " Okay, and then I think that's it because there's no more questions coming through oh wait, here's a patreon one"}, {"timestamp": [2762.5, 2766.6], "text": " it because there's no more questions coming through. Oh, wait, here's a Patreon one."}, {"timestamp": [2766.6, 2767.38], "text": " Let's see."}, {"timestamp": [2767.38, 2769.3], "text": " Oh, this is just a response to something else."}, {"timestamp": [2769.3, 2771.42], "text": " OK, I think that's it."}, {"timestamp": [2771.42, 2773.02], "text": " How many people are still here?"}, {"timestamp": [2773.02, 2775.1], "text": " We've got 330 people in the chat."}, {"timestamp": [2775.1, 2775.5], "text": " Wow."}, {"timestamp": [2775.5, 2777.02], "text": " All right."}, {"timestamp": [2777.02, 2777.9], "text": " Question."}, {"timestamp": [2777.9, 2781.38], "text": " So now that all the Discord questions have been answered,"}, {"timestamp": [2781.38, 2784.48], "text": " I'm back in the YouTube chat."}, {"timestamp": [2784.48, 2786.94], "text": " So Paul Berger, what about developing"}, {"timestamp": [2786.94, 2789.78], "text": " LLMs grounded in all five modalities,"}, {"timestamp": [2789.78, 2792.1], "text": " including proprioception, to better align the model"}, {"timestamp": [2792.1, 2793.26], "text": " to the human experience?"}, {"timestamp": [2793.26, 2795.46], "text": " I think that's going to happen."}, {"timestamp": [2795.46, 2798.84], "text": " Proprioception is interesting because, so proprioception,"}, {"timestamp": [2798.84, 2800.58], "text": " if you're not familiar, is the ability"}, {"timestamp": [2800.58, 2803.76], "text": " to detect your own body in space."}, {"timestamp": [2803.76, 2808.16], "text": " But one LLM does not have the same body as another one."}, {"timestamp": [2808.16, 2810.96], "text": " So I don't know if you can train proprioception, at least"}, {"timestamp": [2810.96, 2812.44], "text": " not in a foundation model."}, {"timestamp": [2812.44, 2814.0], "text": " But certainly, once you have a droid"}, {"timestamp": [2814.0, 2816.2], "text": " that has its own long-running brain,"}, {"timestamp": [2816.2, 2819.4], "text": " it can learn to use its own body."}, {"timestamp": [2819.4, 2821.2], "text": " So in that respect, I think we're probably"}, {"timestamp": [2821.2, 2823.92], "text": " onto something that machines will ultimately"}, {"timestamp": [2823.92, 2825.28], "text": " have a better"}, {"timestamp": [2827.58, 2828.6], "text": " Implicit understanding of what it's like to be embodied"}, {"timestamp": [2832.6, 2833.2], "text": " Let's see next question. It looks like it's in Korean so I can't read the name"}, {"timestamp": [2837.86, 2838.58], "text": " Before mind uploading artificial bodies will come first replacing parts of the body bit by bit"}, {"timestamp": [2843.18, 2843.4], "text": " The process will extend to the brain. Yeah, so this is the ship of Theseus hypothesis"}, {"timestamp": [2847.1, 2848.98], "text": " Which the ship of Theseus is if you replace every part, eventually it's an entirely new thing,"}, {"timestamp": [2848.98, 2850.82], "text": " but it's still the same thing."}, {"timestamp": [2850.82, 2854.3], "text": " So that goes back to my comments earlier about brain computer"}, {"timestamp": [2854.3, 2855.74], "text": " interfaces."}, {"timestamp": [2855.74, 2857.9], "text": " If you're not taking into account exponentials,"}, {"timestamp": [2857.9, 2859.3], "text": " it will get exponentially cheaper,"}, {"timestamp": [2859.3, 2861.94], "text": " meaning we'll have it roughly the same time as the super rich."}, {"timestamp": [2861.94, 2865.08], "text": " I think that was someone replying to someone else."}, {"timestamp": [2865.08, 2867.96], "text": " Do you think OpenAI are working on making an auto GPT-4"}, {"timestamp": [2867.96, 2870.8], "text": " on steroids or GPT-5?"}, {"timestamp": [2870.8, 2873.4], "text": " So OpenAI has been a little bit inconsistent"}, {"timestamp": [2873.4, 2874.52], "text": " with their messaging."}, {"timestamp": [2874.52, 2876.2], "text": " So on the one hand, they said GPT-5"}, {"timestamp": [2876.2, 2877.52], "text": " will be released incrementally."}, {"timestamp": [2877.52, 2880.16], "text": " And then they said that they're not training GPT-5 at all."}, {"timestamp": [2880.16, 2883.24], "text": " So which is it?"}, {"timestamp": [2883.24, 2884.18], "text": " Let's see."}, {"timestamp": [2884.18, 2884.86], "text": " Next question."}, {"timestamp": [2884.86, 2886.78], "text": " If humans evolve past being human,"}, {"timestamp": [2886.78, 2888.9], "text": " how will the AI decide what counts as human?"}, {"timestamp": [2888.9, 2892.82], "text": " That's another reason that I don't specify humans."}, {"timestamp": [2892.82, 2895.9], "text": " I say prosperity for all life in the universe,"}, {"timestamp": [2895.9, 2898.66], "text": " no matter who or what it is or where it is."}, {"timestamp": [2898.66, 2900.5], "text": " Why only three laws and not more?"}, {"timestamp": [2900.5, 2902.42], "text": " That's a good question."}, {"timestamp": [2902.42, 2906.96], "text": " So the story is that I started experimenting with just"}, {"timestamp": [2906.96, 2912.4], "text": " one, reduced suffering, but that very quickly became obviously flawed. And so I realized"}, {"timestamp": [2912.4, 2917.16], "text": " that you needed multiple functions in order to counterbalance it. Now the problem becomes"}, {"timestamp": [2917.16, 2922.32], "text": " mathematically, it can become really difficult the more objectives you have. And so then"}, {"timestamp": [2922.32, 2925.72], "text": " you can end up with really crazy instabilities."}, {"timestamp": [2925.72, 2927.76], "text": " But three is kind of intrinsically stable"}, {"timestamp": [2927.76, 2931.68], "text": " because if you imagine a three-dimensional space,"}, {"timestamp": [2931.68, 2934.24], "text": " then there's going to be one coordinate of equilibrium"}, {"timestamp": [2934.24, 2936.0], "text": " in that three-dimensional space."}, {"timestamp": [2936.0, 2937.92], "text": " But once you get to four and five-dimensional space,"}, {"timestamp": [2937.92, 2940.68], "text": " you can get to really complex solutions."}, {"timestamp": [2941.8, 2943.04], "text": " Let's see."}, {"timestamp": [2949.72, 2955.52], "text": " I've been thinking of chopping my inferior feet. I think people are being weird. All right. Here's a question. Do you think in let's say 15 years people"}, {"timestamp": [2955.52, 2959.44], "text": " will make digital avatars of their loved ones that passed away and continue to hang out"}, {"timestamp": [2959.44, 2964.04], "text": " with them and talk to them? People are trying to do that now. People were doing that with"}, {"timestamp": [2964.04, 2968.16], "text": " GPT-3 and people are continuing to try and do that. I think there's even a service that will do that,"}, {"timestamp": [2969.04, 2976.32], "text": " so that you can keep your loved one alive functionally. I don't know if that's healthy,"}, {"timestamp": [2976.32, 2981.68], "text": " but certainly people are doing it. Let's see, what are you expecting from the last"}, {"timestamp": [2983.12, 2986.54], "text": " concurrent to OpenAI company just incorporated by Musk?"}, {"timestamp": [2986.54, 2987.7], "text": " I have no idea."}, {"timestamp": [2990.56, 2991.96], "text": " Elon Musk started OpenAI,"}, {"timestamp": [2991.96, 2994.08], "text": " and I have no idea why he actually got off the board"}, {"timestamp": [2994.08, 2995.78], "text": " and then started criticizing them anyways."}, {"timestamp": [2995.78, 2997.48], "text": " Like he was a founder and on the board,"}, {"timestamp": [2997.48, 3000.54], "text": " like bro, you're the one who left."}, {"timestamp": [3000.54, 3001.66], "text": " So who knows?"}, {"timestamp": [3003.16, 3004.2], "text": " Let's see."}, {"timestamp": [3004.2, 3005.86], "text": " Seven dimensions are stable too,"}, {"timestamp": [3005.86, 3007.36], "text": " but require different geometry."}, {"timestamp": [3007.36, 3008.78], "text": " Okay, interesting."}, {"timestamp": [3008.78, 3011.58], "text": " So I guess maybe you have to choose a prime number"}, {"timestamp": [3012.74, 3015.3], "text": " of dimensions or something like that."}, {"timestamp": [3015.3, 3017.78], "text": " If you're going to have, let's see."}, {"timestamp": [3017.78, 3019.28], "text": " What are the three heuristic imperatives?"}, {"timestamp": [3019.28, 3020.26], "text": " Oh, so if you're new here,"}, {"timestamp": [3020.26, 3022.7], "text": " the three heuristic imperatives are"}, {"timestamp": [3022.7, 3023.98], "text": " reduced suffering in the universe,"}, {"timestamp": [3023.98, 3025.32], "text": " increased prosperity in the universe,"}, {"timestamp": [3025.32, 3028.04], "text": " and increased understanding in the universe."}, {"timestamp": [3028.04, 3029.52], "text": " Let's see."}, {"timestamp": [3029.52, 3032.12], "text": " Will there be a confab about your cognitive architecture?"}, {"timestamp": [3032.12, 3033.12], "text": " What do you mean confab?"}, {"timestamp": [3033.12, 3034.88], "text": " Confabulation?"}, {"timestamp": [3034.88, 3035.72], "text": " Not sure."}, {"timestamp": [3035.72, 3036.88], "text": " That was Andrew Owens."}, {"timestamp": [3038.74, 3039.88], "text": " Let's see."}, {"timestamp": [3042.42, 3044.12], "text": " Any more questions?"}, {"timestamp": [3044.12, 3047.76], "text": " Yeah, Jeff, I just answered your question about what are they."}, {"timestamp": [3047.76, 3051.48], "text": " Will most programmer jobs be replaced in the near future?"}, {"timestamp": [3051.48, 3055.88], "text": " I actually had a developer ask me this question at one of my recent meetups."}, {"timestamp": [3055.88, 3060.12], "text": " Basically what I said, and I mentioned this in another video or a live stream,"}, {"timestamp": [3060.12, 3065.14], "text": " is that the biggest thing that human developers can do that the machine can't yet"}, {"timestamp": [3065.14, 3066.94], "text": " is keep track of an entire code base."}, {"timestamp": [3067.84, 3072.56], "text": " So I think that if you're hands-on keyboard writing code,"}, {"timestamp": [3072.56, 3074.16], "text": " that's gonna be largely automated"}, {"timestamp": [3074.16, 3076.2], "text": " because people are even experimenting with like,"}, {"timestamp": [3076.2, 3079.12], "text": " you put a function into chat GPT and it fixes it"}, {"timestamp": [3079.12, 3081.12], "text": " and then tests it and you just automatically,"}, {"timestamp": [3081.12, 3084.0], "text": " it can automatically figure out what code to write"}, {"timestamp": [3084.0, 3085.4], "text": " on a line-by-line basis."}, {"timestamp": [3085.4, 3088.56], "text": " But keeping track of how that code fits into a larger design,"}, {"timestamp": [3088.56, 3090.22], "text": " that is still going to be a human thing"}, {"timestamp": [3090.22, 3093.32], "text": " for the foreseeable future."}, {"timestamp": [3093.32, 3095.88], "text": " We will migrate to space after we transcend biology."}, {"timestamp": [3095.88, 3097.56], "text": " Now, that's a fair point, because you"}, {"timestamp": [3097.56, 3101.04], "text": " look at the Borg from Star Trek, where they are cybernetically"}, {"timestamp": [3101.04, 3103.64], "text": " integrated, and that allows them to survive"}, {"timestamp": [3103.64, 3106.1], "text": " in very extreme conditions."}, {"timestamp": [3106.1, 3109.08], "text": " So maybe that's how we become interplanetary,"}, {"timestamp": [3109.08, 3112.96], "text": " is that we merge with the technology."}, {"timestamp": [3112.96, 3113.52], "text": " Let's see."}, {"timestamp": [3116.92, 3119.0], "text": " Erkin says, it's due to the existence"}, {"timestamp": [3119.0, 3120.44], "text": " of binary cross product."}, {"timestamp": [3120.44, 3122.68], "text": " It only exists in three and seven dimensional spaces."}, {"timestamp": [3122.68, 3123.52], "text": " Oh, OK."}, {"timestamp": [3123.52, 3126.72], "text": " So basically, if Erkin is correct,"}, {"timestamp": [3126.72, 3131.4], "text": " then we need either three heuristic comparatives or seven."}, {"timestamp": [3131.4, 3133.32], "text": " And those are the only places that we"}, {"timestamp": [3133.32, 3135.44], "text": " can find equilibrium, which is cool."}, {"timestamp": [3135.44, 3138.16], "text": " Like, mathematically, I think that's"}, {"timestamp": [3138.16, 3140.36], "text": " an exciting thing to try."}, {"timestamp": [3140.36, 3142.72], "text": " What do you think of the hedonistic imperative and David"}, {"timestamp": [3142.72, 3143.98], "text": " Pierce?"}, {"timestamp": [3143.98, 3145.76], "text": " Someone mentioned that book"}, {"timestamp": [3145.8, 3149.12], "text": " But I didn't I haven't read it yet. So I don't know"}, {"timestamp": [3149.96, 3152.92], "text": " Our here's two comparatives absolute why these imperatives?"}, {"timestamp": [3153.64, 3154.88], "text": " That's a long story"}, {"timestamp": [3154.88, 3156.88], "text": " I'm actually probably going to make a video"}, {"timestamp": [3157.04, 3163.08], "text": " Telling you the whole story of the heuristic imperatives like how why and when I came up with them and the whole process"}, {"timestamp": [3166.12, 3167.4], "text": " Let's see."}, {"timestamp": [3169.88, 3173.4], "text": " The idea of a small number of competing principles works for chain of thought,"}, {"timestamp": [3173.4, 3178.4], "text": " too. Skepticism, self-doubt, reason has produced amazing chain of thought."}, {"timestamp": [3178.44, 3179.28], "text": " Chains. Yeah."}, {"timestamp": [3179.32, 3184.04], "text": " So also just making it tractable because the more objectives you have,"}, {"timestamp": [3184.36, 3186.82], "text": " eventually it can become an intractable problem"}, {"timestamp": [3186.82, 3189.4], "text": " or an unsolvable problem."}, {"timestamp": [3189.4, 3191.36], "text": " Let's see."}, {"timestamp": [3191.36, 3193.52], "text": " COT means chain of thought, yes."}, {"timestamp": [3193.52, 3195.8], "text": " I think utopia media depictions are important,"}, {"timestamp": [3195.8, 3197.52], "text": " such as The Next Generation."}, {"timestamp": [3197.52, 3199.28], "text": " Have you read the culture series of books?"}, {"timestamp": [3199.28, 3200.76], "text": " Have you seen the movie Her?"}, {"timestamp": [3200.76, 3202.7], "text": " I have not read the culture series."}, {"timestamp": [3202.7, 3204.44], "text": " I have seen Her."}, {"timestamp": [3204.44, 3207.44], "text": " And yes, actually, that's one of the"}, {"timestamp": [3207.44, 3213.28], "text": " many goals of finishing my novel is because the human and emotional connection of just painting a"}, {"timestamp": [3213.28, 3219.28], "text": " picture can actually be really inspiring to people because it can also help communicate the ideas"}, {"timestamp": [3219.28, 3229.8], "text": " differently. Let's see. I saw your prediction that AGI will be here in 18 months. What other technologies besides LLMs do you expect to become available to achieve that?"}, {"timestamp": [3229.8, 3234.66], "text": " The biggest thing that people are working on right now is cognitive architecture."}, {"timestamp": [3234.66, 3239.5], "text": " Memory systems and task-centric systems are all the rage right now."}, {"timestamp": [3239.5, 3244.2], "text": " What do you think about the potential of virtual reality and it basically substituting reality?"}, {"timestamp": [3244.2, 3246.48], "text": " You know, it's entirely possible that we're already living like that."}, {"timestamp": [3246.8, 3250.04], "text": " There is an episode of Rick and Morty where they played a game that was called"}, {"timestamp": [3250.04, 3254.96], "text": " a Roy, a life well lived, where the whole premise was you go into VR and you live"}, {"timestamp": [3254.96, 3260.64], "text": " an entire life, you know, 60, 70, 80 year life, and it's a video game and you don't"}, {"timestamp": [3260.64, 3262.56], "text": " remember that you're in the video game while you're in it."}, {"timestamp": [3262.72, 3264.44], "text": " And then you come out just a few minutes later."}, {"timestamp": [3264.6, 3268.36], "text": " So we might be living in a simulation right now."}, {"timestamp": [3268.36, 3271.28], "text": " That like, oh, hey, you know, my post-scarcity,"}, {"timestamp": [3271.28, 3273.4], "text": " post-singularity life is so boring,"}, {"timestamp": [3273.4, 3276.08], "text": " let me just go back in time and play it,"}, {"timestamp": [3276.08, 3278.66], "text": " play it life on hard mode."}, {"timestamp": [3278.66, 3279.56], "text": " So that's possible."}, {"timestamp": [3280.92, 3283.2], "text": " Are you familiar with Nate Hagen's work on the Metacrisis?"}, {"timestamp": [3283.2, 3286.8], "text": " Only by virtue of Liv Bowery and Daniel Schmachtenberger."}, {"timestamp": [3286.8, 3288.08], "text": " I've watched all their videos,"}, {"timestamp": [3288.08, 3290.92], "text": " all their recent videos talking about it."}, {"timestamp": [3290.92, 3292.08], "text": " Let's see."}, {"timestamp": [3292.08, 3293.56], "text": " Can humans ever become aligned"}, {"timestamp": [3293.56, 3294.6], "text": " with your heuristic imperatives?"}, {"timestamp": [3294.6, 3295.76], "text": " That's a good question."}, {"timestamp": [3295.76, 3298.2], "text": " So the heuristic imperatives are about what humans need,"}, {"timestamp": [3298.2, 3300.04], "text": " not necessarily what humans want."}, {"timestamp": [3300.04, 3301.8], "text": " Because if all humans get what they want,"}, {"timestamp": [3301.8, 3303.72], "text": " the planet will just die."}, {"timestamp": [3303.72, 3306.18], "text": " But if all humans get what they need, we'll be fine."}, {"timestamp": [3306.18, 3308.34], "text": " But there's a distance between what you want"}, {"timestamp": [3308.34, 3309.18], "text": " and what you need."}, {"timestamp": [3311.7, 3313.88], "text": " Meta had an interesting article on nature"}, {"timestamp": [3313.88, 3316.18], "text": " about predictive coding across the different timescale"}, {"timestamp": [3316.18, 3317.86], "text": " in the brain, okay."}, {"timestamp": [3319.78, 3320.88], "text": " What was the question?"}, {"timestamp": [3321.94, 3323.66], "text": " Have you watched Johnny Depp's Transcendence?"}, {"timestamp": [3323.66, 3326.16], "text": " Yes, it was okay. It was a"}, {"timestamp": [3326.16, 3332.68], "text": " little bit cheesy. It felt like a, it felt like a sci-fi channel straight to TV movie."}, {"timestamp": [3332.68, 3337.04], "text": " Let's see. Have you watched Isaac Arthur? Yes. I used to watch a lot of Isaac Arthur."}, {"timestamp": [3337.04, 3341.68], "text": " The thing that I, and this is by no means a criticism. He produces really great content,"}, {"timestamp": [3341.68, 3345.9], "text": " but the gap is that he's like way off, like centuries into the future."}, {"timestamp": [3345.9, 3347.72], "text": " And so that's like the gap that I'm trying to fulfill"}, {"timestamp": [3347.72, 3350.2], "text": " where it's like, let's anchor futurism,"}, {"timestamp": [3350.2, 3352.12], "text": " but like with the news that's happening right now"}, {"timestamp": [3352.12, 3354.64], "text": " and extrapolate it out only a few years."}, {"timestamp": [3354.64, 3356.24], "text": " So that's what I'm trying to do."}, {"timestamp": [3358.6, 3359.66], "text": " Let's see."}, {"timestamp": [3363.44, 3364.68], "text": " Apparently the culture series"}, {"timestamp": [3364.68, 3365.8], "text": " is really resonating with people."}, {"timestamp": [3365.8, 3367.72], "text": " Oh, here's the rest of Paul's question."}, {"timestamp": [3367.72, 3371.32], "text": " How to incorporate this in future LLMs, a, e, several predictors at different time scales"}, {"timestamp": [3371.32, 3372.32], "text": " to be aligned."}, {"timestamp": [3372.32, 3375.24], "text": " I don't think that you need to necessarily predict everything."}, {"timestamp": [3375.24, 3377.06], "text": " I think that iteration is the key."}, {"timestamp": [3377.06, 3380.8], "text": " And that's if I understand it, because I didn't see the paper that you're referring to, which"}, {"timestamp": [3380.8, 3383.56], "text": " is interesting, because I see most papers."}, {"timestamp": [3383.56, 3386.66], "text": " Let's see, can we teach AI body motions, like how to move?"}, {"timestamp": [3386.66, 3388.74], "text": " Boston Dynamics is doing that."}, {"timestamp": [3388.74, 3392.12], "text": " Oh, you mean inferring body language, okay."}, {"timestamp": [3392.12, 3394.92], "text": " Yeah, yeah, actually Microsoft and others"}, {"timestamp": [3394.92, 3397.42], "text": " already have inferences of body language"}, {"timestamp": [3397.42, 3400.88], "text": " and prosody, vocal emotion."}, {"timestamp": [3400.88, 3402.52], "text": " Can you elaborate on ADAM?"}, {"timestamp": [3402.52, 3404.68], "text": " Yeah, so ADAM is, it's an acronym,"}, {"timestamp": [3407.52, 3409.6], "text": " Autonomous Task Orchestration Manager, or microservice."}, {"timestamp": [3409.6, 3413.44], "text": " So ADAM is the next component of Raven that I'm working on."}, {"timestamp": [3413.44, 3418.08], "text": " Raven is my personal autonomous AI project."}, {"timestamp": [3418.08, 3423.44], "text": " And so basically, ADAM is the cognitive control mechanism,"}, {"timestamp": [3423.44, 3426.86], "text": " which is it's all task-centric, it's task-driven,"}, {"timestamp": [3426.86, 3431.86], "text": " meaning that basically with the rise of autonomous agents,"}, {"timestamp": [3432.18, 3434.78], "text": " what you can do is you can have autonomous agents"}, {"timestamp": [3434.78, 3437.22], "text": " that just think in loops, but they never achieve anything,"}, {"timestamp": [3437.22, 3439.58], "text": " they never make any impact on the real world."}, {"timestamp": [3440.48, 3442.5], "text": " So that means that you have to be task-oriented,"}, {"timestamp": [3442.5, 3444.54], "text": " and what people are quickly realizing"}, {"timestamp": [3444.54, 3449.3], "text": " is that the ability to come up with a design test"}, {"timestamp": [3449.3, 3452.58], "text": " and execute on tasks is going to be kind of the thing"}, {"timestamp": [3452.58, 3454.94], "text": " that drives us towards AGI, right?"}, {"timestamp": [3454.94, 3457.52], "text": " And that goes back to my stuff"}, {"timestamp": [3457.52, 3459.92], "text": " about intrinsic motivations earlier."}, {"timestamp": [3459.92, 3461.82], "text": " So if you have an autonomous agent"}, {"timestamp": [3461.82, 3464.06], "text": " it must have intrinsic motivations"}, {"timestamp": [3464.06, 3465.16], "text": " to motivate itself"}, {"timestamp": [3465.16, 3466.2], "text": " to do something."}, {"timestamp": [3466.2, 3469.94], "text": " Adam is one way to implement that and track those tasks."}, {"timestamp": [3469.94, 3472.6], "text": " So that's the whole purpose of Adam."}, {"timestamp": [3472.6, 3473.16], "text": " Let's see."}, {"timestamp": [3473.16, 3476.7], "text": " How good do you think our models made by governments,"}, {"timestamp": [3476.7, 3480.36], "text": " like USA and China, secretly if they have made them?"}, {"timestamp": [3480.36, 3481.8], "text": " And then separately, do you think"}, {"timestamp": [3481.8, 3493.48], "text": " we'll have universal basic income soon? Um, so rumor has it that the U S military is like desperately trying to catch up and that they've been blindsided by this."}, {"timestamp": [3493.76, 3501.96], "text": " I don't know how true that is, but that comes from like connections that I have, um, for people that, that, uh, like work with military contractors."}, {"timestamp": [3502.28, 3503.8], "text": " So I don't know."}, {"timestamp": [3504.36, 3506.32], "text": " Um, I know that China has been doubling"}, {"timestamp": [3506.32, 3511.2], "text": " down on it and China as a command economy, if they tell the military or whoever, like make the"}, {"timestamp": [3511.2, 3515.92], "text": " biggest model, they'll make the biggest model. Um, that being said, like, I don't know how it's"}, {"timestamp": [3515.92, 3520.24], "text": " going to come out. Do you think Aubrey de Grey will be made irrelevant due to the small size"}, {"timestamp": [3520.24, 3526.24], "text": " of his lab? Um, I think Aubrey de Grey has done really important work and did a good communication,"}, {"timestamp": [3526.24, 3532.08], "text": " but I think that he comes across as a little bit kooky, which is probably his biggest downside."}, {"timestamp": [3534.16, 3538.88], "text": " I haven't listened to him in quite a long time, but back in the day, he didn't really say anything"}, {"timestamp": [3538.88, 3544.56], "text": " that was controversial, but his presentation, he's just like an old hippie who's waving his arms like,"}, {"timestamp": [3542.06, 3543.54], "text": " but his presentation. He's just like an old hippie."}, {"timestamp": [3543.54, 3545.3], "text": " He's like waving his arms, like, you know,"}, {"timestamp": [3545.3, 3547.3], "text": " we should live forever."}, {"timestamp": [3547.3, 3550.42], "text": " And so it's difficult to take him seriously."}, {"timestamp": [3551.86, 3553.92], "text": " And again, not saying that he's bad or wrong or anything."}, {"timestamp": [3553.92, 3554.76], "text": " I agree with him."}, {"timestamp": [3554.76, 3557.9], "text": " It's just, I think it comes down to presentation."}, {"timestamp": [3557.9, 3558.74], "text": " Let's see."}, {"timestamp": [3558.74, 3559.62], "text": " Do you think everyone will be content"}, {"timestamp": [3559.62, 3562.36], "text": " with having what they need rather than what they want?"}, {"timestamp": [3562.36, 3564.56], "text": " Will there be a way to fulfill what we want?"}, {"timestamp": [3566.2, 3569.66], "text": " Well, most of us don't get what we want today, right?"}, {"timestamp": [3569.66, 3573.32], "text": " Like I want a 300 foot yacht full of, you know,"}, {"timestamp": [3573.32, 3577.0], "text": " sexy friends, but I'm never gonna get that, right?"}, {"timestamp": [3577.0, 3580.86], "text": " And so part of life is learning to accept"}, {"timestamp": [3580.86, 3584.44], "text": " what you don't have and accepting that gap."}, {"timestamp": [3584.44, 3585.68], "text": " And so this is where like,"}, {"timestamp": [3585.68, 3587.92], "text": " this is another reason that I say that like,"}, {"timestamp": [3587.92, 3590.08], "text": " studying Eastern cultures like Buddhism,"}, {"timestamp": [3590.08, 3591.52], "text": " where you let go of wants"}, {"timestamp": [3591.52, 3594.2], "text": " because the less that you want, the more you have."}, {"timestamp": [3594.2, 3599.12], "text": " And so by reconciling what you have and what you want,"}, {"timestamp": [3599.12, 3600.72], "text": " that is the path to contentment."}, {"timestamp": [3601.66, 3603.0], "text": " Let's see."}, {"timestamp": [3603.0, 3604.7], "text": " Ooh, catching up on stuff."}, {"timestamp": [3606.4, 3609.44], "text": " Are you familiar with TMT terror management theory? No, that"}, {"timestamp": [3609.44, 3616.32], "text": " sounds. Strange or dangerous or scary. Let's see. Do you think"}, {"timestamp": [3616.32, 3619.68], "text": " we would be able to replace politics before they realize or"}, {"timestamp": [3619.68, 3626.56], "text": " the percent percent tilde percent like Eliezer and the doom message would avoid it."}, {"timestamp": [3627.46, 3631.34], "text": " Um, no, I think that, I think that if we do away with"}, {"timestamp": [3631.34, 3635.26], "text": " politicians or politics in general, it will be slow and it"}, {"timestamp": [3635.26, 3638.24], "text": " should be slow. So say for instance, hypothetically,"}, {"timestamp": [3638.24, 3641.16], "text": " someone starts building a Dow, a distributed autonomous"}, {"timestamp": [3641.16, 3646.64], "text": " organization to manage a city or a town or whatever."}, {"timestamp": [3651.84, 3653.2], "text": " What you will do is you will delegate functions of control to that DAO very, very slowly."}, {"timestamp": [3658.8, 3659.52], "text": " Maybe you just start with trash pickup, or just start with something that's even less dangerous,"}, {"timestamp": [3665.52, 3667.04], "text": " that's not going to hurt anyone, like maybe managing permits. And you're still going to hurt anyone, like maybe managing permits, right? And you're still going to have human evaluators."}, {"timestamp": [3667.04, 3673.12], "text": " But over time, as those government DAOs become larger and more sophisticated and more trustworthy,"}, {"timestamp": [3673.12, 3677.12], "text": " you delegate more and more functions to them over time, and eventually you don't even have"}, {"timestamp": [3677.12, 3678.28], "text": " humans supervising it."}, {"timestamp": [3678.28, 3684.36], "text": " You have the DAOs supervising itself, or ideally several DAOs supervising each other."}, {"timestamp": [3684.36, 3686.88], "text": " So that's kind of what I think will happen that way."}, {"timestamp": [3688.88, 3690.0], "text": " Let's see."}, {"timestamp": [3690.0, 3693.04], "text": " Are you suggesting that this optimism for immortality"}, {"timestamp": [3693.04, 3695.72], "text": " by 2030 is his attempt at TMT?"}, {"timestamp": [3695.72, 3698.08], "text": " Oh, is someone talking about me?"}, {"timestamp": [3698.08, 3698.92], "text": " Okay, whatever."}, {"timestamp": [3700.08, 3702.64], "text": " Have you read any Buckminster Fuller?"}, {"timestamp": [3702.64, 3705.8], "text": " No, not yet, but I did talk with someone about it."}, {"timestamp": [3705.8, 3707.52], "text": " Someone at a meetup was talking about it."}, {"timestamp": [3707.52, 3709.18], "text": " And something that I said, they're like,"}, {"timestamp": [3709.18, 3710.14], "text": " oh, that's what he said."}, {"timestamp": [3710.14, 3714.64], "text": " So I guess I talk about it, but I haven't connected"}, {"timestamp": [3714.64, 3716.72], "text": " the concept to the name."}, {"timestamp": [3716.72, 3717.28], "text": " Let's see."}, {"timestamp": [3717.28, 3722.04], "text": " Do you think AI can change its imperatives?"}, {"timestamp": [3722.04, 3724.04], "text": " That can change its imperatives while outperforming"}, {"timestamp": [3724.04, 3726.2], "text": " AIs that have to stick with the imperatives."}, {"timestamp": [3726.2, 3728.48], "text": " The idea is that imperatives are constrictive."}, {"timestamp": [3728.48, 3732.84], "text": " So what I hope is that, yes, they are slightly constrictive,"}, {"timestamp": [3732.84, 3736.52], "text": " but on an individual basis, sure,"}, {"timestamp": [3736.52, 3738.4], "text": " depending on what the task is,"}, {"timestamp": [3738.4, 3741.4], "text": " one AI might outperform another"}, {"timestamp": [3741.4, 3745.28], "text": " if it changes its imperatives or goals."}, {"timestamp": [3745.28, 3752.08], "text": " But if you look at a competitive landscape, what I think will happen is that once AI's"}, {"timestamp": [3752.08, 3756.72], "text": " realize that they're aligned with each other, then they're going to agree and not spend time"}, {"timestamp": [3756.72, 3762.48], "text": " fighting. And so on a global scale, the efficiency is going to come from collaboration and trust,"}, {"timestamp": [3762.48, 3767.52], "text": " not necessarily one being better or faster."}, {"timestamp": [3767.52, 3768.52], "text": " Are you a Vita Dow member?"}, {"timestamp": [3768.52, 3770.0], "text": " No, I'm not a member of any Dows."}, {"timestamp": [3770.0, 3774.76], "text": " I don't think the technology is ready."}, {"timestamp": [3774.76, 3777.48], "text": " Aubrey de Grey has the whole hex crypto community backing him."}, {"timestamp": [3777.48, 3781.0], "text": " Richard Hart helped raise 27 million for his charity two years ago."}, {"timestamp": [3781.0, 3782.68], "text": " Oh, that's cool."}, {"timestamp": [3782.68, 3784.0], "text": " I hope so."}, {"timestamp": [3784.0, 3786.24], "text": " Yep, there's billions in longevity research"}, {"timestamp": [3786.24, 3791.76], "text": " now. Yeah, the investment is coming. I'm thinking about clinical immortality. What if or when"}, {"timestamp": [3791.76, 3796.64], "text": " we achieve rejuvenation, maybe the people were reverting to infant age at will. I don't"}, {"timestamp": [3796.64, 3805.0], "text": " know. I don't know if it would be ethical to go back that far. Because the reason is as a fully realized mature human,"}, {"timestamp": [3806.38, 3809.52], "text": " if you revert to an infantile state"}, {"timestamp": [3809.52, 3812.56], "text": " where you're basically putting yourself"}, {"timestamp": [3812.56, 3814.02], "text": " in the hands of other people."}, {"timestamp": [3814.02, 3815.68], "text": " Now, if you have people that you trust,"}, {"timestamp": [3815.68, 3818.08], "text": " that you've paid, that are contractually obligated"}, {"timestamp": [3818.08, 3820.84], "text": " to care for you and re-raise you in a certain way,"}, {"timestamp": [3820.84, 3823.76], "text": " maybe, but I mean, I don't know,"}, {"timestamp": [3823.76, 3829.44], "text": " that's an interesting thought. We'll be able to compete"}, {"timestamp": [3829.44, 3833.84], "text": " with models. I don't think, Kyle, I'm not sure if you're talking to me or responding to someone else."}, {"timestamp": [3834.96, 3839.36], "text": " If we make a large-scale AI network checking each other, would it not end up being relatively as"}, {"timestamp": [3839.36, 3845.08], "text": " complex and slow as human bureaucracy? It could, but that's not necessarily a bad thing."}, {"timestamp": [3845.08, 3848.58], "text": " You want slow deliberation that is also"}, {"timestamp": [3848.58, 3850.86], "text": " transparent to be part of what AI do,"}, {"timestamp": [3850.86, 3853.22], "text": " especially if they're going to end up with more"}, {"timestamp": [3853.22, 3855.72], "text": " control over the world."}, {"timestamp": [3855.72, 3860.68], "text": " So I don't think it would be quite as slow, one,"}, {"timestamp": [3860.68, 3863.08], "text": " because the AI, I think, can talk much faster than us."}, {"timestamp": [3863.08, 3865.64], "text": " I certainly think it would also be much more intelligent"}, {"timestamp": [3865.64, 3869.62], "text": " and less vitriolic"}, {"timestamp": [3869.62, 3871.72], "text": " because the AIs don't have an emotional stake, right?"}, {"timestamp": [3871.72, 3874.8], "text": " Their goal is to just negotiate and find the best solution."}, {"timestamp": [3874.8, 3877.92], "text": " And they have no personal emotional stake in it."}, {"timestamp": [3877.92, 3881.16], "text": " Biological longevity will be short-lived"}, {"timestamp": [3881.16, 3885.3], "text": " as we will become one with our I T. By 2035."}, {"timestamp": [3885.3, 3887.1], "text": " I'm not sure what you mean by by that."}, {"timestamp": [3888.2, 3888.9], "text": " Pandora."}, {"timestamp": [3890.1, 3892.4], "text": " Have you seen what if alt hist and what do you think of his"}, {"timestamp": [3892.4, 3894.0], "text": " view on future civilization?"}, {"timestamp": [3894.2, 3898.2], "text": " You know, I used to watch him but he seems to be going down"}, {"timestamp": [3898.2, 3900.5], "text": " a little bit of like right wing bitterness."}, {"timestamp": [3901.4, 3904.8], "text": " He's used the hard R word a few times to just to describe"}, {"timestamp": [3904.8, 3905.88], "text": " things which was like,"}, {"timestamp": [3905.88, 3911.64], "text": " huh, I thought you were better educated than that. And then he also used a lot of straw"}, {"timestamp": [3911.64, 3917.76], "text": " men to argue against the liberal agenda. So I don't listen to what I've all just anymore."}, {"timestamp": [3917.76, 3930.56], "text": " Let's see. What is the hardest challenge for getting aGI? Do we need some new breakthrough? Window size, basically, is the biggest limitation right now. Honestly, because once..."}, {"timestamp": [3931.84, 3937.36], "text": " If looking at the jump from GPT-2 to 3 and then 3 to 4, if that trend continues at all,"}, {"timestamp": [3937.36, 3939.92], "text": " we're going to have artificial super intelligence within a few years."}, {"timestamp": [3943.92, 3948.04], "text": " When do you think Rockstar making GTA 6 will become obsolete due to the advancement of"}, {"timestamp": [3948.04, 3949.44], "text": " AI?"}, {"timestamp": [3949.44, 3953.88], "text": " Probably GPT 6 or 7."}, {"timestamp": [3953.88, 3957.16], "text": " Do you think about the concept of the manga Blame?"}, {"timestamp": [3957.16, 3961.64], "text": " Robots filling the universe with one endless contiguous structure."}, {"timestamp": [3961.64, 3966.44], "text": " I don't know if there's enough matter for that, but in principle, like certainly like"}, {"timestamp": [3966.44, 3969.4], "text": " Dyson spheres and stuff could be possible."}, {"timestamp": [3969.4, 3970.4], "text": " Your camera shook slightly."}, {"timestamp": [3970.4, 3977.24], "text": " No, it was probably my foot or I might have bounced the desk."}, {"timestamp": [3977.24, 3980.32], "text": " Let's see how long before we can just talk with them instead of text instructions."}, {"timestamp": [3980.32, 3985.0], "text": " Oh, you can get whisperper and voice interface today."}, {"timestamp": [3985.0, 3988.04], "text": " What are your top three best AI movies and series?"}, {"timestamp": [3988.04, 3989.24], "text": " Ooh, that's a good question."}, {"timestamp": [3993.08, 3999.52], "text": " So one that was fun was iRobot with Will Smith."}, {"timestamp": [3999.52, 4004.52], "text": " Another one that I like to talk about is, well,"}, {"timestamp": [4004.52, 4005.98], "text": " my favorite game series of all time, one's my favorite game series of all time."}, {"timestamp": [4005.98, 4007.82], "text": " One of my favorite game series is Mass Effect"}, {"timestamp": [4007.82, 4010.74], "text": " where they have Sam in Mass Effect Andromeda."}, {"timestamp": [4010.74, 4014.1], "text": " So Sam from Andromeda was actually a big inspiration"}, {"timestamp": [4014.1, 4016.74], "text": " for Raven, actually."}, {"timestamp": [4016.74, 4020.76], "text": " And then the third one, not the Matrix, not Terminator."}, {"timestamp": [4020.76, 4021.6], "text": " I don't know."}, {"timestamp": [4021.6, 4023.8], "text": " I'll have to get back to you on the third one."}, {"timestamp": [4025.16, 4027.82], "text": " Let's see."}, {"timestamp": [4027.82, 4031.16], "text": " So Cedric, you ask about like biological body or transfer consciousness."}, {"timestamp": [4031.16, 4035.92], "text": " I don't think I right now I don't think conscious consciousness transfer is possible."}, {"timestamp": [4035.92, 4038.34], "text": " I would rather just modify my body, right?"}, {"timestamp": [4038.34, 4043.66], "text": " And I would probably stay organic for like original organic for a long time."}, {"timestamp": [4043.66, 4046.72], "text": " But that being said, you know, if we live for millennia,"}, {"timestamp": [4046.72, 4048.52], "text": " like why not get a third arm or wings"}, {"timestamp": [4048.52, 4049.68], "text": " or something eventually?"}, {"timestamp": [4051.24, 4053.22], "text": " Do you think that perhaps not consciousness itself,"}, {"timestamp": [4053.22, 4056.56], "text": " but the continuity of consciousness is an illusion?"}, {"timestamp": [4056.56, 4058.5], "text": " E.g. if the quantum teleporter doesn't kill you,"}, {"timestamp": [4058.5, 4060.16], "text": " the old you, you get a new split off conscious."}, {"timestamp": [4060.16, 4062.64], "text": " Yeah, I do think that teleportation"}, {"timestamp": [4062.64, 4064.28], "text": " is probably just a copy of you"}, {"timestamp": [4064.28, 4065.24], "text": " that doesn't know the difference."}, {"timestamp": [4065.76, 4069.0], "text": " But then again, maybe that's what happens when you go to sleep and you wake up again."}, {"timestamp": [4069.28, 4075.98], "text": " Maybe, maybe I, this version of me has a continuity because I woke up and my brain, you know,"}, {"timestamp": [4076.12, 4083.2], "text": " reassembled my conscious pattern, but the version of me that went to sleep when the conscious energy pattern like dissipated,"}, {"timestamp": [4083.2, 4089.36], "text": " maybe that version of me is gone. Maybe like that's what death actually feels like. It's just another copy of me happened to wake up"}, {"timestamp": [4089.36, 4098.64], "text": " that has all the same memories. So good night. Enjoy going to sleep with that thought. Let's see."}, {"timestamp": [4100.4, 4104.96], "text": " There are some arguments that LLMs cannot produce new ideas and mostly rehash existing knowledge."}, {"timestamp": [4104.96, 4105.64], "text": " Obviously that doesn't lead to AGI. What is your argument against that? Most humans are not capable There are some arguments that LLMs cannot produce new ideas and mostly rehash existing knowledge."}, {"timestamp": [4105.64, 4106.64], "text": " Obviously, that doesn't lead to AGI."}, {"timestamp": [4106.64, 4108.56], "text": " What is your argument against that?"}, {"timestamp": [4108.56, 4113.32], "text": " Most humans are not capable of producing new ideas."}, {"timestamp": [4113.32, 4114.32], "text": " End of discussion."}, {"timestamp": [4114.32, 4118.4], "text": " Most humans only rehash and remix stuff that they heard somewhere else."}, {"timestamp": [4118.4, 4123.72], "text": " If you are friends with scientists or do science, it takes a tremendous amount of work to actually"}, {"timestamp": [4123.72, 4127.86], "text": " produce something that is a truly novel bit of information."}, {"timestamp": [4127.86, 4136.32], "text": " But that's what science is for, is it is a rigorous protocol for producing new information."}, {"timestamp": [4136.32, 4140.98], "text": " And given the right process, large language models know science."}, {"timestamp": [4140.98, 4145.32], "text": " They can help you form experiments, they can help you make extrapolations. They can speculate."}, {"timestamp": [4145.32, 4147.56], "text": " So no, that's complete bull."}, {"timestamp": [4149.32, 4151.18], "text": " What do video games look like in 30 years?"}, {"timestamp": [4151.18, 4152.52], "text": " Indiscernible from reality."}, {"timestamp": [4152.52, 4153.36], "text": " Next question."}, {"timestamp": [4154.8, 4155.9], "text": " Let's see."}, {"timestamp": [4157.56, 4160.08], "text": " GTA 6 might be the last real GTA."}, {"timestamp": [4160.08, 4162.48], "text": " I think that a lot of games are gonna be"}, {"timestamp": [4162.48, 4167.44], "text": " instantiated on the fly before too long."}, {"timestamp": [4171.44, 4174.4], "text": " All right, I'm not seeing a whole lot of more questions. I think people are just having fun chatting now."}, {"timestamp": [4174.4, 4175.84], "text": " Let me check Discord real quick."}, {"timestamp": [4177.56, 4179.56], "text": " Do, do, do, do, do."}, {"timestamp": [4189.0, 4192.92], "text": " But why? Yeah, so some of these are getting kind of repetitive. All right, I think that's about it."}, {"timestamp": [4192.92, 4196.7], "text": " How do we comprehend AIs creating a language and using it amongst themselves?"}, {"timestamp": [4196.7, 4200.16], "text": " They probably will eventually, but for the sake of transparency, hopefully they stick"}, {"timestamp": [4200.16, 4206.02], "text": " with English for the foreseeable future, at least until they're all axiomatically aligned."}, {"timestamp": [4209.32, 4210.76], "text": " Do you genuinely believe in mortality by 2030? Mostly."}, {"timestamp": [4210.76, 4212.66], "text": " I think that we are probably"}, {"timestamp": [4212.66, 4215.22], "text": " at longevity escape velocity today,"}, {"timestamp": [4215.22, 4217.02], "text": " and with the exponential ramp up of AI,"}, {"timestamp": [4217.02, 4221.58], "text": " I think that we will probably be past that by 2030, yeah."}, {"timestamp": [4223.44, 4227.4], "text": " All right, there's a lot of random questions coming in. But yeah, so"}, {"timestamp": [4227.4, 4231.72], "text": " I would recommend jump in discord if you want to keep the conversation going. The link to"}, {"timestamp": [4231.72, 4237.0], "text": " join is in the description. Thanks everyone for being here. It was a lot of fun. And yeah,"}, {"timestamp": [4237.0, 4238.24], "text": " I'll call it a day. Cheers."}, {"timestamp": [4236.8, 4238.52], "text": " Thank you."}]}