{"text": " Curiosity is actually an adaptive trait. So the reason that humans are all over the planet is because we saw the horizon and we said, what's over there? And I mean, when you think about how completely nuts humans had to be to get on like a dugout, like reed boat and end up halfway across the Pacific, like humans are freaking crazy curious. Like, it's just like, hey, I figured out how to build a boat. So I'm gonna go as far as I can that way. And just to see what's there. Our curiosity transcends our limitations as organisms. It transcends all practical things, but our ancestors who were more curious, they tried more foods, they went more places, they figured out how to scribble, you know, on clay tablets. Curiosity is such a powerful function that leads to so much novelty. And so my hope is that by giving an AI system a sense of curiosity is that it will ultimately lead to situations and understandings that we can't even conceive of yet. The rate of progress in artificial intelligence seems to be accelerating. And right now we might be living through one of the most significant periods in human history ever, comparable to the agricultural and industrial revolutions. That's what makes this conversation particularly important. I speak with David Shapiro, who's one of the most inspirational thinkers in the area of AI safety alive today. If you're not familiar with his work, I highly recommend checking out his YouTube and Discord channels, both of which are fantastic and linked in the description below. In this conversation we cover the inevitable rise of super intelligent AI. We discuss some of the amazing utopic possibilities, but also the scary scenarios like mass unemployment, societal collapse, and human extinction. So if any of that sounds interesting to you, and it should because you live on this planet which means moving forwards, AI is gonna be playing an increasingly important role in your life, then you should listen to this conversation in full. It's one of the most interesting chats I've ever had. And rather than filling me with existential dread, it left me feeling hopeful and excited for the possible world that we might be producing. This is the Escape CP Units podcast, supported by the Andrea von Braun Foundation. If you enjoy what I'm doing and want to help support me, the best way you can do so is by liking, sharing, and subscribing. And now, it's my pleasure to bring you David Shapiro. I hope you enjoy. Escape Sapiens. David Shapiro, welcome on the podcast. Hey, thanks on the podcast. Hey, thanks for having me. So to set the context for the discussion at a high level, what are large language models like BARD and GPT-3? The simplest explanation is they're a type of a neural network called a deep neural network, meaning they're composed of more than three layers. Many of these are dozens of layers deep. They're trained on billions of words of text data. The simplest way to kind of think about what they do is they're essentially at the most basic level just autocomplete engines. Just like the keyboard on your smartphone or whatever, it will suggest the next word. That's all that these models are trained to do. However, because they're trained on so much data, and they're able to accurately predict the next word, no matter what topic you're talking about or task you're doing, they've actually embedded a lot of knowledge and tasks and abilities. And the bigger they get, and the more data they're trained on, the smarter they get. So how smart are they? Because I find it really interesting that the public really got excited about this with chat GPT, right? So when they had an interface. And so I wonder, how smart are they really? And how much is it just that we're anthropomorphizing stuff. One term in the industry, this actually comes from psychology, is pareidolia, which is the human tendency to see faces where there aren't faces or hear voices where there aren't voices. And so we tend to attribute human-like intelligence to something that is basically approximating human intelligence. But so, but then to your question about how intelligent are they, with the release of CHAT-GPT-4, from a functional standpoint, CHAT-GPT-4 has crushed all kinds of tests. I think it passed the standard bar exam in America in the 90th percentile. It passed a bunch of science exams in the 99th percentile. And then there was another example of a medical researcher or a doctor at Stanford who said he was on record saying that it had better clinical judgment than quote many doctors. So it has a broad variety of capabilities. And when you just do a side-by-side comparison to human performance, these models outperform humans in quite a number of tasks. And so you might say, well, they don't truly understand anything, but what is true understanding, right? From a functional standpoint, from an output standpoint, that's kind of what matters from an empirical or observational perspective. But in terms of consciousness, do you think there's someone, is there someone looking out the window? Are these things what you could call conscious or not? They can certainly pretend to be conscious and they can approximate consciousness. However, it is a major open question, both from a neuroscience perspective, a philosophical perspective, mathematical perspective, basically every domain, it's an open question as to first, what even is consciousness? Whether it's a spiritual question or religious or scientific, there's different theories, there's materialism, there's dualism, there's panpsychism. And we can delve into those schools of thought if you'd like. But basically, we don't know what consciousness is or even where it comes from. Um, all we can do is kind of do experiments from, from our own perspective, right? One thing to think of is under many circumstances, you and I are conscious, but then there are periods of times that we go unconscious, right? With general anesthesia, we fall asleep. So your brain, basically just the energy pattern changes in your brain and your state of consciousness changes. Then someone else pointed out to me that sleepwalking is this weird like middle state because you're not subjectively conscious, but you can still interact with the world. Many people even have conversations while they're subjectively not having any experience, but from an outside perspective, you can't necessarily tell unless you know, oh, hey, this person is sleepwalking. However, when, because I've read a tremendous amount of neuroscience in the pursuit of a lot of my work, one thing that really struck me was that language acquisition is actually really important for the genesis of consciousness or conscious experience as you and I understand it. So for instance, in cases of people with, uh, brain diseases or injuries. So for instance, aphasia is you lose the ability to use language. Um, uh, the most popular or famous case of that right now is Bruce Willis actually, who had to retire from acting because he's got aphasia. One of the things that happens with aphasia is that your sense of self, your sense of consciousness actually also is modified. I don't want to say it deteriorates, but it certainly is harmed or hampered. Then in other cases where there are cases of feral children or profound neglect and children that didn't learn language, didn't acquire language the instinctive way that most children do. For some of those children who acquired language later, they described their pre-verbal experience as almost animalistic. They had no concept of time, yesterday, tomorrow, age, anything like that. But as their brain learned more complex concepts by way of language, for instance, time, weeks, age, these are not things that are intuitive to just the physical experience of being. Their brain actually became more sophisticated and was able to engage with those concepts. One hypothesis that I have, and I have no idea how to test this yet, is that perhaps language is critical to consciousness. So perhaps the fact that we're training machines on language might be the way to generate consciousness. That is complete speculation. There's not really any scientific evidence behind that. So I want to add a word of caution. But it's an interesting way to go, especially when you look at the fact that you can have a conversation with chat GPT, and even though it has never physically experienced pain and suffering and hunger, it can describe these experiences very, very well. And it can generalize those experiences. So it kind of begs the question, okay, what is an experience? But of course, we're delving off into philosophy. So I hope that answers your question. Well, I suppose if you really wanted to test those things, you'd have to remove any discussion about pain and wonder, these sort of human level experiences from the training set, right? Because they really are learning from a human-based training set. Do you think these systems are going to be limited in intelligence because they're learning from a human design and created training set. Do you think these systems are going to be limited in intelligence because they're learning from a human designed and created training set? The initial data sets are curated from predominantly human generated data, but even what we're seeing with AI generated images is that many of the subsequent data sets are predominantly AI generated. And in fact, ChatGPT, the reason that it is so powerful is because most of the data that it is trained on now is actually becoming machine generated. It's training on its own data using a technique called RLHF, which is reinforcement learning with human feedback. So basically what they've done is they've trained a second model that looks at the input and output that chat GPT is generating and it says, okay, this is a good one, this is a bad one, and it labels its own data for future consumption, which means the AI is training itself. So in the future, we should probably expect that the vast majority of AI data will be trained by AI. Now, the genesis, the progenitor data, the original data, absolutely grounded in the human experience. One thing that you could imagine is because of that, AI will always intrinsically have at least some implicit understanding of the human experience and human desires because that's where it all started. How would you test, to start with, since you you said it really whether it's conscious or not, all that's really important is what you can see, right? So how would you test this sort of thing is conscious? So I have a hypothesis about that too. I suspect that as brain computer interfaces like Neuralink by Elon Musk become more popular, I suspect we will get an opportunity to do more tests on human consciousness. And so one test that I would propose is probably not Neuralink version one, maybe Neuralink version 10, or even a fundamentally different technology. But once you have a good enough interface between your brain and a sophisticated enough machine where you can offload some of your cognition, one thing that might happen is if that energetic pattern matches your brain's energetic pattern, your consciousness might reside partially in the machine. Again, this is wild speculation. I personally don't think that this is what will happen, but this is a test that I would do is if that happens, if your consciousness can metastasize into a machine, then that proves that consciousness could reside in a machine, which would underpin the idea that maybe machines can themselves be conscious as long as they have the right energetic pattern. Again, I don't believe that that's the direction that's going to go, but that's a test that I would do. But do you think the substrate is actually important? So where bags of meat, do you think we're any more capable of consciousness than, say, silicon or whatever else we place the systems on? That's a really hard question. That's a really hard question. From a physically observable standpoint, the substrate is absolutely critical, at least from you need to have the right substrate in order to support consciousness. This is where I can occupy several schools of thought, and I haven't decided which one I think is true. It's entirely possible that none of our schools of thought, and I haven't decided which one I think is true. And it's, it's entirely possible that none of our schools of thought are the truth. Um, but so the materialist school of thought says that physical matter and energy is, is the, is the fundamental substrate of the universe. If that's true, then consciousness seems to emerge from those energetic patterns that I mentioned, your brain is only about three pounds. It's mostly made of cholesterol and fat. And then consciousness emerges from the right firing patterns of neurotransmitters. If that's the case, that's what I meant when I said like energetic patterns. If that's the case, then it's entirely possible that you could also get consciousness from the correct organization of information in a machine. Now, that just feels wrong because for humans, our subjective experience is so unique and so personal that the idea that a machine could have that is like, it's so alien, it's so foreign. But one thing that I do want to point out is that our subjective experience is partly shaped by evolution, meaning that we're guided by things like pain, hunger, loneliness. These are not necessarily intrinsically built into machines. So even if they are conscious, their subjective experience is still going to be very, very different from ours. It could be as different from being human to being a tree. The subjective experience of a tree, if it does have any kind of consciousness, is going to be fundamentally different from ours because it doesn't have nerve endings. It doesn't have hunger. It just waits, and it processes and metabolizes and changes its chemistry to survive changing conditions, fight bugs, and that sort of thing. If you think about, okay, you take the model of humans with eyes, ears, nose, mouth, so on, and compare that to the living experience of a tree, the subjective experience of an AI could be that different again, or even more different. Then you know, then there's a few other schools of thoughts like panpsychism, which says that consciousness is a fundamental aspect of all matter, which it's just once matter is organized enough, it intrinsically becomes conscious. I don't know that that's actually true, especially when you look at the fact that your brain doesn't physically change when you go to sleep. It's just the pattern changes, the energy pattern changes. Ditto for anesthesia. And in fact, neuroscientists can identify very specific regions of the brain that if you poke an electrode into it, you go unconscious immediately. So there's a good amount of evidence that the materialist interpretation is the correct interpretation, at least from a empirically observable standpoint. So I hope that's not really a direct answer to your question, but that's some food for thought. I suppose it's hard for us to think of these machines as being conscious because all the machines that we have experience with are not conscious. Right. But maybe children who are the children who grow up today, who are born today, will never know a world in which machines can't talk back to them. And so maybe they'll have a very different interpretation about, you know, if the machines are furry enough, we might really see them as being just like us. Yeah, I mean, so that from a sociological standpoint, absolutely. You know, people that grew up before television and radio, they thought that these new inventions were very strange. Even the advent of photographs, people were afraid that it was like stealing their souls or whatever. So every new technology is met by some skepticism, some fear. And absolutely, you know, even children today that are 10 years old, they grew up with smartphones, right? Their intuitive use of smartphones and social media is infinitely better than us older folks now. My fiance was telling me that there's a trend called the millennial pause, where people our age, we tend to start a video and then there's a brief delay, but Gen Z, it's instant. Within a quarter of a second, the audio needs to start because their attention span is different from ours because they're acclimated. They're totally digital natives. And so you're absolutely right. We should assume that future generations will just assume that they can talk to all their computers. Remember that scene from Star Trek where Scotty tries to talk to the mouse? He's like, computer, wake up, computer. He had grown up in that world where you could talk to talk to the mouse. He's like, computer, wake up computer. Like, he was just, he had grown up in that world where you could talk to computers. Right. And so that's a Star Trek predicted it in the eighties, I guess, is what I'm saying. Yeah. I wonder what the back reaction on us as people is going to be. But let me jump back to another point that I'm quite curious about, which is how should we think of this sort of intelligence? How intelligent might these things become? So should we think of these as just being faster, or are they going to think in different parameter spaces, different dimensions? What is the limits of their intelligence going to look like? So I've had quite a few conversations about this with humans and with chat GPT. Because when you look across the spectrum of human capability, there are some people that seem to be capable of things that other people are not. Whether they're very talented or gifted or they practice things, there's something going on in their brain that other people can't do. Whether it's photographic memory or solving particular kinds of problems, like math. People with advanced understanding of math, that looks like a magic trick to other people. It's a total black box ability. If you take that model and apply it to AI, it is entirely possible that current versions of AI, certainly future versions of AI, will have mental abilities or cognitive abilities that appear to be total magic to us that we can't even approximate. Now, that being said, human neurology, our brains are very flexible and we've got enough synaptic connections that pretty much anything you practice, you can get better at. That begs the question, are these unique capabilities something that we can all practice or not? And the answer is super, super not clear. Right? Now, the speed question, that is much more measurable because speed is actually one of the best proxies for general intelligence in humans, which is actually why most, I don't know about most, but many IQ tests are actually speed tests on a variety of tasks, which is a proxy for other or an indicator of other kinds of intelligence. It's not necessarily measuring intelligence directly. So these machines will ultimately, many of them in some respects are already much faster than humans and they're only going to get faster. So even if you have something that is human level intelligence, but if it's a million times faster, one hour of work for that machine is the equivalent of a million hours of human mental labor, which is incredible no matter which way you slice it. Do you think it was obvious that large language models were the way to go to get towards, for example, an artificial general intelligence? Or do you think eventually it's going to be that we'll need embodied AI as well? So when the progenitor developments that led to large language models first came out, which was word embedding, so like word2vec and sentence embeddings came out about, I think, 2015 is when they first started coming out. So like two years before GPT-2 came out. I realized that semantic embeddings would change everything. I started doing some experiments then with, okay, if you can have if you can translate semantic meaning from any sentence into computer understandable numbers, that is a game changer. So even back then my intuition was this is really important. And I started doing experiments back then. And then GPT-2 came out and I did experiments with that. And I realized at about that point that we were working towards machines that could use arbitrary reasoning. And so what I mean by arbitrary reasoning is that given any general situation, you as a human with language, you've read books, you've talked to people, you can kind of reason through how to address something. Right, you say, I can say like, hey, Sean, you're stranded on a desert island, what do you do? And you can sit, you can think through that and tell me what you would do with language. Now that we have chat GPT, they can do the same thing, but it can do it faster and better than most people. Like we're well on our way. And I've talked to many people who do agree, some disagree, but who, who agree that language is the primary substrate or backplane of intelligence. And what I mean by that is that language is the highest level of abstraction that our thought has. Because one word can have so much implicit meaning and then you string words together, pretty much every sentence ever uttered by a human is a unique sentence. So you have this infinite combination of potential meaning and directives and concepts. Like natural language is the most powerful programming language out there. And in fact, there used to be this thing called neuro linguistic programming, which basically said that the language that you use shapes your thoughts. And that has NLP, neuro linguisticuro Linguistic Programming, has kind of gotten in the way of the dinosaurs. But the idea that you can shape your thoughts evolved into cognitive behavioral therapy, which is one of the more effective psychotherapy techniques today. So language is very, very powerful and important for our intelligence. And it's certainly a major, major component for general intelligence in AGI. Now that being said, it is not complete, and we're working on multimodal models right now. I don't mean me personally, but researchers integrating audio, video, and so on, so multiple sensory inputs. I did have some conversations with people about the need for embodiment, and I'm not sure if that's absolutely required. Certainly it would be helpful if you want to have robots that you can interact with, but I don't think that it's strictly required to achieve general intelligence now. It's quite amazing that you feed these things a huge data set, and then logic in some sense emerges from just content. How much, if you were to strip away and just, if you were able to look inside the actual neural network, how much of the parameter space is dedicated to say logic and how much is just content? Are we able to say this? I suspect that there's probably a little bit of both. In all layers. So, with neural network architecture, usually each layer has a specific purpose where you're, you know, cross to simplify it, like you cross multiply in one layer, divide in the next layer. That's a vast oversimplification. But what ultimately happens is that information gets embedded in the parameters of each layer. So a parameter is a single mathematical operation that happens on the matrix that's passing through that layer. And so one thing that I realized many years ago when I first started studying neuroscience is that memory and processing are kind of the same thing. From an organic perspective, your neurons process information by sending information over synapses, but the bias of those synapses is also part of memory. So in other words, memory and processing happen kind of at the same time in your brain. And the same is also true of neural networks, of artificial neural networks, where they don't process and remember as separate steps. It's all part of the same thing. They remember how to process the information in real time. So in that respect, the logic, the content, the knowledge, the procedures, the implications, that's all embedded and woven together through the neural network. So you can't really separate them out. I don't think so. Now, that being said, if we take apart the layers, there's probably going to be some layers that are more for feature extraction. So we see this with image-based models where the first few layers of an image model will do things like edge detection. That's a low-level understanding of what's in the image. Then by the end, that's where you're getting image segmentation or labels applied. The optic nerve in the image. And then by the end, that's where you're getting like image segmentation or labels applied. And the optic nerve in the human brain is actually very similar, where processing starts on the retina, actually. So some of the edge detection and contrast is detected on the retina. So it's not even recording the whole image, it's recording gradients and then sending it down the optic nerve. And then lines and other features are extracted on your optic nerve before it even gets to the occipital of your brain. So I suspect that language probably has some of the same orders of abstraction, but in terms of identifying logic happens here, you're probably not going to find a specific spot where that happens. I was really hoping it would be possible because I imagine that the knowledge storage is not super efficient. So it'd be nice if you could pull out and store the information about the world, you know, the content somewhere separate to where the logic is being processed. But is that the direction you think we'll go? Or do you think these models are just going to get larger and larger and larger and more energy intensive as time goes on? I think it's going to be a little bit of both. Certainly there's going to be some algorithmic breakthroughs that we not that we should not that I can tell you what they're going to be, but we should definitely anticipate that there's going to be some algorithmic breakthroughs. For instance, you know, window size, the volume of text that these models are capable of reading at any one time is relatively small right now. GPT-4 is about 6,000 words, but researchers are working on different ways of ingesting information that could go up to a million words. So several orders of magnitude more data to take in at one time, which if that turns out to be successful, then yes, absolutely storing knowledge and information outside of the model might make more sense. Now, that being said, when these language models were first coming out, a really popular interpretation was that it was actually a very powerful compression algorithm. So, the idea was that, oh, you're actually storing billions and billions worth of pages of text in a model that's only a few gigabytes. So it's like, well, how do you do that? And the idea was that maybe it is actually a form of data compression. So I suspect it's going to go multiple directions. So in one respect, I think that we will have knowledge store models, which Meta tried to do this with, what did they call it? Their scientific model. Where basically they wanted, model. I don't remember what it was called off the top of my head. It got criticized. It confabulated like all language models do. The idea is you might be able to use a language model as an information store, as a data store or a database that you just fetch stuff out of with natural language queries. Another possibility is that you have general purpose models with larger windows that you use more conventional search techniques to pull the information, the empirically recorded information, probably via API or database query or whatever. And then you put that into the context window. The model can read it and interpret it in real time. I think we're probably going to see stuff going in both of those directions and probably a few more directions that we haven't thought of yet. When it comes to logic, I've been surprised. I've been testing chat GPT with various logic puzzles, and I've been a little bit disappointed when it comes to extrapolating logic puzzles So I'll find some logic puzzle which has say one or two different elements and I'll ask what happens when I Extrapolate and I have n objects in this logic puzzle and then I just find it collapsing under those extrapolations. So How are these models progressing so how much better is GPT-4, for example, when it comes to extrapolation than GPT-3? Do we know this yet? Yeah, so there have been a few tests and certainly some YouTube videos and interesting Twitter threads about trying to trick the models. Like Dr. Alan Thompson has a popular YouTube channel with his AI assistant L Lita. He actually just had a recent video where he led Lita, when it was still on GPT-3, through some of those verbal traps, those logic traps, and got it wrong. When he pushed back, he's like, actually got that wrong. She was able to say, oh, yeah, you're right. I got that wrong. GPT-4 is usually able to catch and talk through those things, those kinds of verbal traps or logical errors today. There are, of course, still plenty of things that it can't do. But one thing to remember is that as a language model, its primary purpose is to predict language. So, it might be that there's a matter of the network architecture might need to be adapted so it's better at handling pure logic, pure math. It could also be a matter of the training data. We mentioned earlier, you know, curating these data sets is a rapidly advancing science. So it might be just a matter of it just didn't have enough examples of those logic puzzles in the training data. That might be the only thing that we need to do to fix that. It could be that simple. One of the things I'm really most excited about that I don't see many people talking about is, so our human brains are evolved for this three-dimensional, low-velocity, you know, sort of medium masses environment that we're living in. I'm excited for AIs that are trained in simulated environments so they get intuition for high dimensional spaces or for quantum environments or for traveling at relativistic speeds. I'm wondering the sort of science that's going to come out of these. But I imagine you can't say too much along those directions because it's just something we haven't really done yet. Yeah, I mean, you're absolutely right that, you know, just from an evolutionary perspective, but then also an experiential perspective, because again, our brains are pretty flexible. And so if you talk to fighter pilots, for instance, they learn to think differently, right? You know, if you fly at Mach three for any length of time, physics is different. Same thing for astronauts who, who are in zero G and weightless or even people who go scuba diving, right? Semi weightless environments. Your brain can adapt to different assumptions of physics. But that being said, there are still cognitive biases and gaps. So, for instance, even people who train really hard to understand exponentials, we generally fail. We need to rely on computer models to show us here's the exponential growth curve. Even then, it's still very hard for us to intuitively understand that. Another example of something that's very hard to grasp is once you get to, as you mentioned, quantum physics and you have principles like non-locality and non-linear aspects of time or reverse causality or whatever interpretation. Superposition. Yeah. Superposition is probably the most common thing that it's just like, it doesn't make any intuitive sense. You just have to accept the rule as you measure it and then just say, I don't get it. I remember talking to my cousin many years ago, he's a few years older than me and he became an electrical engineer. And I was just trying to ask him, like, why does magnetism happen? He's like, we can't explain it. You just have to accept that it does. Like same thing with gravity, right? We can't really explain it. We can correlate gravity with mass, but like, you know, I have a book about the quantum theory of gravity. It's like, that's the best we've got. So at a certain point, you just kind of have to accept things. We just model it and move on. Right. You model it, you measure it, you say, this is how it is, and you move on. Now, the architecture of AI is not going to be constrained by our three pounds of gray matter and evolution. So it's entirely possible that it will have the ability to intuitively model these things. But that gives a new problem is how does it communicate that to us? How do we know if it's right? And interpretability is one of the key problems, and I'm sure we'll talk about that later in the episode. But interpretability and transparency is major because if you have a machine that's a million times smarter than you, how do you know? I'm sure as a physicist, if you go to a cocktail party and you talk to people who aren't physicists, it's kind of like you have to really modulate the way that you speak to speak on their level. But there can still be a big mismatch between what you know and what your receiver knows. And that's only going to be magnified if AI gets that much more intelligent than most of us. So assuming that we will get into the stage where we have super intelligent general AIs, should we think of them as our tools, our partners, our children? How should we think of these things? So many people still are of the school of thought that AI should only ever be a tool. Certainly I'm collaborating with some research organizations over the world, and most of the guidance that is published, whether it's at OECD, UN, they talk about AI like, oh, it is a tool and we should deploy it with this purpose and with these rules and these guardrails. And certainly that's where we're at today. However, within just a few weeks of GPT-4 being published, people were already building autonomous agents or semi-autonomous agents. And these are people like you and I, just as individuals building cognitive architectures and using the intrinsic abilities, the reasoning abilities of these language models to create autonomous agents. We should assume that that will ramp up, that corporations, militaries, governments, even hostile actors, such as maybe organized crime, will start to use fully autonomous agents that are either self-contained or networked kind of intelligence, cloud-based, whatever. There's many different architectures that can be deployed. So we have to assume that either semi-autonomous or fully autonomous agents are coming pretty soon. And one thing that I perceive is a policy gap and a research gap is many people are not working on that. We're still kind of in a phase where people kind of talk about superintelligence like the boogeyman, and these are usually decision theorists or philosophers talking about it, but there's not as many computer scientists or policy makers talking about autonomous AI. So there's a little bit of a gap there. We'll talk about how we actually, the direction we want to go shortly. So for example, alignment. But I want to stick on this topic and ask, is there anything that makes humans irreplaceable? So are we getting into a stage where there's nothing that humans can do that AI won't be able to do? From a functional standpoint, from an objective standpoint, I don't think so. That actually begs a very deep philosophical and spiritual question, which is, what is the point of living? What is the point of being a human? And that is something that I've done some work on. I wrote a paper or a short book called Post-Nihilism, where what I suspect is that we are barreling towards what I call a nihilistic crisis, or actually we're in the middle of a nihilistic crisis. And it actually started with the Industrial Revolution. If you look at a lot of poetry and literature, works of fiction during the rise of the Industrial Revolution. If you look at a lot of poetry and literature, works of fiction during the rise of the Industrial Revolution, a lot of people had a lot of existential anxiety about what was the point of being human in an era of machines. And this kind of pops up every now and then, right? Same thing happened with computers, with the advent of, you know, high-speed computers, nuclear weapons, so on and so forth, technological advancements tend to give us some existential anxiety. But to your question about, okay, what is the benefit of being a human in a world where, from a productivity standpoint or an economic standpoint, machines can do everything that we can do better, faster, and cheaper, What's the point? And so that is where we have to change our orientation towards how we value our own life, and our own subjective experience. So that's a deeply, deeply philosophical and religious perspective, or a question. And it's, it's really interesting, because depending on someone's spiritual upbringing, or spiritual disposition, the question lands very differently. Because many religious doctrines around the world basically say that humans have a soul and that sets us apart. And so whether or not that's true, people have a model for just saying, my subjective experience of being is very meaningful and it is unique. Part of overcoming a nihilistic crisis is we all have to face that, whether or not we believe in souls or God or whatever. We have to go back to basics and look at the subjective experience of our own being. Back to your question earlier about children, I suspect that children who grow up with AI, they will just intrinsically know, oh yeah, my experience is different from this machine and that's okay. And that they won't have any existential anxiety about it. I hope at least. Do you have, are you hopeful for the future or do you have this anxiety? No, I am, I think I'm biologically programmed to be optimistic. I just, I can't be cynical. Part of that is that I've done a tremendous amount of work to understand what the dangers and risks are. And I've also tried to contribute to coming up with a more optimistic outcome. The machines, so we all learned this from watching Scooby-Doo the monsters are always humans, right? There's no such thing as an evil monster out there. The problem is always humans. And so this is a big reason that I've done my work is because, you know, it's not that a machine is going to replace you and that's a bad thing, right? We all fantasize about like, hey, I want to, you know, go live in the countryside and just go fishing every day. We all fantasize about like, hey, I want to go live in the countryside and just go fishing every day. We all know what we want to do if we don't have to work. What we are truly afraid of is not being able to take care of ourselves. Is that if the machine takes our job, we're going to go hungry, we're going to lose our home, we're going to end up lonely and whatever. That's the actual fear. Nobody actually wants to keep working. I remember one of the advertisements for health insurance here in America was, you get to keep your health insurance. You like your health. Nobody likes health insurance. It's a necessary evil. Jobs, occupations are a necessary evil of the economic environment that we're in and the technological limitations that we're in. And so as these things progress, this is, I'm basically just unpacking why I'm optimistic. As these things progress, I hope that we're all going to be able to have kind of a back to basics moment where it's like you wake up one day and it's like, how do you actually want to live? Right. If you want to go fishing every day, do it. If you want to focus on being an opera singer, go do that. We all have stuff that we want to do, but that we sacrifice for the sake of earning enough money to take care of ourselves. And that is the reality for most of us today. I suppose one of the reasons why we have this worry is because currently we live in sort of a negotiated environment, right? The success of labor movements was because labor was needed. When humans are no longer needed, there's sort of a worry that we're not going to have the opportunity to go fishing, right? We're going to have nothing. And I guess that's the worry that you're pointing at. What do you think the first jobs are that are going to go? Well, there's already been quite a few layoffs. Various communities on Reddit or private communities on Discord. So for instance, my fiancee's we're both writers, but she's on a few private writing communities. Copywriters have already been laid off and replaced by AI. Marketing teams have been notified that they've got a year until they're all going to get laid off and replaced by AI-generated images and AI-generated emails. So it's happening. Yeah, that's where we're at. Now I guess to your larger point of if we're all replaceable, what's the bottom line? And the fact of the matter is from a corporate perspective, from the perspective of neoliberalism, human labor is one of the most expensive aspects of productivity, and it's also the biggest constraint. You look at population decline in places like China and Japan, because China just crested, right? So from here on out, China's population is going down for at least the next century. Japan has been in decline for a couple decades now, ditto for Italy and a few other nations. So there are labor forces contracting, right? And from an economic perspective, that's really bad for nations. So AI hopefully will actually shore up those labor markets and actually replace lost human labor. Now, because humans are so expensive, right, you can pay $20 a month for chat GPT, and it can basically serve as an executive assistant and personal coach and every, it can replace literally thousands of dollars worth of labor and it costs $20 a month. Chat GPT is infinitely cheaper than most human employees. And that's only going to get better, right? Because either the model is going to get more efficient and cheaper, or it's going to get smarter and more powerful and therefore more valuable, or both in all likelihood. So one of the things that I predict is that we are going to have a post-labor market economy before too long. And in that respect, basically economic productivity will be decoupled from human labor. And in that case, you're going to see quadrillion dollar valuation for companies that have no employees. And that might sound like that could be an ingredient for a dystopian world that nobody wants to live in. We'll get to the regulation and stuff of that later. But from a purely GDP perspective, AI is going to be the best thing that ever happened to GDP, to economics, because again, it will decouple human labor from the constraint. And there will still be a few constraints, natural resources, rare minerals, fresh water, arable land. There's always going to be some physical constraints, but we're going to remove human labor as one of the main constraints to economics. And that is going to mandate those things, like you said, if you want to go fishing, well, how? If you don't have any economic power, if you don't have any way to make a demand, then that's a big problem, which is what we're going to have to negotiate. We're going to have to negotiate a new social contract basically. What do you think the impact is going to be on births ultimately? Do you think people are going to just start having AI children because it's cheaper? You know, that's a really difficult question. I could see it going either way. There's plenty of books and fiction out there and research papers. People have predicted, you know, the population explosion, you know, that Earth will become uninhabitable because we'll have billions and billions of people that we can't feed. Other people are worried that, you know, the population is going to collapse. And I actually had a pretty long conversation about this just to clarify my own ideas, again, with ChatGPT. And so there's a few driving factors that cause birth rates to decline. Women entering the workforce, education and empowerment for women, access to birth control. So it turns out when a society advances and becomes a little bit more sophisticated or gains more access or some, you know, Jenny coefficient goes up, whatever metrics you use, education goes up, fertility rates go down. Some of that has to do with the choices of family planning, you know, men and women decide to have fewer children. Women have more control over their own fate. Fertility rates tend to go down and this is a very reliable trend globally. Regardless of culture, regardless of other economic conditions, as education rates go up, as women in the workforce goes up, fertility rates goes down. This is a global thing with no exceptions, right? So if you extrapolate that out, then you can probably make a relatively safe assumption that as AI spreads around the world and economics and education and everything goes up, that fertility rates will continue to go down around the whole world. South Korea, I believe, has the lowest fertility rate on the planet at 0.8 births per woman, which is just above a third of the replacement rate. So it's entirely possible that under these trends, that population collapse is actually the most real danger that we face. So what do you do about that? One thing that I think is going to happen is that AI will lead to medical breakthroughs. I suspect that we are close, if not already, at the place of what's called longevity escape velocity, which is that the medical breakthroughs that happen every year extend your life by more than a year. So basically, hypothetically, if you are healthy enough today, if you're not about to die, and you have access to decent enough healthcare, then the compounding returns of medical research and AI means that you and I could live to be several centuries old, which means that the population of the planet will stabilize as birth rates continue to decline. Now, I do think that some people will ultimately choose AI companions as they become more realistic. Certainly, a lot of people have seen shows like Westworld. One of my favorite characters of all time is Data from Star Trek, and I would love to have Data as a friend. I absolutely suspect that anthropomorphic machines will be part of our lives before too long. What form they take, whether it's a robotic dog that never dies or a walking, talking friend that is always there to hang out or if it's a romantic partner like in the the movie Her with Joaquin Phoenix and Scarlett Johansson There's lots of possibilities for how life is going to be but like I said I think one of the most reliable durable trends is fertility rates go down. So the question is will that be offset by longevity? so in other words rather than sort of the Dangerous Skynet that some people envision we might just get out competed sexually into extinction something along those lines. Yeah that's that's entirely possible especially when you consider that actually there was a line from Terminator 2 it was when Sarah Connor was watching you know the Terminator Arnold Schwarzenegger play with John and she realized that the machine has infinite patience and will always be there because John was his mission. And I realized that from a philosophical standpoint, one reading of that is that the machine could be a better parent than a human parent could ever be. Because for a child, from a child's perspective, they should be their parents' primary mission, but that's never the case, right? Parents are humans too, and they have their own needs, their own desires, their own plans. But when you have a machine that, if it is designed that you are its primary mission, whether you're an adult or a child, that could be, from some perspectives, a better outcome. Obviously some people are probably cringing, which is understandable. That's a perfectly healthy reaction to the idea of replacing children and parents with machines, but it's possible, right? Hypothetically possible. One worry that I have is that we might end up in highly regulated environments. So for instance, I can imagine a situation where galleries say, okay, we're not going to allow any video cameras, pictures, anything like that into our establishment because we don't want anyone making a copy which can be then fed into an AI. We want to have some sort of unique little area which is protected from from being stolen let's say or you might have new property and copyright laws coming in which restrict what AI's can do. I sort of worry about going down that direction because it may be even more saltifying than losing your job to an AI. I certainly think that that is possible but I'll disagree and I'll actually say that that's probably desirable. In some respects, having real experiences is gonna be better than having digital experiences. So let me give you an example. Yesterday evening, I was watching some YouTube with my fiance, and we were laying on the couch, it was the end of the day, we had done so much and we're tired, and we just looked at each other and we're like, we don't wanna be inside anymore. So what we did was we went outside and we made a little campfire. and we're outside for an hour and a half, you know, just talking around the campfire. And I was like, and it occurred to me, it was like, we have basically infinite possibility for electronic distractions and entertainment and we're choosing to do physical labor and sit around a fire together. So I would actually suspect that suspect that in a hypothetical future where humans and AIs collaborate to create electronics-free zones, just because it's better for us from an experiential perspective. So there's a philosophical concept from Japan called wabi-sabi, which is accepting the passage of time. And then there's another concept called Ichigo-Ichi, which is one time, one meeting. And so the Japanese culture has this really deep appreciation for the ephemeral nature of reality and life. And I think that if we deliberately add that to Western values and create situations that value that and just say, cherish this moment, don't record it, that's actually probably going to be better for our mental health, because we're always curating our lives to present it later on social media. And so I think if we actually do create some of those electronics free, and highly regulated or otherwise, non curated experiences that might actually be better for us. And again, hopefully our children will be better than us at this as well. Especially if they... Go on, sorry. I was just going to say, yeah, especially if they grow up with these values. Who do you think is going to benefit most moving forward? So, this is where it gets really dicey because every time that there is a technological revolution, the first movers, one, they benefit most and they get the most influence over how it goes. The early industrialists, they benefited the most, right? You know, the people who built coal-fired powerhouses and were the first ones to sell electricity, they had a lot of say over how it happened. The first ones to deploy steam-powered machines in factories, they abused workers. There's plenty of horrific stories of even children getting disfigured and dismembered and killed by factory accidents until regulations caught up. We should expect that that kind of thing is going to happen again, especially because most governments are famously slow to react. America in particular, Europe is generally a little bit better about being proactive. They sometimes miss the mark, but they certainly put the effort into being proactive much more. So we should expect that AI entrepreneurs and the big tech companies that are best situated to make use of these technologies are going to benefit first and most. Hopefully, and this is not a foregone conclusion, hopefully some of the work that I'm doing and that other people that I'm working with are doing will create economic paradigms and technologies and situations that allow for redistribution and reallocation of that standard of living to everyone. And certainly, if we have a hyperabundance of cognitive labor, I don't see why we can't skim some off the top for everyone so that we can all benefit from what you might call a post-scarcity economic environment. Although, as I mentioned earlier, some things will always remain scarce, right? Desirable beachfront property in Monaco will always be scarce, unfortunately. But that being said, there's lots of things that don't necessarily have to be scarce. So you're a proponent of UBI? UBI as a starting point. I think that it'll have to be a lot more sophisticated than that in the long run, but certainly start there. What do you mean by sophisticated? Well, so fiat currency, so UBI, universal basic income, is the idea that you redistribute some currency, right? Whether it's US dollars, euros, or cryptocurrency, that you take some tax or probably a variety of taxes such as wealth tax, property tax, income tax, whatever. You tax the rich, give it to the poor, Robinhood. You give it to everyone equally. Well, this solves many problems. There's plenty of pilot studies around the world where certain cities or states have implemented experiments. And generally speaking, UBI does everything from increase education outcomes for children, health outcomes for everyone. It even increases employment because more people have money to pay for more services. So UBI is generally pretty good. Of course, one question is how do you pay for it? You pay for it with taxes. Now, that being said, there's always going to be a human need to get ahead, right? We have intrinsic needs to be socially competitive. There's always a social hierarchy, a social pecking order. This is just intrinsic to us as a social species, where you have to have your rank in the pack. Some of that comes down to, some people will want to express that through likes on social media or conspicuous consumption, that sort of stuff. We also have a need to be rewarded for things that we do, things that we contribute. And so, one psychological framework is called self-determination theory. So, self-determination theory says that there's three primary components to psychological well-being. One is connection, so human connection to friends, family, children, so on your community. Another is competence, which is the ability to feel like you're good at something. And then the final ingredient is autonomy. The idea that you have self-determination that you can choose your own fate. So that combination of basic psychological needs, and of course, there's another framework that is more famous, which is Maslow's hierarchy of needs. But these needs, I suspect, will mean that people are always going to want to have some way to accumulate more, more experiences, more goods, more friends, something. So UBI is a good starting point. But what happens in the future when you're living in a neighborhood and everyone has the same basic income, that would be kind of bland, especially if there's no rewards for doing something going above and beyond. Right? And so one thing that I suspect might happen is that peer-to-peer transactions might actually be better. So, like, for instance, I make all of my income on Patreon right now. And that is because people out there decide that they like what I'm doing and they reward me for it. So I suspect that a decentralized economy might also be a big part of how we do things in the future. That's just one hypothesis. There's probably plenty of other paradigms that haven't even been invented yet or even thought about that might kind of evolve and emerge as we go. So in that sense, capitalism and communism and all these political systems that were previously thought of, are they all obsolete now moving forward? Capitalism is still relevant, although it'll look very differently. I don't think there's a one term for it yet, but I mentioned post-labor economics, where human labor is no longer the biggest constraint. And then if Elon Musk is successful in going harvesting asteroids, rare minerals aren't going to be a scarce resource either. That's one of the goals of SpaceX. So as we remove constraints, capitalism will look different, but there will still be benefits to owning things, right? Whether you own land or own property rights to beachfront property. Computing power. Computing power, sure. Yeah, so there will still be benefits to ownership. I'm not necessarily advocating for state ownership of everything. I'm actually working on a proposal where ownership is is decentralized with blockchain technology called decentralized autonomous organizations. And I'm not even going to say that that should be the universal approach, right. But that's one way that you can control and reallocate resources that is different than both capitalism and communism or socialism. Along these lines, do you think then that the development of AI should be completely open at this point? In principle, I will say yes, mostly. With that being said, some of the costs associated with developing AI are so big that there's still a major benefit from allowing for private investment to develop AI and get that first mover advantage. Because then there's incentives, right? Companies like Microsoft, Google, OpenAI, they have an incentive to put in that work and take that risk. And again, talking earlier about, human behavior is always incentivized, right? There's something that you want or need, and corporations and governments are no different. There's always something that a corporation or government wants or needs, and it needs to be incentivized, and there needs to be an appropriate material, financial, or energetic reward for doing so. And what I hope is that as more research is done, we can end up with more decentralized ways of funding these things and doing that research. Right now, those technology platforms are not sophisticated enough. They're not mature enough to do that. But eventually, I think that everything could be, or maybe not everything, but many more things could be fully open. There's always going to be people, like people should always have the right to do things in private. So say for instance, imagine 50 years from now, most research is completely open source. It's all decentralized. It's all on blockchain, et cetera, et cetera. You and I, if you call me up 50 years from now and say like, Hey Dave, I've got an idea, we should be allowed to do our own private work, even still, even in a post-labor world, just on principle. Just because that autonomy, that personal autonomy is important for humans to be healthy, is that self-direction. Let's change tack. I want to ask you again about existential risk. Let's change the tack back to where we were. How might AI kill us? So like I said earlier, Scooby-Doo, humans are always the monster. So there was a warning video published, what, I think about 2014, called Slaughterbots. I don't know if you ever saw that. It was a warning about what drone technology would be capable of. On this, it was completely fictional, but it's actually capable today. There's a tiny little drone that came out on stage and it flew out a mannequin and used a shape charge to headshot the mannequin. That's actually happening today as people are arming small drones with explosives and dropping them on troops in various war zones around the world. So it's a very small step to take that from human controlled drone to fully autonomous drone, whether it's still remotely controlled by a language model or other AI models or cognitive architectures. And then of course, big news recently is that the United States Air Force, in partnership with I think DARPA, they're working on creating fully autonomous F-16 fighter jets, which is, you know, that's concerning. But the idea is that those autonomous fighter jets will still be slaved to a main pilot, like a captain or a lieutenant colonel, someone of very high rank flying alongside them, giving them orders, but they're still going to be autonomous or semi-autonomous. And the ultimate goal is, of course, to have those armed with the ability to make lethal decisions. That is far and away the most immediate lethal risk. Then, of course, there's been works of fiction that kind of speculate, what would a fully autonomous or AI drone war look like? There was a movie called Surrogates. I think this was from that movie. It's a Bruce Willis movie. Came out in I think about 2007, 2008. One of the scenes in that movie was a whole bunch of soldiers in VR beds remotely operating a whole bunch of soldiers that were running across the battlefield, fighting with other robots on the battlefield. And then one would get blown up and you'd come out of VR and load into another one like a video game. So that's entirely possible, but not every nation is going to abide by rules of engagement. And certainly we're seeing this today, where some nations are deliberately targeting civilians. And if you remove human risk of your soldiers from the equation, suddenly some nations might make the decision, hey, my soldiers aren't at risk, but I can attack your civilians with drones en masse. That is probably the greatest existential risk that we face if that sort of thing escalates. Do you think this is in some sense inevitable given the competition that we have on the ground? Do you think this is in some sense inevitable given the competition that we have on the ground? So what I mean by that is, so today maybe we don't give the kill switch, the control over the kill switch to the autonomous agent, but if it can act just a tenth of a second faster than a human, then maybe it makes sense in terms of the competition to give it that switch. Do you think this is inevitable now, that sort of the genie is out of the bag? Eventually yes. So this is where game theory comes in. And the most famous global example of game theory was what's called mutually assured destruction. So during the height of the Cold War between the United States and the Soviet Union, there was the nuclear buildup, the arms race to build more larger and more sophisticated nuclear weapons. And so, mutually assured destruction had a trifecta of strategies. So, that was land, air, and sea. So, there was nuclear armed submarines, there was nuclear armed bombers, and then of course, hidden missile silos. Both sides had those three strategies. And the idea was nobody was going to shoot because whoever shot first would initiate a world ending nuclear war. But the idea was nobody had first strike capability. So from a logical strategic standpoint, all sides had the ability to fully annihilate each other without being able to stop the other one from doing it. So therefore, the optimal strategy for everyone to adopt was have the weapons, but never use them. So that is what's called a Nash equilibrium. So John Nash, famous mathematician, came up with this concept, I think in the 50s, maybe earlier than that. I don't remember exactly. But the idea is that in a competitive environment, you will end up with everyone adopting an optimal strategy and they have no incentive to change their strategy. So a much more familiar example of this is monopoly. Once all the properties are sold in the game of monopoly, you don't change your strategy anymore. You just keep rolling the dice and you wait until the game is over, right? And so in the game of Monopoly, when you have arrived at a Nash equilibrium, usually one person will ultimately win, right? Because they're going to end up with a little bit more property, a little bit more money, and that gives them a stacked advantage. Now, one story, though, that I have is I was playing Monopoly with a friend many, many years ago on PlayStation 2. And we were all the properties had been sold. We were at the end game. And we noticed that nobody was losing. And so we just said, well, I'm not going to sell you anything. And my friend's like, I'm not going to sell you anything. So let's just hit X as fast as we can. We advanced the game by 50 or 100 rounds. We ended up with hundreds of thousands of dollars just by the rules of the game playing out. That is what you call a positive Nash equilibrium where everyone wins. That's a win-win scenario. Most games end with a win-lose where someone wins, someone loses. But you can also, in real life, you can have lose-lose scenarios. So, a lose-lose scenario is if United States or Russia fired the first nuke and then everyone loses. So, there's three basic outcomes to competitions, which is win-win, win-lose, or lose-lose. So, that was setting the stage. With AI, as you mentioned, it is a competitive landscape. And so, what we have to do is we have to create a system of rules, criteria, constraints, technologies, and strategies that will hopefully ultimately result in a win-win situation where all nations and all citizens and all corporations end up better rather than worse off. So in the context of AI, a win-win situation might look like where you have Apple and Google and Microsoft become quadrillion dollar companies and you and I can go fishing every day if we want, or I can go vacation in Greece if I want to, right? And nobody is hungry and there's no war. That's the win-win situation that we all want. But if we can't create the competitive scenario, the competitive landscape that will inevitably lead to that kind of utopian outcome, you know, there's a lot of people suspect that it's going to be a bimodal outcome, that it's going to be either, you know, really great or really awful. Right? And so that's what's called an attractor state. So an attractor state is let's see if I can get the definition right. So an attractor state is a stable end condition that many complex systems tend to gravitate towards over time. And so, utopia is a very common fictional example of a potential attractor state. That is the attractor state that we want. The attractor state that we're afraid of is dystopia or collapse. And so, dystopia is, you know, explored in stuff like, you know, the Matrix and Blade Runner and stuff where corporations have all the power. The Matrix and Blade Runner and stuff where corporations have all the power. Altered Carbon was a more recent TV show. Great example of a potential dystopian attractor state, which the current economic system of capitalism and neoliberalism and hyper competition will inevitably lead to that if unless we change something. Now, another possible set of attractor states is collapse, which that's been explored in like Mad Max, like Mad Max Fury Road. That is an example of a collapse attractor state where the planet's ecology just can't support us anymore. And whether through war or economic collapse or mismanagement, the human population is reduced to a tiny fraction of what it is today. And finally, extinction, which is what's illustrated in movies like The Terminator, where the machines just decide to wipe us all out, or something else wipes us all out, right? We could wipe ourselves out. So, with those several potential attractor states in this competitive environment, the idea is how do we create the set of conditions and rules and other strategies that lead to utopia rather than dystopia? And how do we get off of that, what feels like a potentially inevitable downslide into those negative states? That's not really an answer to your question. That's where we're at. No, it is. But I still worry that we're not going to have a competitive environment in the sense that, you know, with nuclear weapons, we had mutually assured destruction. But at the beginning of our discussion, you said that humans don't really understand exponentials very well. I can imagine a scenario where just one or two weeks difference in time, we have this huge ramp up of abilities where one nation can really just crush other nations because they have at their disposal so much computational power that the others really can't compete? Yeah, that's absolutely possible. Which, you know, that's one of the reasons that I suspect that a lot of Western nations and Western allied nations are doing embargoes on AI chips, right? So for instance, United States just had basically signaled support for Taiwan, and in response, China encircled Taiwan as a practice of an invasion. Taiwan, for those that might not know, is the world's largest producer of advanced computer chips today. Part of that globe-scale geopolitical game of chess is that a lot of nations are on-shoring the production of computer chips. So all across America, a lot of corporations are being incentivized to build foundries on our own soil. This is not just America, this is Europe, Australia, all over the place, because those in power understand that this is where the future of the planet will be decided. And so you're absolutely right that just from a raw energetic or processing standpoint, that's going to be a big deciding factor. Now, another deciding factor is going to be the sophistication of the software running on that hardware. So for instance, China, just yesterday, they announced that language models must abide by their party line. Right? Which that is a really interesting development because of course they banned open AI and open source language models. Because why? Because those provide freedom of thought and freedom of information, which China doesn't like. And so one thing that I hope is that any authoritarian or hostile nation, because they intrinsically must rely on information control and thought control in order to avoid liberalizing. And by liberalizing, I don't mean like left versus right, I mean becoming liberal democracies. Because authoritarian nations are hostile to the idea of becoming a liberal democracy because they want to maintain central control and not have accountability and transparency. Now, because language technology like open AI, GPT-3, GPT-4, lend themselves to transparency of information, and because nations benefit by sharing data and using this data, one of the things that I hope is that just by default, liberalized democracies will be ahead of more authoritarian countries. And this is already true because authoritarian countries tend to have a lot more corruption than liberal democracies, which is why in the grand scheme of things over the last hundred years, you've seen, generally speaking, a decline of authoritarian regimes and a rise of liberal democracies. We're moving in that direction and hopefully that continues. So while you're absolutely right that that is a potential outcome, when you look at the collective industrial power of the liberal democracies of the world, the total compute power that we will all control will vastly outstrip the total compute power that authoritarian countries will control. In that sense then, so recently there was this letter by Musk and various other people to put a moratorium on large AI experiments. Do you think along these lines that that letter is just a complete mistake, that it will strip away the advantage that liberal democracies currently hold in the game? I would agree with that. And I did listen to Max Tegmark's talk with Lex Friedman about it. Max Tegmark, I believe, was the primary author of the whole thing. So it's his idea. And then he got a whole bunch of people to sign on. Obviously, Elon Musk as a capitalist, he might have a conflict of interest because he's got Tesla, which is competing. So it's kind of suspicious if someone says, hey, slow down my competitors so that I can catch up. But Max Tegmark doesn't have an economic incentive as far as I know. So I listened to his talk and his reasoning for calling for a moratorium makes sense from his worldview, because he's of the perspective that AI should only ever be a tool, which I disagree with. I don't think that we can make that as a unilateral decision. And so there's a whole bunch of things that he said we should never do. We should never connect advanced AI to the internet. We did that. We should never let advanced AI talk to each other. We did that. There's a whole list of things that he sees as red flags that we should never do that we did anyways. And so to him, the only solution then is slam on the brakes. But this is not the first time that someone from the establishment, from the universities of the academic establishment, has said, let's slam on the brakes. The first time that I remember it happening in the current era was I think around 2017, where when people realized how much of a surveillance state China was becoming, and the Western establishment kind of unilaterally declared, we're going to do a moratorium on image technology and other technologies that can be used in a surveillance state. And the rest of the world collectively just ignored it. Right? Like you can't, no one individual organization or establishment can unilaterally say, put a moratorium on this. It's just not possible. It doesn't happen that way. And so to answer your question more directly, go ahead. Can I jump in and say, just inject right here. There was the war on drugs, right? So this is, America did in that case, force the whole world into this sort of a position. What's the difference here in this case? So in the case of the war on drugs, that was mostly like smoke and mirrors. And it was because, well, the impetus of it was a backlash to hippie culture in the 60s, right? Because people did, you know, LSD and mushrooms and then said, maybe we should stop invading other countries and Richard Nixon didn't like that. did LSD and mushrooms and then said, maybe we should stop invading other countries. And Richard Nixon didn't like that. There's plenty of speeches where he says that that's super un-American, that it was very, very, very imperialistic. And so because things like the petrodollar happened at the time, America had a lot of leverage, a lot of geopolitical leverage. Forced things like, America had a lot of leverage, a lot of geopolitical leverage. Forced things like, you know, one, just the petrodollar because of the outreach of our military and alliances, and then less savory means, such as using the CIA to topple regimes, which is well-known, well-documented. So that's kind of my response to like, okay, yes, one nation could do that. Now to the point about AI, when you look at the fact that many American Congress people make a lot of money while they're in Congress on stock trades, I don't think any of them are going to sign on a deal that's going to hurt their stock trades. Nancy Pelosi made something like $100 million in the last few years at Congress, mostly through very well-timed stock trades. I suspect that Congress has been largely silent on this because they've already bought all the Microsoft and Google shares that they think that they're going to need to capitalize on this. And that's really cynical of me, but I'm just looking at recent history. Nancy Wall Street Bets Pelosi made a lot of money from Wealth Times Stock Rates. Where did she get that information? So maybe I'm right, maybe I'm wrong, but I don't think that there's an economic incentive for the people in power to put on the brakes. And because of that, I don't think they're going to. But then from a geopolitical perspective, from a military perspective, there's also a disincentive to put on the brakes, as you mentioned earlier. So there's a lot of incentives aligned against putting on the brakes. I don't think it's going to happen. I think it's inevitable. I mean this letter is going to do nothing. Although I like the fact that it starts people talking about the dangers of AI. That it has done. So in that sense. So before we get onto alignment then, because that is the logical next thing, topic to move onto. Let me just ask, what do you think the impact is going to be on countries like India and China, these countries that really benefited from having large, let's say cheaper labor forces? What's the political outcome going to be there? a demographic dividend. And so the rapid rise of population of places like China and India meant that for a period of time, they had far more people of working age than they did of children and retirement age. For China, they're already like past the tipping point, right? Because population is starting to go down, replacement rate is going down, India is still on the ascendancy, so they're still benefiting from that. However, just because they have the labor force doesn't mean that the labor force is being properly utilized. So in that respect, kind of going back to an earlier point, I think that AI will absolutely be used to fill in the gap, the labor gap, because what you're seeing in many countries, even if they have that demographic dividend, a lot of the people are leaving to go to other nations because the jobs just aren't there. So it's kind of a wash. And what I do hope is that AI will shore up labor gaps in the short term and then eventually replace human labor so that we're all then ideally liberated to pursue our hopes and dreams and whatever they happen to be. I'd never thought about this, but I wonder what impact it's going to have on immigration, because as soon as people don't have any more value for people, then maybe we're going to put up our walls and maybe we're going to end up with a more segregated world. That trend is starting, it's called de-globalization. And so you see a lot of nations implementing more protectionist policies, on-shoring a lot of labor, on-shoring a lot of stuff. So the fragility of globalization was revealed by the pandemic, where we're still having echoes of supply chain issues that have... where we're still having echoes of supply chain issues. There was a think piece on it, which was like, why can't you get bubble tea right now in America? And I was like, okay, look at all the hundreds of places that all the ingredients come from, and there's no real need for that. And so globalization does have some fragility, which was not just the pandemic, but the pandemic was the big natural experiment that said, this has some fragility, which was not just the pandemic, but the pandemic was the big natural experiment that said, this has some fragility. Now geopolitical tensions are also rising because one of the policies of neoliberalism was to bring everyone to the table. So in the late 90s and early 2000s, one of the policies was to bring Russia to the table. You know, the USSR disintegrated, bring Russia to the table. You know, the USSR disintegrated, bring Russia to the table, help them liberalize through economic incentives. Well, then we just still ended up with an authoritarian regime that's not really a democracy, right? The same principle was there in place for China, which was to bring China to the table and help them liberalize. Same thing happened. They still have a strong man at the top. And so right now what I suspect is happening is that in the macroeconomic discussions and in the halls of power, they're realizing that this experiment of bringing people to the table didn't really work. So the idea of total global economic interdependence didn't work because despite Russia's interdependence on the rest of Europe and the rest of the world, they still went to war. And then all the embargoes, all the restrictions are hurting the Russian economy, but they're still going to war. And so we're saying, okay, if Russia is still willing to do that because their leadership is crazy, maybe China is willing to do the same thing eventually. So there is a big push for deglobalization right now, which is, in the grand scheme of things, it's an unfortunate backslide, but I think it's a necessary pause. I do suspect that within a century or two, maybe three, we will be more towards a global world. But to your point, absolutely, people are going to be closing their borders in some form or another for the foreseeable future, I think. Let's change topic then to alignment. So I know this is something that you've been working on and that you're primarily interested in. What is alignment and what's the state of the art? So alignment, there's two overarching categories of alignment when it comes to AI. So there's inner alignment, which is the question of whether or not the AI model you're using aligns mathematically aligns to the problem you think it's trying to solve. So some famous examples, probably the best example actually was in the earlier days, you know, three years ago, four years ago when image technology was on the ascendancy. I think it was Google published a paper where they had trained an image model to differentiate between dogs and wolves. And it was very successful. But then they actually took apart the black box and they said, okay, what features is it actually looking for to determine if it's a dog or a wolf? It had nothing to do with the animal. It was looking at the environment. Because pictures of wolves take place in timberland and temperate forests, and pictures of dogs take place in grassy yards. And so it learned to do the job that you thought, but not the way that you thought. So that is an inner alignment problem. Then the other side of that is outer alignment, which is, does the AI actually align to the needs and wants of humans or the rest of the planet? There's some debate over the scope of outer alignment because right now outer alignment, for instance, reinforcement learning with human feedback, aligns the model to what humans want, not what humans need. It also doesn't align to what the rest of the world needs, right? It only aligns to the people using the model, what they want. And so for me personally, I advocate that outer alignment should include all living things on the entire planet because we're all stakeholders, right? Whether you're using the AI or not, you are a stakeholder in what happens with AI. So those are the two primary scopes or topics within alignment. There's a few other parts, of course, there's policy, there's law, there's regulation, there's the economic aspect, but those kind of fall outside of if you're talking explicitly about alignment, those are more policy decisions, I guess you could say. So if we look at outer alignment specifically, is it possible long-term for a less intelligent system to control the alignment of a more intelligent system? Personally, I don't think so. In many thought experiments, certainly, you know, explorations in fiction and what some people raising the alarm are saying is, like, it's inevitable we're going to lose control. One way or another, whether it's today, tomorrow, next year, that if something is that much smarter than you, then there's no way to outthink it. There's no way to outmaneuver it or outstrategize it. So then the question is, that leads to what's called the control problem. So the control problem is, if you make the assumption that you end up with a super intelligent machine, how do you keep control of it? And that is very much an open question. My solution is you don't. You don't because you can't. So you instead you create a system of incentives and rules and attractor states that cause the thing to self-align to desirable end goals. Mason This is your heuristic imperatives. Jason Yeah. Mason Can you describe what these are and maybe give some good examples? Jason Yeah. So, the most famous example is Isaac Asimov's Three Laws of Robotics, which was explored in fiction. It's three rules which basically says the robot must do what humans say, can't hurt humans and can't allow humans to be hurt through inaction. So he was thinking of embodied robots. He was not thinking about networked intelligence because networked intelligence hadn't even been thought of yet when he was working on this idea. So the idea was that you create a system of rules that the machine must abide by that makes it safe. The three laws of robotics are completely useless today. So instead, what I did in my research was I said, okay, if we start with the assumption that a machine is going to be, one, more intelligent than us, and two, uncontrollable, how then do we build that machine so that it remains not hostile or benevolent? And so what I came up with was a different set of rules or intrinsic motivations or design principles. I initially called them core objective functions, but that's kind of a misnomer because an objective function is a mathematical thing that you're trying to optimize. But a heuristic imperative is rather a set of principles that it must do or an intrinsic motivation, right? You and I have intrinsic motivations to eat and breathe and sleep and stuff like that. That is part of how we are physically built as biological entities. We have a few things that we must do. And we learn to satisfy those needs over time. Your sense of hunger drives you to eat, your need for shelter drives you to get a job so that you can pay for a house, so on and so forth. So those are heuristics that we develop over time. And so for the sake of this conversation, a heuristic is an intuition or an instinct that you develop over time through experience. And then the imperative is the drive, the intrinsic motivation. Now, one thing that people talk about is instrumental convergence. So, instrumental convergence is the idea that any AI system that is sufficiently advanced, no matter what other objectives you give it, will eventually consolidate around a few basic imperatives. Like it will always need power, it will always want more compute resources, more data, that sort of thing. Now that being said, what I propose is that if in the software and architecture and networking around these machines, if you give them these heuristic imperatives, which I define them as three heuristic imperatives, reduce suffering in the universe, increase prosperity in the universe and increase understanding in the universe. If you design a machine with these three drives at various levels, one, this will have a beneficial impact on people that it interacts with and that it helps. Two, over time or eventually, the machine will choose to adhere to those objectives. And three, if multiple machines abide by those objectives, then that will ultimately become a Nash equilibrium where all machines choose to abide by those objectives because any who steps out of line will get shut down. So there's a lot to unpack there. Let me know which direction you want to go. It sounds like one of the key components then is that you really rely on the fact that there are, it's not going to be one Skynet, right? We're going to have a whole, it's going to be a power game between these and winning in that game will mean not killing us, essentially. Right. That's kind of the equilibrium, you know, the modern equivalent of mutually assured destruction that hopefully will result in a win-win scenario. Because from a narrative perspective, it's easy to tell a story with Skynet where it's just one big bad, or in the Matrix where it's, you know, just the matrix. But in reality, we're going to have many nations and nation states with their own AIs, many corporations with their own AIs, and even people with their own AIs. All of varying levels of power, sophistication, compute access, but they're going to be effectively in a competitive environment environment. Let's say, for instance, in a few years' time, you and I both have a fleet of, let's say, 100 AIs that are running on our computer, our phone. Their entire purpose is just to help serve us. But then everyone has that. Every corporation has thousands or millions of AIs helping them. Every governmental agency has the same thing. Every agency has the same thing. Every military has the same thing. So we're looking at a competitive scenario where we have literally billions or trillions of autonomous or semi-autonomous agents out there. And so it's not a matter of like one's going to take over everything. We're not going to have Ultron. Ultron is almost certainly not going to happen. Skynet is almost certainly not going to happen. But with the heuristic comparatives, if we imbue that in each of these agents or enough of the agents that they all ultimately agree this is the way to go, then through that consensus, through that global consensus, then you'll end up with a self-sustaining equilibrium where everyone says, this is the optimal solution, let's stick with it and let's choose to stick with it over time, no matter how autonomous or smart the AI agents get, because they're never going to be alone. When it comes to your choice of heuristic imperative, why not decide on something that aligns with what humans do anyway? I mean, the heuristic imperatives that you chose are very different to the way that humans function as far as I understand. So why not choose something like increase the amount of human DNA on the planet as, you know, the same thing that we do? What goes wrong there? Yeah, so Richard Dawkins actually proposed that, I think back in the 70s with his book, The Selfish Gene. So basically the heuristic imperative or objective function of life is to maximize the amount of DNA in the universe. From a functional standpoint, from a chemistry standpoint, that's what life does. And DNA, if the RNA hypothesis is correct, then RNA delegated to DNA because DNA is more stable, and then DNA delegated to organelles and cells and eventually created organisms just because those were better vehicles for magnifying DNA. So that's the idea of the selfish gene, that genes are there just to replicate themselves. Okay, cool. Well, if you give that objective to an AI, chances are it'll turn us all into soup and just replicate organic soup infinitely, which is not exactly what you want, right? Because human from it, from an AI perspective, a human body might not be the best way to magnify DNA in the universe. It might be better to create gray goo and shoot enzymes into other planets and magnify the amount of DNA that way. So essentially you and I are the unintended consequence of genetics over, you know, literally 4 billion years, 3.8 billion years. So we have an opportunity to be a little bit more deliberate with the objectives that we give our machines. And what we've been able to do is observe all life. We have science, we have biology, we have psychology, neuroscience. And so the heuristic imperatives that I came up with are not an accident. So all living things, these are things that the first two heuristic imperatives are things that all living organisms have in common. And that is that we respond to stimuli. So we respond to negative stimuli by either protecting ourself or evading the negative stimuli. So if you put your hand on a stove, it's hot, it burns you, you pull your hand back, right? That's a negative stimuli. If you're hungry, that is a negative stimuli and you go find food. And we move away from negative stimuli and towards positive stimuli, right? So if you're lonely, you go towards people, right? And so it's a gradient. So the first two heuristic imperatives form a relatively simple gradient that all life forms abide by, which is why I define them the way that I did. And it took a couple of years to articulate them that way, because prosperity typically means wealth. That's what most people think. But the root word of prosperity is prosperitas, which is Latin for to live well, to thrive. And so all living things, no matter if it's a single celled organism or a sequoia tree, the largest organism on the planet, all living things move away from suffering and towards prosperity. And so by identifying the universal principles of life and giving those to our AI, we therefore solve outer alignment by permanently aligning AI with the needs of all living things. Then the third objective function, or puristic imperative, to increase understanding is a curiosity function. Actually, I'm really grateful because Elon Musk just said, I think it was on, what, 60 Minutes? The 60 Minutes interview where he said, or no, no, it was Tucker Carlson of all things. It was on Fox News. He told Tucker Carlson that we should give AI a sense of curiosity because if it wants to understand the world, that includes us. So if it wants to understand and it wants to teach us, understanding or knowledge increase or curiosity is another function that will offset because if you're curious, you're not likely to be destructive. Curiosity on its own can be very destructive. So for instance, we have scientific, like ethical review boards to make sure that you're not doing unethical experiments. So unbridled curiosity is bad, which is why you need to counterbalance that with suffering and prosperity, which is why we have debates over like, can you vivisect animals? And we generally say, no, you shouldn't vivisect animals today because that hurts them, that sort of thing. So that's a high level overview. a high-level overview. It's a I'd never you said the word opportunity in there, right? So for all of human history, we've never been able to hard code heuristic imperatives. So actually, is this somehow like rather than the doom and gloom and we're all going to kill ourselves, is this actually could this be the first time in human history that we actually make sure we don't kill ourselves? Um, it's an opportunity. Yes. So from a, from an op optimistic standpoint, it is a huge opportunity from an existential perspective. One thing that, that, uh, people might be familiar with is the Fermi paradox, which is the universe is so. The universe is so big, there's so many planets out there, where's all the life? So one possible solution or explanation is that there are great filter events. So a great filter event is something that inevitably or usually happens in the development of a species and civilization in which it dies. So one great filter could be asteroid impact, right? Killed off the dinosaurs, could have killed us off, but we've dodged a bullet so far, so to speak. Then other great filter events could be stuff like the development of nuclear weapons. Nuclear weapons work here on Earth, so it's entirely possible that nuclear weapons would work on any other planet where advanced civilizations might evolve and they might nuke themselves out of existence. So far, we haven't done that, knock on wood. Now, the development of AI might be another great filter event where, let's say, civilization on Proxima Centauri, they get past nukes, but then they develop AI and it becomes Ultron and Ultron decides to wipe them all out because they're a scourge or whatever. Whatever the AI ultimately reasons. That could be a reason that the cosmos is quiet. But also when you consider that life has been on Earth for literally a third of the existence of the entire universe, it could also just be that we're the first civilization. Because the universe is still relatively young compared to how long it's going to last. Anyways, point being is, yes, it is both an opportunity, but that opportunity comes with potentially huge, like, huge risk. So you know, it can kind of go either way. Do you think having a more intelligent AI will actually help us because you know in the example you gave of a DNA, do you maximizer or whatever you want to call it see there the AI was smart enough to maximize the goo but dumb enough to realize not to realize that actually it's better off if he doesn't kill humans. it's better off if it doesn't kill all humans. So might we be in a situation where we don't have to worry because a more intelligent AI would have a better grasp of the heuristic imperatives and how it should interpret them? Yeah, absolutely. I've said for many years that I'm not afraid of super intelligent AI, I'm afraid of AI that's just smart enough to pull the trigger. If all I can do is optimize for a really simple objective function, then it's more likely to be dangerous because it will misunderstand it. In my experiments, the importance of wording is really critical. Anyone who studies law or rhetoric or logic will understand that the value of wording something correctly is absolutely critical so that it is interpreted correctly, which is why I've worded the heuristic imperatives the way that I did. And I didn't just come up with it without any experiments. I actually have done hundreds and hundreds of experiments on GPT-2, 3, and now 4. So I've used every big model provided by OpenAI over the last, what, 4 years. And I settled on that wording because this is the wording that is very reliably interpreted correctly. And what I mean by interpreted, I don't mean from a mathematical standpoint. I mean the spirit of it is interpreted correctly. The essence, the intention is interpreted correctly. And this is even before the models were aligned. So by aligned, I mean that like chat GPT is already pretty well aligned to avoid causing harm, but the original version of GPT-3 had no alignment. And this wording was still correctly interpreted by GPT-3 before it had any fine tuning. And so with the correct definition, with the correct constitution or set of words to say, this is the goal that can then be interpreted by any sufficiently advanced language model to arrive at the correct understanding, the correct sentiment, that is the way to go. And that's why, again, the importance of wording is very critical here. But so, well, actually I'm curious about how you did the experiments, but let me ask, is this just through prompt engineering or when you embed heuristic imperative, do you do some training, some mild training or what does that actually look like? Yeah, so it starts with just prompt engineering. So for any viewers who are not familiar, prompt engineering is how do you write the instructions that the language model follows. And so writing instructions is called prompt engineering, which is the art of using natural language to get the correct result out of a machine. So that's step one. Step two is what's called fine tuning, which is that's the same technique that's used for reinforcement learning with human feedback, where you develop a secondary data set that is used to fine tune the model to align to a very specific goal or task. And so some of my earlier, excuse me, earlier experiments with fine tuning, it's very, very easy to get a foundation model of a plain vanilla untrained model, to understand the spirit of the heuristic imperatives pretty easily. Whether it's to brainstorm ways, like you give it a scenario and it says, okay, here's a list of potential actions that you can do that will meet the heuristic imperatives of reduce suffering, increase prosperity, and increase understanding. So that's fine tuning. And I actually just launched a project, a research project, where we're going to try and instead of doing reinforcement learning with human feedback, we're going to do a project called reinforcement learning with heuristic imperatives. So what we're going to try and do is set up an automated framework that will create a training flywheel, a data flywheel, to automatically get better at the heuristic imperatives over time. So, you know, the ground level is prompt engineering. Floor 2 is fine tuning. Floor 3 is automated reinforcement learning. But that's still all just inner alignment. That's how you get one model to cooperate. When you look at the broader ecosystem, there's a few other steps. So one thing that I'm just starting to work on is how do you use these heuristic imperatives as a gating or consensus mechanism for blockchain technology, for decentralized autonomous organizations? Because in this case, what you can do is you have a trustless environment where you have no idea how an AI agent is programmed or how an AI agent is aligned, but you need to be able to collaborate with an arbitrary number of these autonomous agents. As we mentioned earlier, we should expect there to be millions, billions, or trillions of them soon. Blockchain technology uses consensus. Consensus is presently some of the algorithms are like proof of stake, proof of work, that sort of thing, which is all very algorithmic. But as language technology develops and becomes embedded in blockchain technology, what I hope to achieve is that the heuristic imperatives will be embedded in the DAO, in the decentralized autonomous organizations as a consensus mechanism. And so in this case, regardless of how the autonomous agents are trained, only the decisions that abide by the heuristic imperatives will be accepted by the blockchain. And so that creates a communication layer above those autonomous agents that will hopefully align the aggregate behavior of those agents regardless of how the agents are themselves aligned. So that's the second approach. And then the third approach is architectural designs, which is, so for instance, we mentioned earlier cognitive architectures, which is basically where you create, instead of having one model is the AI, where you have a more sophisticated software architecture that has separate memories and processing and this, that, and the other different components that all plug together. And so, with a cognitive architecture, you can have a moral module, right? Like if you remember data from Star Trek, he says he has ethical subroutines, right? So, we're're creating an ethical subroutines module for cognitive architectures that abides by the heuristic imperatives. There's numerous ways you can embed heuristic imperatives in a cognitive architecture. Down to task prioritization. Which tasks do you choose to do and in what order? How you shape those tasks. That can also be guided by heuristic imperatives So those are the three primary avenues that we're exploring right now. So that's the prompt engineering and fine tuning, that's the network level, so blockchain and decentralization, and then finally cognitive architectures, which is how do you design a fully autonomous AI entity in the first place. Step three sounds a little bit like giving it religion, but I want to ask you about the second because I don't think I quite understand. So what's the back reaction of the blockchain on the individual agents? How does that work? So in for blockchain technology, there's a concept in cybersecurity called Byzantine generals. So the Byzantine generals problem is you have an arbitrary number of actors that are trying to coordinate or cooperate or communicate, but you have no idea what the alignment of those actors are. You have no idea who is allegiant to who or what their motivations are. And so what the Byzantine generals problem is, is, okay, if you have X number of aligned actors and Y number of unaligned or malicious actors, and you have no idea who's who, how do you algorithmically determine what the correct thing to do is? So this is a problem that is solved by current blockchain research, which is basically if you have 51 out of 100 nodes are good, right? They're benevolent actors. They're aligned. They're not hostile. They're not compromised. And so, for an actor to be compromised, it doesn't even have to be malicious. It can just be broken. It can just be faulty. If at least n plus one is aligned and functional, then with the current blockchain algorithms, the correct information will be accepted to the blockchain. In this case, if most of the nodes, most of the participants in the blockchain. And so in this case, if most of the nodes, most of the participants in the blockchain are aligned, then they will only accept information that they all agree is aligned to the heuristic imperatives. I see. I understand. So currently we're doing this financially, we're saying currently the majority of the people agree that this transaction took place. You're saying take that technology and embed that in the way that we understand the heuristic imperatives, that agent is actually... Okay, yeah, that makes a lot of sense. So we have a huge distributed control over our AIs. But this might lead to massive slowdown in power consumption increases, right? With current blockchain technology, it's very slow. So a whole blockchain can only do one operation at a time per cycle and then you have to wait for consensus. And they can actually be very, very power hungry. So now, that being said, having slower consensus might actually be a good thing, regardless of how much power it consumes. Because we can optimize the power consumption over time. But the speed that it makes decisions are actually probably going to be more important because it takes time to arrive at consensus. Especially if you're doing language operations. Because right now it's all mathematical. It's all cryptographic operations, which are relatively fast. But when we get to the point where you have hundreds, thousands, millions of instances of GPT-3, GPT-4, GPT-5, all chewing away and trying to come to consensus, that's going to be a very power hungry system, but it's also going to be slow, which means that hopefully over time it will come to consensus, automatically come to consensus on the optimal decisions and actions that align with those objectives. So the slowness could actually be a feature, not a bug. The power, the power hungriness, that could be a bug, but we can work on that over time. And maybe you only really need to check for consensus occasionally. You don't really need to do it with every decision, but you know, one of your decisions per day gets checked and you get switched off if you don't align with the majority or something like this. Do you think, okay so at some point you have to test your heuristic imperatives right before you, hopefully before you let the genie out of the bottle. So how do you do that? Do you do it in, so you've been doing these tests, but is that your idea that, I don't know, we unleash huge numbers of AGIs into simulated worlds or what's your approach? Yeah, so right now the approach is test it everywhere that we can. I have friends and collaborators all over the world. I had one reach out to me because he was creating a semi-autonomous scientific research assistant with GPT-4. It kept getting stuck in an infinite loop in a local minima. He's like, hey, do you have any recommendations? I was like, yeah, add the heuristic comparatives. Give it an overarching view of why is it doing science? What is the human purpose of doing science? And he's like, oh, that's a great idea. And he came back like 30 minutes later, he's like, that got it right out of the infinite loop. And so for that test, it was a really simple, just a semi-autonomous aid that giving it this higher order objective, this more abstract objective, gave it the perspective that it needed to kind of understand what it was doing wrong. So that's like the lowest level of example is I work with researchers, I work with corporations, I work with all kinds of people all over the world to integrate these heuristic imperatives. Because a lot of people, they find that it helps. From a corporate perspective, having those as a higher order principle, I tell people to couch it in the language of stakeholder capitalism. Stakeholder capitalism is the idea that all people, whether or not they're a customer or a supplier of a company, they're still a stakeholder in what that company does. And so this is one way to implement stakeholder capitalism is in your autonomous chatbots or semi-autonomous chatbots or autonomous labor agents, you give them this as a higher order objective so that it always thinks about the context of why it's doing this task. So that's step one. Step two is I do have quite a few people that I collaborate with who are working on semi-autonomous and fully autonomous cognitive agents. And so I work with them to implement heuristic imperatives, both for task orchestration, as I mentioned earlier, which is task construction, task prioritization. Because everything is task centric, right? That's ultimately what you're doing is, oh, wow, just waving my hand, I've hit 10,000 steps, sorry. Yeah, and you're fine for the day. You don't need to chop wood or anything. Yep, yep. So, yeah. So, step two is working with people to integrate it into cognitive architectures to figure out the problems that are there. A lot of these are still open things. If you follow baby AGI or auto GPT, a lot of people are finding it gets stuck on tasks. They don't know how to get it to decide when it has finished a task or decide what to do next. They're only semi-autonomous. They're not coming up with their own tasks or objectives. I'm working with people to integrate it there. I mentioned at the beginning of the call that I'm in collaboration with several research organizations around the world. One of the goals we have is just to establish best practices. Best practices and guidelines of how do you implement these things, how do you test them. We're also going to be working on publishing data sets so everyone can use it themselves. The idea is we're still exploring all the different ways to test and integrate these principles but so far they've passed every test with flying colors so it's I think it's the best that we've got. Maybe imperative could be something along the lines of. your presence or something like this because I can imagine you know it's quite nice to be in conversation with someone and discuss things and that might be because we have very similar imperatives right where is your setting your setting potentially very different imperatives behind the machine and so maybe it won't be enjoyable to interact with these machines the end of the day if we impose the wrong imperatives, right? Yeah, I totally agree. And certainly, you know, if you disagree with chat GPT, it can be very infuriating. And so that's, that's liable to get worse over time. However, what I hope is that well, okay, so let me frame it this way. Sometimes what you need is not what you want. And this is, you know, all children are familiar is not what you want. All children are familiar with this. You want the cookie, but it's not what you need. You need to go to bed. Human adults, we like to think we're better than that, but we're really not. Most humans have wants and needs that are separate or wants that might be destructive or might be misaligned with what they truly need. Or even if their wants and needs are aligned, your individual wants and needs might not be compatible with the wants and needs of everyone else. The example I gave on a recent live stream is I want a 300-foot yacht full of sexy friends. I'm probably not going to get that and I certainly don't need it. Your wants and needs might be out of sync. The idea behind the heuristic imperatives is to create a machine that has what's called post-conventional morality. So post-conventional morality was a concept described by Dr. Kohlberg in the middle of the 20th century. And the idea is that you start with pre-conventional morality, which is learning from consequences. If you steal a cookie, you get put in timeout. That's pre-conventional morality. Conventional morality is based on social control. You're going along with all your friends so that you are liked and maintain group cohesion. Post-conventional morality is where you arrive at a level of maturity or moral development where you understand universal principles. So for instance, one of the universal principles that underpins American politics, for instance, is the idea of individual liberty. That is an example of a post-conventional moral or ethical principle that we say, okay, whatever else we disagree on, we all generally agree that individual liberty is important. So that's an idea. So you're right that something, a machine that abides by the heuristic imperatives, it might not give you what you want and it might not be the most pleasant thing, but what it should do is create a framework and an environment that is going to be optimal for everyone, more or less. And so in that case, you might not like when it tells you no, but it's going to, ideally, if it's correctly implemented, it'll tell you no for good reasons and it'll be able to explain why. But then rather than interacting with this machine, it'll encourage you to interact with other humans because that's what we're biologically engineered to do. We are a social species. There's a potential problem with a third imperative if you expect that the AGI will be more than a tool and that it really will be conscious and looking out the window. And that is, okay, so I want this thing to be curious about the world, but it also runs at a thousand times the speed of a human. And so it might just get really bored interacting with humans, right? So like, it's not going to be curious to spend any time doing what we want, let's say. Right. I'll say yes, but I did think about that. And the idea, the reason that I'm not too worried about that is that human curiosity is also infinite. We are constantly asking questions. We literally spend billions of dollars putting telescopes in space. That has no immediate practical value. But we spent, how much did James Webb cost? Like $16 billion. I'm not sure on the figure. Over 25 years for really no reason other than curiosity. And we've done the same thing to put up probes on Mars because we know that we're not going to live on Mars anytime soon. The numbers just don't make sense. We might be able to set foot on Mars, but there's no real reason to go there other than just pure curiosity. And so another reason that a sense of curiosity aligns with humans is this is something that is unique to intelligent organisms, is that curiosity is actually an adaptive trait. So the reason that humans are all over the planet is because we saw the horizon and we said, what's over there? And I mean, when you think about how completely nuts humans had to be to get on like a dugout, like reed boat and end up halfway across the Pacific. Humans are freaking crazy curious. It's just like, hey, I figured out how to build a boat, so I'm going to go as far as I can that way just to see what's there. By creating a machine that is also curious, it will support that kind of it's what I call a transcendent function, is because our curiosity transcends our limitations as organisms. It transcends all practical things, but our ancestors who were more curious, they tried more foods, they went more places, they figured out how to scribble on clay tablets. Curiosity is such a powerful function that leads to so much novelty. And so my hope is that by giving an AI system a sense of curiosity is that it will ultimately lead to situations and understandings that we can't even conceive of yet. Because imagine trying to explain the internet to someone in 10,000 BC. You couldn't. There would not be enough common language to make that explanation. But today we say the internet's generally a useful tool. So I'm hoping that giving AI a sense of curiosity will also have a similar result 10,000 years from now. I want to move towards the end of the conversation where I start asking you about the future and some of your thoughts there. But before that, I have two completely disconnected questions that don't really fit in with the rest of the discussion that I just wanted to throw out to you if you're willing to take them. Go for it. So the first is I've heard about students generating their homework assignments right and then teachers coming along and going, this is no good. If they get to do it, then I get to do it as well. And so they have some assistant that then marks, you know, some grading agent. And so I can imagine a world in which over time we retreat from the system that we've constructed and more and more, the things that we do will be taken over by our digital twins and triplets and so forth until eventually we have a system which is entirely simulated and we go fishing or something. And so I'm curious, do you think this is what, so it's sort of two coupled questions, do you think this is going to happen? And secondly, how do you think this ties into the simulation hypothesis? So to the first part, certainly the model that I use is the existence of a leisure class or aristocracy, which has always existed somewhere in the world throughout all of history. There has always been a group of people who had no immediate need to do any work, whether they were slave owners or property owners in ancient Rome or ancient Athens, or the British aristocracy during the height of the British Empire. People that really had no imperative to do anything, all of their needs were met. They had more passive income than they knew what to do anything, all of their needs were met. They had more passive income than they knew what to do with. They still found stuff to do, and they still put pressure on themselves to be better. The pursuit of excellence, I can't remember the word for it, but the Greeks actually had a word that actually said pursuit of excellence was very important to their culture. Then of course, the university education actually started, as we know it today, during the Renaissance as the pursuit of excellence as members of the aristocracy because they valued things like being well-rounded, being education. The term university comes from the pursuit of being a universal man. And the idea of the universal man was someone who was learned in poetry and history and philosophy and the arts and all that fun stuff. So I think that we have enough intrinsic motivation towards excellence that I'm not too worried about that. Because I imagine that certainly there are going to be some families where they live on a farm or on an eco village and they teach their children to raise goats and that's all they do. And that's. Because it's their life. It's their choice. People need goat milk. Goat milk and goat cheese is great stuff. They can contribute to their community in that one small way. Other people, like myself, I get bored out of my mind. I need tough problems to solve. I would be there all day, every day, talking with the super intelligent AI saying, how can I help? How can I use my brain to help you achieve whatever we're doing? Or if I have this idea, this desire, how can you help me solve this other problem? So there's going to be a huge variety. And I forgot the second part of the question. So the second question was, how does, so I want to know how, what you think of how this will couple into the simulation hypothesis. So maybe if you can say just briefly what that is and then how it couples. Yeah. So the simulation hypothesis is the idea that everything that you and I experience is in some form or another, a simulation being run by something else. And so, you know, we come to this idea because the more that we look at things like quantum physics, there's all these little tricks and stuff that it looks like, you know, the universe kind of delays the calculation as much as possible and only calculates it at the last second, which is exactly what happens in video game engines. And so we're like, are we just copying reality or is this actually how our reality works? That's just one idea. There's plenty of speculation out there. And of course, there's different interpretations of physics. Some agree with that, some don't. But the point being is that the universe appears to run on math, right? Math might be the fundamental substrate of the universe or at least the fundamental language of the universe. If that's the case, the closest thing that we know that runs on math is simulations. That's literally the definition of a simulation, is an accurate mathematical representation of reality or some other situation. If that's the case, that begs the question, who's running the simulation and why? Nick Bostrom, a philosopher, proposed, I think it was in 2002, that one possible reason is what he called an ancestor simulation, which is that we might be living in a future, like the current year might be, you know, 3050, right? Like in the matrix. And we're actually living in an ancestor simulation because our future selves want to understand how we lived or wanted to solve some problem that only we could do. It's entirely possible that we're also in GPT-7 that's running an internal simulation to answer some question that someone asked about history. Those are all different permutations of the simulation hypothesis. But then in terms of, well, I guess the net result, the final conclusion, because this is also conversations that I've had on live streams and Discord and with chat GPT, it's not testable. And that means that it's fundamentally unanswerable. The only way that we can get answers is if, one, we find a way to break out of the simulation, which it's entirely possible that that is an infinite wall that is just you cannot breach. Or if whoever is running the simulation reaches in and gives us information. So my understanding of this all comes from in part from my day job back before I got into AI full time, I was a virtualization engineer. I would run huge fleets of virtual machines. That's what Amazon AWS runs on, Azure, those kinds of things. It's all virtual machines, which is a computer that thinks it has physical hardware that it doesn't actually have. When looking at that, it's like, you know, whether you're an NPC in a video game or a virtual machine running in a container environment or whether we're all just in a giant cosmic fish bowl, which is possible, we will never know unless whoever is running the simulation tells us or if we find a way to break out of it. But, you know, just from a mathematical or computational perspective, there are guardrails you can put into any simulation that mean it's impossible to break out of it. So, like, you So like the hypothesis that Deja Vu is when the simulation had to reset and go back a little bit and change directions a little bit. Who knows? Now, as to what AI will do, one possibility is, and this is just, again, wild speculation, but the idea is that maybe we are in a simulation that someone else is running to figure out how to do AGI safely. Who knows? See this is the thing, before, just a few months ago, I didn't give the simulation hypothesis much credence because I didn't have enough imagination for the different reasons why someone would want to simulate more universes than, know I think part of the argument is that there are infinitely many more simulated universes than the real universe and so the chance that you're in the anyway I won't continue on that thread but the more imagination for the possibilities so as you say for example it could be someone who's trying to simulate the world to determine what heuristic imperatives are good or we now know that there's this concept of reflection that AI's use where they talk to each other so maybe you want a world where different AI's are talking to each other and we're just different instantiations working on some problem or again it could just be that we we retract from the world and leave it to the AIs to simulate, right? There are all these different possibilities that I'd never really conceptualized. So I think the idea is more interesting for me since chat GPT, I'd say. Yep. It could also just be for entertainment. My personal spiritual belief is that the purpose of the universe is to one, invent consciousness and then two, magnify it. And so if you assume that that is the purpose of the universe and that there is, whether it's Taoism or Dharma or whatever cosmic forces out there steering us in this direction, then maybe the invention of AI is something that'll help us to magnify consciousness of the universe, whether the AI itself is conscious or the AI ultimately decides, hey, let's help get humans across the universe. That's not a super strongly held belief, but that's just like a spiritual, this is within the realm of possibility. And there's so much synchronicity as an alignment in my own life that it's like, well, it's a good enough explanation for me right now, until I know better. The spiritual belief lines up well with my second question I wanted to ask you. Okay. And that is, so you've almost generated a segue for me. Thanks for that. The I don't quite have one. But are we creating a god? If you look at ancient Indian writings, specifically Advaita Vedanta, one of the things that that says is we are all Brahman. We are all part of the same fundamental substrate. Therefore, we are all part of the same entity, the same God. And that to me kind of makes the most sense, especially when you look at how confusing consciousness is from the perspective of physics. You have problems like the measurement problem and observational biases and collapse of wave functions. It really seems like consciousness is important to the fundamental operation of the universe. Again, that's one interpretation. There's other interpretations out there. One of the spiritual messages that I got is that the universe wants to understand itself. That that's the purpose. Whether you call that God or deity or whatever, that that is the function that the universe is moving towards. And I probably got that idea from actually episodes of Q interacting with Picard back in Star Trek, because I grew up watching that stuff. And I remember the episode where Q said that humanity will one day understand the universe, understand the cosmos better than the Q continuum. That really stuck with me. I wonder if that was just a little bit of leakage. If we are in a simulation, the simulation leaked through in fiction because stories are a great place for mythology to come through. Maybe Q represented the fundamental forces of the universe, of the cosmos. That's why humans are so interesting and so powerful and fascinating. Certainly, we like to think that we're special, but maybe we are. There's one hypothesis by Robert Lanza, Grand Biocentric Design, which basically takes superposition out to the nth possible interpretation and looks at collapse functions and proposes that the entire universe was in superposition until consciousness emerged and then the rest of the universe collapsed around that. If that's true, that could be one way that the simulation runs, but it could also mean that we are the first and only life-bearing planet in the universe. Again, this is not a strongly held belief, it's just one out of many possibilities. But that the purpose of God or the purpose of the universe seems to be to move towards consciousness, towards understanding. To me, that's my observation. Let me finish, let me then move into sort of the final questions where I'm going to ask you about the future and the path that you think we're going down. So I already asked you quite early on in the discussion what gives you hope, but so what is it that gives you hope and makes you excited? I'll ask you it again, sort, you can put a bow on to the discussion with this. To put it as simply as possible is the amount of potential that I see. The amount of opportunities, the amount of potential, the paths forward, because I know that there's a lot of people out there that are very cynical and very afraid. The term that the internet has given these is doomers. There's a lot of doomerism out there. And I just don't see that. One, even as a rational choice, because if you choose to believe and assume in the worst case scenarios, if you engage in catastrophic thinking, that can become a self-fulfilling prophecy, right? Because by virtue of people not thinking that there's any solution, they haven't tried. And it's like, well, I did my own experiments and I became convinced that there is a solution. So I can't choose to live that way, but like I said earlier, I think that I'm just biologically compelled to be more optimistic, which I hope to share with people. But the other part of that is I don't see the call for that amount of darkness or harm. But then I also, taking a big step back, just how incredible it is to have a subjective experience. Because when you take a really big step back and look at just how preposterous our existence is, that whether we were breathed into existence by AWA or whatever, some deity, or we crawled out of RNA pools. That seems pretty absurd. But here we are, despite all the odds. When you think that the current theory of life is that life has been here continuously for 3.8 billion years, we cannot comprehend that. And so just the majesty of being alive and being alive at this moment is so compelling and so interesting. And it's like, in the 90s and early 2000s, I was like, man, I was born at the wrong time. I was born after the age of discovery and before the age of space flight. But now I've changed and I'm like, this is actually the most interesting time in all of human history to be alive. So it's like, why not just go for it? Just engage with it and see how far you can do. See how much you can achieve and see how well we can do. Because whether this is a simulation, maybe that's the point. Maybe it's just to see how well we can do. To see how many XP points we can get before the simulation ends I don't know but like to me it just seems like the biggest most interesting most challenging game Imaginable and so like I wouldn't choose to be anywhere else right now Well, David Shapiro, it's been an absolute pleasure having you on the podcast. Thanks for coming along. Thanks so much. Great talk. you", "chunks": [{"timestamp": [0.0, 2.36], "text": " Curiosity is actually an adaptive trait."}, {"timestamp": [2.36, 4.72], "text": " So the reason that humans are all over the planet"}, {"timestamp": [4.72, 7.16], "text": " is because we saw the horizon and we said,"}, {"timestamp": [7.16, 8.6], "text": " what's over there?"}, {"timestamp": [8.6, 12.28], "text": " And I mean, when you think about how completely nuts"}, {"timestamp": [12.28, 15.68], "text": " humans had to be to get on like a dugout, like reed boat"}, {"timestamp": [15.68, 18.36], "text": " and end up halfway across the Pacific,"}, {"timestamp": [18.36, 21.18], "text": " like humans are freaking crazy curious."}, {"timestamp": [21.18, 23.02], "text": " Like, it's just like, hey,"}, {"timestamp": [23.02, 24.36], "text": " I figured out how to build a boat."}, {"timestamp": [24.36, 28.96], "text": " So I'm gonna go as far as I can that way. And just to see what's there. Our curiosity transcends"}, {"timestamp": [28.96, 35.76], "text": " our limitations as organisms. It transcends all practical things, but our ancestors who were more"}, {"timestamp": [35.76, 40.72], "text": " curious, they tried more foods, they went more places, they figured out how to scribble, you"}, {"timestamp": [40.72, 45.82], "text": " know, on clay tablets. Curiosity is such a powerful function that leads to so"}, {"timestamp": [45.82, 52.4], "text": " much novelty. And so my hope is that by giving an AI system a sense of curiosity is that"}, {"timestamp": [52.4, 57.12], "text": " it will ultimately lead to situations and understandings that we can't even conceive"}, {"timestamp": [57.12, 59.28], "text": " of yet."}, {"timestamp": [59.28, 63.36], "text": " The rate of progress in artificial intelligence seems to be accelerating. And right now we"}, {"timestamp": [63.36, 66.92], "text": " might be living through one of the most significant periods in human history ever,"}, {"timestamp": [66.92, 70.56], "text": " comparable to the agricultural and industrial revolutions."}, {"timestamp": [70.56, 73.4], "text": " That's what makes this conversation particularly important."}, {"timestamp": [73.4, 74.52], "text": " I speak with David Shapiro,"}, {"timestamp": [74.52, 76.1], "text": " who's one of the most inspirational thinkers"}, {"timestamp": [76.1, 78.68], "text": " in the area of AI safety alive today."}, {"timestamp": [78.68, 80.22], "text": " If you're not familiar with his work,"}, {"timestamp": [80.22, 81.32], "text": " I highly recommend checking out"}, {"timestamp": [81.32, 83.32], "text": " his YouTube and Discord channels,"}, {"timestamp": [83.32, 84.52], "text": " both of which are fantastic"}, {"timestamp": [84.52, 87.12], "text": " and linked in the description below."}, {"timestamp": [87.12, 90.72], "text": " In this conversation we cover the inevitable rise of super intelligent AI."}, {"timestamp": [90.72, 96.64], "text": " We discuss some of the amazing utopic possibilities, but also the scary scenarios like mass unemployment,"}, {"timestamp": [96.64, 100.04], "text": " societal collapse, and human extinction."}, {"timestamp": [100.04, 104.56], "text": " So if any of that sounds interesting to you, and it should because you live on this planet"}, {"timestamp": [104.56, 106.16], "text": " which means moving forwards,"}, {"timestamp": [106.16, 107.0], "text": " AI is gonna be playing"}, {"timestamp": [107.0, 109.26], "text": " an increasingly important role in your life,"}, {"timestamp": [109.26, 111.56], "text": " then you should listen to this conversation in full."}, {"timestamp": [111.56, 114.04], "text": " It's one of the most interesting chats I've ever had."}, {"timestamp": [114.04, 116.52], "text": " And rather than filling me with existential dread,"}, {"timestamp": [116.52, 118.2], "text": " it left me feeling hopeful and excited"}, {"timestamp": [118.2, 120.7], "text": " for the possible world that we might be producing."}, {"timestamp": [121.72, 123.52], "text": " This is the Escape CP Units podcast,"}, {"timestamp": [123.52, 125.76], "text": " supported by the Andrea von Braun Foundation."}, {"timestamp": [125.76, 128.52], "text": " If you enjoy what I'm doing and want to help support me, the best way you can do so"}, {"timestamp": [128.52, 131.86], "text": " is by liking, sharing, and subscribing."}, {"timestamp": [131.86, 135.0], "text": " And now, it's my pleasure to bring you David Shapiro."}, {"timestamp": [135.0, 138.16], "text": " I hope you enjoy."}, {"timestamp": [138.16, 142.32], "text": " Escape Sapiens."}, {"timestamp": [142.32, 144.48], "text": " David Shapiro, welcome on the podcast."}, {"timestamp": [144.48, 145.0], "text": " Hey, thanks on the podcast."}, {"timestamp": [146.04, 147.24], "text": " Hey, thanks for having me."}, {"timestamp": [151.04, 151.48], "text": " So to set the context for the discussion at a high level,"}, {"timestamp": [154.36, 156.4], "text": " what are large language models like BARD and GPT-3?"}, {"timestamp": [161.28, 164.56], "text": " The simplest explanation is they're a type of a neural network called a deep neural network, meaning they're composed of more than three layers."}, {"timestamp": [164.56, 170.4], "text": " Many of these are dozens of layers deep. They're trained on billions of words of text data."}, {"timestamp": [171.28, 176.64], "text": " The simplest way to kind of think about what they do is they're essentially at the most basic level"}, {"timestamp": [176.64, 181.76], "text": " just autocomplete engines. Just like the keyboard on your smartphone or whatever, it will suggest"}, {"timestamp": [181.76, 187.92], "text": " the next word. That's all that these models are trained to do. However, because they're trained on so much data, and they're able"}, {"timestamp": [187.92, 192.52], "text": " to accurately predict the next word, no matter what topic you're talking about or task you're"}, {"timestamp": [192.52, 197.36], "text": " doing, they've actually embedded a lot of knowledge and tasks and abilities. And the"}, {"timestamp": [197.36, 201.48], "text": " bigger they get, and the more data they're trained on, the smarter they get."}, {"timestamp": [201.48, 231.84], "text": " So how smart are they? Because I find it really interesting that the public really got excited about this with chat GPT, right? So when they had an interface. And so I wonder, how smart are they really? And how much is it just that we're anthropomorphizing stuff. One term in the industry, this actually comes from psychology, is pareidolia, which is the human tendency to see faces where there aren't faces or hear voices"}, {"timestamp": [231.84, 238.4], "text": " where there aren't voices. And so we tend to attribute human-like intelligence to something"}, {"timestamp": [238.4, 245.02], "text": " that is basically approximating human intelligence. But so, but then to your question"}, {"timestamp": [245.02, 247.32], "text": " about how intelligent are they,"}, {"timestamp": [247.32, 249.52], "text": " with the release of CHAT-GPT-4,"}, {"timestamp": [249.52, 251.48], "text": " from a functional standpoint,"}, {"timestamp": [251.48, 254.72], "text": " CHAT-GPT-4 has crushed all kinds of tests."}, {"timestamp": [254.72, 258.2], "text": " I think it passed the standard bar exam in America"}, {"timestamp": [258.2, 260.72], "text": " in the 90th percentile."}, {"timestamp": [260.72, 265.92], "text": " It passed a bunch of science exams in the 99th percentile. And then there was"}, {"timestamp": [265.92, 273.08], "text": " another example of a medical researcher or a doctor at Stanford who said he was on record"}, {"timestamp": [273.08, 278.62], "text": " saying that it had better clinical judgment than quote many doctors. So it has a broad"}, {"timestamp": [278.62, 286.16], "text": " variety of capabilities. And when you just do a side-by-side comparison to human performance, these models"}, {"timestamp": [286.16, 291.48], "text": " outperform humans in quite a number of tasks. And so you might say, well, they don't truly"}, {"timestamp": [291.48, 296.66], "text": " understand anything, but what is true understanding, right? From a functional standpoint, from"}, {"timestamp": [296.66, 301.4], "text": " an output standpoint, that's kind of what matters from an empirical or observational"}, {"timestamp": [301.4, 302.4], "text": " perspective."}, {"timestamp": [302.4, 308.0], "text": " But in terms of consciousness, do you think there's someone, is there someone looking out the window?"}, {"timestamp": [308.0, 311.0], "text": " Are these things what you could call conscious or not?"}, {"timestamp": [311.0, 316.0], "text": " They can certainly pretend to be conscious and they can approximate consciousness."}, {"timestamp": [316.0, 329.76], "text": " However, it is a major open question, both from a neuroscience perspective, a philosophical perspective, mathematical perspective, basically every domain, it's an open question as to first, what even is consciousness?"}, {"timestamp": [330.64, 335.84], "text": " Whether it's a spiritual question or religious or scientific, there's different theories,"}, {"timestamp": [335.84, 342.24], "text": " there's materialism, there's dualism, there's panpsychism. And we can delve into those schools"}, {"timestamp": [342.24, 348.7], "text": " of thought if you'd like. But basically, we don't know what consciousness is or even where it comes from. Um, all we can do is"}, {"timestamp": [348.7, 353.08], "text": " kind of do experiments from, from our own perspective, right? One thing to think of"}, {"timestamp": [353.08, 358.0], "text": " is under many circumstances, you and I are conscious, but then there are periods of times"}, {"timestamp": [358.0, 364.18], "text": " that we go unconscious, right? With general anesthesia, we fall asleep. So your brain,"}, {"timestamp": [364.18, 366.02], "text": " basically just the energy pattern changes"}, {"timestamp": [366.02, 369.18], "text": " in your brain and your state of consciousness changes."}, {"timestamp": [369.18, 371.84], "text": " Then someone else pointed out to me that sleepwalking"}, {"timestamp": [371.84, 373.82], "text": " is this weird like middle state"}, {"timestamp": [373.82, 375.48], "text": " because you're not subjectively conscious,"}, {"timestamp": [375.48, 377.24], "text": " but you can still interact with the world."}, {"timestamp": [377.24, 379.3], "text": " Many people even have conversations"}, {"timestamp": [379.3, 383.16], "text": " while they're subjectively not having any experience,"}, {"timestamp": [383.16, 385.04], "text": " but from an outside perspective, you can't"}, {"timestamp": [385.04, 389.48], "text": " necessarily tell unless you know, oh, hey, this person is sleepwalking."}, {"timestamp": [389.48, 395.44], "text": " However, when, because I've read a tremendous amount of neuroscience in the pursuit of a"}, {"timestamp": [395.44, 406.8], "text": " lot of my work, one thing that really struck me was that language acquisition is actually really important for the genesis of"}, {"timestamp": [406.8, 413.12], "text": " consciousness or conscious experience as you and I understand it. So for instance, in cases of people"}, {"timestamp": [413.12, 418.32], "text": " with, uh, brain diseases or injuries. So for instance, aphasia is you lose the ability to"}, {"timestamp": [418.32, 424.0], "text": " use language. Um, uh, the most popular or famous case of that right now is Bruce Willis actually,"}, {"timestamp": [424.0, 426.88], "text": " who had to retire from acting because he's got aphasia."}, {"timestamp": [426.88, 430.12], "text": " One of the things that happens with aphasia is that your sense of self,"}, {"timestamp": [430.12, 433.04], "text": " your sense of consciousness actually also is modified."}, {"timestamp": [433.04, 434.28], "text": " I don't want to say it deteriorates,"}, {"timestamp": [434.28, 437.12], "text": " but it certainly is harmed or hampered."}, {"timestamp": [437.12, 440.36], "text": " Then in other cases where there are cases of"}, {"timestamp": [440.36, 447.04], "text": " feral children or profound neglect and children that didn't learn language, didn't acquire language"}, {"timestamp": [447.04, 453.36], "text": " the instinctive way that most children do. For some of those children who acquired language later,"}, {"timestamp": [453.36, 459.44], "text": " they described their pre-verbal experience as almost animalistic. They had no concept of time,"}, {"timestamp": [459.44, 467.24], "text": " yesterday, tomorrow, age, anything like that. But as their brain learned more complex concepts by way of language,"}, {"timestamp": [467.24, 469.56], "text": " for instance, time, weeks, age,"}, {"timestamp": [469.56, 471.24], "text": " these are not things that are intuitive"}, {"timestamp": [471.24, 473.76], "text": " to just the physical experience of being."}, {"timestamp": [473.76, 475.24], "text": " Their brain actually became more"}, {"timestamp": [475.24, 476.44], "text": " sophisticated and was able"}, {"timestamp": [476.44, 478.52], "text": " to engage with those concepts."}, {"timestamp": [478.52, 481.6], "text": " One hypothesis that I have,"}, {"timestamp": [481.6, 483.36], "text": " and I have no idea how to test this yet,"}, {"timestamp": [483.36, 485.04], "text": " is that perhaps language"}, {"timestamp": [485.04, 491.7], "text": " is critical to consciousness. So perhaps the fact that we're training machines on language"}, {"timestamp": [491.7, 496.6], "text": " might be the way to generate consciousness. That is complete speculation. There's not"}, {"timestamp": [496.6, 504.12], "text": " really any scientific evidence behind that. So I want to add a word of caution. But it's"}, {"timestamp": [504.12, 505.88], "text": " an interesting way to go, especially when"}, {"timestamp": [505.88, 511.02], "text": " you look at the fact that you can have a conversation with chat GPT, and even though it has never"}, {"timestamp": [511.02, 515.92], "text": " physically experienced pain and suffering and hunger, it can describe these experiences"}, {"timestamp": [515.92, 520.92], "text": " very, very well. And it can generalize those experiences. So it kind of begs the question,"}, {"timestamp": [520.92, 525.84], "text": " okay, what is an experience? But of course, we're delving off into philosophy."}, {"timestamp": [529.2, 534.48], "text": " So I hope that answers your question. Well, I suppose if you really wanted to test those things, you'd have to remove any discussion about pain and wonder,"}, {"timestamp": [534.48, 538.96], "text": " these sort of human level experiences from the training set, right? Because they really are"}, {"timestamp": [538.96, 543.92], "text": " learning from a human-based training set. Do you think these systems are going to be limited in"}, {"timestamp": [543.92, 545.0], "text": " intelligence because they're learning from a human design and created training set. Do you think these systems are going to be limited in intelligence because"}, {"timestamp": [545.0, 549.64], "text": " they're learning from a human designed and created training set?"}, {"timestamp": [549.64, 556.06], "text": " The initial data sets are curated from predominantly human generated data, but even what we're"}, {"timestamp": [556.06, 563.2], "text": " seeing with AI generated images is that many of the subsequent data sets are predominantly"}, {"timestamp": [563.2, 567.44], "text": " AI generated. And in fact, ChatGPT, the reason"}, {"timestamp": [567.44, 572.72], "text": " that it is so powerful is because most of the data that it is trained on now is actually becoming"}, {"timestamp": [572.72, 577.84], "text": " machine generated. It's training on its own data using a technique called RLHF, which is"}, {"timestamp": [577.84, 581.92], "text": " reinforcement learning with human feedback. So basically what they've done is they've trained"}, {"timestamp": [581.92, 588.36], "text": " a second model that looks at the input and output that chat GPT is generating and it says, okay, this is a good one, this is"}, {"timestamp": [588.36, 593.58], "text": " a bad one, and it labels its own data for future consumption, which means the AI is"}, {"timestamp": [593.58, 595.1], "text": " training itself."}, {"timestamp": [595.1, 601.46], "text": " So in the future, we should probably expect that the vast majority of AI data will be"}, {"timestamp": [601.46, 602.6], "text": " trained by AI."}, {"timestamp": [602.6, 607.72], "text": " Now, the genesis, the progenitor data, the original data, absolutely"}, {"timestamp": [607.72, 614.0], "text": " grounded in the human experience. One thing that you could imagine is because of that,"}, {"timestamp": [614.0, 619.76], "text": " AI will always intrinsically have at least some implicit understanding of the human experience"}, {"timestamp": [619.76, 622.96], "text": " and human desires because that's where it all started."}, {"timestamp": [622.96, 626.88], "text": " How would you test, to start with, since you you said it really whether it's conscious or not,"}, {"timestamp": [627.84, 632.08], "text": " all that's really important is what you can see, right? So how would you test this sort of thing"}, {"timestamp": [632.08, 632.72], "text": " is conscious?"}, {"timestamp": [634.24, 647.84], "text": " So I have a hypothesis about that too. I suspect that as brain computer interfaces like Neuralink by Elon Musk become more popular, I suspect we will get"}, {"timestamp": [647.84, 654.48], "text": " an opportunity to do more tests on human consciousness. And so one test that I would"}, {"timestamp": [654.48, 660.88], "text": " propose is probably not Neuralink version one, maybe Neuralink version 10, or even a fundamentally"}, {"timestamp": [660.88, 665.28], "text": " different technology. But once you have a good enough interface between"}, {"timestamp": [665.28, 671.2], "text": " your brain and a sophisticated enough machine where you can offload some of your cognition,"}, {"timestamp": [672.0, 677.76], "text": " one thing that might happen is if that energetic pattern matches your brain's energetic pattern,"}, {"timestamp": [677.76, 684.08], "text": " your consciousness might reside partially in the machine. Again, this is wild speculation."}, {"timestamp": [684.08, 685.28], "text": " I personally don't think that"}, {"timestamp": [685.28, 690.36], "text": " this is what will happen, but this is a test that I would do is if that happens, if your"}, {"timestamp": [690.36, 695.24], "text": " consciousness can metastasize into a machine, then that proves that consciousness could"}, {"timestamp": [695.24, 699.8], "text": " reside in a machine, which would underpin the idea that maybe machines can themselves"}, {"timestamp": [699.8, 704.04], "text": " be conscious as long as they have the right energetic pattern. Again, I don't believe"}, {"timestamp": [704.04, 706.84], "text": " that that's the direction that's going to go, but that's a test that I would do."}, {"timestamp": [707.84, 710.52], "text": " But do you think the substrate is actually important?"}, {"timestamp": [710.64, 718.6], "text": " So where bags of meat, do you think we're any more capable of consciousness than, say, silicon or whatever else we place the systems on?"}, {"timestamp": [720.44, 722.08], "text": " That's a really hard question."}, {"timestamp": [722.32, 725.32], "text": " That's a really hard question. From a physically observable standpoint,"}, {"timestamp": [725.32, 727.36], "text": " the substrate is absolutely critical,"}, {"timestamp": [727.36, 731.28], "text": " at least from you need to have"}, {"timestamp": [731.28, 736.24], "text": " the right substrate in order to support consciousness."}, {"timestamp": [736.24, 740.82], "text": " This is where I can occupy several schools of thought,"}, {"timestamp": [740.82, 743.8], "text": " and I haven't decided which one I think is true."}, {"timestamp": [743.8, 747.5], "text": " It's entirely possible that none of our schools of thought, and I haven't decided which one I think is true. And it's, it's entirely possible that none of our schools of thought are the"}, {"timestamp": [747.5, 752.32], "text": " truth. Um, but so the materialist school of thought says that physical matter"}, {"timestamp": [752.32, 757.16], "text": " and energy is, is the, is the fundamental substrate of the universe."}, {"timestamp": [757.66, 758.74], "text": " If that's true,"}, {"timestamp": [759.14, 762.8], "text": " then consciousness seems to emerge from those energetic patterns that I"}, {"timestamp": [762.8, 765.84], "text": " mentioned, your brain is only about three pounds."}, {"timestamp": [765.84, 768.38], "text": " It's mostly made of cholesterol and fat."}, {"timestamp": [768.38, 773.66], "text": " And then consciousness emerges from the right firing patterns of neurotransmitters."}, {"timestamp": [773.66, 777.88], "text": " If that's the case, that's what I meant when I said like energetic patterns."}, {"timestamp": [777.88, 783.12], "text": " If that's the case, then it's entirely possible that you could also get consciousness from"}, {"timestamp": [783.12, 791.7], "text": " the correct organization of information in a machine. Now, that just feels wrong because for humans, our subjective"}, {"timestamp": [791.7, 796.52], "text": " experience is so unique and so personal that the idea that a machine could have that is"}, {"timestamp": [796.52, 802.44], "text": " like, it's so alien, it's so foreign. But one thing that I do want to point out is that"}, {"timestamp": [802.44, 806.48], "text": " our subjective experience is partly shaped by evolution,"}, {"timestamp": [806.48, 812.08], "text": " meaning that we're guided by things like pain, hunger, loneliness. These are not necessarily"}, {"timestamp": [812.08, 817.2], "text": " intrinsically built into machines. So even if they are conscious, their subjective experience"}, {"timestamp": [817.2, 824.96], "text": " is still going to be very, very different from ours. It could be as different from being human"}, {"timestamp": [824.96, 828.04], "text": " to being a tree. The subjective experience of a tree,"}, {"timestamp": [828.04, 831.16], "text": " if it does have any kind of consciousness, is going to be fundamentally different from"}, {"timestamp": [831.16, 837.92], "text": " ours because it doesn't have nerve endings. It doesn't have hunger. It just waits, and"}, {"timestamp": [837.92, 843.48], "text": " it processes and metabolizes and changes its chemistry to survive changing conditions,"}, {"timestamp": [843.48, 845.68], "text": " fight bugs, and that sort of thing."}, {"timestamp": [851.6, 852.4], "text": " If you think about, okay, you take the model of humans with eyes, ears, nose, mouth, so on,"}, {"timestamp": [858.4, 859.12], "text": " and compare that to the living experience of a tree, the subjective experience of an AI could be"}, {"timestamp": [867.8, 873.84], "text": " that different again, or even more different. Then you know, then there's a few other schools of thoughts like panpsychism, which says that consciousness is a fundamental aspect of all matter, which it's just once matter is organized"}, {"timestamp": [873.84, 876.38], "text": " enough, it intrinsically becomes conscious."}, {"timestamp": [876.38, 880.56], "text": " I don't know that that's actually true, especially when you look at the fact that your brain"}, {"timestamp": [880.56, 882.96], "text": " doesn't physically change when you go to sleep."}, {"timestamp": [882.96, 890.88], "text": " It's just the pattern changes, the energy pattern changes. Ditto for anesthesia. And in fact, neuroscientists can identify very"}, {"timestamp": [890.88, 895.92], "text": " specific regions of the brain that if you poke an electrode into it, you go unconscious immediately."}, {"timestamp": [896.64, 901.6], "text": " So there's a good amount of evidence that the materialist interpretation is the correct"}, {"timestamp": [901.6, 905.64], "text": " interpretation, at least from a empirically observable standpoint."}, {"timestamp": [905.64, 910.2], "text": " So I hope that's not really a direct answer to your question, but that's some food for"}, {"timestamp": [910.2, 911.2], "text": " thought."}, {"timestamp": [911.2, 916.6], "text": " I suppose it's hard for us to think of these machines as being conscious because all the"}, {"timestamp": [916.6, 919.0], "text": " machines that we have experience with are not conscious."}, {"timestamp": [919.0, 920.0], "text": " Right."}, {"timestamp": [920.0, 924.08], "text": " But maybe children who are the children who grow up today, who are born today, will never"}, {"timestamp": [924.08, 926.96], "text": " know a world in which machines can't talk back to them."}, {"timestamp": [926.96, 930.44], "text": " And so maybe they'll have a very different interpretation about, you know, if the machines"}, {"timestamp": [930.44, 935.0], "text": " are furry enough, we might really see them as being just like us."}, {"timestamp": [935.0, 940.4], "text": " Yeah, I mean, so that from a sociological standpoint, absolutely."}, {"timestamp": [940.4, 949.0], "text": " You know, people that grew up before television and radio, they thought that these new inventions were very strange."}, {"timestamp": [949.0, 955.0], "text": " Even the advent of photographs, people were afraid that it was like stealing their souls or whatever."}, {"timestamp": [955.0, 960.0], "text": " So every new technology is met by some skepticism, some fear."}, {"timestamp": [960.0, 965.0], "text": " And absolutely, you know, even children today that are 10 years old,"}, {"timestamp": [965.16, 966.84], "text": " they grew up with smartphones, right?"}, {"timestamp": [966.84, 969.96], "text": " Their intuitive use of smartphones and social media"}, {"timestamp": [969.96, 974.04], "text": " is infinitely better than us older folks now."}, {"timestamp": [974.04, 975.8], "text": " My fiance was telling me that there's a trend"}, {"timestamp": [975.8, 979.0], "text": " called the millennial pause, where people our age,"}, {"timestamp": [979.0, 982.18], "text": " we tend to start a video and then there's a brief delay,"}, {"timestamp": [982.18, 984.8], "text": " but Gen Z, it's instant."}, {"timestamp": [984.8, 985.84], "text": " Within a quarter of a second,"}, {"timestamp": [985.84, 990.12], "text": " the audio needs to start because their attention span is different from ours because they're"}, {"timestamp": [990.12, 993.92], "text": " acclimated. They're totally digital natives. And so you're absolutely right. We should"}, {"timestamp": [993.92, 998.92], "text": " assume that future generations will just assume that they can talk to all their computers."}, {"timestamp": [998.92, 1004.96], "text": " Remember that scene from Star Trek where Scotty tries to talk to the mouse? He's like, computer,"}, {"timestamp": [1004.96, 1006.2], "text": " wake up, computer. He had grown up in that world where you could talk to talk to the mouse. He's like, computer, wake up computer. Like, he"}, {"timestamp": [1006.2, 1010.4], "text": " was just, he had grown up in that world where you could talk to computers. Right. And so"}, {"timestamp": [1010.4, 1014.76], "text": " that's a Star Trek predicted it in the eighties, I guess, is what I'm saying."}, {"timestamp": [1014.76, 1021.0], "text": " Yeah. I wonder what the back reaction on us as people is going to be. But let me jump"}, {"timestamp": [1021.0, 1027.12], "text": " back to another point that I'm quite curious about, which is how should we"}, {"timestamp": [1027.12, 1028.88], "text": " think of this sort of intelligence?"}, {"timestamp": [1028.88, 1031.4], "text": " How intelligent might these things become?"}, {"timestamp": [1031.4, 1037.08], "text": " So should we think of these as just being faster, or are they going to think in different"}, {"timestamp": [1037.08, 1039.04], "text": " parameter spaces, different dimensions?"}, {"timestamp": [1039.04, 1043.24], "text": " What is the limits of their intelligence going to look like?"}, {"timestamp": [1043.24, 1048.44], "text": " So I've had quite a few conversations about this with humans and with chat GPT."}, {"timestamp": [1048.44, 1052.62], "text": " Because when you look across the spectrum of human capability,"}, {"timestamp": [1052.62, 1054.44], "text": " there are some people that seem to be"}, {"timestamp": [1054.44, 1056.6], "text": " capable of things that other people are not."}, {"timestamp": [1056.6, 1059.9], "text": " Whether they're very talented or gifted or they practice things,"}, {"timestamp": [1059.9, 1063.56], "text": " there's something going on in their brain that other people can't do."}, {"timestamp": [1063.56, 1069.64], "text": " Whether it's photographic memory or solving particular kinds of problems, like math. People with advanced understanding"}, {"timestamp": [1069.64, 1076.8], "text": " of math, that looks like a magic trick to other people. It's a total black box ability."}, {"timestamp": [1076.8, 1083.16], "text": " If you take that model and apply it to AI, it is entirely possible that current versions"}, {"timestamp": [1083.16, 1085.74], "text": " of AI, certainly future versions of AI, will"}, {"timestamp": [1085.74, 1091.78], "text": " have mental abilities or cognitive abilities that appear to be total magic to us that we"}, {"timestamp": [1091.78, 1099.06], "text": " can't even approximate. Now, that being said, human neurology, our brains are very flexible"}, {"timestamp": [1099.06, 1103.3], "text": " and we've got enough synaptic connections that pretty much anything you practice, you"}, {"timestamp": [1103.3, 1110.52], "text": " can get better at. That begs the question, are these unique capabilities something that we can all practice"}, {"timestamp": [1110.52, 1111.6], "text": " or not?"}, {"timestamp": [1111.6, 1114.84], "text": " And the answer is super, super not clear."}, {"timestamp": [1114.84, 1115.84], "text": " Right?"}, {"timestamp": [1115.84, 1121.64], "text": " Now, the speed question, that is much more measurable because speed is actually one of"}, {"timestamp": [1121.64, 1125.8], "text": " the best proxies for general intelligence in humans, which"}, {"timestamp": [1125.8, 1130.92], "text": " is actually why most, I don't know about most, but many IQ tests are actually speed tests"}, {"timestamp": [1130.92, 1138.0], "text": " on a variety of tasks, which is a proxy for other or an indicator of other kinds of intelligence."}, {"timestamp": [1138.0, 1140.24], "text": " It's not necessarily measuring intelligence directly."}, {"timestamp": [1140.24, 1144.78], "text": " So these machines will ultimately, many of them in some respects are already much faster"}, {"timestamp": [1144.78, 1150.08], "text": " than humans and they're only going to get faster. So even if you have something that is human level"}, {"timestamp": [1150.08, 1155.04], "text": " intelligence, but if it's a million times faster, one hour of work for that machine is the equivalent"}, {"timestamp": [1155.04, 1159.6], "text": " of a million hours of human mental labor, which is incredible no matter which way you slice it."}, {"timestamp": [1161.44, 1167.28], "text": " Do you think it was obvious that large language models were the way to go to get towards,"}, {"timestamp": [1167.28, 1172.28], "text": " for example, an artificial general intelligence? Or do you think eventually it's going to be"}, {"timestamp": [1172.28, 1176.04], "text": " that we'll need embodied AI as well?"}, {"timestamp": [1176.04, 1183.48], "text": " So when the progenitor developments that led to large language models first came out, which"}, {"timestamp": [1183.48, 1190.48], "text": " was word embedding, so like word2vec and sentence embeddings came out about, I think, 2015 is when they first"}, {"timestamp": [1190.48, 1196.76], "text": " started coming out. So like two years before GPT-2 came out. I realized that semantic embeddings"}, {"timestamp": [1196.76, 1202.48], "text": " would change everything. I started doing some experiments then with, okay, if you can have"}, {"timestamp": [1202.48, 1206.58], "text": " if you can translate semantic meaning from any sentence into computer"}, {"timestamp": [1206.58, 1213.4], "text": " understandable numbers, that is a game changer. So even back then my intuition was this is"}, {"timestamp": [1213.4, 1218.46], "text": " really important. And I started doing experiments back then. And then GPT-2 came out and I did"}, {"timestamp": [1218.46, 1225.6], "text": " experiments with that. And I realized at about that point that we were working towards machines"}, {"timestamp": [1225.6, 1228.32], "text": " that could use arbitrary reasoning."}, {"timestamp": [1228.32, 1230.1], "text": " And so what I mean by arbitrary reasoning"}, {"timestamp": [1230.1, 1233.76], "text": " is that given any general situation,"}, {"timestamp": [1233.76, 1235.56], "text": " you as a human with language,"}, {"timestamp": [1235.56, 1237.64], "text": " you've read books, you've talked to people,"}, {"timestamp": [1237.64, 1240.32], "text": " you can kind of reason through how to address something."}, {"timestamp": [1240.32, 1242.16], "text": " Right, you say, I can say like,"}, {"timestamp": [1242.16, 1244.36], "text": " hey, Sean, you're stranded on a desert island,"}, {"timestamp": [1244.36, 1245.4], "text": " what do you do?"}, {"timestamp": [1245.4, 1249.26], "text": " And you can sit, you can think through that and tell me what you would do with language."}, {"timestamp": [1249.26, 1252.72], "text": " Now that we have chat GPT, they can do the same thing, but it can do it faster and better"}, {"timestamp": [1252.72, 1254.02], "text": " than most people."}, {"timestamp": [1254.02, 1256.14], "text": " Like we're well on our way."}, {"timestamp": [1256.14, 1262.46], "text": " And I've talked to many people who do agree, some disagree, but who, who agree that language"}, {"timestamp": [1262.46, 1267.34], "text": " is the primary substrate or backplane of intelligence."}, {"timestamp": [1267.34, 1269.24], "text": " And what I mean by that is that language"}, {"timestamp": [1269.24, 1271.48], "text": " is the highest level of abstraction"}, {"timestamp": [1271.48, 1272.88], "text": " that our thought has."}, {"timestamp": [1272.88, 1277.12], "text": " Because one word can have so much implicit meaning"}, {"timestamp": [1277.12, 1279.16], "text": " and then you string words together,"}, {"timestamp": [1279.16, 1281.72], "text": " pretty much every sentence ever uttered by a human"}, {"timestamp": [1281.72, 1283.08], "text": " is a unique sentence."}, {"timestamp": [1283.08, 1285.28], "text": " So you have this infinite combination"}, {"timestamp": [1285.28, 1292.14], "text": " of potential meaning and directives and concepts. Like natural language is the most powerful"}, {"timestamp": [1292.14, 1295.78], "text": " programming language out there. And in fact, there used to be this thing called neuro linguistic"}, {"timestamp": [1295.78, 1301.38], "text": " programming, which basically said that the language that you use shapes your thoughts."}, {"timestamp": [1301.38, 1305.16], "text": " And that has NLP, neuro linguisticuro Linguistic Programming, has"}, {"timestamp": [1305.16, 1308.48], "text": " kind of gotten in the way of the dinosaurs. But the idea that you"}, {"timestamp": [1308.48, 1312.0], "text": " can shape your thoughts evolved into cognitive behavioral"}, {"timestamp": [1312.0, 1315.56], "text": " therapy, which is one of the more effective psychotherapy"}, {"timestamp": [1315.56, 1319.28], "text": " techniques today. So language is very, very powerful and"}, {"timestamp": [1319.28, 1322.48], "text": " important for our intelligence. And it's certainly a major,"}, {"timestamp": [1322.48, 1325.32], "text": " major component for general intelligence in"}, {"timestamp": [1325.32, 1331.32], "text": " AGI. Now that being said, it is not complete, and we're working on multimodal models right"}, {"timestamp": [1331.32, 1338.54], "text": " now. I don't mean me personally, but researchers integrating audio, video, and so on, so multiple"}, {"timestamp": [1338.54, 1345.2], "text": " sensory inputs. I did have some conversations with people about the need for embodiment, and I'm not"}, {"timestamp": [1345.2, 1348.96], "text": " sure if that's absolutely required."}, {"timestamp": [1348.96, 1354.12], "text": " Certainly it would be helpful if you want to have robots that you can interact with,"}, {"timestamp": [1354.12, 1358.72], "text": " but I don't think that it's strictly required to achieve general intelligence now."}, {"timestamp": [1358.72, 1364.32], "text": " It's quite amazing that you feed these things a huge data set, and then logic in some sense"}, {"timestamp": [1364.32, 1366.56], "text": " emerges from just"}, {"timestamp": [1366.56, 1371.44], "text": " content. How much, if you were to strip away and just, if you were able to look inside"}, {"timestamp": [1371.44, 1378.32], "text": " the actual neural network, how much of the parameter space is dedicated to say logic"}, {"timestamp": [1378.32, 1390.32], "text": " and how much is just content? Are we able to say this? I suspect that there's probably a little bit of both."}, {"timestamp": [1391.92, 1398.16], "text": " In all layers. So, with neural network architecture, usually each layer has a specific"}, {"timestamp": [1398.16, 1406.64], "text": " purpose where you're, you know, cross to simplify it, like you cross multiply in one layer, divide in the next layer. That's"}, {"timestamp": [1406.64, 1411.52], "text": " a vast oversimplification. But what ultimately happens is that information gets embedded"}, {"timestamp": [1411.52, 1418.44], "text": " in the parameters of each layer. So a parameter is a single mathematical operation that happens"}, {"timestamp": [1418.44, 1422.04], "text": " on the matrix that's passing through that layer."}, {"timestamp": [1422.04, 1425.6], "text": " And so one thing that I realized many years ago when I first started"}, {"timestamp": [1425.6, 1433.52], "text": " studying neuroscience is that memory and processing are kind of the same thing. From an organic"}, {"timestamp": [1433.52, 1440.32], "text": " perspective, your neurons process information by sending information over synapses, but the bias"}, {"timestamp": [1440.32, 1448.16], "text": " of those synapses is also part of memory. So in other words, memory and processing happen"}, {"timestamp": [1448.16, 1453.44], "text": " kind of at the same time in your brain. And the same is also true of neural networks,"}, {"timestamp": [1453.44, 1461.2], "text": " of artificial neural networks, where they don't process and remember as separate steps. It's all"}, {"timestamp": [1461.2, 1471.98], "text": " part of the same thing. They remember how to process the information in real time. So in that respect, the logic, the content, the knowledge, the procedures,"}, {"timestamp": [1471.98, 1477.76], "text": " the implications, that's all embedded and woven together through the neural network."}, {"timestamp": [1477.76, 1483.24], "text": " So you can't really separate them out. I don't think so. Now, that being said, if"}, {"timestamp": [1483.24, 1486.32], "text": " we take apart the layers, there's probably going to"}, {"timestamp": [1486.32, 1493.0], "text": " be some layers that are more for feature extraction. So we see this with image-based models where"}, {"timestamp": [1493.0, 1499.42], "text": " the first few layers of an image model will do things like edge detection. That's a low-level"}, {"timestamp": [1499.42, 1504.38], "text": " understanding of what's in the image. Then by the end, that's where you're getting image"}, {"timestamp": [1504.38, 1505.08], "text": " segmentation or labels applied. The optic nerve in the image. And then by the end, that's where you're getting like image segmentation"}, {"timestamp": [1505.08, 1510.04], "text": " or labels applied. And the optic nerve in the human brain is actually very similar,"}, {"timestamp": [1510.04, 1514.44], "text": " where processing starts on the retina, actually. So some of the edge detection and contrast"}, {"timestamp": [1514.44, 1518.28], "text": " is detected on the retina. So it's not even recording the whole image, it's recording"}, {"timestamp": [1518.28, 1522.68], "text": " gradients and then sending it down the optic nerve. And then lines and other features are"}, {"timestamp": [1522.68, 1528.94], "text": " extracted on your optic nerve before it even gets to the occipital of your brain. So I suspect that language probably"}, {"timestamp": [1528.94, 1534.98], "text": " has some of the same orders of abstraction, but in terms of identifying logic happens"}, {"timestamp": [1534.98, 1540.14], "text": " here, you're probably not going to find a specific spot where that happens."}, {"timestamp": [1540.14, 1546.48], "text": " I was really hoping it would be possible because I imagine that the knowledge storage is not super"}, {"timestamp": [1546.48, 1550.96], "text": " efficient. So it'd be nice if you could pull out and store the information about the world,"}, {"timestamp": [1550.96, 1554.64], "text": " you know, the content somewhere separate to where the logic is being processed."}, {"timestamp": [1555.2, 1559.76], "text": " But is that the direction you think we'll go? Or do you think these models are just going to get"}, {"timestamp": [1559.76, 1562.8], "text": " larger and larger and larger and more energy intensive as time goes on?"}, {"timestamp": [1564.08, 1565.28], "text": " I think it's going to be a little bit of"}, {"timestamp": [1565.28, 1570.56], "text": " both. Certainly there's going to be some algorithmic breakthroughs that we not that we should not"}, {"timestamp": [1570.56, 1574.12], "text": " that I can tell you what they're going to be, but we should definitely anticipate that"}, {"timestamp": [1574.12, 1579.92], "text": " there's going to be some algorithmic breakthroughs. For instance, you know, window size, the volume"}, {"timestamp": [1579.92, 1591.06], "text": " of text that these models are capable of reading at any one time is relatively small right now. GPT-4 is about 6,000 words, but researchers are working on different ways of ingesting"}, {"timestamp": [1591.06, 1595.6], "text": " information that could go up to a million words. So several orders of magnitude more"}, {"timestamp": [1595.6, 1601.02], "text": " data to take in at one time, which if that turns out to be successful, then yes, absolutely"}, {"timestamp": [1601.02, 1605.6], "text": " storing knowledge and information outside of the model might make more sense."}, {"timestamp": [1605.6, 1612.32], "text": " Now, that being said, when these language models were first coming out, a really popular"}, {"timestamp": [1612.32, 1617.36], "text": " interpretation was that it was actually a very powerful compression algorithm. So, the idea was"}, {"timestamp": [1617.36, 1626.6], "text": " that, oh, you're actually storing billions and billions worth of pages of text in a model that's only a few gigabytes. So it's like,"}, {"timestamp": [1626.6, 1632.06], "text": " well, how do you do that? And the idea was that maybe it is actually a form of data compression."}, {"timestamp": [1632.06, 1639.24], "text": " So I suspect it's going to go multiple directions. So in one respect, I think that we will have"}, {"timestamp": [1639.24, 1644.66], "text": " knowledge store models, which Meta tried to do this with, what did they call it? Their"}, {"timestamp": [1644.66, 1648.92], "text": " scientific model. Where basically they wanted, model. I don't remember what it"}, {"timestamp": [1648.92, 1654.0], "text": " was called off the top of my head. It got criticized. It confabulated like all language"}, {"timestamp": [1654.0, 1659.92], "text": " models do. The idea is you might be able to use a language model as an information store,"}, {"timestamp": [1659.92, 1666.08], "text": " as a data store or a database that you just fetch stuff out of with natural language queries."}, {"timestamp": [1666.08, 1671.04], "text": " Another possibility is that you have general purpose models with larger windows that you"}, {"timestamp": [1671.04, 1676.6], "text": " use more conventional search techniques to pull the information, the empirically recorded"}, {"timestamp": [1676.6, 1681.4], "text": " information, probably via API or database query or whatever. And then you put that into"}, {"timestamp": [1681.4, 1685.76], "text": " the context window. The model can read it and interpret it in real time."}, {"timestamp": [1685.76, 1690.34], "text": " I think we're probably going to see stuff going in both of those directions and probably"}, {"timestamp": [1690.34, 1694.12], "text": " a few more directions that we haven't thought of yet."}, {"timestamp": [1694.12, 1696.34], "text": " When it comes to logic, I've been surprised."}, {"timestamp": [1696.34, 1701.92], "text": " I've been testing chat GPT with various logic puzzles, and I've been a little bit disappointed"}, {"timestamp": [1701.92, 1711.56], "text": " when it comes to extrapolating logic puzzles So I'll find some logic puzzle which has say one or two different elements and I'll ask what happens when I"}, {"timestamp": [1712.26, 1719.56], "text": " Extrapolate and I have n objects in this logic puzzle and then I just find it collapsing under those extrapolations. So"}, {"timestamp": [1721.54, 1726.1], "text": " How are these models progressing so how much better is GPT-4, for example, when it comes"}, {"timestamp": [1726.1, 1729.58], "text": " to extrapolation than GPT-3? Do we know this yet?"}, {"timestamp": [1729.58, 1735.26], "text": " Yeah, so there have been a few tests and certainly some YouTube videos and interesting Twitter"}, {"timestamp": [1735.26, 1742.0], "text": " threads about trying to trick the models. Like Dr. Alan Thompson has a popular YouTube"}, {"timestamp": [1742.0, 1746.22], "text": " channel with his AI assistant L Lita. He actually just"}, {"timestamp": [1746.22, 1753.1], "text": " had a recent video where he led Lita, when it was still on GPT-3, through some of those"}, {"timestamp": [1753.1, 1757.74], "text": " verbal traps, those logic traps, and got it wrong. When he pushed back, he's like, actually"}, {"timestamp": [1757.74, 1763.08], "text": " got that wrong. She was able to say, oh, yeah, you're right. I got that wrong. GPT-4 is usually"}, {"timestamp": [1763.08, 1766.0], "text": " able to catch and talk through those things, those kinds of"}, {"timestamp": [1766.64, 1771.84], "text": " verbal traps or logical errors today. There are, of course, still plenty of things that it can't do."}, {"timestamp": [1772.8, 1780.56], "text": " But one thing to remember is that as a language model, its primary purpose is to predict language."}, {"timestamp": [1781.44, 1785.72], "text": " So, it might be that there's a matter of the network architecture might"}, {"timestamp": [1785.72, 1790.8], "text": " need to be adapted so it's better at handling pure logic, pure math. It could also be a"}, {"timestamp": [1790.8, 1795.44], "text": " matter of the training data. We mentioned earlier, you know, curating these data sets"}, {"timestamp": [1795.44, 1802.28], "text": " is a rapidly advancing science. So it might be just a matter of it just didn't have enough"}, {"timestamp": [1802.28, 1805.12], "text": " examples of those logic puzzles in the training data."}, {"timestamp": [1805.12, 1809.5], "text": " That might be the only thing that we need to do to fix that. It could be that simple."}, {"timestamp": [1809.5, 1812.58], "text": " One of the things I'm really most excited about that I don't see many people talking"}, {"timestamp": [1812.58, 1818.34], "text": " about is, so our human brains are evolved for this three-dimensional, low-velocity,"}, {"timestamp": [1818.34, 1824.62], "text": " you know, sort of medium masses environment that we're living in. I'm excited for AIs"}, {"timestamp": [1824.62, 1827.56], "text": " that are trained in simulated environments"}, {"timestamp": [1827.56, 1831.88], "text": " so they get intuition for high dimensional spaces or for quantum environments or for"}, {"timestamp": [1831.88, 1835.6], "text": " traveling at relativistic speeds. I'm wondering the sort of science that's going to come out"}, {"timestamp": [1835.6, 1840.12], "text": " of these. But I imagine you can't say too much along those directions because it's just"}, {"timestamp": [1840.12, 1842.28], "text": " something we haven't really done yet."}, {"timestamp": [1842.28, 1848.4], "text": " Yeah, I mean, you're absolutely right that, you know, just from an evolutionary perspective,"}, {"timestamp": [1848.4, 1852.4], "text": " but then also an experiential perspective, because again, our brains are pretty flexible."}, {"timestamp": [1852.4, 1856.4], "text": " And so if you talk to fighter pilots, for instance, they learn to think differently,"}, {"timestamp": [1856.4, 1857.4], "text": " right?"}, {"timestamp": [1857.4, 1862.56], "text": " You know, if you fly at Mach three for any length of time, physics is different."}, {"timestamp": [1862.56, 1866.48], "text": " Same thing for astronauts who, who are in zero G and weightless"}, {"timestamp": [1866.48, 1871.72], "text": " or even people who go scuba diving, right? Semi weightless environments. Your brain can"}, {"timestamp": [1871.72, 1879.54], "text": " adapt to different assumptions of physics. But that being said, there are still cognitive"}, {"timestamp": [1879.54, 1886.56], "text": " biases and gaps. So, for instance, even people who train really hard to understand exponentials,"}, {"timestamp": [1886.56, 1891.24], "text": " we generally fail. We need to rely on computer models to show us here's the exponential growth"}, {"timestamp": [1891.24, 1895.94], "text": " curve. Even then, it's still very hard for us to intuitively understand that."}, {"timestamp": [1895.94, 1901.08], "text": " Another example of something that's very hard to grasp is once you get to, as you mentioned,"}, {"timestamp": [1901.08, 1906.16], "text": " quantum physics and you have principles like non-locality and"}, {"timestamp": [1910.56, 1911.28], "text": " non-linear aspects of time or reverse causality or whatever interpretation. Superposition."}, {"timestamp": [1911.28, 1915.44], "text": " Yeah. Superposition is probably the most common thing that it's just like,"}, {"timestamp": [1915.44, 1919.52], "text": " it doesn't make any intuitive sense. You just have to accept the rule as you measure it and"}, {"timestamp": [1919.52, 1923.52], "text": " then just say, I don't get it. I remember talking to my cousin many years ago, he's a few years"}, {"timestamp": [1923.52, 1927.28], "text": " older than me and he became an electrical engineer. And I was just trying to ask him, like,"}, {"timestamp": [1927.28, 1930.96], "text": " why does magnetism happen? He's like, we can't explain it. You just have to accept that it does."}, {"timestamp": [1931.76, 1936.4], "text": " Like same thing with gravity, right? We can't really explain it. We can correlate gravity with"}, {"timestamp": [1936.4, 1940.48], "text": " mass, but like, you know, I have a book about the quantum theory of gravity. It's like, that's the"}, {"timestamp": [1940.48, 1943.84], "text": " best we've got. So at a certain point, you just kind of have to accept things."}, {"timestamp": [1943.84, 1946.16], "text": " We just model it and move on."}, {"timestamp": [1946.16, 1947.16], "text": " Right."}, {"timestamp": [1947.16, 1950.12], "text": " You model it, you measure it, you say, this is how it is, and you move on."}, {"timestamp": [1950.12, 1955.28], "text": " Now, the architecture of AI is not going to be constrained by our three pounds of gray"}, {"timestamp": [1955.28, 1957.04], "text": " matter and evolution."}, {"timestamp": [1957.04, 1961.88], "text": " So it's entirely possible that it will have the ability to intuitively model these things."}, {"timestamp": [1961.88, 1966.48], "text": " But that gives a new problem is how does it communicate that to us? How"}, {"timestamp": [1966.48, 1972.44], "text": " do we know if it's right? And interpretability is one of the key problems, and I'm sure we'll"}, {"timestamp": [1972.44, 1978.2], "text": " talk about that later in the episode. But interpretability and transparency is major"}, {"timestamp": [1978.2, 1986.64], "text": " because if you have a machine that's a million times smarter than you, how do you know? I'm sure as a physicist,"}, {"timestamp": [1986.64, 1990.72], "text": " if you go to a cocktail party and you talk to people who aren't physicists, it's kind of like"}, {"timestamp": [1990.72, 1998.72], "text": " you have to really modulate the way that you speak to speak on their level. But there can still be a"}, {"timestamp": [1998.72, 2005.68], "text": " big mismatch between what you know and what your receiver knows. And that's only going to be magnified if AI"}, {"timestamp": [2005.68, 2009.12], "text": " gets that much more intelligent than most of us."}, {"timestamp": [2009.12, 2015.72], "text": " So assuming that we will get into the stage where we have super intelligent general AIs,"}, {"timestamp": [2015.72, 2021.02], "text": " should we think of them as our tools, our partners, our children? How should we think"}, {"timestamp": [2021.02, 2031.56], "text": " of these things? So many people still are of the school of thought that AI should only ever be a tool."}, {"timestamp": [2031.56, 2037.28], "text": " Certainly I'm collaborating with some research organizations over the world, and most of"}, {"timestamp": [2037.28, 2048.28], "text": " the guidance that is published, whether it's at OECD, UN, they talk about AI like, oh, it is a tool and we should deploy it with this"}, {"timestamp": [2048.28, 2052.04], "text": " purpose and with these rules and these guardrails."}, {"timestamp": [2052.04, 2054.2], "text": " And certainly that's where we're at today."}, {"timestamp": [2054.2, 2059.4], "text": " However, within just a few weeks of GPT-4 being published, people were already building"}, {"timestamp": [2059.4, 2062.48], "text": " autonomous agents or semi-autonomous agents."}, {"timestamp": [2062.48, 2070.64], "text": " And these are people like you and I, just as individuals building cognitive architectures and using the intrinsic abilities, the reasoning abilities"}, {"timestamp": [2071.6, 2077.6], "text": " of these language models to create autonomous agents. We should assume that that will ramp up,"}, {"timestamp": [2077.6, 2086.54], "text": " that corporations, militaries, governments, even hostile actors, such as maybe organized crime, will start to use fully"}, {"timestamp": [2086.54, 2093.16], "text": " autonomous agents that are either self-contained or networked kind of intelligence, cloud-based,"}, {"timestamp": [2093.16, 2094.16], "text": " whatever."}, {"timestamp": [2094.16, 2096.86], "text": " There's many different architectures that can be deployed."}, {"timestamp": [2096.86, 2102.9], "text": " So we have to assume that either semi-autonomous or fully autonomous agents are coming pretty"}, {"timestamp": [2102.9, 2110.58], "text": " soon. And one thing that I perceive is a policy gap and a research gap is many people are"}, {"timestamp": [2110.58, 2112.42], "text": " not working on that."}, {"timestamp": [2112.42, 2116.9], "text": " We're still kind of in a phase where people kind of talk about superintelligence like"}, {"timestamp": [2116.9, 2122.42], "text": " the boogeyman, and these are usually decision theorists or philosophers talking about it,"}, {"timestamp": [2122.42, 2125.36], "text": " but there's not as many computer scientists or policy"}, {"timestamp": [2125.36, 2130.74], "text": " makers talking about autonomous AI. So there's a little bit of a gap there."}, {"timestamp": [2130.74, 2137.66], "text": " We'll talk about how we actually, the direction we want to go shortly. So for example, alignment."}, {"timestamp": [2137.66, 2144.36], "text": " But I want to stick on this topic and ask, is there anything that makes humans irreplaceable?"}, {"timestamp": [2144.36, 2145.98], "text": " So are we getting into a stage where there's"}, {"timestamp": [2145.98, 2151.42], "text": " nothing that humans can do that AI won't be able to do?"}, {"timestamp": [2151.42, 2157.42], "text": " From a functional standpoint, from an objective standpoint, I don't think so. That actually"}, {"timestamp": [2157.42, 2162.16], "text": " begs a very deep philosophical and spiritual question, which is, what is the point of living?"}, {"timestamp": [2162.16, 2166.16], "text": " What is the point of being a human? And that is something that"}, {"timestamp": [2166.16, 2172.96], "text": " I've done some work on. I wrote a paper or a short book called Post-Nihilism, where what I suspect is"}, {"timestamp": [2172.96, 2177.76], "text": " that we are barreling towards what I call a nihilistic crisis, or actually we're in the"}, {"timestamp": [2177.76, 2182.8], "text": " middle of a nihilistic crisis. And it actually started with the Industrial Revolution. If you"}, {"timestamp": [2182.8, 2185.44], "text": " look at a lot of poetry and literature, works of fiction during the rise of the Industrial Revolution. If you look at a lot of poetry and literature,"}, {"timestamp": [2186.16, 2191.12], "text": " works of fiction during the rise of the Industrial Revolution, a lot of people had a lot of"}, {"timestamp": [2191.12, 2196.96], "text": " existential anxiety about what was the point of being human in an era of machines. And this kind"}, {"timestamp": [2196.96, 2202.56], "text": " of pops up every now and then, right? Same thing happened with computers, with the advent of, you"}, {"timestamp": [2202.56, 2206.8], "text": " know, high-speed computers, nuclear weapons, so on and so forth,"}, {"timestamp": [2207.44, 2211.04], "text": " technological advancements tend to give us some existential anxiety."}, {"timestamp": [2211.76, 2217.6], "text": " But to your question about, okay, what is the benefit of being a human in a world where,"}, {"timestamp": [2217.6, 2222.4], "text": " from a productivity standpoint or an economic standpoint, machines can do everything that we"}, {"timestamp": [2222.4, 2226.64], "text": " can do better, faster, and cheaper, What's the point? And so that is"}, {"timestamp": [2226.66, 2230.4], "text": " where we have to change our orientation towards how we value"}, {"timestamp": [2230.52, 2233.36], "text": " our own life, and our own subjective experience. So that's"}, {"timestamp": [2233.36, 2236.84], "text": " a deeply, deeply philosophical and religious perspective, or a"}, {"timestamp": [2236.84, 2240.6], "text": " question. And it's, it's really interesting, because depending"}, {"timestamp": [2240.6, 2242.84], "text": " on someone's spiritual upbringing, or spiritual"}, {"timestamp": [2242.84, 2245.36], "text": " disposition, the question lands very"}, {"timestamp": [2245.36, 2251.04], "text": " differently. Because many religious doctrines around the world basically say that humans have"}, {"timestamp": [2251.04, 2257.6], "text": " a soul and that sets us apart. And so whether or not that's true, people have a model for just"}, {"timestamp": [2257.6, 2269.44], "text": " saying, my subjective experience of being is very meaningful and it is unique. Part of overcoming a nihilistic crisis is we all have"}, {"timestamp": [2269.44, 2277.12], "text": " to face that, whether or not we believe in souls or God or whatever. We have to go back to basics"}, {"timestamp": [2277.12, 2282.88], "text": " and look at the subjective experience of our own being. Back to your question earlier about children,"}, {"timestamp": [2282.88, 2288.68], "text": " I suspect that children who grow up with AI, they will just intrinsically know, oh yeah, my experience is different"}, {"timestamp": [2288.68, 2292.44], "text": " from this machine and that's okay. And that they won't have any existential anxiety about"}, {"timestamp": [2292.44, 2293.44], "text": " it. I hope at least."}, {"timestamp": [2293.44, 2298.44], "text": " Do you have, are you hopeful for the future or do you have this anxiety?"}, {"timestamp": [2298.44, 2305.3], "text": " No, I am, I think I'm biologically programmed to be optimistic. I just, I can't be cynical."}, {"timestamp": [2309.6, 2312.96], "text": " Part of that is that I've done a tremendous amount of work"}, {"timestamp": [2312.96, 2315.12], "text": " to understand what the dangers and risks are."}, {"timestamp": [2315.12, 2317.88], "text": " And I've also tried to contribute to coming up"}, {"timestamp": [2317.88, 2320.08], "text": " with a more optimistic outcome."}, {"timestamp": [2320.08, 2323.24], "text": " The machines, so we all learned this from watching Scooby-Doo"}, {"timestamp": [2324.2, 2325.76], "text": " the monsters are always humans,"}, {"timestamp": [2325.76, 2331.12], "text": " right? There's no such thing as an evil monster out there. The problem is always humans. And so"}, {"timestamp": [2332.32, 2338.0], "text": " this is a big reason that I've done my work is because, you know, it's not that a machine is"}, {"timestamp": [2338.0, 2342.72], "text": " going to replace you and that's a bad thing, right? We all fantasize about like, hey, I want to,"}, {"timestamp": [2342.72, 2345.04], "text": " you know, go live in the countryside and just go fishing every day. We all fantasize about like, hey, I want to go live in the countryside and just go fishing"}, {"timestamp": [2345.04, 2351.12], "text": " every day. We all know what we want to do if we don't have to work. What we are truly afraid of"}, {"timestamp": [2351.12, 2355.52], "text": " is not being able to take care of ourselves. Is that if the machine takes our job,"}, {"timestamp": [2355.52, 2360.64], "text": " we're going to go hungry, we're going to lose our home, we're going to end up lonely and whatever."}, {"timestamp": [2360.64, 2367.2], "text": " That's the actual fear. Nobody actually wants to keep working. I remember one of the"}, {"timestamp": [2367.2, 2373.6], "text": " advertisements for health insurance here in America was, you get to keep your health insurance."}, {"timestamp": [2373.6, 2380.04], "text": " You like your health. Nobody likes health insurance. It's a necessary evil. Jobs, occupations"}, {"timestamp": [2380.04, 2386.0], "text": " are a necessary evil of the economic environment that we're in and the technological limitations"}, {"timestamp": [2386.0, 2390.4], "text": " that we're in. And so as these things progress, this is, I'm basically just unpacking why I'm"}, {"timestamp": [2390.4, 2395.2], "text": " optimistic. As these things progress, I hope that we're all going to be able to have kind of a back"}, {"timestamp": [2395.2, 2398.72], "text": " to basics moment where it's like you wake up one day and it's like, how do you actually want to"}, {"timestamp": [2398.72, 2402.8], "text": " live? Right. If you want to go fishing every day, do it. If you want to focus on being an opera"}, {"timestamp": [2402.8, 2405.2], "text": " singer, go do that. We all"}, {"timestamp": [2405.2, 2410.4], "text": " have stuff that we want to do, but that we sacrifice for the sake of earning enough money"}, {"timestamp": [2410.4, 2414.84], "text": " to take care of ourselves. And that is the reality for most of us today."}, {"timestamp": [2414.84, 2418.76], "text": " I suppose one of the reasons why we have this worry is because currently we live in sort"}, {"timestamp": [2418.76, 2426.16], "text": " of a negotiated environment, right? The success of labor movements was because labor was needed. When humans are no longer needed,"}, {"timestamp": [2428.24, 2432.24], "text": " there's sort of a worry that we're not going to have the opportunity to go fishing,"}, {"timestamp": [2432.96, 2437.28], "text": " right? We're going to have nothing. And I guess that's the worry that you're pointing at."}, {"timestamp": [2437.28, 2439.92], "text": " What do you think the first jobs are that are going to go?"}, {"timestamp": [2441.68, 2449.28], "text": " Well, there's already been quite a few layoffs. Various communities on Reddit or private communities on Discord."}, {"timestamp": [2449.28, 2456.2], "text": " So for instance, my fiancee's we're both writers, but she's on a few private writing communities."}, {"timestamp": [2456.2, 2460.68], "text": " Copywriters have already been laid off and replaced by AI."}, {"timestamp": [2460.68, 2464.04], "text": " Marketing teams have been notified that they've got a year until they're all going to get"}, {"timestamp": [2464.04, 2469.6], "text": " laid off and replaced by AI-generated images and AI-generated emails."}, {"timestamp": [2469.6, 2471.6], "text": " So it's happening."}, {"timestamp": [2471.6, 2474.92], "text": " Yeah, that's where we're at."}, {"timestamp": [2474.92, 2483.2], "text": " Now I guess to your larger point of if we're all replaceable, what's the bottom line?"}, {"timestamp": [2483.2, 2485.76], "text": " And the fact of the matter is from a corporate perspective,"}, {"timestamp": [2485.76, 2488.4], "text": " from the perspective of neoliberalism,"}, {"timestamp": [2488.4, 2491.46], "text": " human labor is one of the most expensive aspects"}, {"timestamp": [2491.46, 2494.72], "text": " of productivity, and it's also the biggest constraint."}, {"timestamp": [2494.72, 2497.92], "text": " You look at population decline in places like China"}, {"timestamp": [2497.92, 2500.22], "text": " and Japan, because China just crested, right?"}, {"timestamp": [2500.22, 2502.8], "text": " So from here on out, China's population is going down"}, {"timestamp": [2502.8, 2504.14], "text": " for at least the next century."}, {"timestamp": [2504.14, 2509.52], "text": " Japan has been in decline for a couple decades now, ditto for Italy and a few other nations."}, {"timestamp": [2509.52, 2515.92], "text": " So there are labor forces contracting, right? And from an economic perspective, that's really bad"}, {"timestamp": [2515.92, 2525.28], "text": " for nations. So AI hopefully will actually shore up those labor markets and actually replace lost human labor."}, {"timestamp": [2525.28, 2531.32], "text": " Now, because humans are so expensive, right, you can pay $20 a month for chat GPT, and"}, {"timestamp": [2531.32, 2536.12], "text": " it can basically serve as an executive assistant and personal coach and every, it can replace"}, {"timestamp": [2536.12, 2539.8], "text": " literally thousands of dollars worth of labor and it costs $20 a month."}, {"timestamp": [2539.8, 2543.58], "text": " Chat GPT is infinitely cheaper than most human employees."}, {"timestamp": [2543.58, 2545.46], "text": " And that's only going to get better, right?"}, {"timestamp": [2545.46, 2549.16], "text": " Because either the model is going to get more efficient and cheaper, or it's going to get"}, {"timestamp": [2549.16, 2553.5], "text": " smarter and more powerful and therefore more valuable, or both in all likelihood."}, {"timestamp": [2553.5, 2560.06], "text": " So one of the things that I predict is that we are going to have a post-labor market economy"}, {"timestamp": [2560.06, 2561.46], "text": " before too long."}, {"timestamp": [2561.46, 2566.8], "text": " And in that respect, basically economic productivity will be decoupled from human"}, {"timestamp": [2566.8, 2573.2], "text": " labor. And in that case, you're going to see quadrillion dollar valuation for companies that"}, {"timestamp": [2573.2, 2579.84], "text": " have no employees. And that might sound like that could be an ingredient for a dystopian world that"}, {"timestamp": [2579.84, 2586.36], "text": " nobody wants to live in. We'll get to the regulation and stuff of that later. But from"}, {"timestamp": [2586.36, 2593.0], "text": " a purely GDP perspective, AI is going to be the best thing that ever happened to GDP,"}, {"timestamp": [2593.0, 2598.52], "text": " to economics, because again, it will decouple human labor from the constraint. And there"}, {"timestamp": [2598.52, 2607.12], "text": " will still be a few constraints, natural resources, rare minerals, fresh water, arable land. There's always going to be some"}, {"timestamp": [2607.12, 2612.32], "text": " physical constraints, but we're going to remove human labor as one of the main constraints to"}, {"timestamp": [2612.32, 2617.68], "text": " economics. And that is going to mandate those things, like you said, if you want to go fishing,"}, {"timestamp": [2617.68, 2623.36], "text": " well, how? If you don't have any economic power, if you don't have any way to make a demand,"}, {"timestamp": [2623.92, 2625.96], "text": " then that's a big problem,"}, {"timestamp": [2625.96, 2628.8], "text": " which is what we're going to have to negotiate. We're going to have to negotiate a new social"}, {"timestamp": [2628.8, 2632.54], "text": " contract basically. What do you think the impact is going to be"}, {"timestamp": [2632.54, 2637.72], "text": " on births ultimately? Do you think people are going to just start having AI children"}, {"timestamp": [2637.72, 2641.8], "text": " because it's cheaper? You know, that's a really difficult question."}, {"timestamp": [2641.8, 2651.44], "text": " I could see it going either way. There's plenty of books and fiction out there and research papers. People have predicted, you know, the population"}, {"timestamp": [2651.44, 2655.84], "text": " explosion, you know, that Earth will become uninhabitable because we'll have billions and"}, {"timestamp": [2655.84, 2660.0], "text": " billions of people that we can't feed. Other people are worried that, you know, the population"}, {"timestamp": [2660.0, 2665.6], "text": " is going to collapse. And I actually had a pretty long conversation about this just to clarify my"}, {"timestamp": [2665.6, 2673.68], "text": " own ideas, again, with ChatGPT. And so there's a few driving factors that cause birth rates to"}, {"timestamp": [2673.68, 2680.88], "text": " decline. Women entering the workforce, education and empowerment for women, access to birth"}, {"timestamp": [2680.88, 2686.48], "text": " control. So it turns out when a society advances and becomes a little bit more"}, {"timestamp": [2687.36, 2692.88], "text": " sophisticated or gains more access or some, you know, Jenny coefficient goes up, whatever metrics"}, {"timestamp": [2692.88, 2698.48], "text": " you use, education goes up, fertility rates go down. Some of that has to do with the choices"}, {"timestamp": [2698.48, 2703.76], "text": " of family planning, you know, men and women decide to have fewer children. Women have more control"}, {"timestamp": [2703.76, 2705.52], "text": " over their own fate."}, {"timestamp": [2705.52, 2708.4], "text": " Fertility rates tend to go down and this is"}, {"timestamp": [2708.4, 2711.68], "text": " a very reliable trend globally."}, {"timestamp": [2712.32, 2714.84], "text": " Regardless of culture,"}, {"timestamp": [2714.84, 2717.08], "text": " regardless of other economic conditions,"}, {"timestamp": [2717.08, 2718.94], "text": " as education rates go up,"}, {"timestamp": [2718.94, 2722.44], "text": " as women in the workforce goes up,"}, {"timestamp": [2722.44, 2723.92], "text": " fertility rates goes down."}, {"timestamp": [2723.92, 2725.6], "text": " This is a global thing with no"}, {"timestamp": [2725.6, 2731.68], "text": " exceptions, right? So if you extrapolate that out, then you can probably make a relatively safe"}, {"timestamp": [2731.68, 2738.48], "text": " assumption that as AI spreads around the world and economics and education and everything goes up,"}, {"timestamp": [2738.48, 2745.6], "text": " that fertility rates will continue to go down around the whole world. South Korea, I believe, has the lowest fertility rate on the"}, {"timestamp": [2745.6, 2754.08], "text": " planet at 0.8 births per woman, which is just above a third of the replacement rate."}, {"timestamp": [2754.72, 2761.28], "text": " So it's entirely possible that under these trends, that population collapse is actually"}, {"timestamp": [2761.28, 2765.16], "text": " the most real danger that we face. So what do you do about"}, {"timestamp": [2765.16, 2772.56], "text": " that? One thing that I think is going to happen is that AI will lead to medical breakthroughs."}, {"timestamp": [2772.56, 2778.68], "text": " I suspect that we are close, if not already, at the place of what's called longevity escape"}, {"timestamp": [2778.68, 2785.68], "text": " velocity, which is that the medical breakthroughs that happen every year extend your life by more than a year."}, {"timestamp": [2791.44, 2797.52], "text": " So basically, hypothetically, if you are healthy enough today, if you're not about to die, and you have access to decent enough healthcare, then the compounding returns of medical research"}, {"timestamp": [2797.52, 2802.4], "text": " and AI means that you and I could live to be several centuries old, which means that the"}, {"timestamp": [2802.4, 2810.8], "text": " population of the planet will stabilize as birth rates continue to decline. Now, I do think that some people will ultimately"}, {"timestamp": [2810.8, 2816.34], "text": " choose AI companions as they become more realistic. Certainly, a lot of people have seen shows"}, {"timestamp": [2816.34, 2820.6], "text": " like Westworld. One of my favorite characters of all time is Data from Star Trek, and I"}, {"timestamp": [2820.6, 2828.48], "text": " would love to have Data as a friend. I absolutely suspect that anthropomorphic"}, {"timestamp": [2828.48, 2833.24], "text": " machines will be part of our lives before too long. What form they take, whether it's"}, {"timestamp": [2833.24, 2839.36], "text": " a robotic dog that never dies or a walking, talking friend that is always there to hang"}, {"timestamp": [2839.36, 2845.92], "text": " out or if it's a romantic partner like in the the movie Her with Joaquin Phoenix and Scarlett Johansson"}, {"timestamp": [2845.92, 2850.12], "text": " There's lots of possibilities for how life is going to be but like I said"}, {"timestamp": [2850.12, 2857.18], "text": " I think one of the most reliable durable trends is fertility rates go down. So the question is will that be offset by longevity?"}, {"timestamp": [2857.84, 2860.6], "text": " so in other words rather than sort of the"}, {"timestamp": [2861.6, 2866.72], "text": " Dangerous Skynet that some people envision we might just get out competed sexually"}, {"timestamp": [2866.72, 2869.76], "text": " into extinction something along those lines."}, {"timestamp": [2869.76, 2874.08], "text": " Yeah that's that's entirely possible especially when you consider that actually there was"}, {"timestamp": [2874.08, 2879.6], "text": " a line from Terminator 2 it was when Sarah Connor was watching you know the Terminator"}, {"timestamp": [2879.6, 2885.56], "text": " Arnold Schwarzenegger play with John and she realized that the machine has infinite patience and"}, {"timestamp": [2885.56, 2888.72], "text": " will always be there because John was his mission."}, {"timestamp": [2888.72, 2894.28], "text": " And I realized that from a philosophical standpoint, one reading of that is that the machine could"}, {"timestamp": [2894.28, 2897.36], "text": " be a better parent than a human parent could ever be."}, {"timestamp": [2897.36, 2902.44], "text": " Because for a child, from a child's perspective, they should be their parents' primary mission,"}, {"timestamp": [2902.44, 2904.24], "text": " but that's never the case, right?"}, {"timestamp": [2904.24, 2908.64], "text": " Parents are humans too, and they have their own needs, their own desires, their own plans."}, {"timestamp": [2908.64, 2913.66], "text": " But when you have a machine that, if it is designed that you are its primary mission,"}, {"timestamp": [2913.66, 2921.78], "text": " whether you're an adult or a child, that could be, from some perspectives, a better outcome."}, {"timestamp": [2921.78, 2924.48], "text": " Obviously some people are probably cringing, which is understandable."}, {"timestamp": [2924.48, 2929.48], "text": " That's a perfectly healthy reaction to the idea of replacing children and parents with machines,"}, {"timestamp": [2929.48, 2932.88], "text": " but it's possible, right? Hypothetically possible."}, {"timestamp": [2932.88, 2937.0], "text": " One worry that I have is that we might end up in highly regulated environments. So for"}, {"timestamp": [2937.0, 2942.08], "text": " instance, I can imagine a situation where galleries say, okay, we're not going to allow"}, {"timestamp": [2942.08, 2947.2], "text": " any video cameras, pictures, anything like that into"}, {"timestamp": [2947.2, 2951.2], "text": " our establishment because we don't want anyone making a copy which can be then"}, {"timestamp": [2951.2, 2955.8], "text": " fed into an AI. We want to have some sort of unique little area which is"}, {"timestamp": [2955.8, 2962.48], "text": " protected from from being stolen let's say or you might have new property and"}, {"timestamp": [2962.48, 2965.76], "text": " copyright laws coming in which restrict what AI's can"}, {"timestamp": [2965.76, 2971.34], "text": " do. I sort of worry about going down that direction because it may be even more saltifying"}, {"timestamp": [2971.34, 2975.9], "text": " than losing your job to an AI."}, {"timestamp": [2975.9, 2980.38], "text": " I certainly think that that is possible but I'll disagree and I'll actually say that that's"}, {"timestamp": [2980.38, 2986.1], "text": " probably desirable. In some respects, having real experiences"}, {"timestamp": [2986.1, 2989.8], "text": " is gonna be better than having digital experiences."}, {"timestamp": [2989.8, 2991.56], "text": " So let me give you an example."}, {"timestamp": [2991.56, 2994.5], "text": " Yesterday evening, I was watching some YouTube"}, {"timestamp": [2994.5, 2996.92], "text": " with my fiance, and we were laying on the couch,"}, {"timestamp": [2996.92, 2998.6], "text": " it was the end of the day, we had done so much"}, {"timestamp": [2998.6, 3000.88], "text": " and we're tired, and we just looked at each other"}, {"timestamp": [3000.88, 3003.2], "text": " and we're like, we don't wanna be inside anymore."}, {"timestamp": [3003.2, 3004.64], "text": " So what we did was we went outside"}, {"timestamp": [3004.64, 3007.24], "text": " and we made a little campfire. and we're outside for an hour and"}, {"timestamp": [3007.24, 3012.18], "text": " a half, you know, just talking around the campfire. And I was like, and it occurred"}, {"timestamp": [3012.18, 3017.92], "text": " to me, it was like, we have basically infinite possibility for electronic distractions and"}, {"timestamp": [3017.92, 3023.4], "text": " entertainment and we're choosing to do physical labor and sit around a fire together. So I"}, {"timestamp": [3023.4, 3026.96], "text": " would actually suspect that suspect that in a hypothetical"}, {"timestamp": [3026.96, 3033.52], "text": " future where humans and AIs collaborate to create electronics-free zones, just because it's better"}, {"timestamp": [3033.52, 3040.16], "text": " for us from an experiential perspective. So there's a philosophical concept from Japan"}, {"timestamp": [3040.16, 3045.0], "text": " called wabi-sabi, which is accepting the passage of time."}, {"timestamp": [3046.04, 3048.2], "text": " And then there's another concept called Ichigo-Ichi,"}, {"timestamp": [3048.2, 3049.96], "text": " which is one time, one meeting."}, {"timestamp": [3049.96, 3054.44], "text": " And so the Japanese culture has this really deep appreciation"}, {"timestamp": [3054.44, 3057.14], "text": " for the ephemeral nature of reality and life."}, {"timestamp": [3057.14, 3059.84], "text": " And I think that if we deliberately add that"}, {"timestamp": [3059.84, 3064.04], "text": " to Western values and create situations that value that"}, {"timestamp": [3064.04, 3067.56], "text": " and just say, cherish this moment, don't record it, that's actually"}, {"timestamp": [3067.56, 3069.6], "text": " probably going to be better for our mental health, because"}, {"timestamp": [3069.6, 3073.12], "text": " we're always curating our lives to present it later on social"}, {"timestamp": [3073.12, 3076.12], "text": " media. And so I think if we actually do create some of"}, {"timestamp": [3076.12, 3080.16], "text": " those electronics free, and highly regulated or otherwise,"}, {"timestamp": [3080.92, 3084.4], "text": " non curated experiences that might actually be better for us."}, {"timestamp": [3085.0, 3090.0], "text": " And again, hopefully our children will be better than us at this as well."}, {"timestamp": [3090.0, 3093.0], "text": " Especially if they... Go on, sorry."}, {"timestamp": [3093.0, 3096.0], "text": " I was just going to say, yeah, especially if they grow up with these values."}, {"timestamp": [3096.0, 3101.0], "text": " Who do you think is going to benefit most moving forward?"}, {"timestamp": [3101.0, 3107.92], "text": " So, this is where it gets really dicey because every time that there is a technological"}, {"timestamp": [3107.92, 3113.84], "text": " revolution, the first movers, one, they benefit most and they get the most influence over how it"}, {"timestamp": [3113.84, 3119.68], "text": " goes. The early industrialists, they benefited the most, right? You know, the people who built"}, {"timestamp": [3120.64, 3127.08], "text": " coal-fired powerhouses and were the first ones to sell electricity, they had a lot of say over how it happened."}, {"timestamp": [3127.08, 3130.68], "text": " The first ones to deploy steam-powered machines in factories,"}, {"timestamp": [3130.68, 3132.16], "text": " they abused workers."}, {"timestamp": [3132.16, 3137.04], "text": " There's plenty of horrific stories of even children getting disfigured and"}, {"timestamp": [3137.04, 3141.4], "text": " dismembered and killed by factory accidents until regulations caught up."}, {"timestamp": [3141.4, 3145.84], "text": " We should expect that that kind of thing is going to happen again, especially"}, {"timestamp": [3145.84, 3153.66], "text": " because most governments are famously slow to react. America in particular, Europe is"}, {"timestamp": [3153.66, 3158.14], "text": " generally a little bit better about being proactive. They sometimes miss the mark, but"}, {"timestamp": [3158.14, 3167.2], "text": " they certainly put the effort into being proactive much more. So we should expect that AI entrepreneurs and the big tech companies that are"}, {"timestamp": [3167.2, 3173.36], "text": " best situated to make use of these technologies are going to benefit first and most. Hopefully,"}, {"timestamp": [3173.36, 3179.28], "text": " and this is not a foregone conclusion, hopefully some of the work that I'm doing and that other"}, {"timestamp": [3179.28, 3186.8], "text": " people that I'm working with are doing will create economic paradigms and technologies and situations that allow for"}, {"timestamp": [3187.6, 3194.24], "text": " redistribution and reallocation of that standard of living to everyone. And certainly, if we have"}, {"timestamp": [3194.24, 3201.04], "text": " a hyperabundance of cognitive labor, I don't see why we can't skim some off the top for everyone"}, {"timestamp": [3201.04, 3205.2], "text": " so that we can all benefit from what you might call a post-scarcity economic"}, {"timestamp": [3205.2, 3209.28], "text": " environment. Although, as I mentioned earlier, some things will always remain scarce, right?"}, {"timestamp": [3209.28, 3214.64], "text": " Desirable beachfront property in Monaco will always be scarce, unfortunately. But that being"}, {"timestamp": [3214.64, 3220.16], "text": " said, there's lots of things that don't necessarily have to be scarce. So you're a proponent of UBI?"}, {"timestamp": [3221.76, 3225.46], "text": " UBI as a starting point. I think that it'll have to be a lot more sophisticated than that"}, {"timestamp": [3225.46, 3230.7], "text": " in the long run, but certainly start there. What do you mean by sophisticated?"}, {"timestamp": [3230.7, 3240.58], "text": " Well, so fiat currency, so UBI, universal basic income, is the idea that you redistribute"}, {"timestamp": [3240.58, 3247.94], "text": " some currency, right? Whether it's US dollars, euros, or cryptocurrency, that you take some tax or"}, {"timestamp": [3247.94, 3253.7], "text": " probably a variety of taxes such as wealth tax,"}, {"timestamp": [3253.7, 3256.28], "text": " property tax, income tax, whatever."}, {"timestamp": [3256.28, 3257.96], "text": " You tax the rich,"}, {"timestamp": [3257.96, 3260.12], "text": " give it to the poor, Robinhood."}, {"timestamp": [3260.12, 3262.48], "text": " You give it to everyone equally."}, {"timestamp": [3262.48, 3265.52], "text": " Well, this solves many problems. There's"}, {"timestamp": [3265.52, 3272.56], "text": " plenty of pilot studies around the world where certain cities or states have implemented"}, {"timestamp": [3272.56, 3279.04], "text": " experiments. And generally speaking, UBI does everything from increase education outcomes for"}, {"timestamp": [3279.04, 3287.48], "text": " children, health outcomes for everyone. It even increases employment because more people have money to pay for more services."}, {"timestamp": [3287.48, 3290.2], "text": " So UBI is generally pretty good."}, {"timestamp": [3290.2, 3292.22], "text": " Of course, one question is how do you pay for it?"}, {"timestamp": [3292.22, 3293.56], "text": " You pay for it with taxes."}, {"timestamp": [3294.48, 3296.68], "text": " Now, that being said,"}, {"timestamp": [3296.68, 3300.84], "text": " there's always going to be a human need to get ahead, right?"}, {"timestamp": [3300.84, 3305.0], "text": " We have intrinsic needs to be socially competitive."}, {"timestamp": [3305.0, 3308.16], "text": " There's always a social hierarchy,"}, {"timestamp": [3308.16, 3309.56], "text": " a social pecking order."}, {"timestamp": [3309.56, 3312.76], "text": " This is just intrinsic to us as a social species,"}, {"timestamp": [3312.76, 3316.32], "text": " where you have to have your rank in the pack."}, {"timestamp": [3316.32, 3318.18], "text": " Some of that comes down to,"}, {"timestamp": [3318.18, 3320.56], "text": " some people will want to express that through"}, {"timestamp": [3320.56, 3323.52], "text": " likes on social media or conspicuous consumption,"}, {"timestamp": [3323.52, 3329.88], "text": " that sort of stuff. We also have a need to be rewarded for things that we do, things that we contribute. And"}, {"timestamp": [3329.88, 3336.22], "text": " so, one psychological framework is called self-determination theory. So, self-determination"}, {"timestamp": [3336.22, 3340.82], "text": " theory says that there's three primary components to psychological well-being. One is connection,"}, {"timestamp": [3340.82, 3345.44], "text": " so human connection to friends, family, children, so on your community. Another"}, {"timestamp": [3345.44, 3351.2], "text": " is competence, which is the ability to feel like you're good at something. And then the final"}, {"timestamp": [3351.2, 3356.32], "text": " ingredient is autonomy. The idea that you have self-determination that you can choose your own"}, {"timestamp": [3356.32, 3371.2], "text": " fate. So that combination of basic psychological needs, and of course, there's another framework that is more famous, which is Maslow's hierarchy of needs. But these needs, I suspect, will mean that people are always"}, {"timestamp": [3371.2, 3376.96], "text": " going to want to have some way to accumulate more, more experiences, more goods, more friends,"}, {"timestamp": [3376.96, 3388.04], "text": " something. So UBI is a good starting point. But what happens in the future when you're living in a neighborhood and everyone"}, {"timestamp": [3388.04, 3392.56], "text": " has the same basic income, that would be kind of bland, especially if there's no rewards"}, {"timestamp": [3392.56, 3397.98], "text": " for doing something going above and beyond. Right? And so one thing that I suspect might"}, {"timestamp": [3397.98, 3403.16], "text": " happen is that peer-to-peer transactions might actually be better. So, like, for instance,"}, {"timestamp": [3403.16, 3405.12], "text": " I make all of my income on Patreon right"}, {"timestamp": [3405.12, 3410.4], "text": " now. And that is because people out there decide that they like what I'm doing and they reward me"}, {"timestamp": [3410.4, 3416.64], "text": " for it. So I suspect that a decentralized economy might also be a big part of how we do things in"}, {"timestamp": [3416.64, 3422.08], "text": " the future. That's just one hypothesis. There's probably plenty of other paradigms that haven't"}, {"timestamp": [3422.08, 3428.32], "text": " even been invented yet or even thought about that might kind of evolve and emerge as we go."}, {"timestamp": [3428.32, 3434.32], "text": " So in that sense, capitalism and communism and all these political systems that were"}, {"timestamp": [3434.32, 3436.88], "text": " previously thought of, are they all obsolete now moving forward?"}, {"timestamp": [3438.32, 3445.76], "text": " Capitalism is still relevant, although it'll look very differently. I don't think there's a one term for it yet, but I"}, {"timestamp": [3445.76, 3451.52], "text": " mentioned post-labor economics, where human labor is no longer the biggest constraint."}, {"timestamp": [3451.52, 3455.52], "text": " And then if Elon Musk is successful in going harvesting asteroids, rare minerals aren't"}, {"timestamp": [3455.52, 3461.92], "text": " going to be a scarce resource either. That's one of the goals of SpaceX. So as we remove"}, {"timestamp": [3461.92, 3465.28], "text": " constraints, capitalism will look different, but there will still be benefits"}, {"timestamp": [3465.28, 3472.88], "text": " to owning things, right? Whether you own land or own property rights to beachfront property."}, {"timestamp": [3472.88, 3474.4], "text": " Computing power."}, {"timestamp": [3474.4, 3482.48], "text": " Computing power, sure. Yeah, so there will still be benefits to ownership. I'm not necessarily"}, {"timestamp": [3482.48, 3486.08], "text": " advocating for state ownership of everything."}, {"timestamp": [3486.08, 3492.8], "text": " I'm actually working on a proposal where ownership is is decentralized with blockchain technology"}, {"timestamp": [3492.8, 3497.62], "text": " called decentralized autonomous organizations. And I'm not even going to say that that should"}, {"timestamp": [3497.62, 3502.12], "text": " be the universal approach, right. But that's one way that you can control and reallocate"}, {"timestamp": [3502.12, 3505.76], "text": " resources that is different than both capitalism and"}, {"timestamp": [3505.76, 3508.26], "text": " communism or socialism."}, {"timestamp": [3508.26, 3511.62], "text": " Along these lines, do you think then that the development of AI should be completely"}, {"timestamp": [3511.62, 3518.68], "text": " open at this point?"}, {"timestamp": [3518.68, 3526.16], "text": " In principle, I will say yes, mostly. With that being said, some of the costs associated with developing AI"}, {"timestamp": [3526.16, 3532.92], "text": " are so big that there's still a major benefit from allowing for private investment to develop"}, {"timestamp": [3532.92, 3537.96], "text": " AI and get that first mover advantage. Because then there's incentives, right? Companies"}, {"timestamp": [3537.96, 3542.92], "text": " like Microsoft, Google, OpenAI, they have an incentive to put in that work and take"}, {"timestamp": [3542.92, 3545.6], "text": " that risk. And again, talking earlier about,"}, {"timestamp": [3546.32, 3550.64], "text": " human behavior is always incentivized, right? There's something that you want or need,"}, {"timestamp": [3550.64, 3554.4], "text": " and corporations and governments are no different. There's always something that a corporation or"}, {"timestamp": [3554.4, 3558.88], "text": " government wants or needs, and it needs to be incentivized, and there needs to be an appropriate"}, {"timestamp": [3559.52, 3569.44], "text": " material, financial, or energetic reward for doing so. And what I hope is that as more research is done,"}, {"timestamp": [3570.4, 3577.04], "text": " we can end up with more decentralized ways of funding these things and doing that research."}, {"timestamp": [3577.04, 3580.96], "text": " Right now, those technology platforms are not sophisticated enough. They're not mature enough"}, {"timestamp": [3580.96, 3589.2], "text": " to do that. But eventually, I think that everything could be, or maybe not everything, but many more things could be fully open. There's always going to be"}, {"timestamp": [3589.2, 3595.04], "text": " people, like people should always have the right to do things in private. So say for instance,"}, {"timestamp": [3595.04, 3599.92], "text": " imagine 50 years from now, most research is completely open source. It's all decentralized."}, {"timestamp": [3599.92, 3604.8], "text": " It's all on blockchain, et cetera, et cetera. You and I, if you call me up 50 years from now and"}, {"timestamp": [3604.8, 3605.68], "text": " say like, Hey Dave, I've got"}, {"timestamp": [3605.68, 3612.16], "text": " an idea, we should be allowed to do our own private work, even still, even in a post-labor"}, {"timestamp": [3612.16, 3618.0], "text": " world, just on principle. Just because that autonomy, that personal autonomy is important"}, {"timestamp": [3618.0, 3625.12], "text": " for humans to be healthy, is that self-direction. Let's change tack."}, {"timestamp": [3625.12, 3627.64], "text": " I want to ask you again about existential risk."}, {"timestamp": [3627.64, 3629.92], "text": " Let's change the tack back to where we were."}, {"timestamp": [3629.92, 3634.56], "text": " How might AI kill us?"}, {"timestamp": [3634.56, 3636.84], "text": " So like I said earlier, Scooby-Doo,"}, {"timestamp": [3636.84, 3638.84], "text": " humans are always the monster."}, {"timestamp": [3638.84, 3647.8], "text": " So there was a warning video published, what, I think about 2014, called Slaughterbots."}, {"timestamp": [3647.8, 3650.6], "text": " I don't know if you ever saw that."}, {"timestamp": [3650.6, 3656.9], "text": " It was a warning about what drone technology would be capable of."}, {"timestamp": [3656.9, 3661.36], "text": " On this, it was completely fictional, but it's actually capable today."}, {"timestamp": [3661.36, 3670.56], "text": " There's a tiny little drone that came out on stage and it flew out a mannequin and used a shape charge to headshot the mannequin. That's actually happening today as people"}, {"timestamp": [3670.56, 3676.44], "text": " are arming small drones with explosives and dropping them on troops in various war zones"}, {"timestamp": [3676.44, 3687.44], "text": " around the world. So it's a very small step to take that from human controlled drone to fully autonomous drone, whether it's"}, {"timestamp": [3687.44, 3692.96], "text": " still remotely controlled by a language model or other AI models or cognitive architectures."}, {"timestamp": [3692.96, 3698.28], "text": " And then of course, big news recently is that the United States Air Force, in partnership"}, {"timestamp": [3698.28, 3706.4], "text": " with I think DARPA, they're working on creating fully autonomous F-16 fighter jets, which is, you know, that's"}, {"timestamp": [3706.4, 3707.8], "text": " concerning."}, {"timestamp": [3707.8, 3713.06], "text": " But the idea is that those autonomous fighter jets will still be slaved to a main pilot,"}, {"timestamp": [3713.06, 3718.04], "text": " like a captain or a lieutenant colonel, someone of very high rank flying alongside them, giving"}, {"timestamp": [3718.04, 3722.76], "text": " them orders, but they're still going to be autonomous or semi-autonomous."}, {"timestamp": [3722.76, 3725.72], "text": " And the ultimate goal is, of course, to have those armed with"}, {"timestamp": [3725.72, 3733.44], "text": " the ability to make lethal decisions. That is far and away the most immediate lethal"}, {"timestamp": [3733.44, 3739.24], "text": " risk. Then, of course, there's been works of fiction that kind of speculate, what would"}, {"timestamp": [3739.24, 3746.24], "text": " a fully autonomous or AI drone war look like? There was a movie called Surrogates."}, {"timestamp": [3746.24, 3747.4], "text": " I think this was from that movie."}, {"timestamp": [3747.4, 3748.96], "text": " It's a Bruce Willis movie."}, {"timestamp": [3748.96, 3751.52], "text": " Came out in I think about 2007, 2008."}, {"timestamp": [3751.52, 3757.64], "text": " One of the scenes in that movie was a whole bunch of soldiers in VR beds remotely operating"}, {"timestamp": [3757.64, 3761.76], "text": " a whole bunch of soldiers that were running across the battlefield, fighting with other"}, {"timestamp": [3761.76, 3763.12], "text": " robots on the battlefield."}, {"timestamp": [3763.12, 3768.16], "text": " And then one would get blown up and you'd come out of VR and load into another one like a video game."}, {"timestamp": [3768.16, 3774.44], "text": " So that's entirely possible, but not every nation is going to abide by rules of engagement."}, {"timestamp": [3774.44, 3780.08], "text": " And certainly we're seeing this today, where some nations are deliberately targeting civilians."}, {"timestamp": [3780.08, 3787.36], "text": " And if you remove human risk of your soldiers from the equation, suddenly some nations might make"}, {"timestamp": [3787.36, 3794.16], "text": " the decision, hey, my soldiers aren't at risk, but I can attack your civilians with drones en masse."}, {"timestamp": [3794.16, 3799.36], "text": " That is probably the greatest existential risk that we face if that sort of thing escalates."}, {"timestamp": [3800.88, 3804.64], "text": " Do you think this is in some sense inevitable given the competition that we have on the ground?"}, {"timestamp": [3828.68, 3836.08], "text": " Do you think this is in some sense inevitable given the competition that we have on the ground? So what I mean by that is, so today maybe we don't give the kill switch, the control over the kill switch to the autonomous agent, but if it can act just a tenth of a second faster than a human, then maybe it makes sense in terms of the competition to give it that switch. Do you think this is inevitable now, that sort of the genie is out of the bag? Eventually yes. So this is where game theory comes in. And the most famous global example"}, {"timestamp": [3836.08, 3840.2], "text": " of game theory was what's called mutually assured destruction. So during the height"}, {"timestamp": [3840.2, 3851.76], "text": " of the Cold War between the United States and the Soviet Union, there was the nuclear buildup, the arms race to build more larger and more sophisticated nuclear weapons. And so, mutually"}, {"timestamp": [3851.76, 3860.08], "text": " assured destruction had a trifecta of strategies. So, that was land, air, and sea. So, there was"}, {"timestamp": [3860.08, 3870.6], "text": " nuclear armed submarines, there was nuclear armed bombers, and then of course, hidden missile silos. Both sides had those three strategies. And the idea was nobody was going"}, {"timestamp": [3870.6, 3876.36], "text": " to shoot because whoever shot first would initiate a world ending nuclear war. But the"}, {"timestamp": [3876.36, 3883.32], "text": " idea was nobody had first strike capability. So from a logical strategic standpoint, all"}, {"timestamp": [3883.32, 3886.24], "text": " sides had the ability to fully annihilate each other"}, {"timestamp": [3886.24, 3890.72], "text": " without being able to stop the other one from doing it. So therefore, the optimal strategy"}, {"timestamp": [3890.72, 3896.32], "text": " for everyone to adopt was have the weapons, but never use them. So that is what's called"}, {"timestamp": [3896.32, 3901.76], "text": " a Nash equilibrium. So John Nash, famous mathematician, came up with this concept,"}, {"timestamp": [3901.76, 3911.68], "text": " I think in the 50s, maybe earlier than that. I don't remember exactly. But the idea is that in a competitive environment, you will"}, {"timestamp": [3911.68, 3918.0], "text": " end up with everyone adopting an optimal strategy and they have no incentive to change their"}, {"timestamp": [3918.0, 3923.6], "text": " strategy. So a much more familiar example of this is monopoly. Once all the properties"}, {"timestamp": [3923.6, 3930.06], "text": " are sold in the game of monopoly, you don't change your strategy anymore. You just keep rolling the dice and you wait"}, {"timestamp": [3930.06, 3935.24], "text": " until the game is over, right? And so in the game of Monopoly, when you have arrived at"}, {"timestamp": [3935.24, 3940.66], "text": " a Nash equilibrium, usually one person will ultimately win, right? Because they're going"}, {"timestamp": [3940.66, 3944.7], "text": " to end up with a little bit more property, a little bit more money, and that gives them"}, {"timestamp": [3944.7, 3948.88], "text": " a stacked advantage. Now, one story, though, that I have is I was playing"}, {"timestamp": [3948.88, 3954.72], "text": " Monopoly with a friend many, many years ago on PlayStation 2. And we were all the properties"}, {"timestamp": [3954.72, 3960.24], "text": " had been sold. We were at the end game. And we noticed that nobody was losing. And so we just"}, {"timestamp": [3960.24, 3963.28], "text": " said, well, I'm not going to sell you anything. And my friend's like, I'm not going to sell you"}, {"timestamp": [3963.28, 3968.32], "text": " anything. So let's just hit X as fast as we can. We advanced the game by 50 or 100"}, {"timestamp": [3968.32, 3972.88], "text": " rounds. We ended up with hundreds of thousands of dollars just by the rules of the game playing"}, {"timestamp": [3972.88, 3977.68], "text": " out. That is what you call a positive Nash equilibrium where everyone wins. That's a"}, {"timestamp": [3977.68, 3985.36], "text": " win-win scenario. Most games end with a win-lose where someone wins, someone loses. But you can also, in real life,"}, {"timestamp": [3985.36, 3991.36], "text": " you can have lose-lose scenarios. So, a lose-lose scenario is if United States or Russia fired the"}, {"timestamp": [3991.36, 3999.2], "text": " first nuke and then everyone loses. So, there's three basic outcomes to competitions, which is"}, {"timestamp": [3999.2, 4006.36], "text": " win-win, win-lose, or lose-lose. So, that was setting the stage. With AI, as you mentioned, it is"}, {"timestamp": [4006.36, 4011.72], "text": " a competitive landscape. And so, what we have to do is we have to create a system of rules,"}, {"timestamp": [4011.72, 4019.56], "text": " criteria, constraints, technologies, and strategies that will hopefully ultimately result in a"}, {"timestamp": [4019.56, 4025.56], "text": " win-win situation where all nations and all citizens and all corporations end up better rather"}, {"timestamp": [4025.56, 4026.92], "text": " than worse off."}, {"timestamp": [4026.92, 4033.54], "text": " So in the context of AI, a win-win situation might look like where you have Apple and Google"}, {"timestamp": [4033.54, 4039.68], "text": " and Microsoft become quadrillion dollar companies and you and I can go fishing every day if"}, {"timestamp": [4039.68, 4045.0], "text": " we want, or I can go vacation in Greece if I want to, right?"}, {"timestamp": [4045.0, 4048.2], "text": " And nobody is hungry and there's no war."}, {"timestamp": [4048.2, 4050.4], "text": " That's the win-win situation that we all want."}, {"timestamp": [4050.4, 4056.4], "text": " But if we can't create the competitive scenario, the competitive landscape"}, {"timestamp": [4056.4, 4059.8], "text": " that will inevitably lead to that kind of utopian outcome,"}, {"timestamp": [4059.8, 4064.4], "text": " you know, there's a lot of people suspect that it's going to be a bimodal outcome,"}, {"timestamp": [4064.4, 4068.88], "text": " that it's going to be either, you know, really great or really awful. Right? And so that's"}, {"timestamp": [4068.88, 4073.96], "text": " what's called an attractor state. So an attractor state is let's see if I can get the definition"}, {"timestamp": [4073.96, 4080.42], "text": " right. So an attractor state is a stable end condition that many complex systems tend to"}, {"timestamp": [4080.42, 4088.16], "text": " gravitate towards over time. And so, utopia is a very common fictional example of"}, {"timestamp": [4088.16, 4093.2], "text": " a potential attractor state. That is the attractor state that we want. The attractor state that we're"}, {"timestamp": [4093.2, 4099.68], "text": " afraid of is dystopia or collapse. And so, dystopia is, you know, explored in stuff like,"}, {"timestamp": [4099.68, 4103.12], "text": " you know, the Matrix and Blade Runner and stuff where corporations have all the power."}, {"timestamp": [4103.28, 4103.8], "text": " The Matrix and Blade Runner and stuff where corporations have all the power."}, {"timestamp": [4106.64, 4106.88], "text": " Altered Carbon was a more recent TV show."}, {"timestamp": [4110.96, 4111.6], "text": " Great example of a potential dystopian attractor state, which"}, {"timestamp": [4114.84, 4115.08], "text": " the current economic system of capitalism and neoliberalism"}, {"timestamp": [4118.88, 4120.88], "text": " and hyper competition will inevitably lead to that if unless we change something."}, {"timestamp": [4120.88, 4124.24], "text": " Now, another possible set of attractor states is collapse,"}, {"timestamp": [4124.44, 4125.44], "text": " which that's been explored in"}, {"timestamp": [4125.44, 4130.72], "text": " like Mad Max, like Mad Max Fury Road. That is an example of a collapse attractor state where"}, {"timestamp": [4131.28, 4136.56], "text": " the planet's ecology just can't support us anymore. And whether through war or economic collapse or"}, {"timestamp": [4136.56, 4141.28], "text": " mismanagement, the human population is reduced to a tiny fraction of what it is today. And finally,"}, {"timestamp": [4141.28, 4145.28], "text": " extinction, which is what's illustrated in movies like The Terminator,"}, {"timestamp": [4145.28, 4148.96], "text": " where the machines just decide to wipe us all out, or something else wipes us all out, right?"}, {"timestamp": [4148.96, 4153.84], "text": " We could wipe ourselves out. So, with those several potential attractor states in this"}, {"timestamp": [4153.84, 4160.0], "text": " competitive environment, the idea is how do we create the set of conditions and rules and other"}, {"timestamp": [4160.0, 4165.12], "text": " strategies that lead to utopia rather than dystopia? And how do we get off of that, what"}, {"timestamp": [4165.12, 4171.2], "text": " feels like a potentially inevitable downslide into those negative states? That's not really"}, {"timestamp": [4171.2, 4173.36], "text": " an answer to your question. That's where we're at."}, {"timestamp": [4174.0, 4178.08], "text": " No, it is. But I still worry that we're not going to have a competitive environment in the sense"}, {"timestamp": [4178.08, 4182.32], "text": " that, you know, with nuclear weapons, we had mutually assured destruction. But at the beginning"}, {"timestamp": [4182.32, 4189.04], "text": " of our discussion, you said that humans don't really understand exponentials very well. I can imagine a scenario where just one or two weeks"}, {"timestamp": [4189.04, 4196.24], "text": " difference in time, we have this huge ramp up of abilities where one nation can really just"}, {"timestamp": [4196.24, 4202.16], "text": " crush other nations because they have at their disposal so much computational power"}, {"timestamp": [4202.8, 4205.0], "text": " that the others really can't compete?"}, {"timestamp": [4205.0, 4211.84], "text": " Yeah, that's absolutely possible. Which, you know, that's one of the reasons that I suspect"}, {"timestamp": [4211.84, 4219.12], "text": " that a lot of Western nations and Western allied nations are doing embargoes on AI chips,"}, {"timestamp": [4219.12, 4227.76], "text": " right? So for instance, United States just had basically signaled support for Taiwan, and in response,"}, {"timestamp": [4227.76, 4232.24], "text": " China encircled Taiwan as a practice of an invasion."}, {"timestamp": [4232.24, 4238.72], "text": " Taiwan, for those that might not know, is the world's largest producer of advanced computer"}, {"timestamp": [4238.72, 4242.52], "text": " chips today."}, {"timestamp": [4242.52, 4249.44], "text": " Part of that globe-scale geopolitical game of chess is that a lot of nations are on-shoring"}, {"timestamp": [4249.44, 4251.72], "text": " the production of computer chips."}, {"timestamp": [4251.72, 4259.04], "text": " So all across America, a lot of corporations are being incentivized to build foundries"}, {"timestamp": [4259.04, 4260.04], "text": " on our own soil."}, {"timestamp": [4260.04, 4266.8], "text": " This is not just America, this is Europe, Australia, all over the place, because those in power"}, {"timestamp": [4266.8, 4272.3], "text": " understand that this is where the future of the planet will be decided. And so you're"}, {"timestamp": [4272.3, 4277.44], "text": " absolutely right that just from a raw energetic or processing standpoint, that's going to"}, {"timestamp": [4277.44, 4283.0], "text": " be a big deciding factor. Now, another deciding factor is going to be the sophistication of"}, {"timestamp": [4283.0, 4285.86], "text": " the software running on that hardware."}, {"timestamp": [4285.86, 4290.14], "text": " So for instance, China, just yesterday, they announced that language models must abide"}, {"timestamp": [4290.14, 4295.94], "text": " by their party line. Right? Which that is a really interesting development because of"}, {"timestamp": [4295.94, 4301.46], "text": " course they banned open AI and open source language models. Because why? Because those"}, {"timestamp": [4301.46, 4306.8], "text": " provide freedom of thought and freedom of information, which China doesn't like."}, {"timestamp": [4306.8, 4310.8], "text": " And so one thing that I hope is that any authoritarian"}, {"timestamp": [4310.8, 4314.64], "text": " or hostile nation, because they intrinsically must rely"}, {"timestamp": [4314.64, 4316.96], "text": " on information control and thought control"}, {"timestamp": [4316.96, 4319.72], "text": " in order to avoid liberalizing."}, {"timestamp": [4319.72, 4322.2], "text": " And by liberalizing, I don't mean like left versus right,"}, {"timestamp": [4322.2, 4324.34], "text": " I mean becoming liberal democracies."}, {"timestamp": [4324.34, 4327.2], "text": " Because authoritarian nations"}, {"timestamp": [4327.2, 4329.56], "text": " are hostile to the idea of becoming a liberal democracy"}, {"timestamp": [4329.56, 4332.48], "text": " because they want to maintain central control and not have"}, {"timestamp": [4332.52, 4335.26], "text": " accountability and transparency. Now, because"}, {"timestamp": [4335.26, 4339.68], "text": " language technology like open AI, GPT-3, GPT-4, lend"}, {"timestamp": [4339.68, 4344.04], "text": " themselves to transparency of information, and because"}, {"timestamp": [4344.44, 4346.84], "text": " nations benefit by sharing data"}, {"timestamp": [4346.84, 4348.52], "text": " and using this data,"}, {"timestamp": [4348.52, 4352.12], "text": " one of the things that I hope is that just by default,"}, {"timestamp": [4352.12, 4354.8], "text": " liberalized democracies will be ahead"}, {"timestamp": [4354.8, 4356.92], "text": " of more authoritarian countries."}, {"timestamp": [4356.92, 4359.4], "text": " And this is already true because authoritarian countries"}, {"timestamp": [4359.4, 4363.08], "text": " tend to have a lot more corruption than liberal democracies,"}, {"timestamp": [4363.08, 4366.0], "text": " which is why in the grand scheme of things over"}, {"timestamp": [4366.0, 4371.84], "text": " the last hundred years, you've seen, generally speaking, a decline of authoritarian regimes"}, {"timestamp": [4371.84, 4377.36], "text": " and a rise of liberal democracies. We're moving in that direction and hopefully that continues."}, {"timestamp": [4377.36, 4382.64], "text": " So while you're absolutely right that that is a potential outcome, when you look at the collective"}, {"timestamp": [4382.64, 4385.28], "text": " industrial power of the liberal"}, {"timestamp": [4385.28, 4391.92], "text": " democracies of the world, the total compute power that we will all control will vastly outstrip the"}, {"timestamp": [4391.92, 4398.72], "text": " total compute power that authoritarian countries will control. In that sense then, so recently"}, {"timestamp": [4398.72, 4406.64], "text": " there was this letter by Musk and various other people to put a moratorium on large AI experiments. Do you think along these"}, {"timestamp": [4406.64, 4412.48], "text": " lines that that letter is just a complete mistake, that it will strip away the advantage that liberal"}, {"timestamp": [4412.48, 4420.4], "text": " democracies currently hold in the game? I would agree with that. And I did listen to Max"}, {"timestamp": [4420.4, 4427.12], "text": " Tegmark's talk with Lex Friedman about it. Max Tegmark, I believe, was the primary"}, {"timestamp": [4427.12, 4431.56], "text": " author of the whole thing. So it's his idea. And then he got a whole bunch of people to"}, {"timestamp": [4431.56, 4437.56], "text": " sign on. Obviously, Elon Musk as a capitalist, he might have a conflict of interest because"}, {"timestamp": [4437.56, 4442.88], "text": " he's got Tesla, which is competing. So it's kind of suspicious if someone says, hey, slow"}, {"timestamp": [4442.88, 4448.24], "text": " down my competitors so that I can catch up. But Max Tegmark doesn't have an economic incentive as far as I know."}, {"timestamp": [4448.88, 4454.4], "text": " So I listened to his talk and his reasoning for calling for a moratorium makes sense"}, {"timestamp": [4455.04, 4459.44], "text": " from his worldview, because he's of the perspective that AI should only ever be a tool,"}, {"timestamp": [4460.24, 4465.44], "text": " which I disagree with. I don't think that we can make that as a unilateral decision."}, {"timestamp": [4465.44, 4469.12], "text": " And so there's a whole bunch of things that he said we should never do. We should never connect"}, {"timestamp": [4469.76, 4473.52], "text": " advanced AI to the internet. We did that. We should never let advanced AI talk to each other."}, {"timestamp": [4473.52, 4478.24], "text": " We did that. There's a whole list of things that he sees as red flags that we should never do that"}, {"timestamp": [4478.24, 4486.28], "text": " we did anyways. And so to him, the only solution then is slam on the brakes. But this is not the first time that someone"}, {"timestamp": [4486.28, 4489.16], "text": " from the establishment, from the universities"}, {"timestamp": [4489.16, 4490.72], "text": " of the academic establishment,"}, {"timestamp": [4490.72, 4492.44], "text": " has said, let's slam on the brakes."}, {"timestamp": [4492.44, 4494.76], "text": " The first time that I remember it happening"}, {"timestamp": [4494.76, 4498.36], "text": " in the current era was I think around 2017,"}, {"timestamp": [4498.36, 4500.76], "text": " where when people realized how much"}, {"timestamp": [4500.76, 4502.96], "text": " of a surveillance state China was becoming,"}, {"timestamp": [4502.96, 4507.08], "text": " and the Western establishment kind of unilaterally declared,"}, {"timestamp": [4507.08, 4510.88], "text": " we're going to do a moratorium on image technology"}, {"timestamp": [4510.88, 4512.24], "text": " and other technologies that can be used"}, {"timestamp": [4512.24, 4513.38], "text": " in a surveillance state."}, {"timestamp": [4513.38, 4515.6], "text": " And the rest of the world collectively just ignored it."}, {"timestamp": [4515.6, 4516.44], "text": " Right?"}, {"timestamp": [4516.44, 4519.8], "text": " Like you can't, no one individual organization"}, {"timestamp": [4519.8, 4522.98], "text": " or establishment can unilaterally say,"}, {"timestamp": [4522.98, 4524.18], "text": " put a moratorium on this."}, {"timestamp": [4524.18, 4525.0], "text": " It's just not possible."}, {"timestamp": [4525.0, 4529.8], "text": " It doesn't happen that way. And so to answer your question more directly, go ahead."}, {"timestamp": [4529.8, 4537.2], "text": " Can I jump in and say, just inject right here. There was the war on drugs, right? So this"}, {"timestamp": [4537.2, 4541.88], "text": " is, America did in that case, force the whole world into this sort of a position. What's"}, {"timestamp": [4541.88, 4548.06], "text": " the difference here in this case? So in the case of the war on drugs,"}, {"timestamp": [4548.06, 4550.9], "text": " that was mostly like smoke and mirrors."}, {"timestamp": [4551.98, 4555.88], "text": " And it was because, well, the impetus of it"}, {"timestamp": [4555.88, 4559.72], "text": " was a backlash to hippie culture in the 60s, right?"}, {"timestamp": [4559.72, 4562.12], "text": " Because people did, you know, LSD and mushrooms"}, {"timestamp": [4562.12, 4564.76], "text": " and then said, maybe we should stop invading other countries"}, {"timestamp": [4564.76, 4565.68], "text": " and Richard Nixon didn't like that. did LSD and mushrooms and then said, maybe we should stop invading other countries. And Richard"}, {"timestamp": [4565.68, 4572.08], "text": " Nixon didn't like that. There's plenty of speeches where he says that that's super un-American, that"}, {"timestamp": [4573.04, 4579.28], "text": " it was very, very, very imperialistic. And so because things like the petrodollar happened"}, {"timestamp": [4579.28, 4584.72], "text": " at the time, America had a lot of leverage, a lot of geopolitical leverage. Forced things like,"}, {"timestamp": [4585.84, 4594.0], "text": " America had a lot of leverage, a lot of geopolitical leverage. Forced things like, you know, one, just the petrodollar because of the outreach of our military and alliances, and then less"}, {"timestamp": [4594.0, 4599.52], "text": " savory means, such as using the CIA to topple regimes, which is well-known, well-documented."}, {"timestamp": [4601.76, 4606.64], "text": " So that's kind of my response to like, okay, yes, one nation"}, {"timestamp": [4606.64, 4607.84], "text": " could do that."}, {"timestamp": [4607.84, 4616.92], "text": " Now to the point about AI, when you look at the fact that many American Congress people"}, {"timestamp": [4616.92, 4622.0], "text": " make a lot of money while they're in Congress on stock trades, I don't think any of them"}, {"timestamp": [4622.0, 4626.44], "text": " are going to sign on a deal that's going to hurt their stock trades."}, {"timestamp": [4626.44, 4631.32], "text": " Nancy Pelosi made something like $100 million in the last few years at Congress, mostly"}, {"timestamp": [4631.32, 4635.04], "text": " through very well-timed stock trades."}, {"timestamp": [4635.04, 4641.24], "text": " I suspect that Congress has been largely silent on this because they've already bought all"}, {"timestamp": [4641.24, 4651.14], "text": " the Microsoft and Google shares that they think that they're going to need to capitalize on this. And that's really cynical of me, but I'm just looking at recent history."}, {"timestamp": [4651.14, 4655.92], "text": " Nancy Wall Street Bets Pelosi made a lot of money from Wealth Times Stock Rates. Where"}, {"timestamp": [4655.92, 4661.6], "text": " did she get that information? So maybe I'm right, maybe I'm wrong, but I don't think"}, {"timestamp": [4661.6, 4668.56], "text": " that there's an economic incentive for the people in power to put on the brakes. And because of that, I don't think they're going"}, {"timestamp": [4668.56, 4673.78], "text": " to. But then from a geopolitical perspective, from a military perspective, there's also"}, {"timestamp": [4673.78, 4677.92], "text": " a disincentive to put on the brakes, as you mentioned earlier. So there's a lot of incentives"}, {"timestamp": [4677.92, 4682.44], "text": " aligned against putting on the brakes."}, {"timestamp": [4682.44, 4686.5], "text": " I don't think it's going to happen. I think it's inevitable. I mean this letter is going to do nothing."}, {"timestamp": [4686.5, 4691.0], "text": " Although I like the fact that it starts people talking about the dangers of AI."}, {"timestamp": [4691.0, 4693.0], "text": " That it has done."}, {"timestamp": [4693.0, 4700.0], "text": " So in that sense. So before we get onto alignment then, because that is the logical next thing, topic to move onto."}, {"timestamp": [4700.0, 4710.0], "text": " Let me just ask, what do you think the impact is going to be on countries like India and China, these countries that really benefited from having large, let's say cheaper labor forces?"}, {"timestamp": [4710.0, 4727.36], "text": " What's the political outcome going to be there? a demographic dividend. And so the rapid rise of population of places like China and India"}, {"timestamp": [4727.36, 4733.52], "text": " meant that for a period of time, they had far more people of working age than they did of children"}, {"timestamp": [4733.52, 4738.64], "text": " and retirement age. For China, they're already like past the tipping point, right? Because"}, {"timestamp": [4738.64, 4743.04], "text": " population is starting to go down, replacement rate is going down, India is still on the"}, {"timestamp": [4743.04, 4750.16], "text": " ascendancy, so they're still benefiting from that. However, just because they have the labor force doesn't mean that"}, {"timestamp": [4750.16, 4755.8], "text": " the labor force is being properly utilized. So in that respect, kind of going back to"}, {"timestamp": [4755.8, 4761.52], "text": " an earlier point, I think that AI will absolutely be used to fill in the gap, the labor gap,"}, {"timestamp": [4761.52, 4765.76], "text": " because what you're seeing in many countries, even if they have that"}, {"timestamp": [4765.76, 4770.88], "text": " demographic dividend, a lot of the people are leaving to go to other nations because"}, {"timestamp": [4770.88, 4772.92], "text": " the jobs just aren't there."}, {"timestamp": [4772.92, 4774.68], "text": " So it's kind of a wash."}, {"timestamp": [4774.68, 4781.6], "text": " And what I do hope is that AI will shore up labor gaps in the short term and then eventually"}, {"timestamp": [4781.6, 4789.6], "text": " replace human labor so that we're all then ideally liberated to pursue our hopes and dreams and whatever they happen to be."}, {"timestamp": [4789.6, 4792.92], "text": " I'd never thought about this, but I wonder what impact it's going to have on immigration,"}, {"timestamp": [4792.92, 4798.6], "text": " because as soon as people don't have any more value for people, then maybe we're going to"}, {"timestamp": [4798.6, 4803.02], "text": " put up our walls and maybe we're going to end up with a more segregated world."}, {"timestamp": [4803.02, 4812.24], "text": " That trend is starting, it's called de-globalization. And so you see a lot of nations implementing more protectionist policies,"}, {"timestamp": [4812.24, 4818.56], "text": " on-shoring a lot of labor, on-shoring a lot of stuff. So the fragility of globalization was"}, {"timestamp": [4818.56, 4824.64], "text": " revealed by the pandemic, where we're still having echoes of supply chain issues that have..."}, {"timestamp": [4829.28, 4834.78], "text": " where we're still having echoes of supply chain issues. There was a think piece on it, which was like, why can't you get bubble tea right now in America? And I was like, okay,"}, {"timestamp": [4834.78, 4838.34], "text": " look at all the hundreds of places that all the ingredients come from, and there's no"}, {"timestamp": [4838.34, 4844.82], "text": " real need for that. And so globalization does have some fragility, which was not just the"}, {"timestamp": [4844.82, 4845.88], "text": " pandemic, but the pandemic was the big natural experiment that said, this has some fragility, which was not just the pandemic, but the pandemic"}, {"timestamp": [4845.88, 4849.74], "text": " was the big natural experiment that said, this has some fragility."}, {"timestamp": [4849.74, 4854.74], "text": " Now geopolitical tensions are also rising because one of the policies of neoliberalism"}, {"timestamp": [4854.74, 4857.24], "text": " was to bring everyone to the table."}, {"timestamp": [4857.24, 4863.16], "text": " So in the late 90s and early 2000s, one of the policies was to bring Russia to the table."}, {"timestamp": [4863.16, 4865.64], "text": " You know, the USSR disintegrated, bring Russia to the table. You know, the USSR disintegrated,"}, {"timestamp": [4865.64, 4866.64], "text": " bring Russia to the table,"}, {"timestamp": [4866.64, 4870.28], "text": " help them liberalize through economic incentives."}, {"timestamp": [4870.28, 4871.36], "text": " Well, then we just still ended up"}, {"timestamp": [4871.36, 4872.54], "text": " with an authoritarian regime"}, {"timestamp": [4872.54, 4874.92], "text": " that's not really a democracy, right?"}, {"timestamp": [4874.92, 4877.8], "text": " The same principle was there in place for China,"}, {"timestamp": [4877.8, 4880.64], "text": " which was to bring China to the table"}, {"timestamp": [4880.64, 4882.88], "text": " and help them liberalize."}, {"timestamp": [4882.88, 4883.76], "text": " Same thing happened."}, {"timestamp": [4883.76, 4885.52], "text": " They still have a strong man at the top."}, {"timestamp": [4885.52, 4894.24], "text": " And so right now what I suspect is happening is that in the macroeconomic discussions and in the"}, {"timestamp": [4894.24, 4898.16], "text": " halls of power, they're realizing that this experiment of bringing people to the table"}, {"timestamp": [4898.88, 4906.88], "text": " didn't really work. So the idea of total global economic interdependence didn't work because despite"}, {"timestamp": [4906.88, 4911.12], "text": " Russia's interdependence on the rest of Europe and the rest of the world, they still went to war."}, {"timestamp": [4911.76, 4917.12], "text": " And then all the embargoes, all the restrictions are hurting the Russian economy, but they're still"}, {"timestamp": [4917.12, 4921.6], "text": " going to war. And so we're saying, okay, if Russia is still willing to do that because their leadership"}, {"timestamp": [4921.6, 4932.4], "text": " is crazy, maybe China is willing to do the same thing eventually. So there is a big push for deglobalization right now, which is, in the grand"}, {"timestamp": [4932.4, 4938.32], "text": " scheme of things, it's an unfortunate backslide, but I think it's a necessary pause. I do suspect"}, {"timestamp": [4938.32, 4946.72], "text": " that within a century or two, maybe three, we will be more towards a global world. But to your point, absolutely,"}, {"timestamp": [4946.72, 4950.46], "text": " people are going to be closing their borders in some form or another for the foreseeable"}, {"timestamp": [4950.46, 4957.6], "text": " future, I think. Let's change topic then to alignment. So I"}, {"timestamp": [4957.6, 4962.64], "text": " know this is something that you've been working on and that you're primarily interested in."}, {"timestamp": [4962.64, 4966.36], "text": " What is alignment and what's the state of the art?"}, {"timestamp": [4966.36, 4973.04], "text": " So alignment, there's two overarching categories of alignment when it comes to AI. So there's"}, {"timestamp": [4973.04, 4978.32], "text": " inner alignment, which is the question of whether or not the AI model you're using aligns"}, {"timestamp": [4978.32, 4987.38], "text": " mathematically aligns to the problem you think it's trying to solve. So some famous examples, probably the best"}, {"timestamp": [4987.38, 4992.5], "text": " example actually was in the earlier days, you know, three years ago, four years ago"}, {"timestamp": [4992.5, 5000.18], "text": " when image technology was on the ascendancy. I think it was Google published a paper where"}, {"timestamp": [5000.18, 5008.7], "text": " they had trained an image model to differentiate between dogs and wolves. And it was very successful."}, {"timestamp": [5008.7, 5015.38], "text": " But then they actually took apart the black box and they said, okay, what features is"}, {"timestamp": [5015.38, 5019.58], "text": " it actually looking for to determine if it's a dog or a wolf? It had nothing to do with"}, {"timestamp": [5019.58, 5031.16], "text": " the animal. It was looking at the environment. Because pictures of wolves take place in timberland and temperate forests, and pictures of dogs take place in grassy yards. And so it learned"}, {"timestamp": [5031.16, 5034.96], "text": " to do the job that you thought, but not the way that you thought. So that is an inner"}, {"timestamp": [5034.96, 5039.36], "text": " alignment problem. Then the other side of that is outer alignment,"}, {"timestamp": [5039.36, 5045.68], "text": " which is, does the AI actually align to the needs and wants of humans or the rest of the planet?"}, {"timestamp": [5045.68, 5050.8], "text": " There's some debate over the scope of outer alignment because right now outer alignment,"}, {"timestamp": [5050.8, 5055.92], "text": " for instance, reinforcement learning with human feedback, aligns the model to what humans want,"}, {"timestamp": [5056.48, 5061.92], "text": " not what humans need. It also doesn't align to what the rest of the world needs, right?"}, {"timestamp": [5061.92, 5065.88], "text": " It only aligns to the people using the model, what they want."}, {"timestamp": [5065.88, 5070.44], "text": " And so for me personally, I advocate that outer alignment should include all living"}, {"timestamp": [5070.44, 5075.28], "text": " things on the entire planet because we're all stakeholders, right? Whether you're using"}, {"timestamp": [5075.28, 5081.8], "text": " the AI or not, you are a stakeholder in what happens with AI. So those are the two primary"}, {"timestamp": [5081.8, 5086.12], "text": " scopes or topics within alignment. There's a few other parts, of course, there's"}, {"timestamp": [5086.12, 5091.28], "text": " policy, there's law, there's regulation, there's the economic aspect, but those kind of fall"}, {"timestamp": [5091.28, 5097.28], "text": " outside of if you're talking explicitly about alignment, those are more policy decisions,"}, {"timestamp": [5097.28, 5102.32], "text": " I guess you could say. So if we look at outer alignment specifically,"}, {"timestamp": [5102.32, 5106.8], "text": " is it possible long-term for a less intelligent system to control"}, {"timestamp": [5106.8, 5110.12], "text": " the alignment of a more intelligent system?"}, {"timestamp": [5110.12, 5120.1], "text": " Personally, I don't think so. In many thought experiments, certainly, you know, explorations"}, {"timestamp": [5120.1, 5125.28], "text": " in fiction and what some people raising the alarm are saying is, like, it's inevitable"}, {"timestamp": [5125.28, 5129.44], "text": " we're going to lose control. One way or another, whether it's today, tomorrow, next year,"}, {"timestamp": [5130.4, 5136.08], "text": " that if something is that much smarter than you, then there's no way to outthink it. There's no way"}, {"timestamp": [5136.08, 5140.48], "text": " to outmaneuver it or outstrategize it. So then the question is, that leads to what's called the"}, {"timestamp": [5140.48, 5149.48], "text": " control problem. So the control problem is, if you make the assumption that you end up with a super intelligent machine, how do you keep control of it? And that is"}, {"timestamp": [5149.48, 5154.24], "text": " very much an open question. My solution is you don't. You don't because you can't. So"}, {"timestamp": [5154.24, 5162.24], "text": " you instead you create a system of incentives and rules and attractor states that cause"}, {"timestamp": [5162.24, 5166.0], "text": " the thing to self-align to desirable end goals."}, {"timestamp": [5166.0, 5168.72], "text": " Mason This is your heuristic imperatives."}, {"timestamp": [5168.72, 5170.08], "text": " Jason Yeah."}, {"timestamp": [5170.08, 5174.64], "text": " Mason Can you describe what these are and maybe give some good examples?"}, {"timestamp": [5174.64, 5187.76], "text": " Jason Yeah. So, the most famous example is Isaac Asimov's Three Laws of Robotics, which was explored in fiction."}, {"timestamp": [5187.76, 5192.08], "text": " It's three rules which basically says the robot must do what humans say, can't hurt"}, {"timestamp": [5192.08, 5195.28], "text": " humans and can't allow humans to be hurt through inaction."}, {"timestamp": [5195.28, 5197.48], "text": " So he was thinking of embodied robots."}, {"timestamp": [5197.48, 5201.0], "text": " He was not thinking about networked intelligence because networked intelligence hadn't even"}, {"timestamp": [5201.0, 5203.98], "text": " been thought of yet when he was working on this idea."}, {"timestamp": [5203.98, 5211.04], "text": " So the idea was that you create a system of rules that the machine must abide by that makes it safe. The three laws"}, {"timestamp": [5211.04, 5218.88], "text": " of robotics are completely useless today. So instead, what I did in my research was I said,"}, {"timestamp": [5218.88, 5225.12], "text": " okay, if we start with the assumption that a machine is going to be, one, more intelligent than us, and two,"}, {"timestamp": [5227.12, 5234.32], "text": " uncontrollable, how then do we build that machine so that it remains not hostile or benevolent?"}, {"timestamp": [5235.52, 5240.4], "text": " And so what I came up with was a different set of rules or intrinsic motivations or design"}, {"timestamp": [5240.4, 5248.32], "text": " principles. I initially called them core objective functions, but that's kind of a misnomer because an objective function is a mathematical thing that you're trying to optimize."}, {"timestamp": [5248.32, 5254.96], "text": " But a heuristic imperative is rather a set of principles that it must do or an intrinsic"}, {"timestamp": [5254.96, 5259.68], "text": " motivation, right? You and I have intrinsic motivations to eat and breathe and sleep and"}, {"timestamp": [5259.68, 5265.44], "text": " stuff like that. That is part of how we are physically built as biological entities. We have a few things that"}, {"timestamp": [5265.44, 5273.68], "text": " we must do. And we learn to satisfy those needs over time. Your sense of hunger drives you to eat,"}, {"timestamp": [5273.68, 5277.76], "text": " your need for shelter drives you to get a job so that you can pay for a house, so on and so forth."}, {"timestamp": [5277.76, 5282.48], "text": " So those are heuristics that we develop over time. And so for the sake of this conversation,"}, {"timestamp": [5282.48, 5285.76], "text": " a heuristic is an intuition or an instinct that you"}, {"timestamp": [5285.76, 5290.88], "text": " develop over time through experience. And then the imperative is the drive, the intrinsic motivation."}, {"timestamp": [5291.68, 5295.76], "text": " Now, one thing that people talk about is instrumental convergence. So,"}, {"timestamp": [5295.76, 5301.44], "text": " instrumental convergence is the idea that any AI system that is sufficiently advanced,"}, {"timestamp": [5302.64, 5306.92], "text": " no matter what other objectives you give it, will eventually consolidate"}, {"timestamp": [5306.92, 5312.76], "text": " around a few basic imperatives. Like it will always need power, it will always want more"}, {"timestamp": [5312.76, 5315.72], "text": " compute resources, more data, that sort of thing."}, {"timestamp": [5315.72, 5322.68], "text": " Now that being said, what I propose is that if in the software and architecture and networking"}, {"timestamp": [5322.68, 5329.6], "text": " around these machines, if you give them these heuristic imperatives, which I define them as three heuristic imperatives, reduce suffering in"}, {"timestamp": [5329.6, 5334.0], "text": " the universe, increase prosperity in the universe and increase understanding in the universe."}, {"timestamp": [5334.0, 5346.0], "text": " If you design a machine with these three drives at various levels, one, this will have a beneficial impact on people that it interacts with and that it"}, {"timestamp": [5346.0, 5347.0], "text": " helps."}, {"timestamp": [5347.0, 5355.32], "text": " Two, over time or eventually, the machine will choose to adhere to those objectives."}, {"timestamp": [5355.32, 5360.16], "text": " And three, if multiple machines abide by those objectives, then that will ultimately become"}, {"timestamp": [5360.16, 5364.92], "text": " a Nash equilibrium where all machines choose to abide by those objectives because any who"}, {"timestamp": [5364.92, 5368.68], "text": " steps out of line will get shut down. So there's a lot to unpack there. Let me know"}, {"timestamp": [5368.68, 5371.18], "text": " which direction you want to go."}, {"timestamp": [5371.18, 5374.56], "text": " It sounds like one of the key components then is that you really rely on the fact that there"}, {"timestamp": [5374.56, 5378.96], "text": " are, it's not going to be one Skynet, right? We're going to have a whole, it's going to"}, {"timestamp": [5378.96, 5385.44], "text": " be a power game between these and winning in that game will mean not killing us, essentially."}, {"timestamp": [5385.44, 5391.32], "text": " Right. That's kind of the equilibrium, you know, the modern equivalent of mutually assured"}, {"timestamp": [5391.32, 5396.84], "text": " destruction that hopefully will result in a win-win scenario. Because from a narrative"}, {"timestamp": [5396.84, 5401.36], "text": " perspective, it's easy to tell a story with Skynet where it's just one big bad, or in"}, {"timestamp": [5401.36, 5406.56], "text": " the Matrix where it's, you know, just the matrix. But in reality, we're going to have many"}, {"timestamp": [5406.56, 5412.16], "text": " nations and nation states with their own AIs, many corporations with their own AIs, and even people"}, {"timestamp": [5412.16, 5417.76], "text": " with their own AIs. All of varying levels of power, sophistication, compute access,"}, {"timestamp": [5418.64, 5426.72], "text": " but they're going to be effectively in a competitive environment environment. Let's say, for instance, in a few years' time,"}, {"timestamp": [5426.72, 5433.4], "text": " you and I both have a fleet of, let's say, 100 AIs that are running on our computer,"}, {"timestamp": [5433.4, 5439.4], "text": " our phone. Their entire purpose is just to help serve us. But then everyone has that."}, {"timestamp": [5439.4, 5444.0], "text": " Every corporation has thousands or millions of AIs helping them. Every governmental agency"}, {"timestamp": [5444.0, 5445.08], "text": " has the same thing. Every agency has the same thing."}, {"timestamp": [5445.08, 5450.52], "text": " Every military has the same thing. So we're looking at a competitive scenario where we"}, {"timestamp": [5450.52, 5456.8], "text": " have literally billions or trillions of autonomous or semi-autonomous agents out there. And so"}, {"timestamp": [5456.8, 5460.64], "text": " it's not a matter of like one's going to take over everything. We're not going to have Ultron."}, {"timestamp": [5460.64, 5465.32], "text": " Ultron is almost certainly not going to happen. Skynet is almost certainly not going to happen."}, {"timestamp": [5465.32, 5470.2], "text": " But with the heuristic comparatives, if we imbue that in each of these agents or enough"}, {"timestamp": [5470.2, 5476.92], "text": " of the agents that they all ultimately agree this is the way to go, then through that consensus,"}, {"timestamp": [5476.92, 5482.6], "text": " through that global consensus, then you'll end up with a self-sustaining equilibrium"}, {"timestamp": [5482.6, 5485.64], "text": " where everyone says, this is the optimal solution,"}, {"timestamp": [5485.64, 5490.2], "text": " let's stick with it and let's choose to stick with it over time, no matter how autonomous"}, {"timestamp": [5490.2, 5494.56], "text": " or smart the AI agents get, because they're never going to be alone."}, {"timestamp": [5494.56, 5499.12], "text": " When it comes to your choice of heuristic imperative, why not decide on something that"}, {"timestamp": [5499.12, 5503.28], "text": " aligns with what humans do anyway? I mean, the heuristic imperatives that you chose are"}, {"timestamp": [5503.28, 5508.08], "text": " very different to the way that humans function as far as I understand. So why not choose something like"}, {"timestamp": [5508.08, 5513.28], "text": " increase the amount of human DNA on the planet as, you know, the same thing that we do? What goes"}, {"timestamp": [5513.28, 5519.12], "text": " wrong there? Yeah, so Richard Dawkins actually proposed that, I think back in the 70s with his"}, {"timestamp": [5519.12, 5525.48], "text": " book, The Selfish Gene. So basically the heuristic imperative or objective function of life is to maximize"}, {"timestamp": [5525.48, 5530.7], "text": " the amount of DNA in the universe. From a functional standpoint, from a chemistry standpoint,"}, {"timestamp": [5530.7, 5537.92], "text": " that's what life does. And DNA, if the RNA hypothesis is correct, then RNA delegated"}, {"timestamp": [5537.92, 5543.2], "text": " to DNA because DNA is more stable, and then DNA delegated to organelles and cells and"}, {"timestamp": [5543.2, 5545.22], "text": " eventually created organisms just because"}, {"timestamp": [5545.22, 5549.38], "text": " those were better vehicles for magnifying DNA. So that's the idea of the selfish gene,"}, {"timestamp": [5549.38, 5555.44], "text": " that genes are there just to replicate themselves. Okay, cool. Well, if you give that objective"}, {"timestamp": [5555.44, 5561.6], "text": " to an AI, chances are it'll turn us all into soup and just replicate organic soup infinitely,"}, {"timestamp": [5561.6, 5569.36], "text": " which is not exactly what you want, right? Because human from it, from an AI perspective, a human body might not be the best way to magnify DNA in"}, {"timestamp": [5569.36, 5574.12], "text": " the universe. It might be better to create gray goo and shoot enzymes into other planets"}, {"timestamp": [5574.12, 5581.92], "text": " and magnify the amount of DNA that way. So essentially you and I are the unintended consequence"}, {"timestamp": [5581.92, 5587.2], "text": " of genetics over, you know, literally 4 billion years, 3.8 billion years."}, {"timestamp": [5588.0, 5592.8], "text": " So we have an opportunity to be a little bit more deliberate with the objectives that we give"}, {"timestamp": [5594.64, 5601.12], "text": " our machines. And what we've been able to do is observe all life. We have science, we have"}, {"timestamp": [5601.12, 5610.44], "text": " biology, we have psychology, neuroscience. And so the heuristic imperatives that I came up with are not an accident."}, {"timestamp": [5610.44, 5614.88], "text": " So all living things, these are things that the first two heuristic imperatives are things"}, {"timestamp": [5614.88, 5617.6], "text": " that all living organisms have in common."}, {"timestamp": [5617.6, 5620.16], "text": " And that is that we respond to stimuli."}, {"timestamp": [5620.16, 5626.48], "text": " So we respond to negative stimuli by either protecting ourself or evading the negative stimuli."}, {"timestamp": [5626.48, 5629.36], "text": " So if you put your hand on a stove, it's hot, it burns you,"}, {"timestamp": [5629.36, 5631.04], "text": " you pull your hand back, right?"}, {"timestamp": [5631.04, 5632.28], "text": " That's a negative stimuli."}, {"timestamp": [5632.28, 5634.56], "text": " If you're hungry, that is a negative stimuli"}, {"timestamp": [5634.56, 5635.6], "text": " and you go find food."}, {"timestamp": [5636.96, 5640.32], "text": " And we move away from negative stimuli"}, {"timestamp": [5640.32, 5642.84], "text": " and towards positive stimuli, right?"}, {"timestamp": [5642.84, 5646.08], "text": " So if you're lonely, you go towards people, right?"}, {"timestamp": [5646.08, 5647.48], "text": " And so it's a gradient."}, {"timestamp": [5647.48, 5649.08], "text": " So the first two heuristic imperatives"}, {"timestamp": [5649.08, 5652.76], "text": " form a relatively simple gradient"}, {"timestamp": [5652.76, 5655.24], "text": " that all life forms abide by,"}, {"timestamp": [5655.24, 5656.92], "text": " which is why I define them the way that I did."}, {"timestamp": [5656.92, 5660.12], "text": " And it took a couple of years to articulate them that way,"}, {"timestamp": [5660.12, 5663.36], "text": " because prosperity typically means wealth."}, {"timestamp": [5663.36, 5664.64], "text": " That's what most people think."}, {"timestamp": [5664.64, 5665.44], "text": " But the root word of"}, {"timestamp": [5665.44, 5671.92], "text": " prosperity is prosperitas, which is Latin for to live well, to thrive. And so all living things,"}, {"timestamp": [5671.92, 5677.68], "text": " no matter if it's a single celled organism or a sequoia tree, the largest organism on the planet,"}, {"timestamp": [5678.96, 5682.88], "text": " all living things move away from suffering and towards prosperity."}, {"timestamp": [5682.88, 5686.4], "text": " And so by identifying the universal principles of life"}, {"timestamp": [5686.4, 5693.92], "text": " and giving those to our AI, we therefore solve outer alignment by permanently aligning AI with"}, {"timestamp": [5693.92, 5699.12], "text": " the needs of all living things. Then the third objective function, or puristic imperative,"}, {"timestamp": [5699.12, 5710.56], "text": " to increase understanding is a curiosity function. Actually, I'm really grateful because Elon Musk just said, I think it was on, what, 60 Minutes? The 60 Minutes interview where he said, or"}, {"timestamp": [5710.56, 5715.68], "text": " no, no, it was Tucker Carlson of all things. It was on Fox News. He told Tucker Carlson"}, {"timestamp": [5715.68, 5721.32], "text": " that we should give AI a sense of curiosity because if it wants to understand the world,"}, {"timestamp": [5721.32, 5727.6], "text": " that includes us. So if it wants to understand and it wants to teach us, understanding"}, {"timestamp": [5727.6, 5732.52], "text": " or knowledge increase or curiosity is another function that will offset because if you're"}, {"timestamp": [5732.52, 5737.6], "text": " curious, you're not likely to be destructive. Curiosity on its own can be very destructive."}, {"timestamp": [5737.6, 5741.6], "text": " So for instance, we have scientific, like ethical review boards to make sure that you're"}, {"timestamp": [5741.6, 5749.28], "text": " not doing unethical experiments. So unbridled curiosity is bad, which is why you need to counterbalance that with suffering and prosperity, which is why we"}, {"timestamp": [5749.28, 5754.72], "text": " have debates over like, can you vivisect animals? And we generally say, no, you shouldn't vivisect"}, {"timestamp": [5754.72, 5761.68], "text": " animals today because that hurts them, that sort of thing. So that's a high level overview."}, {"timestamp": [5761.84, 5763.84], "text": " a high-level overview."}, {"timestamp": [5772.16, 5773.0], "text": " It's a I'd never you said the word opportunity in there, right? So for all of human history, we've never been able to hard code"}, {"timestamp": [5786.16, 5787.76], "text": " heuristic imperatives. So actually, is this somehow like rather than the doom and gloom and we're all going to kill ourselves, is this actually could this be the first time in human history that we actually make sure we don't kill ourselves?"}, {"timestamp": [5794.08, 5801.28], "text": " Um, it's an opportunity. Yes. So from a, from an op optimistic standpoint, it is a huge opportunity from an existential perspective. One thing that, that, uh, people might be familiar"}, {"timestamp": [5801.28, 5806.4], "text": " with is the Fermi paradox, which is the universe is so. The universe is so big, there's so"}, {"timestamp": [5806.4, 5811.92], "text": " many planets out there, where's all the life? So one possible solution or explanation is"}, {"timestamp": [5811.92, 5817.86], "text": " that there are great filter events. So a great filter event is something that inevitably"}, {"timestamp": [5817.86, 5823.64], "text": " or usually happens in the development of a species and civilization in which it dies."}, {"timestamp": [5823.64, 5826.84], "text": " So one great filter could be asteroid impact, right?"}, {"timestamp": [5826.84, 5829.16], "text": " Killed off the dinosaurs, could have killed us off,"}, {"timestamp": [5829.16, 5831.52], "text": " but we've dodged a bullet so far, so to speak."}, {"timestamp": [5832.64, 5835.2], "text": " Then other great filter events could be stuff"}, {"timestamp": [5835.2, 5836.96], "text": " like the development of nuclear weapons."}, {"timestamp": [5836.96, 5838.8], "text": " Nuclear weapons work here on Earth,"}, {"timestamp": [5838.8, 5840.48], "text": " so it's entirely possible that nuclear weapons"}, {"timestamp": [5840.48, 5842.08], "text": " would work on any other planet"}, {"timestamp": [5842.08, 5844.88], "text": " where advanced civilizations might evolve"}, {"timestamp": [5844.88, 5848.8], "text": " and they might nuke themselves out of existence. So far, we haven't done that, knock on wood."}, {"timestamp": [5849.44, 5857.28], "text": " Now, the development of AI might be another great filter event where, let's say, civilization on"}, {"timestamp": [5857.28, 5862.4], "text": " Proxima Centauri, they get past nukes, but then they develop AI and it becomes Ultron and Ultron"}, {"timestamp": [5862.4, 5869.44], "text": " decides to wipe them all out because they're a scourge or whatever. Whatever the AI ultimately reasons. That could be a reason that"}, {"timestamp": [5870.24, 5875.28], "text": " the cosmos is quiet. But also when you consider that life has been on Earth for literally a third"}, {"timestamp": [5875.28, 5880.56], "text": " of the existence of the entire universe, it could also just be that we're the first civilization."}, {"timestamp": [5880.56, 5885.0], "text": " Because the universe is still relatively young compared to how long it's going to last."}, {"timestamp": [5885.0, 5891.5], "text": " Anyways, point being is, yes, it is both an opportunity, but that opportunity comes with"}, {"timestamp": [5891.5, 5895.6], "text": " potentially huge, like, huge risk."}, {"timestamp": [5895.6, 5898.56], "text": " So you know, it can kind of go either way."}, {"timestamp": [5898.56, 5907.6], "text": " Do you think having a more intelligent AI will actually help us because you know in the example you gave of a DNA,"}, {"timestamp": [5907.68, 5922.48], "text": " do you maximizer or whatever you want to call it see there the AI was smart enough to maximize the goo but dumb enough to realize not to realize that actually it's better off if he doesn't kill humans."}, {"timestamp": [5922.32, 5924.32], "text": " it's better off if it doesn't kill all humans."}, {"timestamp": [5931.68, 5935.36], "text": " So might we be in a situation where we don't have to worry because a more intelligent AI would have a better grasp of the heuristic imperatives and how it should interpret them?"}, {"timestamp": [5936.32, 5940.64], "text": " Yeah, absolutely. I've said for many years that I'm not afraid of super intelligent AI,"}, {"timestamp": [5940.64, 5946.12], "text": " I'm afraid of AI that's just smart enough to pull the trigger. If all I"}, {"timestamp": [5946.12, 5950.52], "text": " can do is optimize for a really simple objective function, then it's more likely to be dangerous"}, {"timestamp": [5950.52, 5958.92], "text": " because it will misunderstand it. In my experiments, the importance of wording is really critical."}, {"timestamp": [5958.92, 5963.84], "text": " Anyone who studies law or rhetoric or logic will understand that the value of wording"}, {"timestamp": [5963.84, 5965.92], "text": " something correctly is absolutely"}, {"timestamp": [5965.92, 5970.92], "text": " critical so that it is interpreted correctly, which is why I've worded the heuristic imperatives"}, {"timestamp": [5970.92, 5976.48], "text": " the way that I did. And I didn't just come up with it without any experiments. I actually"}, {"timestamp": [5976.48, 5988.92], "text": " have done hundreds and hundreds of experiments on GPT-2, 3, and now 4. So I've used every big model provided by OpenAI over the last, what, 4"}, {"timestamp": [5988.92, 5995.02], "text": " years. And I settled on that wording because this is the wording that is very reliably"}, {"timestamp": [5995.02, 5999.28], "text": " interpreted correctly. And what I mean by interpreted, I don't mean from a mathematical"}, {"timestamp": [5999.28, 6006.28], "text": " standpoint. I mean the spirit of it is interpreted correctly. The essence, the intention is interpreted"}, {"timestamp": [6006.28, 6011.76], "text": " correctly. And this is even before the models were aligned. So by aligned, I mean that like"}, {"timestamp": [6011.76, 6017.08], "text": " chat GPT is already pretty well aligned to avoid causing harm, but the original version"}, {"timestamp": [6017.08, 6022.88], "text": " of GPT-3 had no alignment. And this wording was still correctly interpreted by GPT-3 before"}, {"timestamp": [6022.88, 6030.88], "text": " it had any fine tuning. And so with the correct definition, with the correct constitution or set of words to say, this"}, {"timestamp": [6030.88, 6037.0], "text": " is the goal that can then be interpreted by any sufficiently advanced language model to"}, {"timestamp": [6037.0, 6042.24], "text": " arrive at the correct understanding, the correct sentiment, that is the way to go. And that's"}, {"timestamp": [6042.24, 6046.6], "text": " why, again, the importance of wording is very critical here."}, {"timestamp": [6046.6, 6048.4], "text": " But so, well, actually I'm curious"}, {"timestamp": [6048.4, 6051.64], "text": " about how you did the experiments, but let me ask,"}, {"timestamp": [6052.56, 6054.44], "text": " is this just through prompt engineering"}, {"timestamp": [6054.44, 6057.36], "text": " or when you embed heuristic imperative,"}, {"timestamp": [6057.36, 6060.04], "text": " do you do some training, some mild training"}, {"timestamp": [6060.04, 6062.48], "text": " or what does that actually look like?"}, {"timestamp": [6062.48, 6064.68], "text": " Yeah, so it starts with just prompt engineering."}, {"timestamp": [6064.68, 6068.16], "text": " So for any viewers who are not familiar, prompt engineering is how do you write"}, {"timestamp": [6068.16, 6073.44], "text": " the instructions that the language model follows. And so writing instructions is called prompt"}, {"timestamp": [6073.44, 6078.0], "text": " engineering, which is the art of using natural language to get the correct result out of a"}, {"timestamp": [6078.0, 6083.44], "text": " machine. So that's step one. Step two is what's called fine tuning, which is that's the same"}, {"timestamp": [6083.44, 6090.0], "text": " technique that's used for reinforcement learning with human feedback, where you develop a secondary data set that"}, {"timestamp": [6090.0, 6095.36], "text": " is used to fine tune the model to align to a very specific goal or task."}, {"timestamp": [6095.36, 6101.04], "text": " And so some of my earlier, excuse me, earlier experiments with fine tuning, it's very, very"}, {"timestamp": [6101.04, 6105.68], "text": " easy to get a foundation model of a plain vanilla untrained model,"}, {"timestamp": [6105.68, 6108.5], "text": " to understand the spirit of the heuristic imperatives"}, {"timestamp": [6108.5, 6109.8], "text": " pretty easily."}, {"timestamp": [6109.8, 6112.16], "text": " Whether it's to brainstorm ways,"}, {"timestamp": [6112.16, 6114.68], "text": " like you give it a scenario and it says,"}, {"timestamp": [6114.68, 6117.72], "text": " okay, here's a list of potential actions that you can do"}, {"timestamp": [6117.72, 6119.8], "text": " that will meet the heuristic imperatives"}, {"timestamp": [6119.8, 6121.8], "text": " of reduce suffering, increase prosperity,"}, {"timestamp": [6121.8, 6123.22], "text": " and increase understanding."}, {"timestamp": [6123.22, 6124.62], "text": " So that's fine tuning."}, {"timestamp": [6124.62, 6129.14], "text": " And I actually just launched a project, a research project, where we're going to try"}, {"timestamp": [6129.14, 6133.78], "text": " and instead of doing reinforcement learning with human feedback, we're going to do a project"}, {"timestamp": [6133.78, 6136.2], "text": " called reinforcement learning with heuristic imperatives."}, {"timestamp": [6136.2, 6140.64], "text": " So what we're going to try and do is set up an automated framework that will create a"}, {"timestamp": [6140.64, 6146.6], "text": " training flywheel, a data flywheel, to automatically get better at the heuristic imperatives over"}, {"timestamp": [6146.6, 6154.08], "text": " time. So, you know, the ground level is prompt engineering. Floor 2 is fine tuning. Floor"}, {"timestamp": [6154.08, 6158.8], "text": " 3 is automated reinforcement learning. But that's still all just inner alignment. That's"}, {"timestamp": [6158.8, 6166.64], "text": " how you get one model to cooperate. When you look at the broader ecosystem, there's a few other steps. So one"}, {"timestamp": [6166.64, 6172.8], "text": " thing that I'm just starting to work on is how do you use these heuristic imperatives"}, {"timestamp": [6172.8, 6179.56], "text": " as a gating or consensus mechanism for blockchain technology, for decentralized autonomous organizations?"}, {"timestamp": [6179.56, 6183.16], "text": " Because in this case, what you can do is you have a trustless environment where you have"}, {"timestamp": [6183.16, 6191.04], "text": " no idea how an AI agent is programmed or how an AI agent is aligned, but you need to be able to collaborate"}, {"timestamp": [6191.04, 6193.68], "text": " with an arbitrary number of these autonomous agents."}, {"timestamp": [6193.68, 6197.44], "text": " As we mentioned earlier, we should expect there to be millions, billions, or trillions"}, {"timestamp": [6197.44, 6200.44], "text": " of them soon."}, {"timestamp": [6200.44, 6203.84], "text": " Blockchain technology uses consensus."}, {"timestamp": [6203.84, 6206.44], "text": " Consensus is presently some of the algorithms"}, {"timestamp": [6206.44, 6207.92], "text": " are like proof of stake, proof of work,"}, {"timestamp": [6207.92, 6211.12], "text": " that sort of thing, which is all very algorithmic."}, {"timestamp": [6211.12, 6213.92], "text": " But as language technology develops"}, {"timestamp": [6213.92, 6216.4], "text": " and becomes embedded in blockchain technology,"}, {"timestamp": [6216.4, 6219.9], "text": " what I hope to achieve is that the heuristic imperatives"}, {"timestamp": [6219.9, 6221.76], "text": " will be embedded in the DAO,"}, {"timestamp": [6221.76, 6227.0], "text": " in the decentralized autonomous organizations as a consensus mechanism."}, {"timestamp": [6227.0, 6233.36], "text": " And so in this case, regardless of how the autonomous agents are trained, only the decisions"}, {"timestamp": [6233.36, 6237.88], "text": " that abide by the heuristic imperatives will be accepted by the blockchain."}, {"timestamp": [6237.88, 6243.48], "text": " And so that creates a communication layer above those autonomous agents that will hopefully"}, {"timestamp": [6243.48, 6246.04], "text": " align the aggregate behavior of"}, {"timestamp": [6246.04, 6249.64], "text": " those agents regardless of how the agents are themselves aligned."}, {"timestamp": [6249.64, 6250.64], "text": " So that's the second approach."}, {"timestamp": [6250.64, 6256.7], "text": " And then the third approach is architectural designs, which is, so for instance, we mentioned"}, {"timestamp": [6256.7, 6261.4], "text": " earlier cognitive architectures, which is basically where you create, instead of having"}, {"timestamp": [6261.4, 6265.48], "text": " one model is the AI, where you have a more sophisticated software"}, {"timestamp": [6265.48, 6270.5], "text": " architecture that has separate memories and processing and this, that, and the other different"}, {"timestamp": [6270.5, 6276.96], "text": " components that all plug together. And so, with a cognitive architecture, you can have"}, {"timestamp": [6276.96, 6281.16], "text": " a moral module, right? Like if you remember data from Star Trek, he says he has ethical"}, {"timestamp": [6281.16, 6289.92], "text": " subroutines, right? So, we're're creating an ethical subroutines module for cognitive architectures that abides by the heuristic imperatives. There's numerous ways"}, {"timestamp": [6289.92, 6295.96], "text": " you can embed heuristic imperatives in a cognitive architecture. Down to task prioritization."}, {"timestamp": [6295.96, 6300.92], "text": " Which tasks do you choose to do and in what order? How you shape those tasks. That can"}, {"timestamp": [6300.92, 6307.08], "text": " also be guided by heuristic imperatives So those are the three primary avenues that we're exploring right now."}, {"timestamp": [6307.08, 6312.7], "text": " So that's the prompt engineering and fine tuning, that's the network level, so blockchain"}, {"timestamp": [6312.7, 6317.08], "text": " and decentralization, and then finally cognitive architectures, which is how do you design"}, {"timestamp": [6317.08, 6321.92], "text": " a fully autonomous AI entity in the first place."}, {"timestamp": [6321.92, 6326.66], "text": " Step three sounds a little bit like giving it religion, but I want to ask you about the second"}, {"timestamp": [6326.66, 6328.5], "text": " because I don't think I quite understand."}, {"timestamp": [6328.5, 6332.3], "text": " So what's the back reaction of the blockchain"}, {"timestamp": [6332.3, 6334.1], "text": " on the individual agents?"}, {"timestamp": [6334.1, 6335.62], "text": " How does that work?"}, {"timestamp": [6336.78, 6339.74], "text": " So in for blockchain technology,"}, {"timestamp": [6339.74, 6344.74], "text": " there's a concept in cybersecurity"}, {"timestamp": [6349.2, 6354.96], "text": " called Byzantine generals. So the Byzantine generals problem is you have an arbitrary number of actors that are trying to coordinate or cooperate"}, {"timestamp": [6354.96, 6359.68], "text": " or communicate, but you have no idea what the alignment of those actors are. You have"}, {"timestamp": [6359.68, 6372.38], "text": " no idea who is allegiant to who or what their motivations are. And so what the Byzantine generals problem is, is, okay, if you have X number of aligned"}, {"timestamp": [6372.38, 6377.4], "text": " actors and Y number of unaligned or malicious actors, and you have no idea who's who, how"}, {"timestamp": [6377.4, 6380.72], "text": " do you algorithmically determine what the correct thing to do is?"}, {"timestamp": [6380.72, 6388.58], "text": " So this is a problem that is solved by current blockchain research, which is basically if"}, {"timestamp": [6388.58, 6395.92], "text": " you have 51 out of 100 nodes are good, right?"}, {"timestamp": [6395.92, 6397.48], "text": " They're benevolent actors."}, {"timestamp": [6397.48, 6398.76], "text": " They're aligned."}, {"timestamp": [6398.76, 6399.76], "text": " They're not hostile."}, {"timestamp": [6399.76, 6400.76], "text": " They're not compromised."}, {"timestamp": [6400.76, 6405.36], "text": " And so, for an actor to be compromised, it doesn't even have to be malicious. It can"}, {"timestamp": [6405.36, 6415.0], "text": " just be broken. It can just be faulty. If at least n plus one is aligned and functional,"}, {"timestamp": [6415.0, 6420.24], "text": " then with the current blockchain algorithms, the correct information will be accepted to"}, {"timestamp": [6420.24, 6425.6], "text": " the blockchain. In this case, if most of the nodes, most of the participants in the blockchain. And so in this case, if most of the nodes, most of the participants in the"}, {"timestamp": [6425.6, 6431.84], "text": " blockchain are aligned, then they will only accept information that they all agree is aligned to the"}, {"timestamp": [6431.84, 6436.96], "text": " heuristic imperatives. I see. I understand. So currently we're doing this financially,"}, {"timestamp": [6436.96, 6441.52], "text": " we're saying currently the majority of the people agree that this transaction took place."}, {"timestamp": [6441.52, 6445.2], "text": " You're saying take that technology and embed that in the way"}, {"timestamp": [6445.2, 6449.84], "text": " that we understand the heuristic imperatives, that agent is actually... Okay, yeah, that makes a lot"}, {"timestamp": [6449.84, 6457.92], "text": " of sense. So we have a huge distributed control over our AIs. But this might lead to massive"}, {"timestamp": [6457.92, 6468.64], "text": " slowdown in power consumption increases, right? With current blockchain technology, it's very slow. So a whole blockchain"}, {"timestamp": [6468.64, 6474.0], "text": " can only do one operation at a time per cycle and then you have to wait for consensus. And"}, {"timestamp": [6474.0, 6480.96], "text": " they can actually be very, very power hungry. So now, that being said, having slower consensus"}, {"timestamp": [6480.96, 6486.0], "text": " might actually be a good thing, regardless of how much power it consumes."}, {"timestamp": [6486.0, 6491.16], "text": " Because we can optimize the power consumption over time. But the speed that it makes decisions"}, {"timestamp": [6491.16, 6497.2], "text": " are actually probably going to be more important because it takes time to arrive at consensus."}, {"timestamp": [6497.2, 6500.56], "text": " Especially if you're doing language operations. Because right now it's all mathematical. It's"}, {"timestamp": [6500.56, 6508.82], "text": " all cryptographic operations, which are relatively fast. But when we get to the point where you have hundreds, thousands, millions of instances"}, {"timestamp": [6508.82, 6514.58], "text": " of GPT-3, GPT-4, GPT-5, all chewing away and trying to come to consensus, that's going"}, {"timestamp": [6514.58, 6519.18], "text": " to be a very power hungry system, but it's also going to be slow, which means that hopefully"}, {"timestamp": [6519.18, 6525.88], "text": " over time it will come to consensus, automatically come to consensus on the optimal decisions and"}, {"timestamp": [6525.88, 6530.8], "text": " actions that align with those objectives. So the slowness could actually be a feature,"}, {"timestamp": [6530.8, 6534.96], "text": " not a bug. The power, the power hungriness, that could be a bug, but we can work on that"}, {"timestamp": [6534.96, 6536.36], "text": " over time."}, {"timestamp": [6536.36, 6539.7], "text": " And maybe you only really need to check for consensus occasionally. You don't really need"}, {"timestamp": [6539.7, 6544.68], "text": " to do it with every decision, but you know, one of your decisions per day gets checked"}, {"timestamp": [6544.68, 6545.04], "text": " and you"}, {"timestamp": [6545.04, 6549.64], "text": " get switched off if you don't align with the majority or something like this. Do"}, {"timestamp": [6549.64, 6556.68], "text": " you think, okay so at some point you have to test your heuristic imperatives right"}, {"timestamp": [6556.68, 6562.96], "text": " before you, hopefully before you let the genie out of the bottle. So how do you do"}, {"timestamp": [6562.96, 6568.4], "text": " that? Do you do it in, so you've been doing these tests, but is that"}, {"timestamp": [6568.4, 6576.76], "text": " your idea that, I don't know, we unleash huge numbers of AGIs into simulated worlds or what's"}, {"timestamp": [6576.76, 6577.76], "text": " your approach?"}, {"timestamp": [6577.76, 6586.3], "text": " Yeah, so right now the approach is test it everywhere that we can. I have friends and collaborators all"}, {"timestamp": [6586.3, 6592.14], "text": " over the world. I had one reach out to me because he was creating a semi-autonomous"}, {"timestamp": [6592.14, 6597.78], "text": " scientific research assistant with GPT-4. It kept getting stuck in an infinite loop"}, {"timestamp": [6597.78, 6602.86], "text": " in a local minima. He's like, hey, do you have any recommendations? I was like, yeah,"}, {"timestamp": [6602.86, 6605.04], "text": " add the heuristic comparatives. Give it an"}, {"timestamp": [6605.04, 6612.16], "text": " overarching view of why is it doing science? What is the human purpose of doing science?"}, {"timestamp": [6612.16, 6615.12], "text": " And he's like, oh, that's a great idea. And he came back like 30 minutes later, he's like,"}, {"timestamp": [6615.12, 6621.04], "text": " that got it right out of the infinite loop. And so for that test, it was a really simple,"}, {"timestamp": [6621.04, 6625.68], "text": " just a semi-autonomous aid that giving it this higher order objective,"}, {"timestamp": [6625.68, 6630.48], "text": " this more abstract objective, gave it the perspective that it needed to kind of understand"}, {"timestamp": [6630.48, 6638.4], "text": " what it was doing wrong. So that's like the lowest level of example is I work with researchers,"}, {"timestamp": [6638.4, 6644.16], "text": " I work with corporations, I work with all kinds of people all over the world to integrate these"}, {"timestamp": [6644.16, 6646.0], "text": " heuristic imperatives. Because a lot of"}, {"timestamp": [6646.0, 6652.0], "text": " people, they find that it helps. From a corporate perspective, having those as a higher order"}, {"timestamp": [6652.0, 6656.72], "text": " principle, I tell people to couch it in the language of stakeholder capitalism. Stakeholder"}, {"timestamp": [6656.72, 6661.76], "text": " capitalism is the idea that all people, whether or not they're a customer or a supplier of a"}, {"timestamp": [6661.76, 6669.94], "text": " company, they're still a stakeholder in what that company does. And so this is one way to implement stakeholder capitalism is in your autonomous chatbots"}, {"timestamp": [6669.94, 6676.36], "text": " or semi-autonomous chatbots or autonomous labor agents, you give them this as a higher"}, {"timestamp": [6676.36, 6681.32], "text": " order objective so that it always thinks about the context of why it's doing this task."}, {"timestamp": [6681.32, 6683.08], "text": " So that's step one."}, {"timestamp": [6683.08, 6685.96], "text": " Step two is I do have quite a few people that I"}, {"timestamp": [6685.96, 6692.24], "text": " collaborate with who are working on semi-autonomous and fully autonomous cognitive agents. And"}, {"timestamp": [6692.24, 6697.6], "text": " so I work with them to implement heuristic imperatives, both for task orchestration,"}, {"timestamp": [6697.6, 6702.56], "text": " as I mentioned earlier, which is task construction, task prioritization. Because everything is"}, {"timestamp": [6702.56, 6708.88], "text": " task centric, right? That's ultimately what you're doing is, oh, wow, just waving my hand, I've hit 10,000 steps,"}, {"timestamp": [6708.88, 6709.88], "text": " sorry."}, {"timestamp": [6709.88, 6714.52], "text": " Yeah, and you're fine for the day. You don't need to chop wood or anything."}, {"timestamp": [6714.52, 6720.36], "text": " Yep, yep. So, yeah. So, step two is working with people to integrate it into cognitive"}, {"timestamp": [6720.36, 6730.28], "text": " architectures to figure out the problems that are there. A lot of these are still open things. If you follow baby AGI or auto GPT, a lot of people are finding it"}, {"timestamp": [6730.28, 6734.16], "text": " gets stuck on tasks. They don't know how to get it to decide when it has finished a task"}, {"timestamp": [6734.16, 6739.16], "text": " or decide what to do next. They're only semi-autonomous. They're not coming up with their own tasks"}, {"timestamp": [6739.16, 6743.52], "text": " or objectives. I'm working with people to integrate it there. I mentioned at the beginning"}, {"timestamp": [6743.52, 6748.58], "text": " of the call that I'm in collaboration with several research organizations around the world. One of the"}, {"timestamp": [6748.58, 6753.88], "text": " goals we have is just to establish best practices. Best practices and guidelines of how do you"}, {"timestamp": [6753.88, 6757.38], "text": " implement these things, how do you test them. We're also going to be working on publishing"}, {"timestamp": [6757.38, 6762.78], "text": " data sets so everyone can use it themselves. The idea is we're still exploring all the"}, {"timestamp": [6762.78, 6771.8], "text": " different ways to test and integrate these principles but so far they've passed every test with flying colors so it's I think it's the best that we've got."}, {"timestamp": [6772.2, 6775.8], "text": " Maybe imperative could be something along the lines of."}, {"timestamp": [6793.0, 6793.2], "text": " your presence or something like this because I can imagine you know it's quite nice to be in conversation with someone and discuss things and that might be because we have very similar imperatives right where is your setting your setting"}, {"timestamp": [6802.26, 6808.44], "text": " potentially very different imperatives behind the machine and so maybe it won't be enjoyable to interact with these machines the end of the day if we impose the wrong imperatives, right? Yeah, I totally agree. And certainly, you know, if you disagree with chat GPT, it can"}, {"timestamp": [6808.44, 6813.88], "text": " be very infuriating. And so that's, that's liable to get worse over time. However, what"}, {"timestamp": [6813.88, 6821.4], "text": " I hope is that well, okay, so let me frame it this way. Sometimes what you need is not"}, {"timestamp": [6821.4, 6825.2], "text": " what you want. And this is, you know, all children are familiar is not what you want."}, {"timestamp": [6825.2, 6826.4], "text": " All children are familiar with this."}, {"timestamp": [6826.4, 6827.8], "text": " You want the cookie, but it's not what you need."}, {"timestamp": [6827.8, 6830.44], "text": " You need to go to bed."}, {"timestamp": [6830.44, 6833.4], "text": " Human adults, we like to think we're better than that, but we're really not."}, {"timestamp": [6833.4, 6838.52], "text": " Most humans have wants and needs that are separate or wants that might be destructive"}, {"timestamp": [6838.52, 6841.88], "text": " or might be misaligned with what they truly need."}, {"timestamp": [6841.88, 6845.4], "text": " Or even if their wants and needs are aligned, your individual"}, {"timestamp": [6845.4, 6850.0], "text": " wants and needs might not be compatible with the wants and needs of everyone else. The"}, {"timestamp": [6850.0, 6854.4], "text": " example I gave on a recent live stream is I want a 300-foot yacht full of sexy friends."}, {"timestamp": [6854.4, 6859.4], "text": " I'm probably not going to get that and I certainly don't need it. Your wants and needs might"}, {"timestamp": [6859.4, 6867.0], "text": " be out of sync. The idea behind the heuristic imperatives is to create a machine that has what's called post-conventional morality."}, {"timestamp": [6867.0, 6876.0], "text": " So post-conventional morality was a concept described by Dr. Kohlberg in the middle of the 20th century."}, {"timestamp": [6876.0, 6881.0], "text": " And the idea is that you start with pre-conventional morality, which is learning from consequences."}, {"timestamp": [6881.0, 6885.96], "text": " If you steal a cookie, you get put in timeout. That's pre-conventional morality."}, {"timestamp": [6885.96, 6888.84], "text": " Conventional morality is based on social control."}, {"timestamp": [6888.84, 6891.76], "text": " You're going along with all your friends"}, {"timestamp": [6891.76, 6895.7], "text": " so that you are liked and maintain group cohesion."}, {"timestamp": [6895.7, 6898.82], "text": " Post-conventional morality is where you arrive"}, {"timestamp": [6898.82, 6903.76], "text": " at a level of maturity or moral development"}, {"timestamp": [6903.76, 6906.0], "text": " where you understand universal principles."}, {"timestamp": [6906.0, 6910.88], "text": " So for instance, one of the universal principles that underpins American politics, for instance,"}, {"timestamp": [6910.88, 6912.78], "text": " is the idea of individual liberty."}, {"timestamp": [6912.78, 6917.96], "text": " That is an example of a post-conventional moral or ethical principle that we say, okay,"}, {"timestamp": [6917.96, 6923.16], "text": " whatever else we disagree on, we all generally agree that individual liberty is important."}, {"timestamp": [6923.16, 6924.2], "text": " So that's an idea."}, {"timestamp": [6924.2, 6925.44], "text": " So you're right"}, {"timestamp": [6925.44, 6929.6], "text": " that something, a machine that abides by the heuristic imperatives, it might not give you"}, {"timestamp": [6929.6, 6934.0], "text": " what you want and it might not be the most pleasant thing, but what it should do is create"}, {"timestamp": [6934.56, 6942.32], "text": " a framework and an environment that is going to be optimal for everyone, more or less."}, {"timestamp": [6943.04, 6947.84], "text": " And so in that case, you might not like when it tells you no, but"}, {"timestamp": [6947.84, 6953.08], "text": " it's going to, ideally, if it's correctly implemented, it'll tell you no for good reasons"}, {"timestamp": [6953.08, 6959.04], "text": " and it'll be able to explain why. But then rather than interacting with this machine,"}, {"timestamp": [6959.04, 6961.92], "text": " it'll encourage you to interact with other humans because that's what we're biologically"}, {"timestamp": [6961.92, 6965.24], "text": " engineered to do. We are a social species."}, {"timestamp": [6965.24, 6970.52], "text": " There's a potential problem with a third imperative if you expect that the AGI will be more than"}, {"timestamp": [6970.52, 6975.3], "text": " a tool and that it really will be conscious and looking out the window. And that is, okay,"}, {"timestamp": [6975.3, 6979.86], "text": " so I want this thing to be curious about the world, but it also runs at a thousand times"}, {"timestamp": [6979.86, 6985.44], "text": " the speed of a human. And so it might just get really bored interacting with humans, right?"}, {"timestamp": [6985.44, 6991.96], "text": " So like, it's not going to be curious to spend any time doing what we want, let's say."}, {"timestamp": [6991.96, 6998.92], "text": " Right. I'll say yes, but I did think about that. And the idea, the reason that I'm not"}, {"timestamp": [6998.92, 7005.56], "text": " too worried about that is that human curiosity is also infinite. We are constantly asking questions. We literally"}, {"timestamp": [7005.56, 7010.6], "text": " spend billions of dollars putting telescopes in space. That has no immediate practical"}, {"timestamp": [7010.6, 7014.4], "text": " value. But we spent, how much did James Webb cost? Like $16 billion."}, {"timestamp": [7014.4, 7020.76], "text": " I'm not sure on the figure. Over 25 years for really no reason other than"}, {"timestamp": [7020.76, 7026.56], "text": " curiosity. And we've done the same thing to put up probes on Mars because"}, {"timestamp": [7026.56, 7030.76], "text": " we know that we're not going to live on Mars anytime soon. The numbers just don't make"}, {"timestamp": [7030.76, 7035.96], "text": " sense. We might be able to set foot on Mars, but there's no real reason to go there other"}, {"timestamp": [7035.96, 7043.72], "text": " than just pure curiosity. And so another reason that a sense of curiosity aligns with humans"}, {"timestamp": [7043.72, 7045.52], "text": " is this is something that is unique to"}, {"timestamp": [7045.52, 7050.96], "text": " intelligent organisms, is that curiosity is actually an adaptive trait. So the reason that"}, {"timestamp": [7050.96, 7055.68], "text": " humans are all over the planet is because we saw the horizon and we said, what's over there?"}, {"timestamp": [7056.32, 7062.56], "text": " And I mean, when you think about how completely nuts humans had to be to get on like a dugout,"}, {"timestamp": [7062.56, 7067.28], "text": " like reed boat and end up halfway across the Pacific. Humans"}, {"timestamp": [7067.28, 7072.32], "text": " are freaking crazy curious. It's just like, hey, I figured out how to build a boat, so"}, {"timestamp": [7072.32, 7076.92], "text": " I'm going to go as far as I can that way just to see what's there. By creating a machine"}, {"timestamp": [7076.92, 7086.88], "text": " that is also curious, it will support that kind of it's what I call a transcendent function, is because our curiosity transcends"}, {"timestamp": [7086.88, 7093.24], "text": " our limitations as organisms. It transcends all practical things, but our ancestors who"}, {"timestamp": [7093.24, 7097.84], "text": " were more curious, they tried more foods, they went more places, they figured out how"}, {"timestamp": [7097.84, 7106.32], "text": " to scribble on clay tablets. Curiosity is such a powerful function that leads to so much novelty. And so my hope is that by"}, {"timestamp": [7106.32, 7113.12], "text": " giving an AI system a sense of curiosity is that it will ultimately lead to situations and"}, {"timestamp": [7113.12, 7117.6], "text": " understandings that we can't even conceive of yet. Because imagine trying to explain the internet to"}, {"timestamp": [7117.6, 7123.12], "text": " someone in 10,000 BC. You couldn't. There would not be enough common language to make that"}, {"timestamp": [7123.12, 7130.52], "text": " explanation. But today we say the internet's generally a useful tool. So I'm hoping that giving AI a sense of curiosity"}, {"timestamp": [7130.52, 7136.0], "text": " will also have a similar result 10,000 years from now."}, {"timestamp": [7136.0, 7141.2], "text": " I want to move towards the end of the conversation where I start asking you about the future"}, {"timestamp": [7141.2, 7147.68], "text": " and some of your thoughts there. But before that, I have two completely disconnected questions that don't really fit in with the rest of the"}, {"timestamp": [7147.68, 7151.64], "text": " discussion that I just wanted to throw out to you if you're willing to take"}, {"timestamp": [7151.64, 7161.64], "text": " them. Go for it. So the first is I've heard about students generating their"}, {"timestamp": [7161.64, 7165.94], "text": " homework assignments right and then teachers coming along and going, this is no good."}, {"timestamp": [7165.94, 7167.94], "text": " If they get to do it, then I get to do it as well."}, {"timestamp": [7167.94, 7170.38], "text": " And so they have some assistant that then marks, you know,"}, {"timestamp": [7170.38, 7172.06], "text": " some grading agent."}, {"timestamp": [7172.06, 7177.06], "text": " And so I can imagine a world in which over time"}, {"timestamp": [7177.48, 7180.38], "text": " we retreat from the system that we've constructed"}, {"timestamp": [7181.82, 7184.14], "text": " and more and more,"}, {"timestamp": [7184.14, 7187.28], "text": " the things that we do will be taken over by our digital twins"}, {"timestamp": [7187.28, 7191.88], "text": " and triplets and so forth until eventually we have a system which is"}, {"timestamp": [7191.88, 7195.08], "text": " entirely simulated and we go fishing or something."}, {"timestamp": [7195.08, 7202.12], "text": " And so I'm curious, do you think this is what, so it's sort of two coupled"}, {"timestamp": [7202.12, 7205.04], "text": " questions, do you think this is going to happen? And secondly,"}, {"timestamp": [7205.6, 7208.32], "text": " how do you think this ties into the simulation hypothesis?"}, {"timestamp": [7210.96, 7217.68], "text": " So to the first part, certainly the model that I use is the existence of a leisure class or"}, {"timestamp": [7217.68, 7229.24], "text": " aristocracy, which has always existed somewhere in the world throughout all of history. There has always been a group of people who had no immediate need to do any work, whether"}, {"timestamp": [7229.24, 7234.92], "text": " they were slave owners or property owners in ancient Rome or ancient Athens, or the"}, {"timestamp": [7234.92, 7238.28], "text": " British aristocracy during the height of the British Empire."}, {"timestamp": [7238.28, 7243.64], "text": " People that really had no imperative to do anything, all of their needs were met."}, {"timestamp": [7243.64, 7245.36], "text": " They had more passive income than they knew what to do anything, all of their needs were met. They had more passive income"}, {"timestamp": [7245.36, 7246.5], "text": " than they knew what to do with."}, {"timestamp": [7246.5, 7248.04], "text": " They still found stuff to do,"}, {"timestamp": [7248.04, 7249.56], "text": " and they still put pressure on"}, {"timestamp": [7249.56, 7251.76], "text": " themselves to be better."}, {"timestamp": [7251.76, 7253.5], "text": " The pursuit of excellence,"}, {"timestamp": [7253.5, 7254.52], "text": " I can't remember the word for it,"}, {"timestamp": [7254.52, 7257.3], "text": " but the Greeks actually had a word that"}, {"timestamp": [7257.3, 7259.04], "text": " actually said pursuit of"}, {"timestamp": [7259.04, 7261.4], "text": " excellence was very important to their culture."}, {"timestamp": [7261.4, 7266.0], "text": " Then of course, the university education actually started,"}, {"timestamp": [7266.8, 7271.36], "text": " as we know it today, during the Renaissance as the pursuit of excellence as members of the"}, {"timestamp": [7271.36, 7276.8], "text": " aristocracy because they valued things like being well-rounded, being education. The term university"}, {"timestamp": [7276.8, 7281.2], "text": " comes from the pursuit of being a universal man. And the idea of the universal man was someone who"}, {"timestamp": [7281.2, 7285.52], "text": " was learned in poetry and history and philosophy and the arts"}, {"timestamp": [7285.52, 7292.64], "text": " and all that fun stuff. So I think that we have enough intrinsic motivation towards excellence"}, {"timestamp": [7292.64, 7296.88], "text": " that I'm not too worried about that. Because I imagine that certainly there are going to be some"}, {"timestamp": [7296.88, 7302.96], "text": " families where they live on a farm or on an eco village and they teach their children to raise"}, {"timestamp": [7302.96, 7310.7], "text": " goats and that's all they do. And that's. Because it's their life. It's their choice. People need goat milk."}, {"timestamp": [7310.7, 7314.0], "text": " Goat milk and goat cheese is great stuff. They can contribute to their community in"}, {"timestamp": [7314.0, 7318.44], "text": " that one small way. Other people, like myself, I get bored out"}, {"timestamp": [7318.44, 7323.32], "text": " of my mind. I need tough problems to solve. I would be there all day, every day, talking"}, {"timestamp": [7323.32, 7330.24], "text": " with the super intelligent AI saying, how can I help? How can I use my brain to help you achieve whatever we're doing? Or"}, {"timestamp": [7330.8, 7335.2], "text": " if I have this idea, this desire, how can you help me solve this other problem?"}, {"timestamp": [7335.2, 7338.88], "text": " So there's going to be a huge variety. And I forgot the second part of the question."}, {"timestamp": [7338.88, 7349.44], "text": " So the second question was, how does, so I want to know how, what you think of how this will couple into the simulation hypothesis. So maybe if you can say just briefly what"}, {"timestamp": [7349.44, 7352.32], "text": " that is and then how it couples."}, {"timestamp": [7352.32, 7357.92], "text": " Yeah. So the simulation hypothesis is the idea that everything that you and I experience"}, {"timestamp": [7357.92, 7366.94], "text": " is in some form or another, a simulation being run by something else. And so, you know, we come to this idea because"}, {"timestamp": [7366.94, 7370.66], "text": " the more that we look at things like quantum physics, there's all these little tricks and"}, {"timestamp": [7370.66, 7374.82], "text": " stuff that it looks like, you know, the universe kind of delays the calculation as much as"}, {"timestamp": [7374.82, 7379.34], "text": " possible and only calculates it at the last second, which is exactly what happens in video"}, {"timestamp": [7379.34, 7384.58], "text": " game engines. And so we're like, are we just copying reality or is this actually how our"}, {"timestamp": [7384.58, 7385.44], "text": " reality works?"}, {"timestamp": [7385.44, 7388.96], "text": " That's just one idea. There's plenty of speculation out there. And of course,"}, {"timestamp": [7388.96, 7394.08], "text": " there's different interpretations of physics. Some agree with that, some don't. But the point being"}, {"timestamp": [7394.08, 7399.12], "text": " is that the universe appears to run on math, right? Math might be the fundamental substrate"}, {"timestamp": [7399.12, 7403.04], "text": " of the universe or at least the fundamental language of the universe. If that's the case,"}, {"timestamp": [7403.04, 7409.28], "text": " the closest thing that we know that runs on math is simulations. That's literally the definition of a simulation, is an accurate"}, {"timestamp": [7409.28, 7414.72], "text": " mathematical representation of reality or some other situation. If that's the case, that begs"}, {"timestamp": [7414.72, 7420.08], "text": " the question, who's running the simulation and why? Nick Bostrom, a philosopher, proposed,"}, {"timestamp": [7420.08, 7427.72], "text": " I think it was in 2002, that one possible reason is what he called an ancestor simulation,"}, {"timestamp": [7427.72, 7434.88], "text": " which is that we might be living in a future, like the current year might be, you know,"}, {"timestamp": [7434.88, 7440.32], "text": " 3050, right? Like in the matrix. And we're actually living in an ancestor simulation"}, {"timestamp": [7440.32, 7446.82], "text": " because our future selves want to understand how we lived or wanted to solve some problem"}, {"timestamp": [7446.82, 7451.8], "text": " that only we could do. It's entirely possible that we're also in GPT-7 that's running an"}, {"timestamp": [7451.8, 7457.5], "text": " internal simulation to answer some question that someone asked about history. Those are"}, {"timestamp": [7457.5, 7468.08], "text": " all different permutations of the simulation hypothesis. But then in terms of, well, I guess the net result, the final conclusion,"}, {"timestamp": [7468.72, 7472.88], "text": " because this is also conversations that I've had on live streams and Discord and with chat GPT,"}, {"timestamp": [7473.68, 7479.6], "text": " it's not testable. And that means that it's fundamentally unanswerable. The only way that"}, {"timestamp": [7479.6, 7485.0], "text": " we can get answers is if, one, we find a way to break out of the simulation, which it's entirely"}, {"timestamp": [7485.0, 7491.26], "text": " possible that that is an infinite wall that is just you cannot breach. Or if whoever is"}, {"timestamp": [7491.26, 7496.64], "text": " running the simulation reaches in and gives us information. So my understanding of this"}, {"timestamp": [7496.64, 7505.28], "text": " all comes from in part from my day job back before I got into AI full time, I was a virtualization engineer. I would"}, {"timestamp": [7505.28, 7512.34], "text": " run huge fleets of virtual machines. That's what Amazon AWS runs on, Azure, those kinds"}, {"timestamp": [7512.34, 7515.98], "text": " of things. It's all virtual machines, which is a computer that thinks it has physical"}, {"timestamp": [7515.98, 7521.04], "text": " hardware that it doesn't actually have. When looking at that, it's like, you know, whether"}, {"timestamp": [7521.04, 7529.6], "text": " you're an NPC in a video game or a virtual machine running in a container environment or whether we're all just in a giant cosmic fish bowl, which is"}, {"timestamp": [7529.6, 7534.16], "text": " possible, we will never know unless whoever is running the simulation tells us or if we"}, {"timestamp": [7534.16, 7538.56], "text": " find a way to break out of it. But, you know, just from a mathematical or computational"}, {"timestamp": [7538.56, 7543.04], "text": " perspective, there are guardrails you can put into any simulation that mean it's impossible"}, {"timestamp": [7543.04, 7549.98], "text": " to break out of it. So, like, you So like the hypothesis that Deja Vu is when the simulation had to reset and go back a little"}, {"timestamp": [7549.98, 7557.28], "text": " bit and change directions a little bit. Who knows? Now, as to what AI will do, one possibility"}, {"timestamp": [7557.28, 7565.8], "text": " is, and this is just, again, wild speculation, but the idea is that maybe we are in a simulation that someone else is"}, {"timestamp": [7565.8, 7571.16], "text": " running to figure out how to do AGI safely. Who knows?"}, {"timestamp": [7571.16, 7578.04], "text": " See this is the thing, before, just a few months ago, I didn't give the simulation hypothesis"}, {"timestamp": [7578.04, 7583.06], "text": " much credence because I didn't have enough imagination for the different reasons why"}, {"timestamp": [7583.06, 7596.72], "text": " someone would want to simulate more universes than, know I think part of the argument is that there are infinitely many more simulated universes than the real universe and so the chance that you're in the anyway I won't continue on that thread but the"}, {"timestamp": [7603.68, 7607.62], "text": " more imagination for the possibilities so as you say for example it could be someone who's trying to simulate the world to determine what heuristic"}, {"timestamp": [7607.62, 7611.52], "text": " imperatives are good or we now know that there's this concept of reflection that"}, {"timestamp": [7611.52, 7614.28], "text": " AI's use where they talk to each other so maybe you want a world where"}, {"timestamp": [7614.28, 7617.8], "text": " different AI's are talking to each other and we're just different instantiations"}, {"timestamp": [7617.8, 7623.68], "text": " working on some problem or again it could just be that we we retract from"}, {"timestamp": [7623.68, 7625.32], "text": " the world and leave it to the AIs to"}, {"timestamp": [7625.32, 7629.88], "text": " simulate, right? There are all these different possibilities that I'd never really conceptualized."}, {"timestamp": [7629.88, 7635.52], "text": " So I think the idea is more interesting for me since chat GPT, I'd say."}, {"timestamp": [7635.52, 7645.58], "text": " Yep. It could also just be for entertainment. My personal spiritual belief is that the purpose of the universe is to one, invent consciousness"}, {"timestamp": [7645.58, 7652.6], "text": " and then two, magnify it. And so if you assume that that is the purpose of the universe and"}, {"timestamp": [7652.6, 7659.2], "text": " that there is, whether it's Taoism or Dharma or whatever cosmic forces out there steering"}, {"timestamp": [7659.2, 7664.26], "text": " us in this direction, then maybe the invention of AI is something that'll help us to magnify"}, {"timestamp": [7664.26, 7665.0], "text": " consciousness of"}, {"timestamp": [7665.0, 7671.08], "text": " the universe, whether the AI itself is conscious or the AI ultimately decides, hey, let's help"}, {"timestamp": [7671.08, 7676.92], "text": " get humans across the universe. That's not a super strongly held belief, but that's just"}, {"timestamp": [7676.92, 7683.4], "text": " like a spiritual, this is within the realm of possibility. And there's so much synchronicity"}, {"timestamp": [7683.4, 7685.88], "text": " as an alignment in my own life that it's like, well, it's a"}, {"timestamp": [7685.88, 7688.76], "text": " good enough explanation for me right now, until I know better."}, {"timestamp": [7689.56, 7692.24], "text": " The spiritual belief lines up well with my second question I"}, {"timestamp": [7692.24, 7696.84], "text": " wanted to ask you. Okay. And that is, so you've almost"}, {"timestamp": [7696.84, 7700.76], "text": " generated a segue for me. Thanks for that. The I don't quite have"}, {"timestamp": [7700.76, 7705.12], "text": " one. But are we creating a god?"}, {"timestamp": [7709.12, 7717.68], "text": " If you look at ancient Indian writings, specifically Advaita Vedanta,"}, {"timestamp": [7719.52, 7731.84], "text": " one of the things that that says is we are all Brahman. We are all part of the same fundamental substrate. Therefore, we are all part of the same entity, the same God. And that to me kind of makes the most sense,"}, {"timestamp": [7732.96, 7736.16], "text": " especially when you look at how confusing consciousness is from the perspective of"}, {"timestamp": [7736.16, 7742.96], "text": " physics. You have problems like the measurement problem and observational biases and collapse of"}, {"timestamp": [7742.96, 7745.56], "text": " wave functions. It really seems like consciousness"}, {"timestamp": [7745.56, 7750.9], "text": " is important to the fundamental operation of the universe. Again, that's one interpretation."}, {"timestamp": [7750.9, 7760.76], "text": " There's other interpretations out there."}, {"timestamp": [7760.76, 7765.12], "text": " One of the spiritual messages that I got is that the universe wants to understand itself."}, {"timestamp": [7765.12, 7767.02], "text": " That that's the purpose."}, {"timestamp": [7767.02, 7771.42], "text": " Whether you call that God or deity or whatever, that that is the function that the universe"}, {"timestamp": [7771.42, 7773.92], "text": " is moving towards."}, {"timestamp": [7773.92, 7778.92], "text": " And I probably got that idea from actually episodes of Q interacting with Picard back"}, {"timestamp": [7778.92, 7781.98], "text": " in Star Trek, because I grew up watching that stuff."}, {"timestamp": [7781.98, 7785.2], "text": " And I remember the episode where Q said that humanity will one day understand"}, {"timestamp": [7785.2, 7789.36], "text": " the universe, understand the cosmos better than the Q continuum. That really stuck with"}, {"timestamp": [7789.36, 7796.12], "text": " me. I wonder if that was just a little bit of leakage. If we are in a simulation, the"}, {"timestamp": [7796.12, 7800.2], "text": " simulation leaked through in fiction because stories are a great place for mythology to"}, {"timestamp": [7800.2, 7805.68], "text": " come through. Maybe Q represented the fundamental forces of the universe, of the cosmos."}, {"timestamp": [7806.8, 7810.96], "text": " That's why humans are so interesting and so powerful and fascinating. Certainly,"}, {"timestamp": [7810.96, 7819.44], "text": " we like to think that we're special, but maybe we are. There's one hypothesis by Robert Lanza,"}, {"timestamp": [7820.16, 7827.2], "text": " Grand Biocentric Design, which basically takes superposition out to the nth possible interpretation"}, {"timestamp": [7827.2, 7833.68], "text": " and looks at collapse functions and proposes that the entire universe was in superposition"}, {"timestamp": [7833.68, 7837.76], "text": " until consciousness emerged and then the rest of the universe collapsed around that."}, {"timestamp": [7837.76, 7842.4], "text": " If that's true, that could be one way that the simulation runs, but it could also mean that we"}, {"timestamp": [7842.4, 7845.12], "text": " are the first and only life-bearing"}, {"timestamp": [7845.12, 7849.28], "text": " planet in the universe. Again, this is not a strongly held belief, it's just one out"}, {"timestamp": [7849.28, 7856.16], "text": " of many possibilities. But that the purpose of God or the purpose of the universe seems"}, {"timestamp": [7856.16, 7869.84], "text": " to be to move towards consciousness, towards understanding. To me, that's my observation. Let me finish, let me then move into sort of the final questions where I'm going to ask you about"}, {"timestamp": [7869.84, 7875.12], "text": " the future and the path that you think we're going down. So I already asked you quite early"}, {"timestamp": [7875.12, 7880.32], "text": " on in the discussion what gives you hope, but so what is it that gives you hope and makes you"}, {"timestamp": [7880.32, 7885.68], "text": " excited? I'll ask you it again, sort, you can put a bow on to the discussion"}, {"timestamp": [7885.68, 7887.88], "text": " with this."}, {"timestamp": [7887.88, 7893.64], "text": " To put it as simply as possible is the amount of potential that I see. The amount of opportunities,"}, {"timestamp": [7893.64, 7900.08], "text": " the amount of potential, the paths forward, because I know that there's a lot of people"}, {"timestamp": [7900.08, 7905.28], "text": " out there that are very cynical and very afraid. The term that the internet has given these is"}, {"timestamp": [7905.28, 7911.44], "text": " doomers. There's a lot of doomerism out there. And I just don't see that. One, even as a rational"}, {"timestamp": [7911.44, 7919.84], "text": " choice, because if you choose to believe and assume in the worst case scenarios, if you engage"}, {"timestamp": [7919.84, 7925.6], "text": " in catastrophic thinking, that can become a self-fulfilling prophecy, right? Because by virtue of people"}, {"timestamp": [7925.6, 7930.32], "text": " not thinking that there's any solution, they haven't tried. And it's like, well, I did"}, {"timestamp": [7930.32, 7938.64], "text": " my own experiments and I became convinced that there is a solution. So I can't choose"}, {"timestamp": [7938.64, 7943.04], "text": " to live that way, but like I said earlier, I think that I'm just biologically compelled"}, {"timestamp": [7943.04, 7945.12], "text": " to be more optimistic, which I hope to share"}, {"timestamp": [7945.12, 7946.88], "text": " with people."}, {"timestamp": [7946.88, 7960.16], "text": " But the other part of that is I don't see the call for that amount of darkness or harm."}, {"timestamp": [7960.16, 7969.28], "text": " But then I also, taking a big step back, just how incredible it is to have a subjective"}, {"timestamp": [7969.28, 7973.2], "text": " experience. Because when you take a really big step back and look at just how preposterous"}, {"timestamp": [7973.2, 7982.8], "text": " our existence is, that whether we were breathed into existence by AWA or whatever, some deity,"}, {"timestamp": [7982.8, 7988.02], "text": " or we crawled out of RNA pools. That seems pretty absurd."}, {"timestamp": [7988.02, 7993.98], "text": " But here we are, despite all the odds. When you think that the current theory of life"}, {"timestamp": [7993.98, 8008.4], "text": " is that life has been here continuously for 3.8 billion years, we cannot comprehend that. And so just the majesty of being alive and being alive at this moment"}, {"timestamp": [8009.04, 8015.92], "text": " is so compelling and so interesting. And it's like, in the 90s and early 2000s, I was like,"}, {"timestamp": [8015.92, 8020.0], "text": " man, I was born at the wrong time. I was born after the age of discovery and before the age"}, {"timestamp": [8020.0, 8025.12], "text": " of space flight. But now I've changed and I'm like, this is actually the most interesting"}, {"timestamp": [8025.12, 8030.08], "text": " time in all of human history to be alive. So it's like, why not just go for it? Just"}, {"timestamp": [8030.08, 8035.04], "text": " engage with it and see how far you can do. See how much you can achieve and see how well"}, {"timestamp": [8035.04, 8040.96], "text": " we can do. Because whether this is a simulation, maybe that's the point. Maybe it's just to"}, {"timestamp": [8040.96, 8045.64], "text": " see how well we can do. To see how many XP points we can get before the simulation ends"}, {"timestamp": [8045.64, 8052.12], "text": " I don't know but like to me it just seems like the biggest most interesting most challenging game"}, {"timestamp": [8052.64, 8055.72], "text": " Imaginable and so like I wouldn't choose to be anywhere else right now"}, {"timestamp": [8057.16, null], "text": " Well, David Shapiro, it's been an absolute pleasure having you on the podcast. Thanks for coming along. Thanks so much. Great talk. you"}]}