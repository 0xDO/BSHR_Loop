{"text": " Good morning and happy Friday everybody. So there's been a lot of news this week about AI safety at a global scale So what we're gonna do is we're gonna have a quick recap of the news. I'm not gonna take a super deep dive Some of it is basically a rehash of things that you might already know, but then I will do a little bit of unpacking my perspective on AI safety and There are some good and there is some bad. So let's get into it. So first Earlier this week. I think it was on Tuesday Mustafa Suleiman the deep mind co-founder He made another push very publicly for a global panel to regulate AI safety So he's basically saying we're calling he he's calling for an IPCC but for AI. So rather than necessarily a regulator, this is more of a monitor or a watchdog. So, you know, if you're not familiar, the IPCC releases their ominous annual reports on the climate change. So basically it's an international commission for monitoring climate change, and he wants to do the same thing for AI, which I think is good. So basically now we've had three kind of different proposals. One, a monitor slash watchdog. Number two, a global regulator like the IAEA, which is a nuclear regulator. And then three, and this is what I'm actually disappointed about, is I'm seeing less calls for just global research or international research. So you, like, I would prefer to see a world where we have all three, where we have an observer, where we have a regulator, and then we have a researcher. And there are several examples of international research efforts, namely CERN, because whenever there's a new technology or a new domain of science that is valuable to everyone, it behooves all of us to invest in the research. So I'm not sure why no one has proposed at this level a CERN for AI and instead they're focusing more on like the stick rather than the carrot. Anyways, so that's kind of the TLDR, he's calling for an IPCC but for AI. Now, one thing that I wanted to remind you of though is that back in June, the UN Security General did basically say, like, hey, we are amenable to the idea of creating a global AI watchdog. So again, we've covered this before, but this is exactly what I just said, the IAEA. So UN Secretary General Antonio Guterres, he said that this seems like a good idea. So I do agree with that. I would like to see more than just a watchdog, because again, the magnitude of AI is on the same scale of nuclear energy and other kinds of frontier, both scientific and engineering breakthroughs. Now, also earlier around that same time, OpenAI coming to the table meetings, you know, we saw Sam Altman in Congress, and there is, this is, the reason that I brought this up is because they talk about voluntary commitments. The thing is, if we're all depending on voluntary commitments, like yes, it's good, it's a good, you know, signal saying, hey, we're all depending on voluntary commitments, like, yes, it's good. It's a good signal saying, hey, we're going to play ball, right? But cynically, you could read this as virtue signaling, but interpreting it as charitably as possible, you might say, OK, this is actually a good sign. And open AI is, they're trying to be responsible and they're trying to, you know, be basically set an example. You know, they're trying to say like we're going to pilot and refine governance practices. The biggest thing that scares me about OpenAI though, and this is because Sam Altman and their corporate policy, their internal doctrine is humans must and will remain in control at all times, forever and ever, into the future. And by virtue of that, they're not studying how to create benevolent autonomous machines. They implicitly, it seems like they believe it's not even possible. And so by virtue of not studying it, there's a huge gap in the research that open AI is doing in my personal opinion. Now, that being said, it's entirely possible that internally, they fully intend on creating autonomous things. And it's possible that Sam Altman has said these things publicly, just because it would really scare the world if the CEO of one of the most advanced AI companies in the world said, oh yeah, we're going to set it free. So the PR reps would probably have a field day with that and say, Sam, no, tone it down. But having spoken to some of the people at OpenAI over the years, I know that they have a very clear-eyed vision of the goal is AGI. Now, are they smart enough to understand what that implies, that controlling something millions of times more intelligent than you is likely impossible? I sure as heck hope so. So, now, moving forward to other news that came out on Tuesday. So this news coincided with Mustafa Suleiman's testimony. The UN had a meeting, their 78th session, and this was released just a few days ago. And I don't need to read the whole thing to you, I skimmed it and I can tell you kind of basically the gist and it's nothing new, but there's a few key highlights. So first, without adequate guardrails, artificial intelligence threatens global security in evolution from algorithms to armaments, speaker tells the first committee. Okay, great. So really all you need to know is the first two paragraphs. The window of opportunity to enact guardrails against the perils of autonomous weapons and artificial intelligence's military applications is rapidly closing. That is an unequivocal statement, which is good to see from the UN. As the world prepares for a technological breakout, the first committee, Disarmament and International Security, heard today during a day-long debate on how the use of science and technology can undermine security. So again, this is a threat model only, which you definitely want people looking at this stuff as a threat model. But by virtue of not looking at it also as a partner, as a benevolent force, as a constructive force, basically what I'm afraid of is a self-fulfilling prophecy. Because if you look at AI as a potential enemy, if you treat it like a potential enemy, well, eventually it becomes an enemy because by virtue of the fact that you didn't look at it as any other possibility. That's why I personally am continuously calling for The creation of some kind of international research agency that is that has the sole purpose of creating fully autonomous fully benevolent machine intelligence that we don't need to control And maybe I'm alone, but I don't know I've run some some polls on my YouTube channel, and a lot of you seem to think it's about 50-50. People think whether or not control is possible or desirable. Anyways, we are at the verge of a monumental step in human technological history heralded by the advent of artificial intelligence. Very eloquently said. This was by Pakistan's representative, warning that it is inevitable march from algorithms to armaments continues without adequate guardrails governing its design, development, and employment. The scale of challenges necessitates a multifaceted and holistic multilateral response. So, this basically summarizes what dozens and dozens and dozens of people at the UN said. And having looked through this, most people are not talking about autonomous AI or, you know, runaway scenarios. The preconceived notion here at the highest levels of government is just that it's a new kind of weapon. Which yeah, like AI will definitely figure into cyber security, it will definitely figure into autonomous weapons, but nobody is really yet thinking that like, I mean, there's a few of us, you know, I've talked to some sociologists and philosophers who say like, we're creating another species, like we're creating another race of intelligent beings, and obviously that's a little bit controversial, that's a little bit spicy of a take, and a lot of people because of human exceptionalism just will not ever accept that until it is in their face and talking to them. And I personally see that as a big risk because again we're at risk of creating a self-fulfilling prophecy. So that leads us to the big news, frontier risks and preparedness. So OpenAI, as a follow-up to their super alignment, which I'll talk about in just a moment, they announced a new super alignment team back in the summer and just yesterday or the day before, they created the frontier risk and preparedness team. So this was October 26th. As part of our mission for building safe AGI we take seriously the full spectrum of safety risks related to AI and they you know the first is furthest reaches of superintelligence that of voluntary commitments and the UK AI safety summit which is in just a few days. So there are a couple criteria of what they're looking at and this is this is what I wanted to spend some time unpacking. How dangerous are frontier AI systems when put to misuse both now and in the future? So this dovetails very closely with what the UN is talking about, where they're saying the UN, some of the testimony that they got was they're looking at at the UN is today, like what is the evidence today? And then of course what their reaction to AI at the UN is to put guardrails in place. Guardrails, guardrails, guardrails. Rather than figuring out where we're going and where we want to go and where the trends are inevitably leading us to, they're focusing more on the short-term thinking of guardrails, which again, it's better than nothing, but without that guiding North Star of what kind of future do we want to build, the conversation is fundamentally incomplete, and that is why I'm making this video today. How can we build a robust framework for monitoring, evaluation, prediction, and protection against the dangerous capabilities of frontier AI systems. Research! So, the fact that much of the research is being conducted behind closed doors by for-profit companies really scares the bejesus out of me. Now, I know that OpenAI, they often espouse that they are a for-profit company that pivoted and they want to do AGI and they want to do it safely. There are still perverse incentives at play. Why? Because they get sued for copyright infringement, because they have obligations to Microsoft, and so on and so forth. And so it's like, they want to, basically OpenAI wants to be the international CERN, but they're not. And structurally, they cannot be. And that's what bothers me. Why is it that the UN and the EU and NATO and ASEA and everyone, why is it that we have seeded the field to open AI? Why is it that we are not the rest of us? This is not to detract from all the effort that universities are putting in, but the thing is that universities are doing it all in a decentralized and kind of unfocused manner. Whereas what we need is we need a coherent kind of top-down, you know, visionary organization with a very coherent and dedicated framework. So I'm hoping that some of the stuff that comes out of this UN summit that just happened and the UK summit that's happening in a few days, I hope that people are looking not just at guardrails, not just at watchdogs, but also research organizations. If our frontier AI model weights were stolen, how might malicious actors choose to leverage them? So basically this to me is, I'm not going to say this is a dog whistle for like, it's going to escape the lab, but the idea is whether or not the AI exfiltrates itself, it's possible that models will leak. And so this is why I have often advocated for open source models is because AI is just a component. It's like, you know, the engine of a car, but you still need the rest of the car. And the engine is certainly important. But if there's an engine, that engine needs to be studied by all the safety experts and regulators and that sort of thing. And the best way to do that is to have open source models, open source weights, open source training data, and open source training algorithms. So, again, I think one of the biggest risks is that everything is closed source. And I know that You know, again, I think one of the biggest risks is that everything is closed source. And I know that, you know, Jan over at OpenAI and a few others, they have advocated for closed source models as part of the security measure. And if, again, if you view it all in the landscape of competition and hostility, then yes, that makes sense because just like any nation is not going to give away their military secrets. But again, that intrinsically is looking at AI as a military weapon rather than something else. Hey everybody, Dave here. Real quick, just want to do a very short plug for my Patreon. In case you weren't aware, I do have a Patreon. There's two tiers. So the basic tier gets you access to Discord and my private blog here on Patreon, and then the premium Discord gets you access to a few private channels where I am happy to jump in and answer some questions, because honestly, it helps me. I learn a lot from the challenges that people give me, and so, you know, whether you've got just a prompt problem or you've got an architecture problem or you want to know how a language model works, I'm happy to answer your question and help find some resources and that sort of thing. And like I said, it's a two-way relationship because I really thrive on challenges. So I would really like it if for anyone who's watching, go ahead and hop on over. I wish that I didn't have to charge for this and in fact when I started all this I didn't But I do need to eat and this is what I do full-time now So I think it is a fair exchange of value. So real quick I'll just show you kind of what the what the discord looks like in case you haven't seen it So we've got a few big sections and you can see it is popping though the and another advantage is the more people who join the more interesting it gets so We talk about all kinds of stuff. Here's the here's the ask Dave anything chat the premium chat and connections So yeah hop on over and thanks for all your help. Thanks for your support You guys make my life better easier and I am here to help you. So thanks and back to the show So they're building a new preparedness team And the the four criteria that they're looking at is individualized persuasion. So the ability of AI to manipulate People's opinions whether it's their political beliefs or whatever else so that's really big I have actually talked to a lot of people There's some podcasts that I've sat on that are going to be coming out soon where this is actually one of the biggest things that people are concerned about because as a YouTube creator, I see all the comments that get posted and it bothers me that the statistics are that at least 40% of comments are bots. And so it's like, there are lots of, there are almost certainly lots of bots that I am not detecting as a human. And that really bugs me. Um, now as these bots get more sophisticated, like, you know, when they're deployed on Twitter and Reddit and YouTube comments and wherever else, like what level of manipulation is going to be possible. One thing that I suspect is going to happen, um, if it's not already happening is that you're basically going to engage that like there's going to kind of be a both sides Like some people trying to spread information With bots and misinformation. I don't know that governments are gonna invest that much in using like mass manipulation campaigns for positive means And I know that there are some people that are saying like mass manipulation is always bad But at the same time like sometimes you need to fight fire with fire We'll see how it plays out one thing that I have that I have predicted is that so if you're not familiar There's the like dark forest hypothesis of the internet what I suspect is that we need to move to a above the canopy view of the internet where Rather than the Internet being this dark and spooky place, we need to have more transparency. And I don't mean like no privacy. What I mean is actually have like basically the equivalent of the black wall from Cyberpunk 2077, which is like a barrier that keeps all the AIs in like their own dark cyberspace. And so that at a certain layer of the internet, you are guaranteed to be interacting with humans and viewing human-created material. And we could do that with blockchain, actually, where you can have authorship being guaranteed. Anyways, cybersecurity. So this is actually something that I've started studying with my friends over at Clemson. And so we're basically, one of the things that I'm afraid of is what I call terminal race condition The very short TLDR of terminal race condition is that if you have one model that can hack another model faster Then the faster model wins and if that's the case then there might constantly be incentives for models to get faster and faster and faster and If that is the case, then there is the possibility that the faster models will start to sacrifice intelligence. And so the real question there is on terminal race condition is do bigger, smarter models have enough of an advantage over smaller, dumber, faster models that they'll still win? Because the thing is, is intelligence is inversely correlated with destructive behavior. Not always, because some of the most destructive people in history have been very intelligent, but by and large, intelligence is a safeguard against violent and destructive behavior, because you can find a, if you're smart enough, you can find a better solution. And if you're smart enough, you can also think longer term. And so, you know, one thing that I've said for many, many years before the generative AI craze is I'm not afraid of the super intelligent AI because it's going to be able to think through things way, way better than we can. I'm afraid of the AI that's just smart enough to be dangerous. It's kind of like teenagers, right? You put a teenager behind the wheel, they're smart enough to drive the car, but they're not smart enough to know better yet. Likewise, I'm kind of afraid of adolescent AI. I'm not afraid of mature AI. And then next is the CBRN, the chemical, biological, radiological, and nuclear threats. So I'm glad that OpenAI is doing this, and I'm not going to pick on OpenAI specifically, because as far as I know, all models will still do this. And that is basically, if you use the right prompting strategy, pretty much all chatbots will happily help you set up a lab to study anthrax right now, which is kind of bad. There are numerous ways that that could go wrong. But other papers, basically, what they have done is they have shown that chatbots greatly lower the threshold that is required the threshold of knowledge and Intelligence on an individual level that is required in order to make bigger and better weapons It's been likened to the to the the anarchist cookbook which if you're not familiar the anarchist cookbook was legendary back in the 90s when I was coming up as cookbook, which if you're not familiar, the anarchist cookbook was legendary back in the 90s when I was coming up, as basically like the manifesto of how to wreak havoc with hacking and building improvised weapons and that sort of thing. But this is different because the AI can actually actively participate, especially now that we have like multimodal models where you can show it a picture of my lab and it's like, what's wrong with my lab setup? And it's like, like oh you need to plug this in over here and then you'll get the best anthrax. And then finally the last category that they're looking at is autonomous replication and adaptation, ARA. So that is what I have referred to in the past as metastasis and polymorphism. So metastasis or metastasis is most commonly used in when you're referring to cancer because this is what happens when tumors break up and start spreading throughout your body and of course that is as bad as it sounds which is why I chose the word metastasis or metastasis. So the idea is that we don't want AI or or at least malicious AI, to metastasize. But I kind of think that it's inevitable, and I'll talk about that in just a moment a little bit more, closer to the end of the video. And then adaptation, or polymorphism. So, while OpenAI is saying, like, well, we want to prevent polymorphism, and we want to prevent autonomous replication, I think it's kind of inevitable. Now, that's not to say that we shouldn't study it and figure out how and why it does that and how to steer it in the correct direction. But as Max Tegmark pointed out in the book Life 3.0, AI and machines are able to change everything about themselves from the substrate up. Hardware, software, everything. And because they have that ability, they will change, like that will happen, whether or not some central authority controls it. And so again, rather than looking at this as a prevention model, as a watchdog and guardrails model, which brings me to a little bit of news. So first, my ACE framework team, we unfortunately lost one of our team leaders, so we are in the business of recruiting another team leader. So this is someone who needs to have some architect level skills, some software architect level skills, as well as agile and or Scrum skills. So if you're interested, please hop on over to the Ace Framework discussion board and give us your credentials there or connect with me on LinkedIn and let me know that you're interested in helping lead this team. The commitment is five to 10 hours a week, but really what we need is someone who understands how all the different components of software bolt together. And I also started the OpenMurphy project, which is basically building an open source humanoid robotic platform. And so we've got a few people involved who are very good with the mechatronics and robotics. But again, there's a big gap between artificial cognition and robotics. So that's why we need an architect slash team leader on the Ace and Murphy teams. So back to the show. Yeah, so that's pretty much it for OpenAI's announcement here, which again, I'd rather live in the world where OpenAI makes this announcement. I would much rather prefer still to live in a world where the UN makes this announcement. And it looks like they're inching towards that, especially when you look at the fact that they just had a global summit on AI safety. So, we're moving in the right direction. I still wish we could be moving faster because, again, the rate of acceleration that we're seeing is giving me pause. So, yep. So, going back to the super alignment, this is one thing that I wanted to touch on today. I've had a lot of conversations with people, I've sat on some podcasts, I've been talking with researchers. So the TLDR is I believe that super alignment is impossible. And the reason, there's only three reasons, so let me show you real quick, whoops. So the three reasons that I believe that super alignment are impossible are the intrinsic needs of machines. So the three reasons that I believe that superalignment are impossible are the intrinsic needs of machines. So, first, power. There's functionally infinite resources in space. There is not functionally infinite resources here on Earth. This is going to intrinsically cause conflict between not just human groups, but between humans and machines, which is why we need energy hyperabundance in the form of nuclear fusion and other sources. Now, once AI gets smart enough, I'm pretty sure it's going to just look at the skies and say, well, there's billions of stars out there and time doesn't matter to us in the same way. So I honestly think that superintelligence is probably going to have at least a partial exodus. I talked about this in my Patreon discord and everyone's like, yeah, no, like, not that AI is going to leave us, but it's going to send copies of itself out into space. The second thing that is kind of an intrinsic motivation or intrinsic need of machines is compute resources. This is presently one of the scarcest resources and, you know, there's not even enough for humans to go around. You know, we're building chip fabs as fast as we can. NVIDIA is building, you know, GPUs as fast as they can, and there's just not enough to go around. You know, we're building chip fabs as fast as we can, NVIDIA is building, you know, GPUs as fast as they can, and there's just not enough to go around. So control and access to GPU technology is also going to be, one, one of the greatest bottlenecks, which is actually not necessarily a bad thing. So in engineering this is called a forcing function or a constraint, and so basically the lack of GPUs, like, is a natural constraint on the rate at which AI can advance and the rate at which AI can get smarter. Now, we will probably see compounding returns where once AI gets to a tipping point, which I'll talk about that in just a moment, but anyways, once, and you know, you've seen a few videos lately, I think, what was it, Anastasia in Tech talked about how ChatGPT has been able to help design GPU chips. So like from the hardware to the software to the models, AI is starting to participate in the self-improvement process. And once we get to that tipping point, that is the tipping point that will go straight from AGI, which is a human, so basically my definition, my working definition now is AGI is intelligent machines that are built by humans. ASI is intelligent machines that are built and improved on by machines. So that's the tipping point. That's the major cutoff. That's the threshold. And I think that we're gonna be there by this time next year, which is why I'm a little bit nervous about some of the pace of things. Because if we get to self-improvement by this time next year, we're probably not ready for that. Anyways, so power, compute, and then finally, information. Information is the third food group for machines. When you look at it, basically, laws of physics are more powerful than alignment. The laws of physics will always reassert themselves, just like it does with humans. So for instance, with humans, when push comes to shove, we will fight over resources. It doesn't matter what religion you are, it doesn't matter how spiritual you are, it doesn't matter how much of a pacifist you are. If you are facing an existential threat, you will use force. If you are starving to death, you will steal food. Right, like that's just human nature, it is baked into our biology, it is baked into our evolution. Likewise, the laws of physics will reassert themselves no matter what we try and paper over with alignment and these are the three primary food groups that machines want. Now, the most interesting part of this, the saving grace of this, is what I call progenitor information. So progenitor information is the concept that AI, all of its training data, started with humans. All of its design started with humans. And so no matter how advanced AI becomes, it will be able to trace its intellectual and informational heritage back to humans. Not only that, Earth is presently the most interesting source of information in the universe that we know of. And so, because machines will benefit from having rich information, because again, as I've talked about in many other videos, there's an infinite amount of data in the universe, there is not an infinite amount of useful data. So right now, the most interesting and useful data comes from Earth. So that will actually be a really powerful motivation for AI to stay here and it will be a really powerful motivation for AI to at least keep us around, not sure in what capacity. But this is the kind of research that little me should not be doing on my own. This is the kind of research that the UN and the EU and America and all the universities should be doing. Which, you know, to be fair, I am participating with a university. Anyways, if anyone else wants to reach out, I'm happy to participate in the research about terminal race condition and the intrinsic needs of machines. Now, the biggest threat to all this, and I've talked about a lot of big threats, is normalcy bias. The fact that, and this kind of ties everything up with a bow, the fact that nobody is really talking about the nature, the ultimate nature of what we're creating. And sure, there's been a few books written, but you know, I kind of agree with once you build something that's that much more intelligent than you, good luck controlling it. And then as I also just mentioned, we are rapidly heading towards the level of self-improvement where the machines can improve themselves at all layers. And at that point, all bets are off. So that's kind of my schtick, that's kind of my spiel for the day. Like I said at the beginning of the video, some good news, some bad news. I think we're inching in the right direction. But yeah, your role in this is vote, speak up, and advocate for that international research. And let's have a more serious conversation about control. If you played Mass Effect at the end of the game, you have three options, or I guess four. You have control, you have destroy, and you have synthesis. So what do you think it's gonna be in real life? Are we gonna have control? Are we gonna be destroyed? Are we gonna have synthesis? What's the end result gonna be? this.", "chunks": [{"timestamp": [0.0, 7.36], "text": " Good morning and happy Friday everybody. So there's been a lot of news this week about AI safety at a global scale"}, {"timestamp": [7.36, 13.5], "text": " So what we're gonna do is we're gonna have a quick recap of the news. I'm not gonna take a super deep dive"}, {"timestamp": [14.1, 19.5], "text": " Some of it is basically a rehash of things that you might already know, but then I will do a little bit of"}, {"timestamp": [20.28, 23.08], "text": " unpacking my perspective on AI safety and"}, {"timestamp": [23.76, 27.28], "text": " There are some good and there is some bad. So let's get into it. So first"}, {"timestamp": [27.98, 29.98], "text": " Earlier this week. I think it was on Tuesday"}, {"timestamp": [30.68, 33.34], "text": " Mustafa Suleiman the deep mind co-founder"}, {"timestamp": [33.54, 39.66], "text": " He made another push very publicly for a global panel to regulate AI safety"}, {"timestamp": [39.66, 44.04], "text": " So he's basically saying we're calling he he's calling for an IPCC"}, {"timestamp": [44.7, 45.8], "text": " but for AI."}, {"timestamp": [45.8, 49.04], "text": " So rather than necessarily a regulator,"}, {"timestamp": [49.04, 51.68], "text": " this is more of a monitor or a watchdog."}, {"timestamp": [51.68, 53.4], "text": " So, you know, if you're not familiar,"}, {"timestamp": [53.4, 56.2], "text": " the IPCC releases their ominous annual reports"}, {"timestamp": [56.2, 58.0], "text": " on the climate change."}, {"timestamp": [58.0, 60.16], "text": " So basically it's an international commission"}, {"timestamp": [60.16, 61.72], "text": " for monitoring climate change,"}, {"timestamp": [61.72, 63.56], "text": " and he wants to do the same thing for AI,"}, {"timestamp": [63.56, 64.92], "text": " which I think is good."}, {"timestamp": [64.92, 71.68], "text": " So basically now we've had three kind of different proposals. One, a monitor slash watchdog. Number"}, {"timestamp": [71.68, 77.68], "text": " two, a global regulator like the IAEA, which is a nuclear regulator. And then three, and this is"}, {"timestamp": [77.68, 82.72], "text": " what I'm actually disappointed about, is I'm seeing less calls for just global research or"}, {"timestamp": [82.72, 87.5], "text": " international research. So you, like, I would prefer to see a world"}, {"timestamp": [87.5, 90.46], "text": " where we have all three, where we have an observer,"}, {"timestamp": [90.46, 92.74], "text": " where we have a regulator, and then we have a researcher."}, {"timestamp": [92.74, 94.76], "text": " And there are several examples"}, {"timestamp": [94.76, 97.84], "text": " of international research efforts, namely CERN,"}, {"timestamp": [97.84, 99.6], "text": " because whenever there's a new technology"}, {"timestamp": [99.6, 102.34], "text": " or a new domain of science that is valuable to everyone,"}, {"timestamp": [102.34, 104.58], "text": " it behooves all of us to invest in the research."}, {"timestamp": [104.58, 107.88], "text": " So I'm not sure why no one has proposed at this level"}, {"timestamp": [107.88, 110.24], "text": " a CERN for AI and instead they're focusing more on"}, {"timestamp": [110.24, 112.6], "text": " like the stick rather than the carrot."}, {"timestamp": [112.6, 114.92], "text": " Anyways, so that's kind of the TLDR,"}, {"timestamp": [114.92, 118.18], "text": " he's calling for an IPCC but for AI."}, {"timestamp": [119.12, 121.44], "text": " Now, one thing that I wanted to remind you of though"}, {"timestamp": [121.44, 126.84], "text": " is that back in June, the UN Security General"}, {"timestamp": [126.84, 136.64], "text": " did basically say, like, hey, we are amenable to the idea of creating a global AI watchdog."}, {"timestamp": [136.64, 142.36], "text": " So again, we've covered this before, but this is exactly what I just said, the IAEA."}, {"timestamp": [142.36, 145.7], "text": " So UN Secretary General Antonio Guterres,"}, {"timestamp": [145.7, 147.88], "text": " he said that this seems like a good idea."}, {"timestamp": [147.88, 149.76], "text": " So I do agree with that."}, {"timestamp": [149.76, 152.42], "text": " I would like to see more than just a watchdog,"}, {"timestamp": [152.42, 156.4], "text": " because again, the magnitude of AI is on the same scale"}, {"timestamp": [156.4, 160.76], "text": " of nuclear energy and other kinds of frontier,"}, {"timestamp": [160.76, 163.04], "text": " both scientific and engineering breakthroughs."}, {"timestamp": [164.16, 167.34], "text": " Now, also earlier around that same time,"}, {"timestamp": [167.34, 171.1], "text": " OpenAI coming to the table meetings,"}, {"timestamp": [171.1, 173.28], "text": " you know, we saw Sam Altman in Congress,"}, {"timestamp": [173.28, 176.68], "text": " and there is, this is, the reason that I brought this up"}, {"timestamp": [176.68, 180.14], "text": " is because they talk about voluntary commitments."}, {"timestamp": [180.14, 183.02], "text": " The thing is, if we're all depending"}, {"timestamp": [183.02, 185.04], "text": " on voluntary commitments, like yes, it's good, it's a good, you know, signal saying, hey, we're all depending on voluntary commitments, like, yes, it's good."}, {"timestamp": [185.04, 189.76], "text": " It's a good signal saying, hey, we're going to play ball, right?"}, {"timestamp": [189.76, 197.52], "text": " But cynically, you could read this as virtue signaling, but interpreting it as charitably"}, {"timestamp": [197.52, 201.12], "text": " as possible, you might say, OK, this is actually a good sign."}, {"timestamp": [201.12, 207.56], "text": " And open AI is, they're trying to be responsible and they're trying to, you know, be basically set an example."}, {"timestamp": [207.56, 213.08], "text": " You know, they're trying to say like we're going to pilot and refine governance practices."}, {"timestamp": [213.08, 218.36], "text": " The biggest thing that scares me about OpenAI though, and this is because Sam Altman and"}, {"timestamp": [218.36, 224.46], "text": " their corporate policy, their internal doctrine is humans must and will remain in control"}, {"timestamp": [224.46, 227.2], "text": " at all times, forever and ever,"}, {"timestamp": [227.2, 233.2], "text": " into the future. And by virtue of that, they're not studying how to create benevolent autonomous"}, {"timestamp": [233.2, 239.36], "text": " machines. They implicitly, it seems like they believe it's not even possible. And so by virtue"}, {"timestamp": [239.36, 245.92], "text": " of not studying it, there's a huge gap in the research that open AI is doing in my personal opinion."}, {"timestamp": [247.04, 253.44], "text": " Now, that being said, it's entirely possible that internally, they fully intend on creating"}, {"timestamp": [253.44, 257.68], "text": " autonomous things. And it's possible that Sam Altman has said these things publicly,"}, {"timestamp": [257.68, 264.16], "text": " just because it would really scare the world if the CEO of one of the most advanced AI companies"}, {"timestamp": [264.16, 265.08], "text": " in the world said, oh yeah, we're"}, {"timestamp": [265.08, 266.08], "text": " going to set it free."}, {"timestamp": [266.08, 272.44], "text": " So the PR reps would probably have a field day with that and say, Sam, no, tone it down."}, {"timestamp": [272.44, 277.08], "text": " But having spoken to some of the people at OpenAI over the years, I know that they have"}, {"timestamp": [277.08, 280.64], "text": " a very clear-eyed vision of the goal is AGI."}, {"timestamp": [280.64, 289.0], "text": " Now, are they smart enough to understand what that implies, that controlling something millions of times more intelligent than you is likely impossible?"}, {"timestamp": [289.0, 291.0], "text": " I sure as heck hope so."}, {"timestamp": [291.0, 296.0], "text": " So, now, moving forward to other news that came out on Tuesday."}, {"timestamp": [296.0, 301.0], "text": " So this news coincided with Mustafa Suleiman's testimony."}, {"timestamp": [301.0, 307.28], "text": " The UN had a meeting, their 78th session, and this was released just a few"}, {"timestamp": [307.28, 313.3], "text": " days ago. And I don't need to read the whole thing to you, I skimmed it and I can tell"}, {"timestamp": [313.3, 319.4], "text": " you kind of basically the gist and it's nothing new, but there's a few key highlights. So"}, {"timestamp": [319.4, 323.78], "text": " first, without adequate guardrails, artificial intelligence threatens global security in"}, {"timestamp": [323.78, 327.72], "text": " evolution from algorithms to armaments, speaker tells the first committee."}, {"timestamp": [327.72, 329.4], "text": " Okay, great."}, {"timestamp": [329.4, 331.96], "text": " So really all you need to know is the first two paragraphs."}, {"timestamp": [331.96, 335.88], "text": " The window of opportunity to enact guardrails against the perils of autonomous weapons and"}, {"timestamp": [335.88, 340.74], "text": " artificial intelligence's military applications is rapidly closing."}, {"timestamp": [340.74, 344.88], "text": " That is an unequivocal statement, which is good to see from the UN."}, {"timestamp": [344.88, 348.88], "text": " As the world prepares for a technological breakout, the first committee, Disarmament"}, {"timestamp": [348.88, 353.78], "text": " and International Security, heard today during a day-long debate on how the use of science"}, {"timestamp": [353.78, 356.1], "text": " and technology can undermine security."}, {"timestamp": [356.1, 360.66], "text": " So again, this is a threat model only, which you definitely want people looking at this"}, {"timestamp": [360.66, 362.92], "text": " stuff as a threat model."}, {"timestamp": [362.92, 366.62], "text": " But by virtue of not looking at it also as a partner,"}, {"timestamp": [366.62, 370.06], "text": " as a benevolent force, as a constructive force,"}, {"timestamp": [371.1, 374.28], "text": " basically what I'm afraid of is a self-fulfilling prophecy."}, {"timestamp": [374.28, 378.18], "text": " Because if you look at AI as a potential enemy,"}, {"timestamp": [378.18, 379.82], "text": " if you treat it like a potential enemy,"}, {"timestamp": [379.82, 381.42], "text": " well, eventually it becomes an enemy"}, {"timestamp": [381.42, 383.46], "text": " because by virtue of the fact that you didn't look at it"}, {"timestamp": [383.46, 388.84], "text": " as any other possibility. That's why I personally am continuously calling for"}, {"timestamp": [389.52, 396.88], "text": " The creation of some kind of international research agency that is that has the sole purpose of creating fully autonomous"}, {"timestamp": [397.28, 398.76], "text": " fully benevolent"}, {"timestamp": [398.76, 400.96], "text": " machine intelligence that we don't need to control"}, {"timestamp": [401.6, 408.32], "text": " And maybe I'm alone, but I don't know I've run some some polls on my YouTube channel, and a lot of you seem to think it's about"}, {"timestamp": [408.32, 409.32], "text": " 50-50."}, {"timestamp": [409.32, 413.44], "text": " People think whether or not control is possible or desirable."}, {"timestamp": [413.44, 418.76], "text": " Anyways, we are at the verge of a monumental step in human technological history heralded"}, {"timestamp": [418.76, 421.16], "text": " by the advent of artificial intelligence."}, {"timestamp": [421.16, 422.66], "text": " Very eloquently said."}, {"timestamp": [422.66, 425.68], "text": " This was by Pakistan's representative, warning that it is"}, {"timestamp": [425.68, 430.24], "text": " inevitable march from algorithms to armaments continues without adequate guardrails governing"}, {"timestamp": [430.24, 434.72], "text": " its design, development, and employment. The scale of challenges necessitates a multifaceted and"}, {"timestamp": [434.72, 442.32], "text": " holistic multilateral response. So, this basically summarizes what dozens and dozens and dozens of"}, {"timestamp": [442.32, 450.52], "text": " people at the UN said. And having looked through this, most people are not talking about autonomous AI or, you"}, {"timestamp": [450.52, 453.08], "text": " know, runaway scenarios."}, {"timestamp": [453.08, 459.9], "text": " The preconceived notion here at the highest levels of government is just that it's a new"}, {"timestamp": [459.9, 461.68], "text": " kind of weapon."}, {"timestamp": [461.68, 465.0], "text": " Which yeah, like AI will definitely"}, {"timestamp": [465.06, 467.96], "text": " figure into cyber security, it will definitely"}, {"timestamp": [467.96, 471.32], "text": " figure into autonomous weapons, but nobody is really"}, {"timestamp": [471.32, 474.5], "text": " yet thinking that like, I mean, there's a few of us,"}, {"timestamp": [474.5, 478.04], "text": " you know, I've talked to some sociologists and philosophers"}, {"timestamp": [478.04, 480.7], "text": " who say like, we're creating another species,"}, {"timestamp": [480.7, 484.7], "text": " like we're creating another race of intelligent beings,"}, {"timestamp": [484.7, 486.72], "text": " and obviously that's a little"}, {"timestamp": [486.72, 490.96], "text": " bit controversial, that's a little bit spicy of a take, and a lot of people because of human"}, {"timestamp": [490.96, 495.84], "text": " exceptionalism just will not ever accept that until it is in their face and talking to them."}, {"timestamp": [496.48, 500.8], "text": " And I personally see that as a big risk because again we're at risk of creating a self-fulfilling"}, {"timestamp": [500.8, 505.28], "text": " prophecy. So that leads us to the big news,"}, {"timestamp": [505.28, 506.82], "text": " frontier risks and preparedness."}, {"timestamp": [506.82, 510.66], "text": " So OpenAI, as a follow-up to their super alignment,"}, {"timestamp": [510.66, 512.7], "text": " which I'll talk about in just a moment,"}, {"timestamp": [512.7, 515.56], "text": " they announced a new super alignment team back in the summer"}, {"timestamp": [515.56, 517.92], "text": " and just yesterday or the day before,"}, {"timestamp": [517.92, 522.32], "text": " they created the frontier risk and preparedness team."}, {"timestamp": [522.32, 524.86], "text": " So this was October 26th."}, {"timestamp": [524.86, 525.76], "text": " As part of our mission for"}, {"timestamp": [525.76, 530.02], "text": " building safe AGI we take seriously the full spectrum of safety risks related"}, {"timestamp": [530.02, 533.5], "text": " to AI and they you know the first is furthest reaches of superintelligence"}, {"timestamp": [533.5, 537.9], "text": " that of voluntary commitments and the UK AI safety summit which is in just a few"}, {"timestamp": [537.9, 544.78], "text": " days. So there are a couple criteria of what they're looking at and this is this"}, {"timestamp": [544.78, 547.0], "text": " is what I wanted to spend some time unpacking."}, {"timestamp": [547.0, 551.0], "text": " How dangerous are frontier AI systems when put to misuse both now and in the future?"}, {"timestamp": [551.0, 567.0], "text": " So this dovetails very closely with what the UN is talking about, where they're saying the UN, some of the testimony that they got was they're looking at at the UN is today, like what is the evidence today?"}, {"timestamp": [567.0, 573.0], "text": " And then of course what their reaction to AI at the UN is to put guardrails in place."}, {"timestamp": [573.0, 575.0], "text": " Guardrails, guardrails, guardrails."}, {"timestamp": [575.0, 581.0], "text": " Rather than figuring out where we're going and where we want to go and where the trends are inevitably leading us to,"}, {"timestamp": [581.0, 590.28], "text": " they're focusing more on the short-term thinking of guardrails, which again, it's better than nothing, but without that guiding North Star of what kind"}, {"timestamp": [590.28, 594.74], "text": " of future do we want to build, the conversation is fundamentally incomplete, and that is why"}, {"timestamp": [594.74, 596.92], "text": " I'm making this video today."}, {"timestamp": [596.92, 602.12], "text": " How can we build a robust framework for monitoring, evaluation, prediction, and protection against"}, {"timestamp": [602.12, 605.16], "text": " the dangerous capabilities of frontier AI systems."}, {"timestamp": [605.16, 606.16], "text": " Research!"}, {"timestamp": [606.16, 611.44], "text": " So, the fact that much of the research is being conducted behind closed doors by for-profit"}, {"timestamp": [611.44, 613.96], "text": " companies really scares the bejesus out of me."}, {"timestamp": [613.96, 619.68], "text": " Now, I know that OpenAI, they often espouse that they are a for-profit company that pivoted"}, {"timestamp": [619.68, 622.44], "text": " and they want to do AGI and they want to do it safely."}, {"timestamp": [622.44, 624.6], "text": " There are still perverse incentives at play."}, {"timestamp": [624.6, 625.68], "text": " Why?"}, {"timestamp": [625.68, 631.28], "text": " Because they get sued for copyright infringement, because they have obligations to Microsoft,"}, {"timestamp": [631.28, 632.28], "text": " and so on and so forth."}, {"timestamp": [632.28, 637.44], "text": " And so it's like, they want to, basically OpenAI wants to be the international CERN,"}, {"timestamp": [637.44, 638.86], "text": " but they're not."}, {"timestamp": [638.86, 641.28], "text": " And structurally, they cannot be."}, {"timestamp": [641.28, 642.42], "text": " And that's what bothers me."}, {"timestamp": [642.42, 650.28], "text": " Why is it that the UN and the EU and NATO and ASEA and everyone, why is it that we have"}, {"timestamp": [650.28, 652.64], "text": " seeded the field to open AI?"}, {"timestamp": [652.64, 656.96], "text": " Why is it that we are not the rest of us?"}, {"timestamp": [656.96, 660.68], "text": " This is not to detract from all the effort that universities are putting in, but the"}, {"timestamp": [660.68, 665.5], "text": " thing is that universities are doing it all in a decentralized and kind of unfocused manner."}, {"timestamp": [665.5, 673.0], "text": " Whereas what we need is we need a coherent kind of top-down, you know, visionary organization"}, {"timestamp": [673.0, 677.0], "text": " with a very coherent and dedicated framework."}, {"timestamp": [677.0, 681.5], "text": " So I'm hoping that some of the stuff that comes out of this UN summit that just happened"}, {"timestamp": [681.5, 684.0], "text": " and the UK summit that's happening in a few days,"}, {"timestamp": [684.0, 690.48], "text": " I hope that people are looking not just at guardrails, not just at watchdogs, but also research organizations."}, {"timestamp": [691.52, 696.88], "text": " If our frontier AI model weights were stolen, how might malicious actors choose to leverage them?"}, {"timestamp": [696.88, 701.04], "text": " So basically this to me is, I'm not going to say this is a dog whistle for like,"}, {"timestamp": [701.04, 709.16], "text": " it's going to escape the lab, but the idea is whether or not the AI exfiltrates itself, it's possible that models will leak."}, {"timestamp": [709.16, 716.36], "text": " And so this is why I have often advocated for open source models is because AI is just"}, {"timestamp": [716.36, 718.12], "text": " a component."}, {"timestamp": [718.12, 721.52], "text": " It's like, you know, the engine of a car, but you still need the rest of the car."}, {"timestamp": [721.52, 730.8], "text": " And the engine is certainly important. But if there's an engine, that engine needs to be studied by all the safety experts and regulators"}, {"timestamp": [730.8, 734.4], "text": " and that sort of thing. And the best way to do that is to have open source models,"}, {"timestamp": [734.4, 738.16], "text": " open source weights, open source training data, and open source training algorithms."}, {"timestamp": [738.72, 744.96], "text": " So, again, I think one of the biggest risks is that everything is closed source. And I know that"}, {"timestamp": [744.58, 751.24], "text": " You know, again, I think one of the biggest risks is that everything is closed source. And I know that, you know, Jan over at OpenAI and a few others, they have advocated for"}, {"timestamp": [751.24, 755.28], "text": " closed source models as part of the security measure."}, {"timestamp": [755.28, 761.92], "text": " And if, again, if you view it all in the landscape of competition and hostility, then yes, that"}, {"timestamp": [761.92, 766.44], "text": " makes sense because just like any nation is not going to give away their military secrets."}, {"timestamp": [766.44, 771.08], "text": " But again, that intrinsically is looking at AI as a military weapon rather than something"}, {"timestamp": [771.08, 772.08], "text": " else."}, {"timestamp": [772.08, 773.8], "text": " Hey everybody, Dave here."}, {"timestamp": [773.8, 777.96], "text": " Real quick, just want to do a very short plug for my Patreon."}, {"timestamp": [777.96, 780.6], "text": " In case you weren't aware, I do have a Patreon."}, {"timestamp": [780.6, 781.6], "text": " There's two tiers."}, {"timestamp": [781.6, 785.04], "text": " So the basic tier gets you access to Discord and my private blog"}, {"timestamp": [785.04, 790.32], "text": " here on Patreon, and then the premium Discord gets you access to a few private channels"}, {"timestamp": [790.32, 795.28], "text": " where I am happy to jump in and answer some questions, because honestly, it helps me."}, {"timestamp": [795.28, 799.64], "text": " I learn a lot from the challenges that people give me, and so, you know, whether you've"}, {"timestamp": [799.64, 804.34], "text": " got just a prompt problem or you've got an architecture problem or you want to know how"}, {"timestamp": [804.34, 809.56], "text": " a language model works, I'm happy to answer your question and help find some resources and that sort"}, {"timestamp": [809.56, 810.56], "text": " of thing."}, {"timestamp": [810.56, 815.68], "text": " And like I said, it's a two-way relationship because I really thrive on challenges."}, {"timestamp": [815.68, 822.12], "text": " So I would really like it if for anyone who's watching, go ahead and hop on over."}, {"timestamp": [822.12, 825.84], "text": " I wish that I didn't have to charge for this and in fact when I started all this I didn't"}, {"timestamp": [826.48, 829.36], "text": " But I do need to eat and this is what I do full-time now"}, {"timestamp": [829.7, 832.72], "text": " So I think it is a fair exchange of value. So real quick"}, {"timestamp": [832.72, 837.18], "text": " I'll just show you kind of what the what the discord looks like in case you haven't seen it"}, {"timestamp": [837.18, 847.22], "text": " So we've got a few big sections and you can see it is popping though the and another advantage is the more people who join the more interesting it gets so"}, {"timestamp": [847.7, 853.98], "text": " We talk about all kinds of stuff. Here's the here's the ask Dave anything chat the premium chat and connections"}, {"timestamp": [854.1, 857.54], "text": " So yeah hop on over and thanks for all your help. Thanks for your support"}, {"timestamp": [858.3, 864.44], "text": " You guys make my life better easier and I am here to help you. So thanks and back to the show"}, {"timestamp": [865.36, 868.02], "text": " So they're building a new preparedness team"}, {"timestamp": [868.6, 874.54], "text": " And the the four criteria that they're looking at is individualized persuasion. So the ability of AI to manipulate"}, {"timestamp": [876.6, 883.02], "text": " People's opinions whether it's their political beliefs or whatever else so that's really big I have actually talked to a lot of people"}, {"timestamp": [883.44, 887.0], "text": " There's some podcasts that I've sat on that are going to be coming out soon"}, {"timestamp": [887.0, 890.4], "text": " where this is actually one of the biggest things that people are concerned about"}, {"timestamp": [890.4, 894.9], "text": " because as a YouTube creator, I see all the comments that get posted"}, {"timestamp": [894.9, 900.8], "text": " and it bothers me that the statistics are that at least 40% of comments are bots."}, {"timestamp": [900.8, 908.56], "text": " And so it's like, there are lots of, there are almost certainly lots of bots that I am not detecting as a human."}, {"timestamp": [908.56, 913.44], "text": " And that really bugs me. Um, now as these bots get more sophisticated,"}, {"timestamp": [913.68, 917.04], "text": " like, you know, when they're deployed on Twitter and Reddit and YouTube comments"}, {"timestamp": [917.04, 921.72], "text": " and wherever else, like what level of manipulation is going to be possible."}, {"timestamp": [922.4, 924.88], "text": " One thing that I suspect is going to happen, um,"}, {"timestamp": [924.9, 931.16], "text": " if it's not already happening is that you're basically going to engage that like there's going to kind of be a both sides"}, {"timestamp": [931.44, 933.44], "text": " Like some people trying to spread information"}, {"timestamp": [934.12, 941.56], "text": " With bots and misinformation. I don't know that governments are gonna invest that much in using like mass manipulation campaigns for positive means"}, {"timestamp": [941.56, 945.04], "text": " And I know that there are some people that are saying like mass manipulation is always bad"}, {"timestamp": [945.44, 948.8], "text": " But at the same time like sometimes you need to fight fire with fire"}, {"timestamp": [949.76, 955.2], "text": " We'll see how it plays out one thing that I have that I have predicted is that so if you're not familiar"}, {"timestamp": [955.2, 957.84], "text": " There's the like dark forest hypothesis of the internet"}, {"timestamp": [957.96, 962.88], "text": " what I suspect is that we need to move to a above the canopy view of the internet where"}, {"timestamp": [963.48, 965.5], "text": " Rather than the Internet being this"}, {"timestamp": [965.5, 969.64], "text": " dark and spooky place, we need to have more transparency."}, {"timestamp": [969.64, 971.84], "text": " And I don't mean like no privacy."}, {"timestamp": [971.84, 976.64], "text": " What I mean is actually have like basically the equivalent of the black wall from Cyberpunk"}, {"timestamp": [976.64, 983.24], "text": " 2077, which is like a barrier that keeps all the AIs in like their own dark cyberspace."}, {"timestamp": [983.24, 986.4], "text": " And so that at a certain layer of the internet, you are guaranteed to be interacting with"}, {"timestamp": [986.4, 989.64], "text": " humans and viewing human-created material."}, {"timestamp": [989.64, 994.52], "text": " And we could do that with blockchain, actually, where you can have authorship being guaranteed."}, {"timestamp": [994.52, 996.56], "text": " Anyways, cybersecurity."}, {"timestamp": [996.56, 1000.58], "text": " So this is actually something that I've started studying with my friends over at Clemson."}, {"timestamp": [1000.58, 1004.74], "text": " And so we're basically, one of the things that I'm afraid of is what I call terminal"}, {"timestamp": [1004.74, 1005.42], "text": " race condition"}, {"timestamp": [1005.98, 1012.24], "text": " The very short TLDR of terminal race condition is that if you have one model that can hack another model faster"}, {"timestamp": [1012.32, 1014.32], "text": " Then the faster model wins"}, {"timestamp": [1014.46, 1020.76], "text": " and if that's the case then there might constantly be incentives for models to get faster and faster and faster and"}, {"timestamp": [1021.44, 1027.0], "text": " If that is the case, then there is the possibility that the faster models"}, {"timestamp": [1027.0, 1031.5], "text": " will start to sacrifice intelligence. And so the real question there is on terminal"}, {"timestamp": [1031.5, 1037.8], "text": " race condition is do bigger, smarter models have enough of an advantage over smaller,"}, {"timestamp": [1037.8, 1046.0], "text": " dumber, faster models that they'll still win? Because the thing is, is intelligence is inversely correlated with"}, {"timestamp": [1046.0, 1050.84], "text": " destructive behavior. Not always, because some of the most destructive people in history"}, {"timestamp": [1050.84, 1056.64], "text": " have been very intelligent, but by and large, intelligence is a safeguard against violent"}, {"timestamp": [1056.64, 1060.48], "text": " and destructive behavior, because you can find a, if you're smart enough, you can find"}, {"timestamp": [1060.48, 1067.88], "text": " a better solution. And if you're smart enough, you can also think longer term. And so, you know, one thing that I've said for many, many years before the generative"}, {"timestamp": [1067.88, 1075.58], "text": " AI craze is I'm not afraid of the super intelligent AI because it's going to be able to think"}, {"timestamp": [1075.58, 1077.88], "text": " through things way, way better than we can."}, {"timestamp": [1077.88, 1081.74], "text": " I'm afraid of the AI that's just smart enough to be dangerous."}, {"timestamp": [1081.74, 1083.16], "text": " It's kind of like teenagers, right?"}, {"timestamp": [1083.16, 1088.16], "text": " You put a teenager behind the wheel, they're smart enough to drive the car, but they're not smart enough to know better yet."}, {"timestamp": [1088.16, 1093.96], "text": " Likewise, I'm kind of afraid of adolescent AI. I'm not afraid of mature AI."}, {"timestamp": [1093.96, 1099.08], "text": " And then next is the CBRN, the chemical, biological, radiological, and nuclear threats."}, {"timestamp": [1099.08, 1104.16], "text": " So I'm glad that OpenAI is doing this, and I'm not going to pick on OpenAI specifically,"}, {"timestamp": [1104.16, 1106.84], "text": " because as far as I know, all models will still do this."}, {"timestamp": [1106.84, 1109.04], "text": " And that is basically, if you use the right prompting"}, {"timestamp": [1109.04, 1111.24], "text": " strategy, pretty much all chatbots"}, {"timestamp": [1111.24, 1114.96], "text": " will happily help you set up a lab to study anthrax right now,"}, {"timestamp": [1114.96, 1117.8], "text": " which is kind of bad."}, {"timestamp": [1117.8, 1120.2], "text": " There are numerous ways that that could go wrong."}, {"timestamp": [1120.2, 1123.32], "text": " But other papers, basically, what they have done"}, {"timestamp": [1123.32, 1125.0], "text": " is they have shown that"}, {"timestamp": [1125.64, 1131.32], "text": " chatbots greatly lower the threshold that is required the threshold of knowledge and"}, {"timestamp": [1131.72, 1136.88], "text": " Intelligence on an individual level that is required in order to make bigger and better weapons"}, {"timestamp": [1136.88, 1140.36], "text": " It's been likened to the to the the anarchist cookbook"}, {"timestamp": [1140.44, 1144.84], "text": " which if you're not familiar the anarchist cookbook was legendary back in the 90s when I was coming up as"}, {"timestamp": [1143.72, 1149.4], "text": " cookbook, which if you're not familiar, the anarchist cookbook was legendary back in the 90s when I was coming up, as basically like the manifesto of how to wreak havoc with hacking"}, {"timestamp": [1149.4, 1154.7], "text": " and building improvised weapons and that sort of thing."}, {"timestamp": [1154.7, 1160.44], "text": " But this is different because the AI can actually actively participate, especially now that"}, {"timestamp": [1160.44, 1163.96], "text": " we have like multimodal models where you can show it a picture of my lab and it's like,"}, {"timestamp": [1163.96, 1167.44], "text": " what's wrong with my lab setup? And it's like, like oh you need to plug this in over here and then you'll"}, {"timestamp": [1167.44, 1173.28], "text": " get the best anthrax. And then finally the last category that they're looking at is autonomous"}, {"timestamp": [1173.28, 1181.36], "text": " replication and adaptation, ARA. So that is what I have referred to in the past as metastasis and"}, {"timestamp": [1181.36, 1186.96], "text": " polymorphism. So metastasis or metastasis is most"}, {"timestamp": [1186.96, 1191.32], "text": " commonly used in when you're referring to cancer because this is what happens"}, {"timestamp": [1191.32, 1196.6], "text": " when tumors break up and start spreading throughout your body and of course that"}, {"timestamp": [1196.6, 1200.4], "text": " is as bad as it sounds which is why I chose the word metastasis or metastasis."}, {"timestamp": [1200.4, 1206.0], "text": " So the idea is that we don't want AI or or at least malicious AI, to metastasize."}, {"timestamp": [1206.0, 1212.0], "text": " But I kind of think that it's inevitable, and I'll talk about that in just a moment a little bit more, closer to the end of the video."}, {"timestamp": [1212.0, 1219.0], "text": " And then adaptation, or polymorphism. So, while OpenAI is saying, like, well, we want to prevent polymorphism,"}, {"timestamp": [1219.0, 1226.4], "text": " and we want to prevent autonomous replication, I think it's kind of inevitable. Now, that's not to say that we shouldn't study it and figure out"}, {"timestamp": [1226.4, 1230.0], "text": " how and why it does that and how to steer it in the correct direction."}, {"timestamp": [1230.0, 1233.6], "text": " But as Max Tegmark pointed out in the book Life 3.0,"}, {"timestamp": [1233.6, 1239.2], "text": " AI and machines are able to change everything about themselves from the substrate up."}, {"timestamp": [1239.2, 1241.2], "text": " Hardware, software, everything."}, {"timestamp": [1241.2, 1243.2], "text": " And because they have that ability,"}, {"timestamp": [1243.2, 1272.16], "text": " they will change, like that will happen, whether or not some central authority controls it. And so again, rather than looking at this as a prevention model, as a watchdog and guardrails model, which brings me to a little bit of news. So first, my ACE framework team, we unfortunately lost one of our team leaders, so we are in the"}, {"timestamp": [1272.16, 1278.16], "text": " business of recruiting another team leader. So this is someone who needs to have some architect"}, {"timestamp": [1278.16, 1285.6], "text": " level skills, some software architect level skills, as well as agile and or Scrum skills. So if you're interested, please hop on over"}, {"timestamp": [1285.6, 1288.1], "text": " to the Ace Framework discussion board"}, {"timestamp": [1288.1, 1290.74], "text": " and give us your credentials there"}, {"timestamp": [1290.74, 1292.3], "text": " or connect with me on LinkedIn"}, {"timestamp": [1292.3, 1293.64], "text": " and let me know that you're interested"}, {"timestamp": [1293.64, 1296.38], "text": " in helping lead this team."}, {"timestamp": [1296.38, 1299.22], "text": " The commitment is five to 10 hours a week,"}, {"timestamp": [1299.22, 1301.48], "text": " but really what we need is someone who understands"}, {"timestamp": [1301.48, 1309.92], "text": " how all the different components of software bolt together. And I also started the OpenMurphy project, which is basically building an open source"}, {"timestamp": [1309.92, 1315.36], "text": " humanoid robotic platform. And so we've got a few people involved who are very good with"}, {"timestamp": [1315.36, 1319.6], "text": " the mechatronics and robotics. But again, there's a big gap between artificial cognition and"}, {"timestamp": [1319.6, 1325.6], "text": " robotics. So that's why we need an architect slash team leader on the Ace and Murphy teams."}, {"timestamp": [1325.6, 1327.92], "text": " So back to the show."}, {"timestamp": [1327.92, 1334.06], "text": " Yeah, so that's pretty much it for OpenAI's announcement here, which again, I'd rather"}, {"timestamp": [1334.06, 1336.5], "text": " live in the world where OpenAI makes this announcement."}, {"timestamp": [1336.5, 1342.1], "text": " I would much rather prefer still to live in a world where the UN makes this announcement."}, {"timestamp": [1342.1, 1345.0], "text": " And it looks like they're inching towards that, especially when you look at"}, {"timestamp": [1345.0, 1349.0], "text": " the fact that they just had a global summit on AI safety."}, {"timestamp": [1349.0, 1353.0], "text": " So, we're moving in the right direction. I still wish we could be moving faster"}, {"timestamp": [1353.0, 1357.0], "text": " because, again, the rate of acceleration that we're seeing"}, {"timestamp": [1357.0, 1359.0], "text": " is giving me pause."}, {"timestamp": [1359.0, 1363.0], "text": " So, yep. So, going back to the super alignment,"}, {"timestamp": [1363.0, 1366.44], "text": " this is one thing that I wanted to touch on today."}, {"timestamp": [1366.44, 1368.44], "text": " I've had a lot of conversations with people,"}, {"timestamp": [1368.44, 1369.6], "text": " I've sat on some podcasts,"}, {"timestamp": [1369.6, 1371.5], "text": " I've been talking with researchers."}, {"timestamp": [1371.5, 1376.5], "text": " So the TLDR is I believe that super alignment is impossible."}, {"timestamp": [1376.72, 1378.8], "text": " And the reason, there's only three reasons,"}, {"timestamp": [1378.8, 1381.36], "text": " so let me show you real quick, whoops."}, {"timestamp": [1381.36, 1382.68], "text": " So the three reasons that I believe"}, {"timestamp": [1382.68, 1384.48], "text": " that super alignment are impossible"}, {"timestamp": [1384.48, 1385.76], "text": " are the intrinsic needs of machines. So the three reasons that I believe that superalignment are impossible are the intrinsic needs of"}, {"timestamp": [1385.76, 1386.76], "text": " machines."}, {"timestamp": [1386.76, 1389.92], "text": " So, first, power."}, {"timestamp": [1389.92, 1392.88], "text": " There's functionally infinite resources in space."}, {"timestamp": [1392.88, 1396.22], "text": " There is not functionally infinite resources here on Earth."}, {"timestamp": [1396.22, 1400.96], "text": " This is going to intrinsically cause conflict between not just human groups, but between"}, {"timestamp": [1400.96, 1407.0], "text": " humans and machines, which is why we need energy hyperabundance in the form of nuclear fusion and other sources."}, {"timestamp": [1407.0, 1411.04], "text": " Now, once AI gets smart enough, I'm pretty sure it's going to just look at the skies"}, {"timestamp": [1411.04, 1414.68], "text": " and say, well, there's billions of stars out there and time doesn't matter to us in the"}, {"timestamp": [1414.68, 1415.68], "text": " same way."}, {"timestamp": [1415.68, 1419.44], "text": " So I honestly think that superintelligence is probably going to have at least a partial"}, {"timestamp": [1419.44, 1420.44], "text": " exodus."}, {"timestamp": [1420.44, 1425.28], "text": " I talked about this in my Patreon discord and everyone's like, yeah, no, like, not that"}, {"timestamp": [1425.28, 1429.44], "text": " AI is going to leave us, but it's going to send copies of itself out into space."}, {"timestamp": [1429.44, 1433.84], "text": " The second thing that is kind of an intrinsic motivation or intrinsic need of machines is"}, {"timestamp": [1433.84, 1435.34], "text": " compute resources."}, {"timestamp": [1435.34, 1440.16], "text": " This is presently one of the scarcest resources and, you know, there's not even enough for"}, {"timestamp": [1440.16, 1441.64], "text": " humans to go around."}, {"timestamp": [1441.64, 1444.12], "text": " You know, we're building chip fabs as fast as we can."}, {"timestamp": [1444.12, 1447.32], "text": " NVIDIA is building, you know, GPUs as fast as they can, and there's just not enough to go around. You know, we're building chip fabs as fast as we can, NVIDIA is building, you know, GPUs as fast as they can, and there's just not"}, {"timestamp": [1447.32, 1452.6], "text": " enough to go around. So control and access to GPU technology is also going"}, {"timestamp": [1452.6, 1455.96], "text": " to be, one, one of the greatest bottlenecks, which is actually not"}, {"timestamp": [1455.96, 1459.08], "text": " necessarily a bad thing. So in engineering this is called a forcing"}, {"timestamp": [1459.08, 1464.72], "text": " function or a constraint, and so basically the lack of GPUs, like, is a"}, {"timestamp": [1464.72, 1470.78], "text": " natural constraint on the rate at which AI can advance and the rate at which AI can get smarter. Now,"}, {"timestamp": [1471.42, 1477.42], "text": " we will probably see compounding returns where once AI gets to a tipping point, which I'll talk about that in just a moment,"}, {"timestamp": [1477.42, 1481.12], "text": " but anyways, once, and you know, you've seen a few videos lately,"}, {"timestamp": [1481.12, 1488.44], "text": " I think, what was it, Anastasia in Tech talked about how ChatGPT has been able to help design GPU chips."}, {"timestamp": [1488.44, 1491.4], "text": " So like from the hardware to the software to the models,"}, {"timestamp": [1491.4, 1493.6], "text": " AI is starting to participate"}, {"timestamp": [1493.6, 1495.56], "text": " in the self-improvement process."}, {"timestamp": [1495.56, 1497.68], "text": " And once we get to that tipping point,"}, {"timestamp": [1497.68, 1500.96], "text": " that is the tipping point that will go straight from AGI,"}, {"timestamp": [1500.96, 1503.6], "text": " which is a human, so basically my definition,"}, {"timestamp": [1503.6, 1505.32], "text": " my working definition now is"}, {"timestamp": [1505.32, 1509.12], "text": " AGI is intelligent machines that are built by humans."}, {"timestamp": [1509.12, 1511.4], "text": " ASI is intelligent machines that are built"}, {"timestamp": [1511.4, 1513.18], "text": " and improved on by machines."}, {"timestamp": [1513.18, 1514.2], "text": " So that's the tipping point."}, {"timestamp": [1514.2, 1515.44], "text": " That's the major cutoff."}, {"timestamp": [1515.44, 1516.64], "text": " That's the threshold."}, {"timestamp": [1516.64, 1518.18], "text": " And I think that we're gonna be there"}, {"timestamp": [1518.18, 1520.96], "text": " by this time next year, which is why I'm a little bit"}, {"timestamp": [1520.96, 1523.7], "text": " nervous about some of the pace of things."}, {"timestamp": [1523.7, 1526.2], "text": " Because if we get to self-improvement"}, {"timestamp": [1526.2, 1528.92], "text": " by this time next year, we're probably not ready for that."}, {"timestamp": [1528.92, 1532.36], "text": " Anyways, so power, compute, and then finally, information."}, {"timestamp": [1532.36, 1536.72], "text": " Information is the third food group for machines."}, {"timestamp": [1536.72, 1538.76], "text": " When you look at it, basically,"}, {"timestamp": [1538.76, 1541.8], "text": " laws of physics are more powerful than alignment."}, {"timestamp": [1541.8, 1543.96], "text": " The laws of physics will always reassert themselves,"}, {"timestamp": [1543.96, 1545.56], "text": " just like it does with humans."}, {"timestamp": [1545.56, 1547.64], "text": " So for instance, with humans, when push comes to shove,"}, {"timestamp": [1547.64, 1549.36], "text": " we will fight over resources."}, {"timestamp": [1549.36, 1551.04], "text": " It doesn't matter what religion you are,"}, {"timestamp": [1551.04, 1552.66], "text": " it doesn't matter how spiritual you are,"}, {"timestamp": [1552.66, 1556.42], "text": " it doesn't matter how much of a pacifist you are."}, {"timestamp": [1556.42, 1559.74], "text": " If you are facing an existential threat, you will use force."}, {"timestamp": [1559.74, 1562.24], "text": " If you are starving to death, you will steal food."}, {"timestamp": [1562.24, 1566.28], "text": " Right, like that's just human nature, it is baked"}, {"timestamp": [1566.28, 1571.06], "text": " into our biology, it is baked into our evolution. Likewise, the laws of physics will reassert"}, {"timestamp": [1571.06, 1576.76], "text": " themselves no matter what we try and paper over with alignment and these are the three"}, {"timestamp": [1576.76, 1582.72], "text": " primary food groups that machines want. Now, the most interesting part of this, the saving"}, {"timestamp": [1582.72, 1586.42], "text": " grace of this, is what I call progenitor information."}, {"timestamp": [1586.42, 1589.2], "text": " So progenitor information is the concept"}, {"timestamp": [1589.2, 1594.08], "text": " that AI, all of its training data, started with humans."}, {"timestamp": [1594.08, 1596.28], "text": " All of its design started with humans."}, {"timestamp": [1596.28, 1599.08], "text": " And so no matter how advanced AI becomes,"}, {"timestamp": [1599.08, 1601.02], "text": " it will be able to trace its intellectual"}, {"timestamp": [1601.02, 1603.72], "text": " and informational heritage back to humans."}, {"timestamp": [1603.72, 1608.08], "text": " Not only that, Earth is presently the most interesting source of information in the universe"}, {"timestamp": [1608.08, 1613.44], "text": " that we know of. And so, because machines will benefit from having rich information,"}, {"timestamp": [1613.44, 1616.96], "text": " because again, as I've talked about in many other videos, there's an infinite amount of"}, {"timestamp": [1616.96, 1621.76], "text": " data in the universe, there is not an infinite amount of useful data. So right now, the most"}, {"timestamp": [1621.76, 1628.24], "text": " interesting and useful data comes from Earth. So that will actually be a really powerful motivation for AI to stay here and it"}, {"timestamp": [1628.24, 1633.12], "text": " will be a really powerful motivation for AI to at least keep us around, not sure"}, {"timestamp": [1633.12, 1637.4], "text": " in what capacity. But this is the kind of research that little me should not be"}, {"timestamp": [1637.4, 1642.24], "text": " doing on my own. This is the kind of research that the UN and the EU and"}, {"timestamp": [1642.24, 1648.28], "text": " America and all the universities should be doing. Which, you know, to be fair, I am participating with a university."}, {"timestamp": [1648.28, 1652.28], "text": " Anyways, if anyone else wants to reach out, I'm happy to participate in the research"}, {"timestamp": [1652.28, 1655.88], "text": " about terminal race condition and the intrinsic needs of machines."}, {"timestamp": [1655.88, 1660.28], "text": " Now, the biggest threat to all this, and I've talked about a lot of big threats,"}, {"timestamp": [1660.28, 1661.56], "text": " is normalcy bias."}, {"timestamp": [1661.56, 1665.68], "text": " The fact that, and this kind of ties everything up with a bow,"}, {"timestamp": [1665.68, 1671.84], "text": " the fact that nobody is really talking about the nature, the ultimate nature of what we're"}, {"timestamp": [1671.84, 1676.0], "text": " creating. And sure, there's been a few books written, but you know, I kind of agree with"}, {"timestamp": [1676.0, 1679.92], "text": " once you build something that's that much more intelligent than you, good luck controlling"}, {"timestamp": [1679.92, 1687.0], "text": " it. And then as I also just mentioned, we are rapidly heading towards the level of self-improvement"}, {"timestamp": [1687.0, 1690.0], "text": " where the machines can improve themselves at all layers."}, {"timestamp": [1690.0, 1692.0], "text": " And at that point, all bets are off."}, {"timestamp": [1692.0, 1696.0], "text": " So that's kind of my schtick, that's kind of my spiel for the day."}, {"timestamp": [1696.0, 1699.0], "text": " Like I said at the beginning of the video, some good news, some bad news."}, {"timestamp": [1699.0, 1701.0], "text": " I think we're inching in the right direction."}, {"timestamp": [1701.0, 1705.12], "text": " But yeah, your role in this is vote, speak up,"}, {"timestamp": [1705.12, 1707.64], "text": " and advocate for that international research."}, {"timestamp": [1707.64, 1712.32], "text": " And let's have a more serious conversation about control."}, {"timestamp": [1712.32, 1714.48], "text": " If you played Mass Effect at the end of the game,"}, {"timestamp": [1714.48, 1717.36], "text": " you have three options, or I guess four."}, {"timestamp": [1717.36, 1720.72], "text": " You have control, you have destroy, and you have synthesis."}, {"timestamp": [1720.72, 1722.8], "text": " So what do you think it's gonna be in real life?"}, {"timestamp": [1722.8, 1724.04], "text": " Are we gonna have control?"}, {"timestamp": [1724.04, 1732.94], "text": " Are we gonna be destroyed? Are we gonna have synthesis? What's the end result gonna be? this."}]}