{"text": " One of the questions that I get all the time is, Dave, how do you keep up with research and how are you so productive? And I frequently get questions about, you know, what's my workflow and what's my process. And I'm not gonna show you my full process because, well, it's really kind of custom personalized to me but what I will show you is a tool that I just built that's based on, you know, my scripting abilities and all that sort of stuff. And this tool is just too useful to keep private, so it is already public. It's called Weekly underscore Archive. And that's just kind of a general description of what it does. Basically what it does is it's this nice, handy dandy little Python script that it just asks you for a lookup query and then it goes to the to the archive API and we'll search for that query and download the latest 200 abstracts and then it will format it into Markdown for you. So let me show you an example of what the output looks like. So I put in just LLMs. So LLMS is the query, and it gave me this nice searchable document that renders nicely, and I've got it so that it gives you the title, the link to the paper, list the authors, so if you want to search by author, and then the summary. So for instance, I was doing research and I wanted to learn about driving. And so it turns out there are four papers out in the last week alone that have to do with using large language models for driving, for autonomous driving. So first is Language MPC, so large language models as decision makers for autonomous driving. Driving with LLMs, fusing object-level vector modality for explainable autonomous driving. And then, what was the next one? The next one was GPT driver learning to drive with GPT, so that's pretty cool. And then finally drive GPT-4 interpretable end-to-end autonomous driving via large language model. So, that's really cool! But, the archive website is not necessarily the most searchable thing. And the reason is, and the primary reason is because one, you can only have up to 200 results per page, and you have to expand the abstracts. I was like, okay, well, I wanted to download this page and re-render it without violating, you know, archives thing, but also I wanted it to be eminently renderable so that I could drop it into Claude. Because Claude, you can see where I just copy-pasted a whole bunch of stuff, basically titles and abstracts, into Claude and then I'm like, hey, tell me what the latest trends are. And so, for instance, I discovered, you know, many papers are focusing on LLM's abilities to do things like reasoning, planning, and knowledge intensive tasks. And then you can scroll down and say, okay, cool, give me a list of the papers that do this. So instead of manually scrolling through, you know, like hundreds and hundreds of archive papers in order to get a feel for where the industry is, I just like said, okay, hey, tell me what some of the trends are to Claude. Oh, with Claude you have to be very, very explicit. I said, please write a summary of the benchmarking trends as elucidated by the text I gave you. If you don't do that, it'll just make stuff up. Claude is still really, really bad about hallucinating. And the thing is there is very clearly some archive papers in its training data. So it'll, it'll, if you don't tell it only use what I gave you, then it'll start either making up archive papers or citing papers that are much older. And I'm like, no, make sure you only pull from the information I gave you in this conversation. And then like it will change the formatting on you if you're not careful. And I'm like, why did you change the formatting on me? Anyways, so they still have a little bit of work to do on Clouds, but that's not the point of the video. The point of the video is that they have a nice free open source search that gives you this gigantic wad of XML, which is a nightmare. But you know, it is what it is. It's XML. And it's free, so don't look a gift horse in the mouth. And, yeah, so how you use this... And actually, let me show you a couple other ones. So here's the first one that I did. So if you want to look at today, the last week's stuff of large language models, click on that one. I did one where I just searched for bacon as a test to see what it would come up with and apparently there's a lot of authors named bacon but there's not a whole lot of archived papers on the food bacon so that was really disappointing. But you know whatever. And then I did another one for quantum computing because I wanted to make sure that the input would be able to handle spaces. So basically it has to be URL encoded which is really simple you just replace it with %20 and away it goes. You can see it's taking a minute to render. I'm not sure why it's taking so long. I think I broke GitHub. Anyways, we will wait. So yeah, really simple tool, really straightforward. Let's do a quick refresh. There we go. Okay, so check on quantum computing. It might not load. But yeah, so very straightforward. You just run this pie, this this pie file. And so I'll walk you through the script real quick, just in case you want to see it. But it just so the first thing is it takes the input, what you want to look up, you type it in, it does it in lowercase and replaces any whitespace with a percent 20. It'll go, it'll populate the URL. Oh, another thing, you can change this URL if you want to search by something else, if you want to search for author or change the max results. I have the sort by last updated date and sort order descending. So this will get the latest stuff because really the problem I was trying to solve was everything is moving so fast so it's like I wanted to just have a way of figuring out what is going on with the world at a glance in terms of large language models because it's like there's LoRa and QLoRa and there's all these kinds of benchmarks coming out. And so what you can do is you can just like come to this and say, alright, let's look up benchmarks. So from Words to Watch, benchmarking the energy cost of large language model. Cool! Great paper, glad this exists. And you can see there's 115 references to benchmarks in this. T dollar cubed bench, benchmarking current progress in text-to-3D generation. Great, cool. Now I know that we're benchmarking text-to-3D generation. Let's see, shadow alignment. So that sounds cool. It's like shadow banning people. Do they still do that on Reddit and Twitter? Anyways, yeah, so GPT fuzzer, right. Okay, great. Anyways, idea here is that you can download hundreds, if not thousands of the latest, latest papers on any particular topic from archive, whether or not it's like large language models. You can you can put in quantum computing, like I said, but for whatever reason it's not rendering directly on GitHub. I broke it. But yeah, you can download an arbitrary number of things. It's sorted by date, by release date. It is in descending order so you know you've got the latest and greatest. Yeah, I think that's about it. So, thanks for watching. Cheers, and yep, I definitely broke it. Have a good one. greatest. Yeah, I think that's about it. So thanks for watching. Cheers and yep, I definitely broke it. Have a good one.", "chunks": [{"timestamp": [0.0, 2.5], "text": " One of the questions that I get all the time is,"}, {"timestamp": [2.5, 4.24], "text": " Dave, how do you keep up with research"}, {"timestamp": [4.24, 6.52], "text": " and how are you so productive?"}, {"timestamp": [6.52, 8.32], "text": " And I frequently get questions about,"}, {"timestamp": [8.32, 10.8], "text": " you know, what's my workflow and what's my process."}, {"timestamp": [10.8, 13.52], "text": " And I'm not gonna show you my full process"}, {"timestamp": [13.52, 17.12], "text": " because, well, it's really kind of custom personalized to me"}, {"timestamp": [17.12, 20.6], "text": " but what I will show you is a tool that I just built"}, {"timestamp": [20.6, 23.24], "text": " that's based on, you know, my scripting abilities"}, {"timestamp": [23.24, 25.64], "text": " and all that sort of stuff."}, {"timestamp": [25.64, 28.6], "text": " And this tool is just too useful to keep private,"}, {"timestamp": [28.6, 30.74], "text": " so it is already public."}, {"timestamp": [30.74, 32.96], "text": " It's called Weekly underscore Archive."}, {"timestamp": [32.96, 35.72], "text": " And that's just kind of a general description"}, {"timestamp": [35.72, 36.56], "text": " of what it does."}, {"timestamp": [36.56, 39.28], "text": " Basically what it does is it's this nice,"}, {"timestamp": [39.28, 41.28], "text": " handy dandy little Python script"}, {"timestamp": [41.28, 46.08], "text": " that it just asks you for a lookup query and then it goes to the"}, {"timestamp": [46.08, 53.68], "text": " to the archive API and we'll search for that query and download the latest 200"}, {"timestamp": [53.68, 58.68], "text": " abstracts and then it will format it into Markdown for you. So let me show you"}, {"timestamp": [58.68, 67.12], "text": " an example of what the output looks like. So I put in just LLMs. So LLMS is the query, and it gave me this nice"}, {"timestamp": [68.56, 72.8], "text": " searchable document that renders nicely, and I've got it so that it gives you the title,"}, {"timestamp": [72.8, 77.84], "text": " the link to the paper, list the authors, so if you want to search by author, and then the summary."}, {"timestamp": [77.84, 86.0], "text": " So for instance, I was doing research and I wanted to learn about driving. And so it turns out there are four papers out"}, {"timestamp": [86.0, 89.0], "text": " in the last week alone that have to do with"}, {"timestamp": [89.0, 92.0], "text": " using large language models for driving, for autonomous driving."}, {"timestamp": [92.0, 95.0], "text": " So first is Language MPC, so large language"}, {"timestamp": [95.0, 98.0], "text": " models as decision makers for autonomous driving."}, {"timestamp": [98.0, 101.0], "text": " Driving with LLMs, fusing object-level vector modality"}, {"timestamp": [101.0, 104.0], "text": " for explainable autonomous driving. And then, what was the next one?"}, {"timestamp": [104.0, 106.0], "text": " The next one was"}, {"timestamp": [106.0, 108.0], "text": " GPT driver learning to drive"}, {"timestamp": [108.0, 110.0], "text": " with GPT, so that's pretty cool."}, {"timestamp": [110.0, 112.0], "text": " And then finally"}, {"timestamp": [112.0, 114.0], "text": " drive GPT-4 interpretable end-to-end"}, {"timestamp": [114.0, 116.0], "text": " autonomous driving via large language"}, {"timestamp": [116.0, 118.0], "text": " model. So, that's really"}, {"timestamp": [118.0, 120.0], "text": " cool! But,"}, {"timestamp": [120.0, 122.0], "text": " the archive website"}, {"timestamp": [122.0, 124.0], "text": " is not necessarily"}, {"timestamp": [124.0, 125.5], "text": " the most searchable thing."}, {"timestamp": [125.5, 129.5], "text": " And the reason is, and the primary reason is because one,"}, {"timestamp": [129.5, 133.0], "text": " you can only have up to 200 results per page,"}, {"timestamp": [133.0, 137.5], "text": " and you have to expand the abstracts."}, {"timestamp": [137.5, 143.0], "text": " I was like, okay, well, I wanted to download this page and re-render it without violating, you know,"}, {"timestamp": [143.0, 146.0], "text": " archives thing, but also I wanted it to be"}, {"timestamp": [146.0, 149.0], "text": " eminently renderable so that I could drop it"}, {"timestamp": [149.0, 152.0], "text": " into Claude. Because Claude, you can see where I just"}, {"timestamp": [152.0, 155.0], "text": " copy-pasted a whole bunch of stuff, basically"}, {"timestamp": [155.0, 158.0], "text": " titles and abstracts, into Claude"}, {"timestamp": [158.0, 161.0], "text": " and then I'm like, hey, tell me what the latest trends are."}, {"timestamp": [161.0, 164.0], "text": " And so, for instance, I discovered, you know, many papers"}, {"timestamp": [164.0, 166.08], "text": " are focusing on LLM's abilities to"}, {"timestamp": [166.08, 170.48], "text": " do things like reasoning, planning, and knowledge intensive tasks. And then you can scroll down and"}, {"timestamp": [170.48, 176.96], "text": " say, okay, cool, give me a list of the papers that do this. So instead of manually scrolling through,"}, {"timestamp": [176.96, 182.4], "text": " you know, like hundreds and hundreds of archive papers in order to get a feel for where the"}, {"timestamp": [183.36, 188.0], "text": " industry is, I just like said, okay, hey, tell me what some of the trends are"}, {"timestamp": [188.0, 192.0], "text": " to Claude. Oh, with Claude you have to be very, very explicit."}, {"timestamp": [192.0, 196.0], "text": " I said, please write a summary of the benchmarking trends as elucidated by the text"}, {"timestamp": [196.0, 200.0], "text": " I gave you. If you don't do that, it'll just make stuff up. Claude is still"}, {"timestamp": [200.0, 204.0], "text": " really, really bad about hallucinating. And the thing is"}, {"timestamp": [204.0, 209.64], "text": " there is very clearly some archive papers in its training data."}, {"timestamp": [209.64, 214.08], "text": " So it'll, it'll, if you don't tell it only use what I gave you, then it'll start either"}, {"timestamp": [214.08, 218.96], "text": " making up archive papers or citing papers that are much older."}, {"timestamp": [218.96, 224.88], "text": " And I'm like, no, make sure you only pull from the information I gave you in this conversation."}, {"timestamp": [224.88, 228.0], "text": " And then like it will change the formatting on you if you're not careful."}, {"timestamp": [228.0, 232.0], "text": " And I'm like, why did you change the formatting on me? Anyways, so they still have a little bit"}, {"timestamp": [232.0, 236.0], "text": " of work to do on Clouds, but that's not the point of the video. The point of the video is that they have"}, {"timestamp": [236.0, 240.0], "text": " a nice free open source search that gives you this gigantic"}, {"timestamp": [240.0, 244.0], "text": " wad of XML, which is a nightmare. But you know,"}, {"timestamp": [244.0, 246.0], "text": " it is what it is. It's XML."}, {"timestamp": [246.0, 249.0], "text": " And it's free, so don't look a gift horse in the mouth."}, {"timestamp": [249.0, 252.0], "text": " And, yeah, so how you use this..."}, {"timestamp": [252.0, 254.0], "text": " And actually, let me show you a couple other ones."}, {"timestamp": [254.0, 256.0], "text": " So here's the first one that I did."}, {"timestamp": [256.0, 259.0], "text": " So if you want to look at today, the last week's stuff"}, {"timestamp": [259.0, 262.0], "text": " of large language models, click on that one."}, {"timestamp": [262.0, 265.28], "text": " I did one where I just searched for bacon as a test to see what it would"}, {"timestamp": [265.28, 268.48], "text": " come up with and apparently there's a lot of authors named bacon but there's not a whole lot"}, {"timestamp": [268.48, 275.2], "text": " of archived papers on the food bacon so that was really disappointing. But you know whatever. And"}, {"timestamp": [275.2, 279.2], "text": " then I did another one for quantum computing because I wanted to make sure that the input"}, {"timestamp": [279.2, 286.0], "text": " would be able to handle spaces. So basically it has to be URL encoded which is really simple you just replace it with"}, {"timestamp": [286.0, 291.0], "text": " %20 and away it goes. You can see it's taking a minute to render."}, {"timestamp": [291.0, 295.0], "text": " I'm not sure why it's taking so long. I think I broke GitHub."}, {"timestamp": [295.0, 298.0], "text": " Anyways, we will wait."}, {"timestamp": [298.0, 301.0], "text": " So yeah, really simple tool, really straightforward."}, {"timestamp": [301.0, 304.0], "text": " Let's do a quick refresh."}, {"timestamp": [304.0, 309.6], "text": " There we go. Okay, so check on quantum computing."}, {"timestamp": [309.6, 316.04], "text": " It might not load. But yeah, so very straightforward. You just run this pie, this this pie file."}, {"timestamp": [316.04, 319.2], "text": " And so I'll walk you through the script real quick, just in case you want to see it. But"}, {"timestamp": [319.2, 326.0], "text": " it just so the first thing is it takes the input, what you want to look up, you type it in, it does it in lowercase"}, {"timestamp": [326.0, 328.0], "text": " and replaces any"}, {"timestamp": [328.0, 330.0], "text": " whitespace with a percent 20."}, {"timestamp": [330.0, 332.0], "text": " It'll go, it'll"}, {"timestamp": [332.0, 334.0], "text": " populate the URL. Oh, another thing,"}, {"timestamp": [334.0, 336.0], "text": " you can change this URL if you"}, {"timestamp": [336.0, 338.0], "text": " want to search by something else, if you want to search"}, {"timestamp": [338.0, 340.0], "text": " for author or change the max results."}, {"timestamp": [340.0, 342.0], "text": " I have the"}, {"timestamp": [342.0, 344.0], "text": " sort by last updated date"}, {"timestamp": [344.0, 348.0], "text": " and sort order descending."}, {"timestamp": [348.0, 352.0], "text": " So this will get the latest stuff because really the problem I was trying to solve was"}, {"timestamp": [352.0, 356.0], "text": " everything is moving so fast so it's like I wanted to just have a way of"}, {"timestamp": [356.0, 360.0], "text": " figuring out what is going on with the world at a glance"}, {"timestamp": [360.0, 364.0], "text": " in terms of large language models because it's like there's LoRa and QLoRa"}, {"timestamp": [364.0, 366.2], "text": " and there's all these kinds of benchmarks coming out."}, {"timestamp": [366.2, 369.5], "text": " And so what you can do is you can just like come to this and say,"}, {"timestamp": [369.5, 371.2], "text": " alright, let's look up benchmarks."}, {"timestamp": [371.2, 374.8], "text": " So from Words to Watch, benchmarking the energy cost of large language model."}, {"timestamp": [374.8, 377.7], "text": " Cool! Great paper, glad this exists."}, {"timestamp": [377.7, 383.0], "text": " And you can see there's 115 references to benchmarks in this."}, {"timestamp": [383.0, 388.64], "text": " T dollar cubed bench, benchmarking current progress in text-to-3D"}, {"timestamp": [388.64, 393.92], "text": " generation. Great, cool. Now I know that we're benchmarking text-to-3D generation."}, {"timestamp": [395.68, 399.2], "text": " Let's see, shadow alignment. So that sounds cool. It's like shadow banning people."}, {"timestamp": [399.76, 406.0], "text": " Do they still do that on Reddit and Twitter? Anyways, yeah, so GPT"}, {"timestamp": [406.0, 410.96], "text": " fuzzer, right. Okay, great. Anyways, idea here is that you"}, {"timestamp": [410.96, 416.04], "text": " can download hundreds, if not thousands of the latest, latest"}, {"timestamp": [416.04, 419.52], "text": " papers on any particular topic from archive, whether or not"}, {"timestamp": [419.52, 423.32], "text": " it's like large language models. You can you can put in quantum"}, {"timestamp": [423.32, 426.0], "text": " computing, like I said, but for whatever reason it's not rendering"}, {"timestamp": [426.0, 428.0], "text": " directly on GitHub."}, {"timestamp": [428.0, 430.0], "text": " I broke it."}, {"timestamp": [430.0, 432.0], "text": " But yeah, you can download"}, {"timestamp": [432.0, 434.0], "text": " an arbitrary number of things."}, {"timestamp": [434.0, 436.0], "text": " It's sorted by date, by release date."}, {"timestamp": [436.0, 438.0], "text": " It is in descending"}, {"timestamp": [438.0, 440.0], "text": " order so you know you've got the latest"}, {"timestamp": [440.0, 442.0], "text": " and greatest. Yeah, I think that's about it."}, {"timestamp": [442.0, 444.0], "text": " So, thanks for watching."}, {"timestamp": [444.0, 446.0], "text": " Cheers, and yep, I definitely broke it."}, {"timestamp": [446.0, 448.0], "text": " Have a good one."}, {"timestamp": [441.71, 446.63], "text": " greatest. Yeah, I think that's about it. So thanks for watching. Cheers and yep, I"}, {"timestamp": [446.63, 450.17], "text": " definitely broke it. Have a good one."}]}