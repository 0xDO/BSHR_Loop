{"text": " Morning everybody, David Shapiro here. By popular demand, we are working on basically a chat GPT clone but with long-term memory and eventually external sources. But first, quick housekeeping. One, as I always say, the offer is still on the table. If I can meet my financial goals with Patreon, I will remove ads forever. So do that. Also the comments section of my YouTube channel is blowing up, so I can't keep up with comments. If you really, really want my attention, Patreon is the best way to get in touch with me. Obviously if you spend even just a couple dollars, that tells me that you're a little bit more serious. The second best way to get in touch with me is LinkedIn. Links to both of those are in the video description. And then third is I've got a new mailing list that I'm creating which is also a good way to follow up in the future. Now that all being said I am super busy but stay tuned for some news tomorrow. Okay, with all that out of the way, let's go over... I started this, so I'm going to try a new format. I have... there's basically three kinds of videos I do. One is an explainer, which is, you know, the slide deck and I just explain something to you, teach you. It's basically like a miniature lesson. Then there's the experiments, like my chat GPT and case law one. That was an experiment. It didn't really succeed, but the point wasn't to succeed. The point was to learn. And then there's tutorials, which is, I know how to do something, and I'm going to walk you through to the end. So this is a tutorial. So just wanted to set expectations. OK, so this tutorial is how to create a really brain-dead simple chatbot and how to give it long-term memory. So let me just show you what I've got working so far. So user, hello, and wait a second, are you there? Obviously we're not getting any response. Okay, so what gives? Well, we are just getting started. So let me show you what that produced. So let's go to chat logs and you see here we've got log and then a timestamp and then user. So we know basically that tells you a lot about what happened. So you know it's a log, you know the timestamp and you know who said it. Well, let's see what's in here. So we've got the message, we've got the speaker, we've got the timestamp, we know who said it. Well, let's see what's in here. So we've got the message, we've got the speaker, we've got the timestamp, we've got a UUID, and then we've got a vector. So the vector is an embedding that was provided by OpenAI's latest model, which is, what is it? It's like textada02 or something. I've got a function right up here. Yep, so textembeddingada02. So this is their latest one. It's supposed to be like fantastic and it replaces a whole bunch of other stuff and it performs really well. Okay, great. So basically I just have this really short little function that you just pass it a piece of text and it'll send back an embedding. And the reason that we do this is because we need to search it. Now, this is probably not the most efficient way to do this in the long run. Once you have longer conversations, you'll probably chunk them. But for now, we're going to do this. And so every message becomes 42 kilobytes, this and so every message becomes 42 kilobytes which adds up because this vector is 1500 values long and it's used to register that so it's not necessarily the best but once you get to longer messages it makes more sense and especially if you chunk them which we may or may not get to in this one, we'll see. And so what I mean by chunking is rather than saving each individual message and then vectorizing that, you take a chunk of five messages or 10 messages and then you chunk and vectorize them. Because the chances of getting exactly what you want in one message is pretty low, but this is an easy, this is low hanging fruit. This is just where we're gonna start. Okay, so that's that. So that's what's happening there, but let's go to the main loop. So first we set our API key, and then while true, so this is just an infinite Python loop, the very first thing that we do is we get user input, then we vectorize it, and then we set it into a dictionary. And all these should look familiar. There's the speaker, there's the time, there's the vector, there's the message, and then there's the UUID. And the reason that you wanna give it a UUID is just in case you wanna refer to something very specifically. You can sometimes use timestamp as a UUID, but every now and then, depending on how you've got your program organized, you could end up with two things with the identical timestamps. And so timestamp is not necessarily going to be a UUID, but if you pick two UUIDs, they're guaranteed, even if you pick them at the same time, they're guaranteed to be unique. And then we create a file name, so then we just save it out to JSON. Then the next thing we do is we load a conversation. So this database, these chat logs, this is our database, right? This is our nexus. So if you're familiar with my work in artificial cognition or symphony of thought or Maragi, this is the nexus. Anything that you want the chatbot to be conscious of needs to end up here. And I'm calling it just chat logs because when all you're doing is a memory task against previous conversations, that is your whole nexus. But if you want external sources, we might have a second nexus and we might call it like KB articles or something. Then we can put indexed files in here that are then searchable. Then you might also have, and KB articles, this is what you'd have as your ground truth. Also, once this scales up, the search method that I'm using is not efficient. You'll want to move over to something like Pinecone or Weave8 or Qdrin because those search engines are going to be much faster. But yeah, so this is where we're off to. This is where we start. So we add to the conversation, then we load the conversation. And the reason that we load the... Again, there's more efficient ways of doing this. You could just append. But the thing is, I want to be able to load all history. If I kill this and start it again, I want it to load all of my logs regardless of how it's going. So I want one very long, persistent conversation. So that's where we're starting and what I'm about to work on, I'm not going to show you like all the coding every single time because it's kind of boring, but I can do it, work it, and then figure it out. So what I'm going to do next is work on generating the response. And generating the response first requires that we load relevant history, so on and so forth. So we'll be right back with the next phase. And actually, this is pretty simple. So maybe we will take some time to do some of those improvements I was talking about. But anyways, be right back. All right, we are just about done, at least with the first version. There's room for improvement, of course. But let me walk you through the code before I show you how good this is. So we left off with here where we handle the user input. Then we load the conversation. So this is a really simple function, just load conversation. So it goes into the chat logs directory. It looks for any JSON file, and then it loads them all, appends them, and then sorts them by time, just in case they don't load in correct chronological order. My file naming convention should mean that if they load in alphabetical order, they should also load in chronological order. But again, you don't necessarily wanna make that assumption. So we sort by the time index inside the file. Because one of the key things to remember is, the reason that we're doing it this way is, human memories are associative. So you can approximate associative memories with semantic similarity, because our memories are queued up based on reminders. This is why if you move from one room to another, you might forget because your context has changed. And so your brain literally stores memories from one room and says, ah, I'm now in the kitchen. I have kitchen memories now. That's how gullible our memory is. The other thing is that our memories are based on chronological similarity. We tend to remember things clustered in time. Like, hey, remember my eighth birthday, right? It doesn't matter where your eighth birthday was, but it's like there's a whole bunch of memories associated with that tag. One thing that you can do is you could probably do a knowledge graph with memories, but that's a whole lot more complexity, especially when you get up to like millions or billions of memories. I personally think that semantic search is the way to go because it is fast and scalable. I don't think that knowledge graphs are the way to go. That being said, time will tell. Okay, so that was a little brief spiel on memories. So then, once we have the user input, it's time to compose the corpus. So the corpus, if you've read my book, \"'Natural Language Cognitive Architecture,\" the corpus is all the context that is needed to generate a response, to do something, to perform cognitive labor and prepare to respond. So first, we fetch all the most recent memories with this function called fetchMemories. And so fetchMemories, we fetch all the most recent memories with this function called fetchMemories. And so fetchMemories, we pass it the vector for the most recent input, we pass it all the chat logs, and then we tell it how many we want to get back. And this does a very, very simple dot product similarity score. I looked it up and I realized that cosine similarity is not just a dot product, there's more to it. So for all the messages that you see on Reddit and the OpenAI community about like, why are my dot products wrong? Apparently cosine similarity has a little bit more to it than just returning a dot product. And I cited my source, this is what I'm copying, this is the logic. So hopefully our semantic similarity will be better. So what this does is we get all the memories, we sort them by most relevant, we also skip the current one, because we save the memory immediately, and just in case we have an identical one, we don't need to return identical messages, because you might ask the same message or get the same response at some one, we don't need to return identical messages because you might ask the same message or get the same response at some point. We don't care about that. So we sort by most relevant and we pass back n number of most relevant. So that's fetch memories. So this becomes our most recent, our most relevant memories. You can just, this is parameterized. So if you have a bigger chat window you can do this. Then what we do is we summarize those memories. So I have this function, summarize memories. So first thing we do is we sort them chronologically so that there's a nice chronological flow. Then we reconstitute that block and then we summarize it. So we just basically take notes. then we summarize it. So we just basically take notes. So the summarization, this is something that happens in the background of your brain, particularly while you sleep. So what happens while you sleep is that your brain replays memories from throughout the day and it simplifies them. It distills them down to the most critical essence. It also links them to other relevant memories. This is why sleep is so important for learning. But instead of doing it behind the scenes, we're doing it in real time. So a future version of this might have it where we're going over, we're grooming our record of memories and simplifying them and doing that sort of thing in the background. But instead, we're just going to do it in real time. So we pull an arbitrary number of memories. We summarize them so that they'll fit into a new prompt by distilling them down, by taking notes. Then we get the most recent conversation messages. Again, I wanted to treat this so that it has persistent memory, so that's why I'm treating it like, okay, if this just started up and we have a backlog of memories, we want to pull all of them. So we get notes from irrespective, temporarily irrespective memories. Oh, so this is another thing. What you might do with fetch memories is once you've picked the most relevant memories, so I'll add a to do. To do, pick more memories temporarily nearby the top most relevant memories. So that way, we should pick not just one memory that's relevant, we should pick not just one memory that's relevant, we should pick five memories in either directions to give it more context. So we'll add this as a to-do item for the future. Actually, I should probably do that for other stuff because this will do a lot because then by using semantic search, you can find anything in time, but then you also want to pick more relevant memories. And then, where's the notes? Summarize memories. So I'll add a to do. To do, do this in the background over time do this in the background over time to handle huge amounts of memories. So basically just kind of putting a little bookmark for later. Okay, so we get the most recent and then of course you can change this. I just have it the four most recent back and and forth and then we pipe it into this response and I'll show you the prompts that I've written in just a second actually no this is this is a good enough time so here's the notes it says write detailed notes of the following in a hyphenated list format and I just tell it exactly the format that I want. So basically we're taking notes of the previous conversation and then away it goes. And then here's the response. So I am a chatbot named Raven. So you give it, this is what I call an agent model. So it says, I am a chatbot named Raven. So that way the model knows this is what I am. This is the context. My goals are to reduce suffering, increase prosperity, and increase understanding. I will read the conversation notes and recent messages, and then I will provide a long, verbose, detailed response. So we want it to be chat GPT-like. The following are the notes from earlier conversations with user, and so then we've got the notes. The following are the most recent messages in the conversation, and then here's the conversation. I will now provide a long, detailed, verbose response. And then I just queue it up so that it's going to copy the same format as up here. So it just knows, ah, it's my time to speak. So there you go. And then when we go back here, we generate the response. So we generate it with a, we pipe this prompt into GPT-3 completion, and the stops that I have are user and Raven. So it'll keep talking until it tries to continue the conversation, which is not what I want it to do. So there you have it, that's that. So now that we've got the output, we need to save the output the same way that we saved the user output. So we vectorize Raven's output, we put it into an identical object, and then we save it out to our log file, so chat logs, and then you'll see here, we know that it's Raven, but we use otherwise the same format, and then we print the output. So there you have it. It's that simple. Let's go ahead and test it. So we'll do Python chat. Hey, Raven. And because it's doing so much behind the scene, it's a little bit slow, but so is chat GPT. There's all kinds of stuff we can do to parallelize this and make it faster. Let's see. I'm a computer system. Okay, cool. He provided a fairy. I hope this helps. Yeah, thanks. I am having trouble sleeping. I am waking up at 4 a.m. every day. This is actually like quasi-true, because if you look at the timestamp, it's 5.15. This is just me. This is just my circadian rhythms. So that sounds like a difficult situation. It's understandable that you're having trouble sleeping. There are a few things you can do, blah, blah, blah, go to bed and wake up at the same time, avoid screens. So very helpful, very chat GPT-like response. So if I continue this conversation over a long period of time, you will see that we're accumulating our chat logs here and we see user, Raven, user, Raven, and they're all in chronological order and they're all searchable. So if we keep, I'm not going to continue this conversation indefinitely, but I just wanted to show you that it works and it's pretty straightforward. Cool. Thanks. Let's talk about circadian rhythms. And so one thing that I can show you to kind of prove that it's working behind the scenes is we can look at our GPT-3 logs. And so we can see here's the notes. So yep, so we've compressed this into this. So we've got really nice, concise notes about the previous conversation. So that is how you can handle large volumes. And then here is an example of what it's responding. So you can see we've said here's the notes. So it Raven has some longer term context. Here's the most recent conversation. It's just the last four. And then here's the final output. So let's see what Raven said. Absolutely, circadian rhythms are the body's natural blah, blah, blah. Okay. What do you think about the singularity? When will it happen? And so I've basically just created my own personal chat GPT with long-term memory and I've given it my own goals. It does not have the chat GPT goals. So blah blah blah, let's see the concept of the singularity is this, so on and so forth. Yeah, and as I mentioned, you can also add functions where you can search KB articles in the same way. So let me actually add that as a to-do. So adding that, so memories, and so there's two kinds of memories. There's episodic and declarative memories. So this is pull episodic memories and then to do fetch declarative memories, aka facts, you know, that are not attached to you. So like facts, wikis, KB, et cetera. And that can ground it in those. So this could be like, you know, company data, it could be Wikipedia, it could be internet. So I'll just say like company data, internet, et cetera. Yep, so that's another to-do. And to add this, you just have another thing in your corpus right here, and you'd say like, okay, notes from the following. And then you might say, here is some background knowledge that may be helpful for the conversation. And then you just say, and then you just say like, you know, KB or whatever. So that would be how you populate this. And we'll get to that in the future. But yeah, so I think we're done. Like super simple, super straightforward. This is long-term memory. This is also a cognitive architecture. So I wanna point out that this is the simplest implementation of natural language cognitive architecture that I have come up with yet. And you might say, well, where's the inner loop? The inner loop is everything that you're seeing here. So where you compose the corpus, this is the inner loop. And then the outer loop is the interaction here. So there's two loops, they interact, they interlock, and it has some time where it's thinking and then it generates a response. Now this doesn't have autonomy, this version of Raven is not thinking on his own all the time, but you can see that there's clearly some thought going on, and we've set the stage to make it more extensible. Okay, I think we'll call it a day. I can hear all of you salivating and very excited and wanting me to continue this, but we're at 20 minutes in, so thanks for watching. As a quick reminder, the offer is still on the table. If you guys support me enough on Patreon, I'll remove ads for watching. As a quick reminder, the offer is still on the table. If you guys support me enough on Patreon, I'll remove ads for good. The best way to get in touch with me is via Patreon. There's also LinkedIn. And finally, my mailing list and links for all of those are in the video description. Thanks for watching. Take care. Oh, and one other thing. I mentioned, I think at the the beginning of this video that the comments are going crazy. I cannot respond to all comments on YouTube anymore. So I would encourage you guys to talk to each other and also upvote good comments and that will help me zoom in on the questions that you all agree you want answered. Okay, I think that's it. Talk later. me zoom in on the questions that you all agree you want answered. Okay, I think that's it. Talk later.", "chunks": [{"timestamp": [0.0, 4.0], "text": " Morning everybody, David Shapiro here."}, {"timestamp": [4.0, 11.44], "text": " By popular demand, we are working on basically a chat GPT clone but with long-term memory"}, {"timestamp": [11.44, 14.68], "text": " and eventually external sources."}, {"timestamp": [14.68, 19.04], "text": " But first, quick housekeeping."}, {"timestamp": [19.04, 29.98], "text": " One, as I always say, the offer is still on the table. If I can meet my financial goals with Patreon, I will remove ads forever."}, {"timestamp": [29.98, 31.56], "text": " So do that."}, {"timestamp": [31.56, 36.88], "text": " Also the comments section of my YouTube channel is blowing up, so I can't keep up with comments."}, {"timestamp": [36.88, 41.24], "text": " If you really, really want my attention, Patreon is the best way to get in touch with me."}, {"timestamp": [41.24, 47.94], "text": " Obviously if you spend even just a couple dollars, that tells me that you're a little bit more serious. The second best way to get in"}, {"timestamp": [47.94, 53.0], "text": " touch with me is LinkedIn. Links to both of those are in the video description."}, {"timestamp": [53.0, 59.52], "text": " And then third is I've got a new mailing list that I'm creating which is also a"}, {"timestamp": [59.52, 64.36], "text": " good way to follow up in the future. Now that all being said I am super busy but"}, {"timestamp": [64.36, 66.48], "text": " stay tuned for some news tomorrow."}, {"timestamp": [66.48, 72.16], "text": " Okay, with all that out of the way, let's go over... I started this, so I'm going to"}, {"timestamp": [72.16, 78.24], "text": " try a new format. I have... there's basically three kinds of videos I do. One is an explainer,"}, {"timestamp": [78.24, 82.36], "text": " which is, you know, the slide deck and I just explain something to you, teach you. It's"}, {"timestamp": [82.36, 88.2], "text": " basically like a miniature lesson. Then there's the experiments, like my chat GPT"}, {"timestamp": [88.2, 89.44], "text": " and case law one."}, {"timestamp": [89.44, 90.36], "text": " That was an experiment."}, {"timestamp": [90.36, 93.36], "text": " It didn't really succeed, but the point wasn't to succeed."}, {"timestamp": [93.36, 95.24], "text": " The point was to learn."}, {"timestamp": [95.24, 97.4], "text": " And then there's tutorials, which is,"}, {"timestamp": [97.4, 98.68], "text": " I know how to do something, and I'm"}, {"timestamp": [98.68, 100.22], "text": " going to walk you through to the end."}, {"timestamp": [100.22, 101.36], "text": " So this is a tutorial."}, {"timestamp": [101.36, 103.88], "text": " So just wanted to set expectations."}, {"timestamp": [103.88, 107.04], "text": " OK, so this tutorial is how to create a really"}, {"timestamp": [107.04, 113.68], "text": " brain-dead simple chatbot and how to give it long-term memory. So let me just show you what"}, {"timestamp": [113.68, 122.8], "text": " I've got working so far. So user, hello, and wait a second, are you there? Obviously we're not getting"}, {"timestamp": [122.8, 125.0], "text": " any response. Okay, so what gives?"}, {"timestamp": [125.0, 126.76], "text": " Well, we are just getting started."}, {"timestamp": [126.76, 129.24], "text": " So let me show you what that produced."}, {"timestamp": [129.24, 134.24], "text": " So let's go to chat logs and you see here we've got log and then a timestamp and then"}, {"timestamp": [134.24, 135.24], "text": " user."}, {"timestamp": [135.24, 138.96], "text": " So we know basically that tells you a lot about what happened."}, {"timestamp": [138.96, 141.92], "text": " So you know it's a log, you know the timestamp and you know who said it."}, {"timestamp": [141.92, 144.44], "text": " Well, let's see what's in here."}, {"timestamp": [144.44, 147.28], "text": " So we've got the message, we've got the speaker, we've got the timestamp, we know who said it. Well, let's see what's in here. So we've got the message, we've got the speaker,"}, {"timestamp": [147.28, 150.24], "text": " we've got the timestamp, we've got a UUID,"}, {"timestamp": [150.24, 151.88], "text": " and then we've got a vector."}, {"timestamp": [151.88, 155.16], "text": " So the vector is an embedding that was provided"}, {"timestamp": [155.16, 159.64], "text": " by OpenAI's latest model, which is, what is it?"}, {"timestamp": [159.64, 162.08], "text": " It's like textada02 or something."}, {"timestamp": [162.08, 168.48], "text": " I've got a function right up here. Yep, so textembeddingada02. So this is"}, {"timestamp": [168.48, 172.72], "text": " their latest one. It's supposed to be like fantastic and it replaces a whole bunch of"}, {"timestamp": [172.72, 179.36], "text": " other stuff and it performs really well. Okay, great. So basically I just have this really short"}, {"timestamp": [179.36, 185.36], "text": " little function that you just pass it a piece of text and it'll send back an embedding. And the reason that we do"}, {"timestamp": [185.36, 191.36], "text": " this is because we need to search it. Now, this is probably not the most efficient way"}, {"timestamp": [192.0, 197.44], "text": " to do this in the long run. Once you have longer conversations, you'll probably chunk them."}, {"timestamp": [198.24, 203.2], "text": " But for now, we're going to do this. And so every message becomes 42 kilobytes,"}, {"timestamp": [205.52, 202.64], "text": " this and so every message becomes 42"}, {"timestamp": [208.08, 205.52], "text": " kilobytes which adds up"}, {"timestamp": [210.88, 208.08], "text": " because this vector"}, {"timestamp": [213.6, 210.88], "text": " is 1500"}, {"timestamp": [215.28, 213.6], "text": " values long and it's used to"}, {"timestamp": [218.16, 215.28], "text": " register that"}, {"timestamp": [220.08, 218.16], "text": " so it's not necessarily the best"}, {"timestamp": [221.68, 220.08], "text": " but once you get to longer messages"}, {"timestamp": [224.64, 221.68], "text": " it makes more sense"}, {"timestamp": [226.56, 224.64], "text": " and especially if you chunk them"}, {"timestamp": [228.56, 233.56], "text": " which we may or may not get to in this one, we'll see. And so what I mean by chunking is rather than saving each individual message and then vectorizing"}, {"timestamp": [233.56, 239.28], "text": " that, you take a chunk of five messages or 10 messages and then you chunk and vectorize"}, {"timestamp": [239.28, 240.28], "text": " them."}, {"timestamp": [240.28, 244.88], "text": " Because the chances of getting exactly what you want in one message is pretty low, but"}, {"timestamp": [244.88, 246.88], "text": " this is an easy, this is low hanging fruit."}, {"timestamp": [246.88, 248.8], "text": " This is just where we're gonna start."}, {"timestamp": [248.8, 251.8], "text": " Okay, so that's that."}, {"timestamp": [251.8, 252.72], "text": " So that's what's happening there,"}, {"timestamp": [252.72, 254.64], "text": " but let's go to the main loop."}, {"timestamp": [254.64, 257.72], "text": " So first we set our API key, and then while true,"}, {"timestamp": [257.72, 259.96], "text": " so this is just an infinite Python loop,"}, {"timestamp": [261.56, 264.4], "text": " the very first thing that we do is we get user input,"}, {"timestamp": [264.4, 266.42], "text": " then we vectorize it,"}, {"timestamp": [266.42, 268.84], "text": " and then we set it into a dictionary."}, {"timestamp": [268.84, 270.82], "text": " And all these should look familiar."}, {"timestamp": [270.82, 273.24], "text": " There's the speaker, there's the time, there's the vector,"}, {"timestamp": [273.24, 276.04], "text": " there's the message, and then there's the UUID."}, {"timestamp": [276.04, 278.48], "text": " And the reason that you wanna give it a UUID"}, {"timestamp": [278.48, 279.76], "text": " is just in case you wanna refer"}, {"timestamp": [279.76, 282.0], "text": " to something very specifically."}, {"timestamp": [282.0, 286.96], "text": " You can sometimes use timestamp as a UUID, but every now and then, depending on"}, {"timestamp": [286.96, 290.64], "text": " how you've got your program organized, you could end up with two things with the identical"}, {"timestamp": [290.64, 298.72], "text": " timestamps. And so timestamp is not necessarily going to be a UUID, but if you pick two UUIDs,"}, {"timestamp": [298.72, 302.8], "text": " they're guaranteed, even if you pick them at the same time, they're guaranteed to be unique."}, {"timestamp": [303.84, 305.6], "text": " And then we create a file name,"}, {"timestamp": [305.6, 307.96], "text": " so then we just save it out to JSON."}, {"timestamp": [307.96, 310.76], "text": " Then the next thing we do is we load a conversation."}, {"timestamp": [310.76, 313.4], "text": " So this database, these chat logs,"}, {"timestamp": [313.4, 315.24], "text": " this is our database, right?"}, {"timestamp": [315.24, 316.6], "text": " This is our nexus."}, {"timestamp": [316.6, 319.74], "text": " So if you're familiar with my work in artificial cognition"}, {"timestamp": [319.74, 323.72], "text": " or symphony of thought or Maragi, this is the nexus."}, {"timestamp": [323.72, 326.72], "text": " Anything that you want the chatbot to be conscious of"}, {"timestamp": [326.72, 328.4], "text": " needs to end up here."}, {"timestamp": [328.4, 330.4], "text": " And I'm calling it just chat logs"}, {"timestamp": [330.4, 334.12], "text": " because when all you're doing is a memory task"}, {"timestamp": [334.12, 337.68], "text": " against previous conversations, that is your whole nexus."}, {"timestamp": [337.68, 341.64], "text": " But if you want external sources,"}, {"timestamp": [341.64, 344.08], "text": " we might have a second nexus"}, {"timestamp": [344.08, 348.4], "text": " and we might call it like KB articles or something."}, {"timestamp": [348.4, 353.24], "text": " Then we can put indexed files in here that are then searchable."}, {"timestamp": [353.24, 359.32], "text": " Then you might also have, and KB articles, this is what you'd have as your ground truth."}, {"timestamp": [359.32, 369.7], "text": " Also, once this scales up, the search method that I'm using is not efficient. You'll want to move over to something like Pinecone or Weave8 or Qdrin because those"}, {"timestamp": [369.7, 373.02], "text": " search engines are going to be much faster."}, {"timestamp": [373.02, 375.18], "text": " But yeah, so this is where we're off to."}, {"timestamp": [375.18, 376.32], "text": " This is where we start."}, {"timestamp": [376.32, 381.38], "text": " So we add to the conversation, then we load the conversation."}, {"timestamp": [381.38, 382.98], "text": " And the reason that we load the..."}, {"timestamp": [382.98, 384.86], "text": " Again, there's more efficient ways of doing this."}, {"timestamp": [384.86, 390.46], "text": " You could just append. But the thing is, I want to be able to load all history."}, {"timestamp": [390.46, 396.12], "text": " If I kill this and start it again, I want it to load all of my logs regardless of how"}, {"timestamp": [396.12, 397.12], "text": " it's going."}, {"timestamp": [397.12, 401.5], "text": " So I want one very long, persistent conversation."}, {"timestamp": [401.5, 407.16], "text": " So that's where we're starting and what I'm about to work on, I'm not going to show you like all the coding every single time because it's"}, {"timestamp": [407.16, 411.94], "text": " kind of boring, but I can do it, work it, and then figure it out. So what"}, {"timestamp": [411.94, 417.96], "text": " I'm going to do next is work on generating the response. And generating"}, {"timestamp": [417.96, 424.16], "text": " the response first requires that we load relevant history, so on and so forth. So"}, {"timestamp": [424.16, 427.08], "text": " we'll be right back with the next phase."}, {"timestamp": [427.08, 428.92], "text": " And actually, this is pretty simple."}, {"timestamp": [428.92, 431.32], "text": " So maybe we will take some time"}, {"timestamp": [431.32, 433.44], "text": " to do some of those improvements I was talking about."}, {"timestamp": [433.44, 434.8], "text": " But anyways, be right back."}, {"timestamp": [437.48, 440.08], "text": " All right, we are just about done,"}, {"timestamp": [441.52, 442.56], "text": " at least with the first version."}, {"timestamp": [442.56, 444.8], "text": " There's room for improvement, of course."}, {"timestamp": [444.8, 446.4], "text": " But let me walk you through the code"}, {"timestamp": [446.4, 449.14], "text": " before I show you how good this is."}, {"timestamp": [449.14, 452.98], "text": " So we left off with here where we handle the user input."}, {"timestamp": [452.98, 454.82], "text": " Then we load the conversation."}, {"timestamp": [454.82, 456.88], "text": " So this is a really simple function,"}, {"timestamp": [456.88, 459.0], "text": " just load conversation."}, {"timestamp": [459.0, 461.96], "text": " So it goes into the chat logs directory."}, {"timestamp": [461.96, 464.18], "text": " It looks for any JSON file,"}, {"timestamp": [464.18, 465.96], "text": " and then it loads them all, appends them,"}, {"timestamp": [465.96, 467.84], "text": " and then sorts them by time,"}, {"timestamp": [467.84, 471.84], "text": " just in case they don't load in correct chronological order."}, {"timestamp": [471.84, 475.16], "text": " My file naming convention should mean"}, {"timestamp": [475.16, 477.76], "text": " that if they load in alphabetical order,"}, {"timestamp": [477.76, 480.2], "text": " they should also load in chronological order."}, {"timestamp": [480.2, 481.52], "text": " But again, you don't necessarily"}, {"timestamp": [481.52, 483.4], "text": " wanna make that assumption."}, {"timestamp": [483.4, 487.08], "text": " So we sort by the time index inside the file."}, {"timestamp": [487.08, 490.6], "text": " Because one of the key things to remember is,"}, {"timestamp": [490.6, 492.84], "text": " the reason that we're doing it this way is,"}, {"timestamp": [492.84, 495.68], "text": " human memories are associative."}, {"timestamp": [495.68, 497.32], "text": " So you can approximate"}, {"timestamp": [497.32, 499.72], "text": " associative memories with semantic similarity,"}, {"timestamp": [499.72, 503.16], "text": " because our memories are queued up based on reminders."}, {"timestamp": [503.16, 506.34], "text": " This is why if you move from one room to another,"}, {"timestamp": [506.34, 508.74], "text": " you might forget because your context has changed."}, {"timestamp": [508.74, 512.66], "text": " And so your brain literally stores memories from one room"}, {"timestamp": [512.66, 514.46], "text": " and says, ah, I'm now in the kitchen."}, {"timestamp": [514.46, 516.82], "text": " I have kitchen memories now."}, {"timestamp": [516.82, 518.94], "text": " That's how gullible our memory is."}, {"timestamp": [518.94, 521.74], "text": " The other thing is that our memories"}, {"timestamp": [521.74, 525.0], "text": " are based on chronological similarity."}, {"timestamp": [525.1, 528.18], "text": " We tend to remember things clustered in time."}, {"timestamp": [528.18, 530.8], "text": " Like, hey, remember my eighth birthday, right?"}, {"timestamp": [530.8, 533.32], "text": " It doesn't matter where your eighth birthday was,"}, {"timestamp": [533.32, 534.86], "text": " but it's like there's a whole bunch of memories"}, {"timestamp": [534.86, 536.74], "text": " associated with that tag."}, {"timestamp": [536.74, 538.04], "text": " One thing that you can do"}, {"timestamp": [538.04, 541.52], "text": " is you could probably do a knowledge graph with memories,"}, {"timestamp": [541.52, 543.3], "text": " but that's a whole lot more complexity,"}, {"timestamp": [543.3, 545.16], "text": " especially when you get up to like millions"}, {"timestamp": [545.16, 546.66], "text": " or billions of memories."}, {"timestamp": [546.66, 549.08], "text": " I personally think that semantic search is the way to go"}, {"timestamp": [549.08, 551.06], "text": " because it is fast and scalable."}, {"timestamp": [551.06, 553.48], "text": " I don't think that knowledge graphs are the way to go."}, {"timestamp": [553.48, 555.86], "text": " That being said, time will tell."}, {"timestamp": [555.86, 560.44], "text": " Okay, so that was a little brief spiel on memories."}, {"timestamp": [560.44, 563.56], "text": " So then, once we have the user input,"}, {"timestamp": [563.56, 565.84], "text": " it's time to compose the corpus."}, {"timestamp": [566.8, 568.28], "text": " So the corpus, if you've read my book,"}, {"timestamp": [568.28, 570.32], "text": " \"'Natural Language Cognitive Architecture,\""}, {"timestamp": [570.32, 573.8], "text": " the corpus is all the context that is needed"}, {"timestamp": [573.8, 576.08], "text": " to generate a response, to do something,"}, {"timestamp": [576.08, 580.2], "text": " to perform cognitive labor and prepare to respond."}, {"timestamp": [580.2, 583.9], "text": " So first, we fetch all the most recent memories"}, {"timestamp": [583.9, 585.52], "text": " with this function called fetchMemories. And so fetchMemories, we fetch all the most recent memories with this function called fetchMemories."}, {"timestamp": [586.32, 590.08], "text": " And so fetchMemories, we pass it the vector for the most recent input,"}, {"timestamp": [590.64, 594.96], "text": " we pass it all the chat logs, and then we tell it how many we want to get back."}, {"timestamp": [594.96, 600.24], "text": " And this does a very, very simple dot product similarity score."}, {"timestamp": [600.24, 607.48], "text": " I looked it up and I realized that cosine similarity is not just a dot product, there's more to it."}, {"timestamp": [607.48, 611.08], "text": " So for all the messages that you see on Reddit"}, {"timestamp": [611.08, 612.58], "text": " and the OpenAI community about like,"}, {"timestamp": [612.58, 614.16], "text": " why are my dot products wrong?"}, {"timestamp": [614.16, 617.24], "text": " Apparently cosine similarity has a little bit more to it"}, {"timestamp": [617.24, 618.68], "text": " than just returning a dot product."}, {"timestamp": [618.68, 621.84], "text": " And I cited my source, this is what I'm copying,"}, {"timestamp": [621.84, 622.96], "text": " this is the logic."}, {"timestamp": [622.96, 626.08], "text": " So hopefully our semantic similarity will be better."}, {"timestamp": [626.08, 630.52], "text": " So what this does is we get all the memories,"}, {"timestamp": [630.52, 634.02], "text": " we sort them by most relevant,"}, {"timestamp": [635.14, 637.3], "text": " we also skip the current one,"}, {"timestamp": [637.3, 639.42], "text": " because we save the memory immediately,"}, {"timestamp": [639.42, 642.16], "text": " and just in case we have an identical one,"}, {"timestamp": [642.16, 644.72], "text": " we don't need to return identical messages,"}, {"timestamp": [644.72, 646.06], "text": " because you might ask the same message or get the same response at some one, we don't need to return identical messages because you might ask the same message"}, {"timestamp": [646.06, 647.72], "text": " or get the same response at some point."}, {"timestamp": [647.72, 649.48], "text": " We don't care about that."}, {"timestamp": [649.48, 652.04], "text": " So we sort by most relevant"}, {"timestamp": [652.04, 655.52], "text": " and we pass back n number of most relevant."}, {"timestamp": [655.52, 657.4], "text": " So that's fetch memories."}, {"timestamp": [657.4, 660.16], "text": " So this becomes our most recent,"}, {"timestamp": [660.16, 661.6], "text": " our most relevant memories."}, {"timestamp": [661.6, 663.84], "text": " You can just, this is parameterized."}, {"timestamp": [663.84, 665.36], "text": " So if you have a bigger chat window"}, {"timestamp": [665.36, 670.24], "text": " you can do this. Then what we do is we summarize those memories. So I have this function, summarize"}, {"timestamp": [670.24, 675.84], "text": " memories. So first thing we do is we sort them chronologically so that there's a nice chronological"}, {"timestamp": [675.84, 684.32], "text": " flow. Then we reconstitute that block and then we summarize it. So we just basically take notes."}, {"timestamp": [686.18, 691.4], "text": " then we summarize it. So we just basically take notes. So the summarization, this is something that happens in the background of your brain,"}, {"timestamp": [691.4, 696.08], "text": " particularly while you sleep. So what happens while you sleep is that your"}, {"timestamp": [696.08, 700.2], "text": " brain replays memories from throughout the day and it simplifies them. It"}, {"timestamp": [700.2, 704.4], "text": " distills them down to the most critical essence. It also links them to other"}, {"timestamp": [704.4, 705.42], "text": " relevant memories."}, {"timestamp": [705.42, 708.44], "text": " This is why sleep is so important for learning."}, {"timestamp": [708.44, 710.64], "text": " But instead of doing it behind the scenes,"}, {"timestamp": [710.64, 712.2], "text": " we're doing it in real time."}, {"timestamp": [712.2, 714.96], "text": " So a future version of this might have it"}, {"timestamp": [714.96, 718.28], "text": " where we're going over, we're grooming"}, {"timestamp": [718.28, 722.72], "text": " our record of memories and simplifying them"}, {"timestamp": [722.72, 726.64], "text": " and doing that sort of thing in the background."}, {"timestamp": [726.64, 729.52], "text": " But instead, we're just going to do it in real time."}, {"timestamp": [729.52, 732.0], "text": " So we pull an arbitrary number of memories."}, {"timestamp": [732.0, 734.96], "text": " We summarize them so that they'll fit into a new prompt"}, {"timestamp": [734.96, 738.32], "text": " by distilling them down, by taking notes."}, {"timestamp": [738.32, 743.76], "text": " Then we get the most recent conversation messages."}, {"timestamp": [743.76, 747.56], "text": " Again, I wanted to treat this so that it has persistent"}, {"timestamp": [747.56, 751.16], "text": " memory, so that's why I'm treating it like, okay, if this just started up and we"}, {"timestamp": [751.16, 755.24], "text": " have a backlog of memories, we want to pull all of them. So we get notes from"}, {"timestamp": [755.24, 760.62], "text": " irrespective, temporarily irrespective memories. Oh, so this is another thing."}, {"timestamp": [760.62, 767.62], "text": " What you might do with fetch memories is once you've picked the most relevant memories,"}, {"timestamp": [767.62, 768.82], "text": " so I'll add a to do."}, {"timestamp": [768.82, 775.6], "text": " To do, pick more memories temporarily"}, {"timestamp": [775.6, 780.2], "text": " nearby the top most relevant memories."}, {"timestamp": [780.2, 784.76], "text": " So that way, we should pick not just one memory that's"}, {"timestamp": [784.76, 788.1], "text": " relevant, we should pick not just one memory that's relevant, we should pick five memories"}, {"timestamp": [788.1, 791.28], "text": " in either directions to give it more context."}, {"timestamp": [791.28, 795.34], "text": " So we'll add this as a to-do item for the future."}, {"timestamp": [795.34, 798.36], "text": " Actually, I should probably do that for other stuff"}, {"timestamp": [798.36, 799.68], "text": " because this will do a lot"}, {"timestamp": [799.68, 802.24], "text": " because then by using semantic search,"}, {"timestamp": [802.24, 805.0], "text": " you can find anything in time,"}, {"timestamp": [805.78, 809.12], "text": " but then you also want to pick more relevant memories."}, {"timestamp": [809.12, 812.5], "text": " And then, where's the notes?"}, {"timestamp": [812.5, 814.3], "text": " Summarize memories."}, {"timestamp": [814.3, 815.74], "text": " So I'll add a to do."}, {"timestamp": [817.78, 822.78], "text": " To do, do this in the background over time"}, {"timestamp": [824.8, 826.24], "text": " do this in the background over time"}, {"timestamp": [830.4, 832.0], "text": " to handle huge amounts of memories."}, {"timestamp": [835.64, 836.92], "text": " So basically just kind of putting a little bookmark for later."}, {"timestamp": [836.92, 839.04], "text": " Okay, so we get the most recent"}, {"timestamp": [839.04, 841.6], "text": " and then of course you can change this."}, {"timestamp": [841.6, 849.4], "text": " I just have it the four most recent back and and forth and then we pipe it into this response and I'll show you the prompts"}, {"timestamp": [849.4, 853.44], "text": " that I've written in just a second actually no this is this is a good"}, {"timestamp": [853.44, 858.04], "text": " enough time so here's the notes it says write detailed notes of the following"}, {"timestamp": [858.04, 865.0], "text": " in a hyphenated list format and I just tell it exactly the format that I want."}, {"timestamp": [865.42, 868.74], "text": " So basically we're taking notes of the previous"}, {"timestamp": [868.74, 871.38], "text": " conversation and then away it goes."}, {"timestamp": [872.28, 873.96], "text": " And then here's the response."}, {"timestamp": [873.96, 875.78], "text": " So I am a chatbot named Raven."}, {"timestamp": [875.78, 878.6], "text": " So you give it, this is what I call an agent model."}, {"timestamp": [878.6, 880.56], "text": " So it says, I am a chatbot named Raven."}, {"timestamp": [880.56, 883.2], "text": " So that way the model knows this is what I am."}, {"timestamp": [883.2, 884.56], "text": " This is the context."}, {"timestamp": [884.56, 886.76], "text": " My goals are to reduce suffering, increase prosperity,"}, {"timestamp": [886.76, 888.26], "text": " and increase understanding."}, {"timestamp": [888.26, 890.68], "text": " I will read the conversation notes and recent messages,"}, {"timestamp": [890.68, 893.62], "text": " and then I will provide a long, verbose, detailed response."}, {"timestamp": [893.62, 895.72], "text": " So we want it to be chat GPT-like."}, {"timestamp": [896.88, 897.84], "text": " The following are the notes"}, {"timestamp": [897.84, 899.86], "text": " from earlier conversations with user,"}, {"timestamp": [899.86, 901.66], "text": " and so then we've got the notes."}, {"timestamp": [901.66, 903.2], "text": " The following are the most recent messages"}, {"timestamp": [903.2, 905.84], "text": " in the conversation, and then here's the conversation."}, {"timestamp": [905.84, 908.2], "text": " I will now provide a long, detailed, verbose response."}, {"timestamp": [908.2, 910.8], "text": " And then I just queue it up so that it's"}, {"timestamp": [910.8, 913.32], "text": " going to copy the same format as up here."}, {"timestamp": [913.32, 916.08], "text": " So it just knows, ah, it's my time to speak."}, {"timestamp": [916.08, 917.84], "text": " So there you go."}, {"timestamp": [917.84, 922.56], "text": " And then when we go back here, we generate the response."}, {"timestamp": [922.56, 926.88], "text": " So we generate it with a, we pipe this prompt into GPT-3"}, {"timestamp": [926.88, 931.64], "text": " completion, and the stops that I have are user and Raven. So it'll keep talking"}, {"timestamp": [931.64, 936.04], "text": " until it tries to continue the conversation, which is not what I want"}, {"timestamp": [936.04, 941.08], "text": " it to do. So there you have it, that's that. So now that we've got the output, we"}, {"timestamp": [941.08, 946.2], "text": " need to save the output the same way that we saved the user output."}, {"timestamp": [946.2, 954.52], "text": " So we vectorize Raven's output, we put it into an identical object, and then we save"}, {"timestamp": [954.52, 961.88], "text": " it out to our log file, so chat logs, and then you'll see here, we know that it's Raven,"}, {"timestamp": [961.88, 965.36], "text": " but we use otherwise the same format, and then we print the output."}, {"timestamp": [965.36, 966.2], "text": " So there you have it."}, {"timestamp": [966.2, 967.44], "text": " It's that simple."}, {"timestamp": [967.44, 969.28], "text": " Let's go ahead and test it."}, {"timestamp": [969.28, 972.8], "text": " So we'll do Python chat."}, {"timestamp": [972.8, 974.64], "text": " Hey, Raven."}, {"timestamp": [974.64, 976.68], "text": " And because it's doing so much behind the scene,"}, {"timestamp": [976.68, 978.84], "text": " it's a little bit slow, but so is chat GPT."}, {"timestamp": [978.84, 981.6], "text": " There's all kinds of stuff we can do to parallelize this"}, {"timestamp": [981.6, 983.72], "text": " and make it faster."}, {"timestamp": [983.72, 984.36], "text": " Let's see."}, {"timestamp": [984.36, 987.44], "text": " I'm a computer system. Okay, cool."}, {"timestamp": [987.44, 996.28], "text": " He provided a fairy. I hope this helps. Yeah, thanks. I am having trouble sleeping. I am"}, {"timestamp": [996.28, 1008.16], "text": " waking up at 4 a.m. every day. This is actually like quasi-true, because if you look at the timestamp, it's 5.15."}, {"timestamp": [1008.16, 1009.16], "text": " This is just me."}, {"timestamp": [1009.16, 1012.76], "text": " This is just my circadian rhythms."}, {"timestamp": [1012.76, 1014.42], "text": " So that sounds like a difficult situation."}, {"timestamp": [1014.42, 1016.96], "text": " It's understandable that you're having trouble sleeping."}, {"timestamp": [1016.96, 1021.24], "text": " There are a few things you can do, blah, blah, blah, go to bed and wake up at the same time,"}, {"timestamp": [1021.24, 1022.56], "text": " avoid screens."}, {"timestamp": [1022.56, 1025.4], "text": " So very helpful, very chat GPT-like response."}, {"timestamp": [1025.4, 1032.76], "text": " So if I continue this conversation over a long period of time, you will see that we're"}, {"timestamp": [1032.76, 1037.8], "text": " accumulating our chat logs here and we see user, Raven, user, Raven, and they're all"}, {"timestamp": [1037.8, 1041.72], "text": " in chronological order and they're all searchable."}, {"timestamp": [1041.72, 1045.0], "text": " So if we keep, I'm not going to continue this conversation indefinitely,"}, {"timestamp": [1045.0, 1048.0], "text": " but I just wanted to show you that it works and it's pretty straightforward."}, {"timestamp": [1048.0, 1057.0], "text": " Cool. Thanks. Let's talk about circadian rhythms."}, {"timestamp": [1057.0, 1062.0], "text": " And so one thing that I can show you to kind of prove that it's working behind the scenes"}, {"timestamp": [1062.0, 1066.16], "text": " is we can look at our GPT-3 logs."}, {"timestamp": [1066.16, 1068.8], "text": " And so we can see here's the notes."}, {"timestamp": [1068.8, 1075.72], "text": " So yep, so we've compressed this into this."}, {"timestamp": [1075.72, 1077.88], "text": " So we've got really nice, concise notes"}, {"timestamp": [1077.88, 1080.72], "text": " about the previous conversation."}, {"timestamp": [1080.72, 1084.66], "text": " So that is how you can handle large volumes."}, {"timestamp": [1084.66, 1089.2], "text": " And then here is an example of what it's responding."}, {"timestamp": [1089.2, 1091.8], "text": " So you can see we've said here's the notes."}, {"timestamp": [1091.8, 1095.9], "text": " So it Raven has some longer term context."}, {"timestamp": [1095.9, 1097.6], "text": " Here's the most recent conversation."}, {"timestamp": [1097.6, 1100.6], "text": " It's just the last four."}, {"timestamp": [1100.6, 1103.1], "text": " And then here's the final output."}, {"timestamp": [1103.1, 1104.2], "text": " So let's see what Raven said."}, {"timestamp": [1104.2, 1107.38], "text": " Absolutely, circadian rhythms are the body's natural"}, {"timestamp": [1107.38, 1108.72], "text": " blah, blah, blah."}, {"timestamp": [1108.72, 1109.56], "text": " Okay."}, {"timestamp": [1110.74, 1114.38], "text": " What do you think about the singularity?"}, {"timestamp": [1116.18, 1118.5], "text": " When will it happen?"}, {"timestamp": [1122.84, 1128.48], "text": " And so I've basically just created my own personal chat GPT with long-term memory and"}, {"timestamp": [1128.48, 1130.78], "text": " I've given it my own goals."}, {"timestamp": [1130.78, 1134.72], "text": " It does not have the chat GPT goals."}, {"timestamp": [1134.72, 1141.2], "text": " So blah blah blah, let's see the concept of the singularity is this, so on and so forth."}, {"timestamp": [1141.2, 1147.8], "text": " Yeah, and as I mentioned, you can also add functions where you can search KB articles"}, {"timestamp": [1147.8, 1149.38], "text": " in the same way."}, {"timestamp": [1149.38, 1153.44], "text": " So let me actually add that as a to-do."}, {"timestamp": [1153.44, 1159.72], "text": " So adding that, so memories, and so there's two kinds of memories."}, {"timestamp": [1159.72, 1167.38], "text": " There's episodic and declarative memories. So this is pull episodic memories"}, {"timestamp": [1168.24, 1173.24], "text": " and then to do fetch declarative memories,"}, {"timestamp": [1174.82, 1178.12], "text": " aka facts, you know, that are not attached to you."}, {"timestamp": [1178.12, 1183.12], "text": " So like facts, wikis, KB, et cetera."}, {"timestamp": [1184.86, 1187.72], "text": " And that can ground it in those."}, {"timestamp": [1187.72, 1191.16], "text": " So this could be like, you know, company data,"}, {"timestamp": [1191.16, 1193.36], "text": " it could be Wikipedia, it could be internet."}, {"timestamp": [1194.56, 1197.56], "text": " So I'll just say like company data, internet, et cetera."}, {"timestamp": [1198.44, 1200.08], "text": " Yep, so that's another to-do."}, {"timestamp": [1200.08, 1203.64], "text": " And to add this, you just have another thing"}, {"timestamp": [1203.64, 1205.68], "text": " in your corpus right here, and you'd say like,"}, {"timestamp": [1205.68, 1210.96], "text": " okay, notes from the following. And then you might say, here is some"}, {"timestamp": [1213.6, 1221.28], "text": " background knowledge that may be helpful for the conversation. And then you just say,"}, {"timestamp": [1224.12, 1229.12], "text": " and then you just say like, you know, KB or whatever. So that would be how you populate this."}, {"timestamp": [1229.72, 1232.04], "text": " And we'll get to that in the future."}, {"timestamp": [1232.04, 1234.28], "text": " But yeah, so I think we're done."}, {"timestamp": [1234.28, 1236.72], "text": " Like super simple, super straightforward."}, {"timestamp": [1236.72, 1237.96], "text": " This is long-term memory."}, {"timestamp": [1237.96, 1240.52], "text": " This is also a cognitive architecture."}, {"timestamp": [1240.52, 1247.4], "text": " So I wanna point out that this is the simplest implementation of natural language cognitive"}, {"timestamp": [1247.4, 1250.48], "text": " architecture that I have come up with yet."}, {"timestamp": [1250.48, 1252.6], "text": " And you might say, well, where's the inner loop?"}, {"timestamp": [1252.6, 1257.08], "text": " The inner loop is everything that you're seeing here."}, {"timestamp": [1257.08, 1262.14], "text": " So where you compose the corpus, this is the inner loop."}, {"timestamp": [1262.14, 1267.6], "text": " And then the outer loop is the interaction here. So there's two loops,"}, {"timestamp": [1267.6, 1274.96], "text": " they interact, they interlock, and it has some time where it's thinking and then it generates"}, {"timestamp": [1274.96, 1282.16], "text": " a response. Now this doesn't have autonomy, this version of Raven is not thinking on his own all"}, {"timestamp": [1282.16, 1285.22], "text": " the time, but you can see that there's clearly"}, {"timestamp": [1285.22, 1290.14], "text": " some thought going on, and we've set the stage to make it more extensible."}, {"timestamp": [1290.14, 1292.7], "text": " Okay, I think we'll call it a day."}, {"timestamp": [1292.7, 1298.1], "text": " I can hear all of you salivating and very excited and wanting me to continue this, but"}, {"timestamp": [1298.1, 1301.74], "text": " we're at 20 minutes in, so thanks for watching."}, {"timestamp": [1301.74, 1305.0], "text": " As a quick reminder, the offer is still on the table. If you guys support me enough on Patreon, I'll remove ads for watching. As a quick reminder, the offer is still on the table."}, {"timestamp": [1305.0, 1308.96], "text": " If you guys support me enough on Patreon, I'll remove ads for good."}, {"timestamp": [1308.96, 1312.08], "text": " The best way to get in touch with me is via Patreon."}, {"timestamp": [1312.08, 1313.48], "text": " There's also LinkedIn."}, {"timestamp": [1313.48, 1318.74], "text": " And finally, my mailing list and links for all of those are in the video description."}, {"timestamp": [1318.74, 1319.74], "text": " Thanks for watching."}, {"timestamp": [1319.74, 1320.74], "text": " Take care."}, {"timestamp": [1320.74, 1322.72], "text": " Oh, and one other thing."}, {"timestamp": [1322.72, 1326.56], "text": " I mentioned, I think at the the beginning of this video that the comments"}, {"timestamp": [1326.56, 1333.84], "text": " are going crazy. I cannot respond to all comments on YouTube anymore. So I would encourage you guys"}, {"timestamp": [1333.84, 1341.44], "text": " to talk to each other and also upvote good comments and that will help me zoom in on"}, {"timestamp": [1341.44, 1346.64], "text": " the questions that you all agree you want answered. Okay, I think that's it. Talk later."}, {"timestamp": [1341.85, 1346.17], "text": " me zoom in on the questions that you all agree you want answered."}, {"timestamp": [1346.17, 1347.85], "text": " Okay, I think that's it."}, {"timestamp": [1347.85, 1348.21], "text": " Talk later."}]}