{"text": " All right, gang. So David Shapiro here. One of the questions that I get the most on Patreon and LinkedIn and everywhere else is how do I make a QA chatbot? I get questions like how do I answer specific questions, whether it's in a high risk situation. And I mean like law, medicine, mental health, all kinds of stuff. And of course, you know, chat GPT is out and you can do some stuff, but you know, it's bounded, right? They have their own things, but with the chat GPT API, we can do our own stuff. And natural language interfaces are certainly the way of the future, or at least the way of right now for how we're gonna be interacting with data and computers. So I did a little poll and a very, very clear majority of people want QA chatbots. Now, I was laying in bed last night and I was like, I want to do, I want to accelerate longevity and regenerative medicine because that's one of the things that I really am looking forward to. I've got an old shoulder injury, and I'm getting old, and getting old sucks. And I just saw that Sam Altman invested $180 million in this stuff. So I was like, okay, well, what if I can combine these two? So here's what we're going to do. We're going to go through the whole process of using the latest and greatest AI tools to make even more AI tools to help accelerate longevity and regenerative medicine. And we're going to do it with the chat GPT API. So this is going to be a series. I don't know, it's going to take a while to unpack. And I'm also going to do a little bit of real-time editing with pausing and stuff. So, anyways, here's where I'm starting. And I often, in past experiments, I'll often collect the data before, like without showing you guys, and then I'll forget how I got it. So, I'm documenting how I got the data in the first place. So, I'll come over to Bing AI, which Bing chat searches the Internet, and I can tell it what I'm looking for. Also speaking of, we're up to 10. So that's good. It'll be better once they figure out how to get us back to unlimited conversations, but we're going in the right direction. So I asked for sources for primary research, and I said like NIH and archive and of course it just said NIH and archive, but it also added nature. And so basically what I'm doing is I'm going to gather some sources to download some like PDFs or other articles and use that as my data. Now, obviously, this is probably not, I probably will not succeed in advancing science on my own, but what I can do is I can demonstrate how, and then someone else can take it and put it into a product. Speaking of, I have had on occasion, some people reach out to me and say like, oh, I incorporated your work into my startup or into my business. Like, please let me know what you're actually using my work for. One, it's just nice to know, but two, it also helps me understand where I'm adding value, right? Because my whole goal, I'm not here to make a ton of money. I am here to help bring about post-scarcity and singularity and all that fun stuff. All right, so anyways, let's see. I'm specifically trying to accelerate longevity and regenerative research with AI. So I'm looking for papers that can be integrated into automatic NLP stuff. Yeah, that's basically it. So I think the kinds of papers that will be most helpful are likely to include specific techniques, proteins, enzymes, enzymes, that doesn't look right, enzymes, and therapies. Let's see, does that help narrow it down? All right, so if I tell Bing what I'm doing, hopefully it'll have a better understanding of what I'm trying to achieve and then give me a little bit more specific stuff. So anyways, regenerative medicine. So nature.com slash regenerative. Looks like this is pretty good. So there's plenty of stuff that's open access. So in order to respect the data, I'm only going to download stuff that has open access, which it's great that they do this. Actually, I'm going to. Favorites bar, because that's good information. Let's see, yes, that helps a lot. Thank you for sharing. Artificial intelligence and longevity medicine, which discusses, interesting. So there's, oh, okay. So I found a Nature article talking about it. Oh, this is only a preview, okay. So I think we'll need to specify that we need open access stuff. Let's see. Let's see. Do you want me to show you how to download any of these papers? No, some of them are not open access, but I'm not looking for AI and aging papers. I'm doing the AI. I'm looking for therapeutic techniques and candidates. Can you try again? And you try again. Yep, so therapeutic techniques and candidates for longevity in regenerative medicine. So what it's doing is it's distilling down my search query, which that's actually a super easy thing to do with prompt engineering. You just say, take this paragraph and convert it into a Google search query, or in this case, a Bing search query. All right. Let's see, regenerative medicine. All right, the Lancet, so that's a good one. Envisioning future trends in regenerative medicine. So some of these, yes, except all cookies. Where did it go? Okay. All right. So this is not, this is requiring stuff. So here's one thing where I am really skeptical because like all of this gatekeeping and paywalls is actually really going to slow down research, especially in the age of AI. That being said, it's like, okay, there is some value added by these things. Okay, let's focus on open access sources of regenerative medicine research. Can you find any more aside from archive. And nature. Because, like, okay, the fact that, like, this one is here, so medical express, but this is, like, a no-name site. So it's, like, I don't even know if, like, this is, yeah, this is just someone restating something else. And it's just pointing at nature anyways. Okay. So it looks like nature open access is gonna be our best source so far. We can also have archive. So let's see. If we go to archive and we look for, like, let's see, where is it? Well, oh, it looks like we're looking for bio archive. Okay. Well, oh, looks like we're looking for bioarchive. Okay. Oh, man, this website is like from the 90s. Okay, this is going to be a pain. Ouch. All right, neuroscience, molecular, so molecular biology, immunology, genomics, genetics, let's see, cell biology, cancer biology, biochemistry, synthetic biology, that's fun. Okay, structural bias, so a lot of this stuff is not going to be like directly relevant, which is interesting. Okay, sure. Open journal of Regenerative Medicine. Hey, that looks... Stem Cell Research and Regenerative Medicine and NPJ. There we go. Open Journal of Regenerative Medicine. So let's bookmark that. Favorites bar. Open Access Journal Stem Cell Research research and regenerative medicine. I'm not seeing any papers. Let's see. It looks like they only have like a couple of issues per year. This looks low quality. Okay. All right. Well, we've got a couple, right? So let me bookmark bioRxiv as well. Okay, so if we come here, got bioRxiv, we got Open Journal of Regenerative Medicine, and then we've got Nature. So we've got three sources of data. So what I'll do now is, I'm not gonna make you watch me download everything, but now you see how I've gone about finding this information. And so then I'll download just a whole mess of this stuff, and then save it, and we'll start to slice and dice it. All right, pause it, download stuff, we'll be right back. Okay. And we're back. So I downloaded about, let's see, 81 papers. So this is obviously 81 research papers on a variety of topics. This is not a really coherent search strategy. This is just a proof of concept because this is to demonstrate like, cause this is an intractable amount of material. Sure, a professional scientist will probably easily skim through more than this number while performing a literature review. But imagine that you've got 2000,000 or 10,000 of these that you need to sift through and get information from. So imagine that in your citations page, instead of having 30 to 100, what if you have like 30,000 citations? And we can do that automatically with AI and we can find them quickly through chatbots. So that's step one was downloading stuff, which I just showed you. And then step two is let's convert it. So I have this handy dandy public archive right here called document scraping, and I've got a convert PDF file here. So what we're gonna do next is we're going to, and I'll show you, it's really simple, convert PDF to text. And what it'll do is it'll grab everything in that folder that's a PDF and then dump a text version out to the next folder. And then it breaks it up by page. So I'll just say new page, right? It's not a big deal. And this forms a nice, easy way of seeing it. So I'll show you what it does. So CD to chat GPT regenerative medicine, and then we'll do Python step 01 convert. And so what it's doing is it's going through and converting them one by one, and it's dropping them here. So you can see here is the information. And obviously this is not going to grab the, what should we call it? It's not gonna grab the graphics and stuff. And that's fine, because we don't have multimodal models yet. Apparently GPT-4 is going to be multimodal but at least this should give us all the information that we need in terms of text information. So this is running, it'll take a minute. It's obviously very much taking its time so I'll go ahead and pause it again and show you the end result. Also let's see we're only at 12 minutes, so we'll see how much further we can get. Yeah, we'll be right back. All right, so we've got 81 text files. Now, I always check my data, and so here's a problem, is some of the lines have no spaces. Some of them do, some of them don't. It's really weird. I don't know if this is a problem with the PDF plumber or with the underlying PDF itself. So I don't know what's going on. But from there and here, like, okay, you see this one, this one looks like it's formatted fine. I'm not gonna delete it because it's like, okay, you see this one, this one looks like it's formatted fine. I'm not gonna delete it because it's like, okay, we still want this information. And also, even if it is, even if the formatting is botched like that, as it is in some cases, GPT-3 can still often read these. So let me just grab this and show you what I mean. So let's grab this and go to and then we say, let's see, complete, fix the formatting issue. Fixed. And so then if we, let's see, do that up to 500, zoom out a little. So you can see it can still read it and add the spaces back in, which means that it can comprehend the word boundary problems. So yeah, GPT-3 can understand it. It's honestly probably been trained on a lot of data that is malformed like this. So it's fine. Yeah, that's good. So now let's come over here. And what we're going to do is we need to convert these text files into something that's usable. So one of the advantages of having new page right here is that we can open it and then split it into individual pages again that are text. So what we're going to do is we're going to have a folder called papers underscore JSON. And now I'm going to ask chat-gpt, write a Python script that opens all text files from the puts them into a list of strings with the demarcator new page. Then take each page and get an embedding by passing the string to a function that I call, let's see, gpt3 embed into a JSON file, one JSON file for each original text file. The JSON should have elements such as original text, and finally the embedding. Okay, so that should be enough. Sure, here's a Python script. Wow, it's fast. I guess nobody else is using it right now. Okay, pages, embeddings, GPTT three embed page for page and page. Oh wow. Okay. That's fun. Um, yep. Original file name, file name pages equals I plus one text page embedding to list for I and page embedding enumerate zip pages. Dang. I think this is it. Um, is dang, I think this is it. This function is wrong, but that's fine. Yep, okay, so let's copy this and come out here. All right, so let's get rid of those guys. All right, so for file name in OSLister, dir path, and dir path is here. So we actually want to make this a function and then call it from if name equals main. So process text, it's defining a function within. That doesn't make any sense. That is what, it's fine. Yep. Let's see. No, stop, stop, stop. Let's remove the gpt3 embedding function from being nested inside another function. That is not PEP 8 approved. At least I don't think so. Sure, here's an updated, yep. And so basically I'm just going to ignore that, because I wrote that function elsewise. OK, so for this and that, et cetera, et cetera. There we go. And then. Let's see, oh, one problem. One more problem, the output directory needs to be specified as papers underscore JSON. And see, this is why I don't like using Copilot, because you don't have a dialogue, right? It's just guessing what you want and then what I can do is I can just look at this and say, okay, why is it not allowing me to pass Papers, JSON, outpath. Why is the outpath hardcoded and not parameterized? Does that make any sense to you. I'm way too passive aggressive with this thing, I apologize. There was an error generating a response. I'm... No, do not. So here's the thing, when you do regenerate and it just continues, no, stop, stop, stop. Because then it's broken up into two. So what I'm going to do instead is I'm going to come here and just, one, remove the saltiness, please fix, and let's start over. Good catch. So OpenAI, maybe you can have it as a setting, but honestly, if there's an error, I would rather it just like save what it did and then pipe that in. I don't know. Like, yeah, there we go. Much better. Okay. There we go. Much better. Okay, so let's copy this and come back over here. We don't need that function. So basically what I did here is I copied a few functions from another script. So here's my embedding one. Oh, because we're using data from the internet, I always do this where I force it, I encode it to ASCII and then decode, and that fixes Unicode errors because GPT-3 often does not do well with some forms of Unicode. I still haven't figured it out. Maybe it's not even an issue anymore, but if it is, it's still out there. Okay, so if it doesn't exist, make it great for file name, NOSLister. If it ends with .text, that's great. Split it into pages, embed it, and then here's the output, so we get all the pages, excellent. I'm just gonna trust that this works because I respond with this. So the vector, I don't think it's a numpy array, so that's fine. Embedding to list, I think it's already, yeah. So embeddings equals, so this is already a list I believe, so I think this will break, that's fine. And then we zip the pages and embeddings. Oh, interesting, okay. We'll see if this is formatted correctly. And then let's see, save it. So it just replaces the file name, that's good, with OS. But I actually already have a save JSON thing, so we just have the file path and the payload. And in this one I specify ensure ASCII false, sort keys true, and then indent. So this formats it nice and pretty. So what we'll do instead is we'll comment this out and then we'll just do save JSON. And what is the order? I have it file path and payload. So then the file path is this guy. And then the payload is output dict. Yes. All right, so that should be good. Now if name equals main, et cetera, et cetera. So what I want'll do is... Hmm. I guess this is just going to be messy no matter what we do. So let's add some output here. So we'll do print content to embed. And then we'll do content and then we'll print vector. Vector. All right, so that way we'll be able to see it embedding as it goes and that should be fine and then yeah that'll be fine. All then, yeah, that'll be fine. All right, so let's save this and come over here. So Python step 02. Hey, look, we had an issue, Unicode decode error. Okay, in text f read, oh, yep. So here we go. All right, so I'm gonna ask, I know what the problem is, but I'm gonna ask chat GPT. Got an error, can you fix it? There we go. Do, do, do, do, do, do. So basically it's pretty simple. All you have to do is specify, make sure those are spaces, okay, yes. So you specify the encoding is UTF-8. And so now it knows better. All right, so let's try that again. Yeah, it should not go that fast. Okay, so the fact that it's going that fast and it's not outputting the vector means that it's barfing somewhere here. So let's comment out this because it's probably returning none. Which I had that because there was another project that I was working on where some of the things failed. Yeah. Always use notepad. Let's see, embedding null. Yeah, so you see it was not embedding. So let's delete these. It worked mostly. Let me close these because they're superfluous. And let's see why it's blowing up. Do, do, do, do, do, do. Oh, you need an API key. What do you mean I need an API key? Here, let me fix this. I'll be right back. Okay, I think I fixed it. So I added this here and then I copied my get ignore and API key right here. So we should be good. Let's give it one last try. And also I did a time check. We're at almost 30 minutes. So this will probably be it for today. But we're off to a pretty good start if I do say so myself. All right. So let's do, let's try and do some embeddings. That's still seen. Oh, no, it's just that fast. Look at that. Look at that. All right, so let's try and do some embeddings. That still seems, oh, no, it's just that fast. Look at that, look at that. All right, so let's take a look at the output. Whoops. All right, so the embedding. So for each page, we get embedding, page number, text we get embedding page number text, embedding page number text. And then we keep the original file name as well. And so by using this, we can trace it back to the original PDF as well, because we're going to basically need to be able to cite our sources. All right, cool. So this is tearing through this. Let's see, make sure it hasn't blown up. All right, it. So this is tearing through this. Let's see, make sure it hasn't blown up. All right, it's still going. And so you see, basically what we're doing is we're getting one embedding for every single page. Now, there's a lot more that we're going to need to do in order to make this usable, because some of these are like 80 pages long, which is way too much for it to read. So then it's like, okay, well, if you search it based on the embedding, that'll get you close, but then what else do you do? There's a lot of problems to solve, but the fact of the matter is, this is going to give us basically a super advanced chat-based scientific search engine. Now, because I'm also working on cognitive architecture, we can have it do a lot of thinking for you in the background. That's going to be the real game changer. Not just a chatbot that allows you to search, but a chatbot that you can ask it scientific questions, and it will go think about the problem for you. So I'm going to let this finish. I'm going to go ahead and stop the recording, and we'll come back tomorrow for part two. Thanks for watching. I'm going to go ahead and stop the recording and we'll come back tomorrow for part two. Thanks for watching.", "chunks": [{"timestamp": [0.0, 7.28], "text": " All right, gang. So David Shapiro here. One of the questions that I get the most on Patreon"}, {"timestamp": [7.28, 12.96], "text": " and LinkedIn and everywhere else is how do I make a QA chatbot? I get questions like"}, {"timestamp": [12.96, 19.64], "text": " how do I answer specific questions, whether it's in a high risk situation. And I mean"}, {"timestamp": [19.64, 25.0], "text": " like law, medicine, mental health, all kinds of stuff."}, {"timestamp": [25.0, 27.24], "text": " And of course, you know, chat GPT is out"}, {"timestamp": [27.24, 31.24], "text": " and you can do some stuff, but you know, it's bounded, right?"}, {"timestamp": [31.24, 34.36], "text": " They have their own things, but with the chat GPT API,"}, {"timestamp": [34.36, 37.12], "text": " we can do our own stuff."}, {"timestamp": [37.12, 40.68], "text": " And natural language interfaces are certainly"}, {"timestamp": [40.68, 43.0], "text": " the way of the future, or at least the way of right now"}, {"timestamp": [43.0, 46.0], "text": " for how we're gonna be interacting with data and computers."}, {"timestamp": [46.72, 54.96], "text": " So I did a little poll and a very, very clear majority of people want QA chatbots."}, {"timestamp": [54.96, 61.84], "text": " Now, I was laying in bed last night and I was like, I want to do, I want to accelerate longevity and regenerative medicine"}, {"timestamp": [62.72, 65.68], "text": " because that's one of the things that I really am looking"}, {"timestamp": [65.68, 72.16], "text": " forward to. I've got an old shoulder injury, and I'm getting old, and getting old sucks."}, {"timestamp": [72.88, 78.24], "text": " And I just saw that Sam Altman invested $180 million in this stuff. So I was like,"}, {"timestamp": [78.24, 88.56], "text": " okay, well, what if I can combine these two? So here's what we're going to do. We're going to go through the whole process of using the"}, {"timestamp": [88.98, 92.62], "text": " latest and greatest AI tools to make even more AI tools to"}, {"timestamp": [92.62, 96.84], "text": " help accelerate longevity and regenerative medicine."}, {"timestamp": [96.84, 101.06], "text": " And we're going to do it with the chat GPT API."}, {"timestamp": [101.06, 102.56], "text": " So this is going to be a series."}, {"timestamp": [102.56, 104.88], "text": " I don't know, it's going to take a while to unpack."}, {"timestamp": [104.88, 106.72], "text": " And I'm also going to do a little bit of real-time"}, {"timestamp": [106.72, 108.28], "text": " editing with pausing and stuff."}, {"timestamp": [108.28, 112.28], "text": " So, anyways, here's where I'm starting."}, {"timestamp": [112.28, 118.4], "text": " And I often, in past experiments, I'll often collect the data before, like without showing"}, {"timestamp": [118.4, 120.28], "text": " you guys, and then I'll forget how I got it."}, {"timestamp": [120.28, 123.52], "text": " So, I'm documenting how I got the data in the first place."}, {"timestamp": [123.52, 125.76], "text": " So, I'll come over to Bing AI,"}, {"timestamp": [125.76, 128.4], "text": " which Bing chat searches the Internet,"}, {"timestamp": [128.4, 130.64], "text": " and I can tell it what I'm looking for."}, {"timestamp": [130.64, 135.12], "text": " Also speaking of, we're up to 10. So that's good."}, {"timestamp": [135.12, 138.0], "text": " It'll be better once they figure out how to get us"}, {"timestamp": [138.0, 140.16], "text": " back to unlimited conversations,"}, {"timestamp": [140.16, 142.6], "text": " but we're going in the right direction."}, {"timestamp": [142.6, 148.0], "text": " So I asked for sources for primary research, and I said like NIH and archive and of course"}, {"timestamp": [148.0, 152.24], "text": " it just said NIH and archive, but it also added nature."}, {"timestamp": [152.24, 158.36], "text": " And so basically what I'm doing is I'm going to gather some sources to download some like"}, {"timestamp": [158.36, 166.9], "text": " PDFs or other articles and use that as my data. Now, obviously, this is probably not,"}, {"timestamp": [166.9, 170.78], "text": " I probably will not succeed in advancing science on my own,"}, {"timestamp": [170.78, 173.76], "text": " but what I can do is I can demonstrate how,"}, {"timestamp": [173.76, 176.8], "text": " and then someone else can take it and put it into a product."}, {"timestamp": [176.8, 179.76], "text": " Speaking of, I have had on occasion,"}, {"timestamp": [179.76, 181.12], "text": " some people reach out to me and say like,"}, {"timestamp": [181.12, 184.54], "text": " oh, I incorporated your work into my startup"}, {"timestamp": [184.54, 185.52], "text": " or into my business."}, {"timestamp": [185.52, 192.08], "text": " Like, please let me know what you're actually using my work for. One, it's just nice to know,"}, {"timestamp": [192.08, 198.32], "text": " but two, it also helps me understand where I'm adding value, right? Because my whole goal,"}, {"timestamp": [198.32, 205.52], "text": " I'm not here to make a ton of money. I am here to help bring about post-scarcity and singularity and all that fun stuff."}, {"timestamp": [205.52, 207.52], "text": " All right, so anyways, let's see."}, {"timestamp": [207.52, 232.08], "text": " I'm specifically trying to accelerate longevity and regenerative research with AI. So I'm looking for papers that can be integrated into automatic NLP"}, {"timestamp": [232.08, 245.0], "text": " stuff. Yeah, that's basically it. So I think the kinds of papers that will be most helpful"}, {"timestamp": [246.24, 251.24], "text": " are likely to include specific techniques,"}, {"timestamp": [252.96, 255.8], "text": " proteins, enzymes, enzymes,"}, {"timestamp": [259.46, 264.08], "text": " that doesn't look right, enzymes, and therapies."}, {"timestamp": [271.44, 277.04], "text": " Let's see, does that help narrow it down? All right, so if I tell Bing what I'm doing, hopefully it'll have a better understanding of what I'm trying"}, {"timestamp": [277.04, 286.8], "text": " to achieve and then give me a little bit more specific stuff. So anyways, regenerative medicine. So nature.com slash regenerative."}, {"timestamp": [286.8, 306.16], "text": " Looks like this is pretty good. So there's plenty of stuff that's open access. So in order to respect the data, I'm only going to download stuff that has open access, which it's great that they do this. Actually, I'm going to. Favorites bar, because that's good information."}, {"timestamp": [306.16, 307.56], "text": " Let's see, yes, that helps a lot."}, {"timestamp": [307.56, 309.48], "text": " Thank you for sharing."}, {"timestamp": [309.48, 311.6], "text": " Artificial intelligence and longevity medicine,"}, {"timestamp": [311.6, 313.72], "text": " which discusses, interesting."}, {"timestamp": [313.72, 318.14], "text": " So there's, oh, okay."}, {"timestamp": [318.14, 320.96], "text": " So I found a Nature article talking about it."}, {"timestamp": [320.96, 324.08], "text": " Oh, this is only a preview, okay."}, {"timestamp": [324.08, 327.96], "text": " So I think we'll need to specify that"}, {"timestamp": [327.96, 335.12], "text": " we need open access stuff. Let's see. Let's see. Do you want me to show you how to download"}, {"timestamp": [335.12, 350.8], "text": " any of these papers? No, some of them are not open access, but I'm not looking for AI and aging papers. I'm doing the AI. I'm"}, {"timestamp": [350.8, 363.56], "text": " looking for therapeutic techniques and candidates. Can you try again?"}, {"timestamp": [362.0, 368.5], "text": " And you try again."}, {"timestamp": [372.5, 376.5], "text": " Yep, so therapeutic techniques and candidates for longevity in regenerative medicine. So what it's doing is it's distilling down my search query,"}, {"timestamp": [376.5, 379.5], "text": " which that's actually a super easy thing to do with prompt engineering."}, {"timestamp": [379.5, 384.5], "text": " You just say, take this paragraph and convert it into a Google search query,"}, {"timestamp": [384.5, 385.52], "text": " or in this case, a"}, {"timestamp": [385.52, 387.32], "text": " Bing search query."}, {"timestamp": [387.32, 389.68], "text": " All right."}, {"timestamp": [389.68, 390.84], "text": " Let's see, regenerative medicine."}, {"timestamp": [390.84, 393.68], "text": " All right, the Lancet, so that's a good one."}, {"timestamp": [393.68, 397.48], "text": " Envisioning future trends in regenerative medicine."}, {"timestamp": [397.48, 400.28], "text": " So some of these, yes, except all cookies."}, {"timestamp": [400.28, 402.28], "text": " Where did it go?"}, {"timestamp": [402.28, 403.28], "text": " Okay."}, {"timestamp": [403.28, 404.28], "text": " All right."}, {"timestamp": [404.28, 406.84], "text": " So this is not, this is requiring stuff."}, {"timestamp": [406.84, 414.44], "text": " So here's one thing where I am really skeptical because like all of this gatekeeping and paywalls"}, {"timestamp": [414.44, 419.84], "text": " is actually really going to slow down research, especially in the age of AI."}, {"timestamp": [419.84, 426.48], "text": " That being said, it's like, okay, there is some value added by these things."}, {"timestamp": [426.48, 438.8], "text": " Okay, let's focus on open access sources of regenerative medicine research."}, {"timestamp": [438.8, 447.7], "text": " Can you find any more aside from archive."}, {"timestamp": [447.7, 448.7], "text": " And nature."}, {"timestamp": [448.7, 452.5], "text": " Because, like, okay, the fact that, like, this one is here, so medical express, but"}, {"timestamp": [452.5, 454.9], "text": " this is, like, a no-name site."}, {"timestamp": [454.9, 462.06], "text": " So it's, like, I don't even know if, like, this is, yeah, this is just someone restating"}, {"timestamp": [462.06, 463.06], "text": " something else."}, {"timestamp": [463.06, 465.2], "text": " And it's just pointing at nature anyways."}, {"timestamp": [465.2, 466.04], "text": " Okay."}, {"timestamp": [466.04, 467.78], "text": " So it looks like nature open access"}, {"timestamp": [467.78, 469.98], "text": " is gonna be our best source so far."}, {"timestamp": [469.98, 471.72], "text": " We can also have archive."}, {"timestamp": [473.18, 474.98], "text": " So let's see."}, {"timestamp": [474.98, 478.1], "text": " If we go to archive and we look for,"}, {"timestamp": [478.1, 479.8], "text": " like, let's see, where is it?"}, {"timestamp": [481.16, 484.02], "text": " Well, oh, it looks like we're looking for bio archive."}, {"timestamp": [484.02, 484.86], "text": " Okay."}, {"timestamp": [485.0, 486.0], "text": " Well, oh, looks like we're looking for bioarchive. Okay."}, {"timestamp": [486.0, 490.76], "text": " Oh, man, this website is like from the 90s."}, {"timestamp": [490.76, 494.24], "text": " Okay, this is going to be a pain."}, {"timestamp": [494.24, 495.24], "text": " Ouch."}, {"timestamp": [495.24, 513.56], "text": " All right, neuroscience, molecular, so molecular biology, immunology, genomics, genetics, let's see, cell biology, cancer biology, biochemistry, synthetic biology,"}, {"timestamp": [513.56, 514.56], "text": " that's fun."}, {"timestamp": [514.56, 520.76], "text": " Okay, structural bias, so a lot of this stuff is not going to be like directly relevant,"}, {"timestamp": [520.76, 521.76], "text": " which is interesting."}, {"timestamp": [521.76, 522.76], "text": " Okay, sure."}, {"timestamp": [522.76, 525.46], "text": " Open journal of Regenerative Medicine. Hey,"}, {"timestamp": [525.46, 532.42], "text": " that looks... Stem Cell Research and Regenerative Medicine and NPJ. There we"}, {"timestamp": [532.42, 540.28], "text": " go. Open Journal of Regenerative Medicine. So let's bookmark that. Favorites bar."}, {"timestamp": [540.28, 545.0], "text": " Open Access Journal Stem Cell Research research and regenerative medicine."}, {"timestamp": [545.0, 548.0], "text": " I'm not seeing any papers."}, {"timestamp": [551.0, 553.0], "text": " Let's see."}, {"timestamp": [555.0, 559.0], "text": " It looks like they only have like a couple of issues per year."}, {"timestamp": [559.0, 563.0], "text": " This looks low quality."}, {"timestamp": [564.0, 568.76], "text": " Okay. All right. Well, we've got a couple, right?"}, {"timestamp": [568.76, 572.34], "text": " So let me bookmark bioRxiv as well."}, {"timestamp": [576.08, 578.92], "text": " Okay, so if we come here, got bioRxiv,"}, {"timestamp": [578.92, 581.74], "text": " we got Open Journal of Regenerative Medicine,"}, {"timestamp": [581.74, 583.28], "text": " and then we've got Nature."}, {"timestamp": [583.28, 587.64], "text": " So we've got three sources of data."}, {"timestamp": [587.64, 589.64], "text": " So what I'll do now is,"}, {"timestamp": [589.64, 592.08], "text": " I'm not gonna make you watch me download everything,"}, {"timestamp": [592.08, 595.84], "text": " but now you see how I've gone about finding this information."}, {"timestamp": [596.72, 601.72], "text": " And so then I'll download just a whole mess of this stuff,"}, {"timestamp": [601.96, 605.68], "text": " and then save it, and we'll start to slice and dice it."}, {"timestamp": [605.68, 608.44], "text": " All right, pause it, download stuff, we'll be right back."}, {"timestamp": [609.16, 609.66], "text": " Okay."}, {"timestamp": [609.8, 610.5], "text": " And we're back."}, {"timestamp": [610.54, 617.44], "text": " So I downloaded about, let's see, 81 papers."}, {"timestamp": [617.96, 623.22], "text": " So this is obviously 81 research papers on a variety of topics."}, {"timestamp": [623.22, 625.64], "text": " This is not a really coherent search strategy."}, {"timestamp": [625.64, 628.04], "text": " This is just a proof of concept because this is"}, {"timestamp": [629.5, 630.4], "text": " to demonstrate like,"}, {"timestamp": [630.4, 634.24], "text": " cause this is an intractable amount of material."}, {"timestamp": [634.24, 637.54], "text": " Sure, a professional scientist will probably"}, {"timestamp": [637.54, 640.8], "text": " easily skim through more than this number"}, {"timestamp": [640.8, 643.18], "text": " while performing a literature review."}, {"timestamp": [643.18, 648.24], "text": " But imagine that you've got 2000,000 or 10,000 of these"}, {"timestamp": [648.24, 650.92], "text": " that you need to sift through and get information from."}, {"timestamp": [650.92, 653.76], "text": " So imagine that in your citations page,"}, {"timestamp": [653.76, 655.86], "text": " instead of having 30 to 100,"}, {"timestamp": [655.86, 658.56], "text": " what if you have like 30,000 citations?"}, {"timestamp": [658.56, 661.36], "text": " And we can do that automatically with AI"}, {"timestamp": [661.36, 664.28], "text": " and we can find them quickly through chatbots."}, {"timestamp": [664.28, 668.08], "text": " So that's step one was downloading stuff,"}, {"timestamp": [668.08, 669.36], "text": " which I just showed you."}, {"timestamp": [669.36, 671.94], "text": " And then step two is let's convert it."}, {"timestamp": [671.94, 676.88], "text": " So I have this handy dandy public archive right here"}, {"timestamp": [676.88, 678.08], "text": " called document scraping,"}, {"timestamp": [678.08, 682.52], "text": " and I've got a convert PDF file here."}, {"timestamp": [682.52, 687.48], "text": " So what we're gonna do next is we're going to, and I'll show you, it's really simple,"}, {"timestamp": [687.48, 689.98], "text": " convert PDF to text."}, {"timestamp": [689.98, 697.84], "text": " And what it'll do is it'll grab everything in that folder that's a PDF and then dump"}, {"timestamp": [697.84, 702.0], "text": " a text version out to the next folder."}, {"timestamp": [702.0, 704.36], "text": " And then it breaks it up by page."}, {"timestamp": [704.36, 705.96], "text": " So I'll just say new page, right?"}, {"timestamp": [705.96, 707.32], "text": " It's not a big deal."}, {"timestamp": [707.32, 711.74], "text": " And this forms a nice, easy way of seeing it."}, {"timestamp": [711.74, 713.36], "text": " So I'll show you what it does."}, {"timestamp": [713.36, 717.8], "text": " So CD to chat GPT regenerative medicine,"}, {"timestamp": [717.8, 721.7], "text": " and then we'll do Python step 01 convert."}, {"timestamp": [721.7, 723.2], "text": " And so what it's doing is it's going through"}, {"timestamp": [723.2, 725.52], "text": " and converting them one by one,"}, {"timestamp": [725.52, 727.56], "text": " and it's dropping them here."}, {"timestamp": [727.56, 731.5], "text": " So you can see here is the information."}, {"timestamp": [734.0, 737.2], "text": " And obviously this is not going to grab the,"}, {"timestamp": [738.84, 739.68], "text": " what should we call it?"}, {"timestamp": [739.68, 742.0], "text": " It's not gonna grab the graphics and stuff."}, {"timestamp": [742.0, 742.84], "text": " And that's fine,"}, {"timestamp": [742.84, 744.48], "text": " because we don't have multimodal models yet."}, {"timestamp": [744.48, 751.44], "text": " Apparently GPT-4 is going to be multimodal but at least this should give us all the information"}, {"timestamp": [751.44, 757.84], "text": " that we need in terms of text information. So this is running, it'll take a minute."}, {"timestamp": [759.44, 763.44], "text": " It's obviously very much taking its time so I'll go ahead and pause it again and show you the end"}, {"timestamp": [763.44, 766.48], "text": " result. Also let's see we're only at 12 minutes,"}, {"timestamp": [766.48, 769.18], "text": " so we'll see how much further we can get."}, {"timestamp": [769.18, 770.84], "text": " Yeah, we'll be right back."}, {"timestamp": [772.04, 776.56], "text": " All right, so we've got 81 text files."}, {"timestamp": [776.56, 780.36], "text": " Now, I always check my data, and so here's a problem,"}, {"timestamp": [780.36, 785.0], "text": " is some of the lines have no spaces."}, {"timestamp": [785.36, 786.76], "text": " Some of them do, some of them don't."}, {"timestamp": [786.76, 788.2], "text": " It's really weird."}, {"timestamp": [788.2, 792.56], "text": " I don't know if this is a problem with the PDF plumber"}, {"timestamp": [792.56, 795.0], "text": " or with the underlying PDF itself."}, {"timestamp": [796.36, 798.36], "text": " So I don't know what's going on."}, {"timestamp": [798.36, 803.04], "text": " But from there and here, like, okay, you see this one,"}, {"timestamp": [803.04, 804.88], "text": " this one looks like it's formatted fine."}, {"timestamp": [804.88, 805.24], "text": " I'm not gonna delete it because it's like, okay, you see this one, this one looks like it's formatted fine."}, {"timestamp": [805.24, 806.9], "text": " I'm not gonna delete it because it's like,"}, {"timestamp": [806.9, 808.8], "text": " okay, we still want this information."}, {"timestamp": [808.8, 813.06], "text": " And also, even if it is,"}, {"timestamp": [813.06, 815.92], "text": " even if the formatting is botched like that,"}, {"timestamp": [815.92, 818.0], "text": " as it is in some cases,"}, {"timestamp": [818.0, 820.72], "text": " GPT-3 can still often read these."}, {"timestamp": [821.64, 830.0], "text": " So let me just grab this and show you what I mean. So let's grab this and go to"}, {"timestamp": [830.0, 846.88], "text": " and then we say, let's see, complete, fix the formatting issue. Fixed."}, {"timestamp": [851.2, 853.84], "text": " And so then if we, let's see, do that up to 500, zoom out a little."}, {"timestamp": [857.04, 865.3], "text": " So you can see it can still read it and add the spaces back in, which means that it can comprehend the word boundary problems. So yeah, GPT-3 can understand it."}, {"timestamp": [865.3, 868.22], "text": " It's honestly probably been trained on a lot of data"}, {"timestamp": [868.22, 870.94], "text": " that is malformed like this."}, {"timestamp": [870.94, 873.02], "text": " So it's fine."}, {"timestamp": [873.02, 874.7], "text": " Yeah, that's good."}, {"timestamp": [874.7, 875.98], "text": " So now let's come over here."}, {"timestamp": [875.98, 878.06], "text": " And what we're going to do is we need"}, {"timestamp": [878.06, 881.74], "text": " to convert these text files into something that's usable."}, {"timestamp": [881.74, 887.04], "text": " So one of the advantages of having new page right here is that we can open it and"}, {"timestamp": [887.04, 893.84], "text": " then split it into individual pages again that are text. So what we're going to do is we're going"}, {"timestamp": [893.84, 931.36], "text": " to have a folder called papers underscore JSON. And now I'm going to ask chat-gpt, write a Python script that opens all text files from the puts them into a list of strings with the demarcator new page."}, {"timestamp": [931.36, 977.6], "text": " Then take each page and get an embedding by passing the string to a function that I call, let's see, gpt3 embed into a JSON file, one JSON file for each original"}, {"timestamp": [977.6, 1011.68], "text": " text file. The JSON should have elements such as original text, and finally the embedding."}, {"timestamp": [1011.68, 1014.32], "text": " Okay, so that should be enough."}, {"timestamp": [1014.32, 1015.92], "text": " Sure, here's a Python script."}, {"timestamp": [1015.92, 1017.68], "text": " Wow, it's fast."}, {"timestamp": [1017.68, 1020.6], "text": " I guess nobody else is using it right now."}, {"timestamp": [1020.6, 1026.12], "text": " Okay, pages, embeddings, GPTT three embed page for page and page."}, {"timestamp": [1026.12, 1026.72], "text": " Oh wow."}, {"timestamp": [1026.72, 1027.2], "text": " Okay."}, {"timestamp": [1027.32, 1027.96], "text": " That's fun."}, {"timestamp": [1029.24, 1030.2], "text": " Um, yep."}, {"timestamp": [1030.24, 1037.28], "text": " Original file name, file name pages equals I plus one text page embedding to list"}, {"timestamp": [1037.28, 1040.2], "text": " for I and page embedding enumerate zip pages."}, {"timestamp": [1040.84, 1041.34], "text": " Dang."}, {"timestamp": [1041.96, 1043.04], "text": " I think this is it."}, {"timestamp": [1043.56, 1044.2], "text": " Um,"}, {"timestamp": [1043.28, 1049.42], "text": " is dang, I think this is it."}, {"timestamp": [1051.44, 1052.88], "text": " This function is wrong, but that's fine."}, {"timestamp": [1056.68, 1057.52], "text": " Yep, okay, so let's copy this and come out here."}, {"timestamp": [1062.2, 1063.82], "text": " All right, so let's get rid of those guys."}, {"timestamp": [1070.7, 1100.76], "text": " All right, so for file name in OSLister, dir path, and dir path is here. So we actually want to make this a function and then call it from if name equals main."}, {"timestamp": [1100.76, 1104.62], "text": " So process text, it's defining a function within."}, {"timestamp": [1104.62, 1106.0], "text": " That doesn't make any sense."}, {"timestamp": [1106.0, 1110.0], "text": " That is what, it's fine."}, {"timestamp": [1113.0, 1117.0], "text": " Yep. Let's see. No, stop, stop, stop."}, {"timestamp": [1117.0, 1131.24], "text": " Let's remove the gpt3 embedding function from being nested inside another function."}, {"timestamp": [1131.24, 1133.88], "text": " That is not PEP 8 approved."}, {"timestamp": [1133.88, 1139.08], "text": " At least I don't think so."}, {"timestamp": [1139.08, 1145.16], "text": " Sure, here's an updated, yep."}, {"timestamp": [1146.96, 1147.46], "text": " And so basically I'm just going to ignore that,"}, {"timestamp": [1150.9, 1151.5], "text": " because I wrote that function elsewise."}, {"timestamp": [1155.24, 1156.24], "text": " OK, so for this and that, et cetera, et cetera."}, {"timestamp": [1156.96, 1160.44], "text": " There we go."}, {"timestamp": [1161.4, 1163.74], "text": " And then."}, {"timestamp": [1165.0, 1165.98], "text": " Let's see, oh, one problem."}, {"timestamp": [1170.98, 1171.72], "text": " One more problem, the output directory needs to be"}, {"timestamp": [1175.78, 1179.4], "text": " specified as papers underscore JSON."}, {"timestamp": [1184.4, 1184.78], "text": " And see, this is why I don't like using Copilot,"}, {"timestamp": [1187.76, 1197.04], "text": " because you don't have a dialogue, right? It's just guessing what you want and then what I can do is I can just look at this and say, okay, why is it not allowing me to"}, {"timestamp": [1198.48, 1200.48], "text": " pass"}, {"timestamp": [1212.52, 1217.72], "text": " Papers, JSON, outpath. Why is the outpath hardcoded and not parameterized?"}, {"timestamp": [1217.72, 1236.72], "text": " Does that make any sense to you. I'm way too passive aggressive with this thing, I apologize. There was an"}, {"timestamp": [1236.72, 1246.44], "text": " error generating a response. I'm... No, do not."}, {"timestamp": [1246.44, 1251.24], "text": " So here's the thing, when you do regenerate and it just continues, no, stop, stop, stop."}, {"timestamp": [1251.24, 1253.18], "text": " Because then it's broken up into two."}, {"timestamp": [1253.18, 1258.84], "text": " So what I'm going to do instead is I'm going to come here and just, one, remove the saltiness,"}, {"timestamp": [1258.84, 1262.2], "text": " please fix, and let's start over."}, {"timestamp": [1262.2, 1273.4], "text": " Good catch. So OpenAI, maybe you can have it as a setting, but honestly, if there's an error, I would"}, {"timestamp": [1273.4, 1277.8], "text": " rather it just like save what it did and then pipe that in."}, {"timestamp": [1277.8, 1278.8], "text": " I don't know."}, {"timestamp": [1278.8, 1282.56], "text": " Like, yeah, there we go."}, {"timestamp": [1282.56, 1283.56], "text": " Much better."}, {"timestamp": [1283.56, 1287.08], "text": " Okay. There we go. Much better. Okay, so let's copy this and come back over here."}, {"timestamp": [1290.9, 1292.5], "text": " We don't need that function."}, {"timestamp": [1292.5, 1294.98], "text": " So basically what I did here is I copied a few functions"}, {"timestamp": [1294.98, 1296.82], "text": " from another script."}, {"timestamp": [1296.82, 1299.36], "text": " So here's my embedding one."}, {"timestamp": [1299.36, 1302.14], "text": " Oh, because we're using data from the internet,"}, {"timestamp": [1302.14, 1303.9], "text": " I always do this where I force it,"}, {"timestamp": [1303.9, 1306.78], "text": " I encode it to ASCII and then decode,"}, {"timestamp": [1306.78, 1311.2], "text": " and that fixes Unicode errors because GPT-3 often"}, {"timestamp": [1311.2, 1314.1], "text": " does not do well with some forms of Unicode."}, {"timestamp": [1314.1, 1315.5], "text": " I still haven't figured it out."}, {"timestamp": [1315.5, 1317.62], "text": " Maybe it's not even an issue anymore,"}, {"timestamp": [1317.62, 1319.34], "text": " but if it is, it's still out there."}, {"timestamp": [1319.34, 1323.64], "text": " Okay, so if it doesn't exist, make it great"}, {"timestamp": [1323.64, 1327.92], "text": " for file name, NOSLister. If it ends with .text, that's great."}, {"timestamp": [1327.92, 1330.96], "text": " Split it into pages, embed it,"}, {"timestamp": [1330.96, 1332.44], "text": " and then here's the output,"}, {"timestamp": [1332.44, 1335.12], "text": " so we get all the pages, excellent."}, {"timestamp": [1336.88, 1338.84], "text": " I'm just gonna trust that this works"}, {"timestamp": [1340.44, 1342.8], "text": " because I respond with this."}, {"timestamp": [1342.8, 1345.92], "text": " So the vector, I don't think it's a numpy array, so that's"}, {"timestamp": [1345.92, 1347.32], "text": " fine."}, {"timestamp": [1347.32, 1352.76], "text": " Embedding to list, I think it's already, yeah."}, {"timestamp": [1352.76, 1359.32], "text": " So embeddings equals, so this is already a list I believe, so I think this will break,"}, {"timestamp": [1359.32, 1362.16], "text": " that's fine."}, {"timestamp": [1362.16, 1367.64], "text": " And then we zip the pages and embeddings."}, {"timestamp": [1368.84, 1369.76], "text": " Oh, interesting, okay."}, {"timestamp": [1371.92, 1374.36], "text": " We'll see if this is formatted correctly. And then let's see, save it."}, {"timestamp": [1375.56, 1380.56], "text": " So it just replaces the file name, that's good, with OS."}, {"timestamp": [1380.86, 1383.4], "text": " But I actually already have a save JSON thing,"}, {"timestamp": [1383.4, 1386.0], "text": " so we just have the file path and the payload."}, {"timestamp": [1386.0, 1391.6], "text": " And in this one I specify ensure ASCII false, sort keys true, and then indent."}, {"timestamp": [1391.6, 1394.42], "text": " So this formats it nice and pretty."}, {"timestamp": [1394.42, 1402.18], "text": " So what we'll do instead is we'll comment this out and then we'll just do save JSON."}, {"timestamp": [1402.18, 1403.18], "text": " And what is the order?"}, {"timestamp": [1403.18, 1406.6], "text": " I have it file path and payload."}, {"timestamp": [1406.6, 1418.5], "text": " So then the file path is this guy."}, {"timestamp": [1418.5, 1424.0], "text": " And then the payload is output dict."}, {"timestamp": [1424.0, 1433.0], "text": " Yes. All right, so that should be good. Now if name equals main, et cetera,"}, {"timestamp": [1433.0, 1445.56], "text": " et cetera. So what I want'll do is..."}, {"timestamp": [1445.56, 1448.28], "text": " Hmm."}, {"timestamp": [1448.28, 1452.2], "text": " I guess this is just going to be messy no matter what we do."}, {"timestamp": [1452.2, 1459.48], "text": " So let's add some output here."}, {"timestamp": [1459.48, 1464.78], "text": " So we'll do print content to embed."}, {"timestamp": [1464.78, 1471.2], "text": " And then we'll do content and then we'll print vector."}, {"timestamp": [1475.28, 1479.2], "text": " Vector. All right, so that way we'll be able to see it embedding as it goes"}, {"timestamp": [1480.64, 1485.1], "text": " and that should be fine and then yeah that'll be fine. All then, yeah, that'll be fine."}, {"timestamp": [1485.1, 1488.94], "text": " All right, so let's save this and come over here."}, {"timestamp": [1488.94, 1491.24], "text": " So Python step 02."}, {"timestamp": [1494.6, 1499.6], "text": " Hey, look, we had an issue, Unicode decode error."}, {"timestamp": [1500.14, 1503.94], "text": " Okay, in text f read, oh, yep."}, {"timestamp": [1505.34, 1506.58], "text": " So here we go."}, {"timestamp": [1511.1, 1512.46], "text": " All right, so I'm gonna ask,"}, {"timestamp": [1512.46, 1515.02], "text": " I know what the problem is, but I'm gonna ask chat GPT."}, {"timestamp": [1515.02, 1517.42], "text": " Got an error, can you fix it?"}, {"timestamp": [1528.12, 1532.44], "text": " There we go."}, {"timestamp": [1533.88, 1535.92], "text": " Do, do, do, do, do, do. So basically it's pretty simple."}, {"timestamp": [1535.92, 1539.24], "text": " All you have to do is specify,"}, {"timestamp": [1541.4, 1543.32], "text": " make sure those are spaces, okay, yes."}, {"timestamp": [1543.32, 1546.12], "text": " So you specify the encoding is UTF-8."}, {"timestamp": [1547.12, 1548.56], "text": " And so now it knows better."}, {"timestamp": [1548.56, 1550.26], "text": " All right, so let's try that again."}, {"timestamp": [1551.2, 1554.98], "text": " Yeah, it should not go that fast."}, {"timestamp": [1562.36, 1564.28], "text": " Okay, so the fact that it's going that fast"}, {"timestamp": [1564.28, 1566.72], "text": " and it's not outputting the vector means that it's"}, {"timestamp": [1566.72, 1574.4], "text": " barfing somewhere here. So let's comment out this because it's probably returning none."}, {"timestamp": [1576.64, 1586.0], "text": " Which I had that because there was another project that I was working on where some of the things failed. Yeah."}, {"timestamp": [1587.26, 1588.92], "text": " Always use notepad."}, {"timestamp": [1590.6, 1592.88], "text": " Let's see, embedding null. Yeah, so you see it was not embedding."}, {"timestamp": [1592.88, 1596.2], "text": " So let's delete these."}, {"timestamp": [1598.26, 1599.46], "text": " It worked mostly."}, {"timestamp": [1602.24, 1604.36], "text": " Let me close these because they're superfluous."}, {"timestamp": [1604.36, 1606.76], "text": " And let's see why it's blowing up."}, {"timestamp": [1610.0, 1611.76], "text": " Do, do, do, do, do, do."}, {"timestamp": [1611.76, 1615.64], "text": " Oh, you need an API key."}, {"timestamp": [1615.64, 1617.56], "text": " What do you mean I need an API key?"}, {"timestamp": [1617.56, 1618.4], "text": " Here, let me fix this."}, {"timestamp": [1618.4, 1619.22], "text": " I'll be right back."}, {"timestamp": [1620.6, 1621.88], "text": " Okay, I think I fixed it."}, {"timestamp": [1621.88, 1624.32], "text": " So I added this here and then I copied my"}, {"timestamp": [1624.32, 1627.54], "text": " get ignore and API key right here."}, {"timestamp": [1627.54, 1628.68], "text": " So we should be good."}, {"timestamp": [1628.68, 1630.26], "text": " Let's give it one last try."}, {"timestamp": [1630.26, 1631.6], "text": " And also I did a time check."}, {"timestamp": [1631.6, 1633.6], "text": " We're at almost 30 minutes."}, {"timestamp": [1633.6, 1636.28], "text": " So this will probably be it for today."}, {"timestamp": [1636.28, 1639.32], "text": " But we're off to a pretty good start if I do say so myself."}, {"timestamp": [1639.32, 1640.54], "text": " All right."}, {"timestamp": [1640.54, 1643.86], "text": " So let's do, let's try and do some embeddings."}, {"timestamp": [1643.86, 1644.86], "text": " That's still seen."}, {"timestamp": [1644.86, 1646.4], "text": " Oh, no, it's just that fast. Look at that. Look at that. All right, so let's try and do some embeddings. That still seems, oh, no, it's just that fast."}, {"timestamp": [1646.4, 1647.8], "text": " Look at that, look at that."}, {"timestamp": [1649.6, 1652.12], "text": " All right, so let's take a look at the output."}, {"timestamp": [1658.36, 1659.4], "text": " Whoops."}, {"timestamp": [1659.4, 1660.94], "text": " All right, so the embedding."}, {"timestamp": [1663.04, 1667.3], "text": " So for each page, we get embedding, page number, text we get embedding page number text,"}, {"timestamp": [1667.3, 1669.04], "text": " embedding page number text."}, {"timestamp": [1669.04, 1672.88], "text": " And then we keep the original file name as well."}, {"timestamp": [1672.88, 1675.4], "text": " And so by using this, we can trace it back"}, {"timestamp": [1675.4, 1677.8], "text": " to the original PDF as well, because we're"}, {"timestamp": [1677.8, 1680.96], "text": " going to basically need to be able to cite our sources."}, {"timestamp": [1680.96, 1682.08], "text": " All right, cool."}, {"timestamp": [1682.08, 1685.6], "text": " So this is tearing through this. Let's see, make sure it hasn't blown up. All right, it. So this is tearing through this."}, {"timestamp": [1685.6, 1687.0], "text": " Let's see, make sure it hasn't blown up."}, {"timestamp": [1687.0, 1688.88], "text": " All right, it's still going."}, {"timestamp": [1688.88, 1692.96], "text": " And so you see, basically what we're doing is we're getting one embedding for every single"}, {"timestamp": [1692.96, 1693.96], "text": " page."}, {"timestamp": [1693.96, 1700.04], "text": " Now, there's a lot more that we're going to need to do in order to make this usable, because"}, {"timestamp": [1700.04, 1704.88], "text": " some of these are like 80 pages long, which is way too much for it to read."}, {"timestamp": [1704.88, 1706.16], "text": " So then it's like, okay, well,"}, {"timestamp": [1706.16, 1708.76], "text": " if you search it based on the embedding,"}, {"timestamp": [1708.76, 1711.34], "text": " that'll get you close, but then what else do you do?"}, {"timestamp": [1711.34, 1712.84], "text": " There's a lot of problems to solve,"}, {"timestamp": [1712.84, 1714.72], "text": " but the fact of the matter is,"}, {"timestamp": [1714.72, 1718.36], "text": " this is going to give us basically a super advanced"}, {"timestamp": [1718.36, 1721.62], "text": " chat-based scientific search engine."}, {"timestamp": [1721.62, 1727.76], "text": " Now, because I'm also working on cognitive architecture, we can have it do a lot of thinking for you in the background."}, {"timestamp": [1727.8, 1729.8], "text": " That's going to be the real game changer."}, {"timestamp": [1729.8, 1734.74], "text": " Not just a chatbot that allows you to search, but a chatbot that you can ask it scientific questions,"}, {"timestamp": [1734.74, 1737.24], "text": " and it will go think about the problem for you."}, {"timestamp": [1738.56, 1745.48], "text": " So I'm going to let this finish. I'm going to go ahead and stop the recording, and we'll come back tomorrow for part two."}, {"timestamp": [1746.08, 1748.08], "text": " Thanks for watching."}, {"timestamp": [1743.33, 1749.69], "text": " I'm going to go ahead and stop the recording and we'll come back tomorrow for part two."}, {"timestamp": [1749.69, 1750.33], "text": " Thanks for watching."}]}