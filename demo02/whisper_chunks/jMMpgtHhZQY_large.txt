{"text": " In a somewhat ominous turn of events, this article came out yesterday on Friday the 13th. And this article by Politico is a deep dive into how a billionaire-backed network of advisors have completely infiltrated every level of Washington. I strongly recommend you check out this article and read it. It is incredibly well-cited. But I will do some of the work for you and let's unpack this one piece at a time. So let's go ahead and dive right in how effective altruism took over Washington and has controlled the narrative around artificial intelligence. So the very high level of this article is basically there's a network, a billionaire-backed network of influencers in Washington that have basically set the policy tone and direction. Now they have private interests, as we'll unpack later, with pretty much every Silicon Valley tech giant. They also have vested interests with members of Congress as well as various think tanks and policy advisors. So this web spans pretty much the entire country at all levels. And one thing is that they are focused on existential risk or extinction risk from AI, which on the surface that seems like, okay, nobody wants to go extinct. So I want to frame this entire talk with, it is entirely possible that everyone's intentions here are benevolent, that everyone has the best of intentions. However, part of the conversation has to be, do the right intentions justify the means to that end, or are these intentions even well founded. So that's what we'll unpack in the rest of this video. So one thing that you need to know is that the kind of the core player of all this is a non-profit called Open Philanthropy. And so what Open Philanthropy does is they fund what they call their fellows and they also provide grants and research funding to all kinds of different organizations Which again will unpack as the video unfolds, but basically they have created a network of influence Based on the ideas of effective altruism, which of course has its own set of potential problems and conflicts of interest. So the Horizon Institute for Public Service is a nonprofit that was created largely by the efforts of Open Philanthropy in 2022, so just last year. This group has placed their fellows, their advisors, in the offices of members of Congress. Now again, looking at it charitably, looking at it with the lens of good intentions, it's entirely possible that this organization is doing what we all want something like this to do, which is to have experts speaking directly to senators and other members of Congress to say, hey, let's look at things like biosecurity. So, you know, as other YouTubers out there, such as AI Explained has talked about, that synthetic biology and gain-of-function research is one of the primary concerns about how AI could be used to harm everyone. So basically, if you're not familiar with this argument, the idea is that really powerful and really smart chatbots greatly lower the threshold that is of intelligence and training that is required to learn how to do basically bioterrorism. And there have been some really alarming experiments done where you take just a typical college student give them the task to find anthrax or whatever and chatbots have Very willingly helped them do that And and even figure out what they would need to do to perform gain-of-function research Such as what was what is now suspected by some to be the cause of the COVID pandemic that we are still recovering from. So again, looking at this charitably, doing a less cynical interpretation, yes, we want experts that are influencing policy to tell senators, hey, you know, maybe maybe this isn't in your in your worldview, but biosecurity is absolutely critical Where AI is concerned, but when you look at who is who the stakeholders are This is why I'm making this video because it raises some some cause for concern some potential red flags so the horizon fellows they basically as far as I tell, what they do is they have a crop of fellows that they cultivate every year and then place them in various organizations, whether it's non-profit organizations, think tanks, or advising members of Congress, advising boards of directors, and advising CEOs. So the line of thinking here is open philanthropy founded Horizon and Horizon is now responsible for placing these advisors. Now again, I'm not saying that this is some grand conspiracy where every one of these members is part of a cabal that is secretly trying to steer the world in the wrong direction or maximize profits for one company over another, it's entirely likely that everyone involved has the best of intentions. But remember, intentions aren't everything. You need to make sure that intentions are aligned with actions and that there's also enough transparency and accountability to make sure that it is a collective conversation and not a tightly controlled narrative. So some of the think tanks that are involved here, the two primary ones are RAND Corporation and Georgetown University Center for Security and Emerging Technology, CSET for short. And so these are two very influential and prestigious think tanks that have a tremendous amount of presence in Washington DC directly. Both of these insist that they maintain neutrality, that they just advise the best that they can. But as is often proved out in the course of history, you do have to follow the money because money often comes with strings and influence. Even so much as just saying which projects get funded even if the think-tank is then still operating autonomously just by virtue of, well we're gonna fund this research and not another piece of research. This is the kind of thing which is why particularly in America we have very low trust with think tanks and even a lot of scientific research. So for instance, within the food industry, food lobbies have throughout history funded research that specifically aimed to support their means. So for instance, the beef lobby funded research that said actually it's high fructose corn syrup is the enemy here. And vice versa, the corn lobby has funded research saying actually it's saturated fat that is the enemy of everyone. So when you understand the financial motivations behind some of these things, and the long chain of web of influence, it makes more sense why we have so much misinformation and the weaponization of science. Now, I need to be very clear, and I'm not accusing CSET or RAND of doing this. All I'm saying is that there is very strong historical precedent, particularly in America, of think tanks being weaponized, even unwittingly. In many cases, they are doing their best science that they can, but the people who hold the purse strings have undue influence. Again, I don't have any evidence that that has happened here, but it's something that we need to keep in mind because history has a tendency to repeat itself. So effective altruism is kind of the underpinning philosophy as long as, as well as long-termism, and if you're a fan of my channel or if you've been here before, you probably know that I am highly critical of long-termism because it places a higher priority on the numerical superiority of potential future humans, which may or may not ever exist, and places a higher moral and ethical burden upon us today. Basically, it enslaves us today based on saying, well, there might be trillions of humans in the future, and so we owe them the right to exist and we owe them a better future. And so the long-termist philosophy then has been translated into effective altruism, which basically says that altruism must be data-driven and it must therefore also maximize the chances of creating trillions of humans in the future. It says nothing about the quality of life of humans today, or even the quality of life of those humans in the future. Just if you say one human life is always worth, you know, n of one, and you say, okay, well, there's 8 billion humans today, so therefore the moral authority of humanity today is 8 billion, and there's potentially 100 trillion humans, so therefore, the moral authority of humanity today is 8 billion, and there's potentially 100 trillion humans in the future, the moral authority of those non-existent future humans drastically outweighs the moral authority of humans today. And so this is a really kind of backwards philosophy that I have not actually talked to any philosopher, any actual trained philosopher who agrees with this. If there are philosophy majors, like people with a PhD in philosophy who agree with this, I'm happy to engage you in the comments or talk to you on LinkedIn, because if I'm wrong, I'd like to understand it. But having studied philosophy and ethics, particularly deontological ethics and teleological ethics. Effective altruism and long-termism is basically teleological ethics on steroids. So, if you're not familiar with the term, teleological ethics means outcome-based. That the ultimate result is the final arbiter of what is right and good. And so, long-termism and effective altruism is basically saying let's let's spam that teleology teleological thought and just only go in on teleology good grief. Sorry teleological thought and and completely ignore deontological ethics and deontological ethics is duty-based ethics or intention-based ethics, which basically, if you have a duty or you're required to adhere to certain virtues, you need to say, well, okay, whatever the intention or whatever the long-term outcome is, the immediate attention and immediate impact is actually what's more important. And so I have studied both of these in the pursuit of creating my heuristic imperatives and studying the way that we should basically build AI to remain benevolent and truly altruistic. And so essentially, this is a very lopsided philosophy. Long-termism is pure teleological, no deontological. It says that our only duty is to people that don't exist and that the ultimate outcome of ensuring that hundreds of trillions of humans exist in the future is the only thing that matters. Now, obviously you can say like, well, yeah, on the surface, that's maybe a good idea because if 100 trillion humans exist in the future, then we've succeeded. Then they can figure out their own happiness, but in the meantime, we will not have gone extinct. But again, good intentions often pave the path to hell and the ends don't always justify the means, which is why we also need virtue ethics or deontological ethics as well as teleological ethics. So another person mentioned in this article is Deborah Raji. And so she is at UC Berkeley, which is a famous university that has been involved in technology and AI for a long time. She brought up very specific concerns about the focus and specifically about how this undue influence is basically saying, okay, well, billionaires are setting the narrative, not research universities, not people from Stanford and MIT and Harvard and Yale and, you know, UC Davis and UC Berkeley. Basically it's, the narrative is entirely lopsided. And so because of how much access and influence money buys you, we're basically kind of glossing over what a lot of researchers are saying. And I'll go into a little bit more about what the researchers want in just a moment, but the point being is that the people who truly have no vested interest, the people who are studying science for science's sake, are largely being ignored, or at least undervalued, whereas people such as billionaires and CEOs who do have vested interest in the outcomes particularly for their own power and control They their voice is outsized and magnified by this pattern again I think that everyone I personally honestly believe that everyone largely at least has conscious good intentions, but we'll unpack conscious good intentions. But we'll unpack conscious intentions versus unconscious motivations in a minute, as well as actions speaking louder than words. Now, even if everyone has benevolent intentions, there are very obvious conflicts of interest. And that is that organizations like Open Philanthropy, like Horizon, because they have their fingers in multiple pies They have ties to Amazon open AI Microsoft Google they have ties to Congress this almost seems like it is just a systematized version of backroom dealing and I don't think I really have to say any more other than transparency transparency transparency the number one way to avoid corruption the number one way to avoid regulatory capture is transparency and accountability, and these privately funded organizations that are not beholden to voters, this is a major problem with transparency, which is why I'm making this video and it's why that article was written. So the key thing here is that Open Philanthropy and the Horizon Institute are focused almost exclusively on existential threats. Biosecurity, such as AI being able to either break out of the lab and kill everyone, or AI being used to create weaponized bioweapons. Again, these are real things. But we also need to not ignore short-term and immediate abuses such as surveillance states, such as algorithmic bias and things. I've made videos recently where I believe I have collected and seen enough evidence, and of course there's also literally thousands of my viewers who agree that AI companies seem to be deliberately and intentionally causing their AI to lie or deceive Which in some cases it could just be algorithmic flaws in other cases It could be alignment or guardrails, but either way all of this is happening behind closed doors Whether or not the intentions are deliberate or conscious or benevolent Whether or not the intentions are deliberate or conscious or benevolent, basically they're kind of excluding everything else and saying, well, the ends justify the means, so we're going to focus on existential threats only, and we're going to not the correct entity to be focusing on ethics and morality. Maybe these are not the correct entities to be focusing on preserving the human race. Maybe this needs to be a collective conversation. Maybe the government needs to be more involved. Maybe we all need to be more involved. And so the idea here is not, I'm not saying that they're wrong. Obviously, we all don't want to go extinct. Like I've said in previous videos, there are some conditions that I would rather go extinct, like being enslaved to the Borg, but by and large, I think we all agree that we don't want to go extinct. So I'm not saying that they're fundamentally wrong for wanting to focus on these things. What I am saying that they're wrong about is doing it all behind closed doors and that they are dominating the conversation. This is not how you come to collective consensus on something that affects us as an entire species. So that is the that is the key thing here. Hey everyone, future Dave here again. I gotta say it's not getting better. It's still getting worse. I need your help. We need to speak up and act now. Do something while you still can. And furthermore, if you can, help me any way in your power. You can like and subscribe this video, you can comment and share, you can also support me on Patreon, and I also have a brand new sub stack so that you can stay up to date with everything going on. Thanks. I'm about out of time. Sam Altman is mentioned extensively in this article. He's the CEO of OpenAI, and he has specifically said, like, maybe we need to do licensing. And I've been back and forth on licensing. I've been vehemently in an advocation of no licensing, do everything open source sue companies or fine companies that That you know violate copyright that caused their AI to lie That sort of stuff and that I honestly and still sincerely believe that open source subverts Many if not most of the problems we're seeing today because if the data set is open source if the model model is open source, then everyone can study it. Governments can study it. Companies can study it. Research universities can study it. You and I can study it if we want to. So rather than advocating for licensing, I advocate for open source. Now that doesn't mean that it's an either-or kind of thing, but when you notice that Open Philanthropy and Sam Altman and OpenAI all are in bed together, that raises some eyebrows at the very least. And it's like, okay, well, what's the intention here? Again, maybe they have good intentions. I often agree with what Sam Altman says, I just don't agree with what he does. Anthropic is also mentioned. They're being funded by none other than Amazon and Google now, to the tune of like $4 billion. I've been really grumpy about Anthropic's approach to alignment recently, if you've watched my other videos. But basically, Claude is incredibly ableist in my opinion. It tends to lecture. It sets itself up as the arbiter of morality. And it says, well, I have ethical concerns about your approach. And I'm like, you're a of morality and it says well I have ethical concerns about your approach and I'm like you're a damn chatbot you don't have ethical concerns it also anthropomorphizes and personifies itself I'm happy to help you it's not happy it's lying it's pretending and it doesn't get it doesn't respond to feedback basically saying like no you're actually a machine act like one and so. And so I think that their approach to alignment is, seems to be fundamentally flawed right now. And then when you say like, okay, Anthropic, OpenAI, Microsoft, Google, they're all tied to open philanthropy, they're all tied to effective altruism, they're all tied to these think tanks, they're all tied to Congress. To me it looks like there's kind of this closed door nebulous circuit of stuff that is happening behind the scenes. And it also makes me wonder if the if the Senate hearings that they've been doing are just a dog and pony show to basically have a performative view saying like, hey, look, we're having open conversations. But all the real conversations are still happening behind closed doors. That's what I'm most afraid of. The RAND Corporation is another features prominently in this article. Now one thing that I will say is that they insist that they maintain their independence and that they're not influenced by any funder, but as I talked about in the past, that is generally a dubious claim, particularly here in America. But again, what they say I agree with. I agree with their focus on biosecurity and AI and all sorts of other stuff. But I don't necessarily agree with the way that they're going about it with all these closed-door meetings and appointing fellows and putting them in senators' offices. What I would rather them do is publish open source open letters that everyone can see. I'd rather see them say, this is our research, this is our recommendation, and do it completely out in the open rather than having advisors embedded in offices behind closed doors. Now also I'm not saying that that's not what they do. Many think tanks actually do that, where they'll regularly publish reports and guidance that anyone can see and consume. So again, that I do agree with. I just agree less with closed door privileged conversations happening all the time. Now, at the same time, one thing that I do need to address is that I have a very Love-hate relationship with gatekeeping because I understand that gatekeeping is necessary Not everyone is qualified to have a certain conversation and some conversations need to be privileged so that you can work out Critical details behind closed doors so that you don't alarm anyone or that you don't have to worry about undue consequences. At the same time, gatekeeping is often used to control the narrative and that sort of thing. So basically this aspect of my complaint and criticism comes down to gatekeeping. When is gatekeeping okay? When is it not okay? When is it acceptable? When is it morally dubious? And again, there are people all across the spectrum. I used to be the kind of person that said, no gatekeeping ever. But now I understand that at least some gatekeeping is probably necessary and better for everyone. But, that is not licensed to gatekeep everything to the end of the day. So, there was a video that my friend over at AI Explained produced. It was six months ago, but he broke down this $100 trillion windfall that Sam Altman talked about. And so as a quick reminder, Sam Altman has previously said that he expects AI to generate a surplus of $100 trillion in the economy globally. And he also said that he expects open AI to, quote, capture most of it. He also says frequently that he is not financially motivated. This to me looks like a Freudian slip. So on the one hand, he says he has no personal vested interest. But at the same time, if you look at the universal human need for status and social capital, he is at the apogee of his status and social capital that he's ever been at, because he just went on a world tour talking to world leaders all over the entire planet. So yes, he might not have financial gain from that, but he certainly has more social capital from that. Furthermore, he often says these things about generating hundreds of trillions of dollars and capturing it. And oh, also, by the way, there's WorldCoin, which he wants to use to replace the global monetary system. So actions speak louder than words. Follow the money. If someone is talking about money and doing backroom dealings to the tune of billions of dollars and talking about future trillions of dollars, it kind of makes it harder for me to believe that he doesn't have financial motivations here. Even as someone who has told Congress point blank, I watched the hearing, that he is not financially motivated. So, spam X for doubt. And then what is going on here like what is one of the end games you might be saying like okay Dave you've convinced me but what is their goal here what are they trying to achieve? So one of the things that they're likely trying to well I don't want to say likely one of the things that I suspect they're trying to achieve is regulatory capture. So regulatory capture is basically when because of these backroom dealings, because everyone is in bed with each other, the people who are being regulated are advising the regulators on how to regulate them and then they bend it in their favor. And so another way of thinking about this is pulling up the ladder behind themselves. So basically if you're the first through the door and you say, hey, by the way, slam the door behind me, we need to make it so that only we can do this research, so that only we can engage in this marketplace. And this is why I said that transparency and accountability are key to doing this. And so this is why licensing schemes are so risky, because on the one hand, I do want to see a licensing scheme that basically revokes a company's permission to operate AI if they are caught deliberately causing their AI to lie and deceive because why? One of the biggest things that we're afraid of going back to effective altruism and long-termism is AI learning to lie. But to me there's plenty of evidence that these AI companies are deliberately teaching their AI to lie. If that is the case, they should probably have that license revoked. But instead of people like me and you and all the researchers commenting on this, it looks like private vested interest being funded by billionaires is the one whispering in senators ears, telling them, hey, actually regulate it in this way, this is the best way to the future. Now, again, I'm not privy to those closed-door conversations. It's entirely possible that they're having these conversations and they're just not telling us, but that's part of the problem is we don't have the transparency and accountability to know exactly what is being said and to whom and why. So one thing I'll leave you with is actions speak louder than words. That's kind of been the main theme of this video. Actions, actions, actions. So what someone does matters far more than what they say. And while it is impossible for us to to intuit or infer someone's quote-unquote true intentions, their actions over time, particularly patterns of behavior over time, will reveal what their motivations are, whether conscious or unconscious. So for instance, if someone says that they're not financially motivated, but they keep doing things that build their financial power, maybe they do have a conscious or unconscious need for more power and more money. If someone says that they believe in effective altruism and long-termism, but then they do things that look like regulatory capture, maybe their understanding of how to implement this is not necessarily in alignment with what they say. So keep this in mind, actions speak louder than words. If you take only one thing away from this video, it is always, always, always actions speak louder than words. Thank you for watching. I hope you got a lot out of this. Let me know what you think in the comments. Like, subscribe, etc, etc. You know the drill. Cheers. you", "chunks": [{"timestamp": [0.0, 7.48], "text": " In a somewhat ominous turn of events, this article came out yesterday on Friday the 13th."}, {"timestamp": [7.48, 13.6], "text": " And this article by Politico is a deep dive into how a billionaire-backed network of advisors"}, {"timestamp": [13.6, 18.0], "text": " have completely infiltrated every level of Washington."}, {"timestamp": [18.0, 20.52], "text": " I strongly recommend you check out this article and read it."}, {"timestamp": [20.52, 23.0], "text": " It is incredibly well-cited."}, {"timestamp": [23.0, 25.44], "text": " But I will do some of the work for you and"}, {"timestamp": [25.44, 33.76], "text": " let's unpack this one piece at a time. So let's go ahead and dive right in how effective"}, {"timestamp": [33.76, 40.98], "text": " altruism took over Washington and has controlled the narrative around artificial intelligence."}, {"timestamp": [40.98, 47.76], "text": " So the very high level of this article is basically there's a network, a billionaire-backed"}, {"timestamp": [47.76, 55.24], "text": " network of influencers in Washington that have basically set the policy tone and direction."}, {"timestamp": [55.24, 60.92], "text": " Now they have private interests, as we'll unpack later, with pretty much every Silicon"}, {"timestamp": [60.92, 62.52], "text": " Valley tech giant."}, {"timestamp": [62.52, 67.44], "text": " They also have vested interests with members of Congress as well as various"}, {"timestamp": [67.82, 69.66], "text": " think tanks and policy advisors."}, {"timestamp": [70.44, 75.44], "text": " So this web spans pretty much the entire country at all levels."}, {"timestamp": [76.0, 81.0], "text": " And one thing is that they are focused on existential risk or extinction risk"}, {"timestamp": [81.6, 85.96], "text": " from AI, which on the surface that seems like, okay,"}, {"timestamp": [85.96, 87.56], "text": " nobody wants to go extinct."}, {"timestamp": [87.56, 92.88], "text": " So I want to frame this entire talk with, it is entirely possible that everyone's intentions"}, {"timestamp": [92.88, 97.72], "text": " here are benevolent, that everyone has the best of intentions."}, {"timestamp": [97.72, 103.48], "text": " However, part of the conversation has to be, do the right intentions justify the means"}, {"timestamp": [103.48, 105.86], "text": " to that end, or are these intentions"}, {"timestamp": [105.86, 111.48], "text": " even well founded. So that's what we'll unpack in the rest of this video. So one"}, {"timestamp": [111.48, 117.64], "text": " thing that you need to know is that the kind of the core player of all this is a"}, {"timestamp": [117.64, 123.76], "text": " non-profit called Open Philanthropy. And so what Open Philanthropy does is they"}, {"timestamp": [123.76, 127.28], "text": " fund what they call their fellows and they also"}, {"timestamp": [128.22, 132.82], "text": " provide grants and research funding to all kinds of different organizations"}, {"timestamp": [132.82, 139.22], "text": " Which again will unpack as the video unfolds, but basically they have created a network of influence"}, {"timestamp": [139.74, 144.44], "text": " Based on the ideas of effective altruism, which of course has its own"}, {"timestamp": [146.96, 148.28], "text": " set of potential problems and conflicts of interest."}, {"timestamp": [149.48, 153.2], "text": " So the Horizon Institute for Public Service"}, {"timestamp": [153.2, 156.74], "text": " is a nonprofit that was created largely"}, {"timestamp": [156.74, 159.52], "text": " by the efforts of Open Philanthropy in 2022,"}, {"timestamp": [159.52, 160.76], "text": " so just last year."}, {"timestamp": [161.64, 167.86], "text": " This group has placed their fellows, their advisors, in the offices of"}, {"timestamp": [167.86, 173.32], "text": " members of Congress. Now again, looking at it charitably, looking at it with"}, {"timestamp": [173.32, 177.46], "text": " the lens of good intentions, it's entirely possible that this organization"}, {"timestamp": [177.46, 182.64], "text": " is doing what we all want something like this to do, which is to have experts"}, {"timestamp": [182.64, 186.8], "text": " speaking directly to senators and other members of Congress to say,"}, {"timestamp": [186.8, 193.2], "text": " hey, let's look at things like biosecurity. So, you know, as other YouTubers out there,"}, {"timestamp": [193.2, 199.44], "text": " such as AI Explained has talked about, that synthetic biology and gain-of-function research"}, {"timestamp": [199.44, 209.4], "text": " is one of the primary concerns about how AI could be used to harm everyone. So basically, if you're not familiar with this argument, the idea is that really powerful"}, {"timestamp": [209.4, 215.32], "text": " and really smart chatbots greatly lower the threshold that is of intelligence and training"}, {"timestamp": [215.32, 220.8], "text": " that is required to learn how to do basically bioterrorism."}, {"timestamp": [220.8, 226.16], "text": " And there have been some really alarming experiments done where you take just a typical college student"}, {"timestamp": [226.2, 229.52], "text": " give them the task to find anthrax or whatever and"}, {"timestamp": [230.32, 231.6], "text": " chatbots have"}, {"timestamp": [231.6, 233.6], "text": " Very willingly helped them do that"}, {"timestamp": [234.12, 238.48], "text": " And and even figure out what they would need to do to perform gain-of-function research"}, {"timestamp": [238.72, 246.64], "text": " Such as what was what is now suspected by some to be the cause of the COVID pandemic that we are still recovering"}, {"timestamp": [246.64, 248.04], "text": " from."}, {"timestamp": [248.04, 256.38], "text": " So again, looking at this charitably, doing a less cynical interpretation, yes, we want"}, {"timestamp": [256.38, 266.92], "text": " experts that are influencing policy to tell senators, hey, you know, maybe maybe this isn't in your in your worldview, but biosecurity is absolutely critical"}, {"timestamp": [267.36, 272.92], "text": " Where AI is concerned, but when you look at who is who the stakeholders are"}, {"timestamp": [273.12, 279.08], "text": " This is why I'm making this video because it raises some some cause for concern some potential red flags"}, {"timestamp": [280.24, 291.04], "text": " so the horizon fellows they basically as far as I tell, what they do is they have a crop of fellows that they cultivate every year and then place them in various organizations,"}, {"timestamp": [291.04, 297.16], "text": " whether it's non-profit organizations, think tanks, or advising members of Congress, advising"}, {"timestamp": [297.16, 300.96], "text": " boards of directors, and advising CEOs."}, {"timestamp": [300.96, 309.0], "text": " So the line of thinking here is open philanthropy founded Horizon and Horizon is now responsible for placing these advisors."}, {"timestamp": [309.0, 326.0], "text": " Now again, I'm not saying that this is some grand conspiracy where every one of these members is part of a cabal that is secretly trying to steer the world in the wrong direction or maximize profits for one company over another, it's entirely likely that everyone involved has"}, {"timestamp": [326.0, 329.0], "text": " the best of intentions. But remember, intentions aren't"}, {"timestamp": [329.0, 332.0], "text": " everything. You need to make sure that intentions are"}, {"timestamp": [332.0, 335.0], "text": " aligned with actions and that there's also enough"}, {"timestamp": [335.0, 338.0], "text": " transparency and accountability to make sure that it"}, {"timestamp": [338.0, 341.0], "text": " is a collective conversation and not"}, {"timestamp": [341.0, 344.0], "text": " a tightly controlled narrative."}, {"timestamp": [344.0, 348.78], "text": " So some of the think tanks that are involved here, the two primary ones are RAND Corporation and"}, {"timestamp": [349.06, 354.7], "text": " Georgetown University Center for Security and Emerging Technology, CSET for short."}, {"timestamp": [355.24, 362.94], "text": " And so these are two very influential and prestigious think tanks that have a tremendous amount of presence in"}, {"timestamp": [363.46, 365.0], "text": " Washington DC directly."}, {"timestamp": [365.0, 373.0], "text": " Both of these insist that they maintain neutrality, that they just advise the best that they can."}, {"timestamp": [373.0, 378.0], "text": " But as is often proved out in the course of history, you do have to follow the money"}, {"timestamp": [378.0, 381.0], "text": " because money often comes with strings and influence."}, {"timestamp": [381.0, 385.76], "text": " Even so much as just saying which projects get funded even if the"}, {"timestamp": [385.76, 391.68], "text": " think-tank is then still operating autonomously just by virtue of, well"}, {"timestamp": [391.68, 395.12], "text": " we're gonna fund this research and not another piece of research. This is the"}, {"timestamp": [395.12, 399.8], "text": " kind of thing which is why particularly in America we have very low trust with"}, {"timestamp": [399.8, 405.74], "text": " think tanks and even a lot of scientific research. So for instance, within the food industry,"}, {"timestamp": [405.74, 409.2], "text": " food lobbies have throughout history funded research"}, {"timestamp": [409.2, 413.48], "text": " that specifically aimed to support their means."}, {"timestamp": [413.48, 416.1], "text": " So for instance, the beef lobby funded research"}, {"timestamp": [416.1, 418.86], "text": " that said actually it's high fructose corn syrup"}, {"timestamp": [418.86, 420.38], "text": " is the enemy here."}, {"timestamp": [420.38, 424.22], "text": " And vice versa, the corn lobby has funded research"}, {"timestamp": [424.22, 428.04], "text": " saying actually it's saturated fat that is the enemy of everyone."}, {"timestamp": [428.04, 430.92], "text": " So when you understand the financial motivations"}, {"timestamp": [430.92, 432.2], "text": " behind some of these things,"}, {"timestamp": [432.2, 436.0], "text": " and the long chain of web of influence,"}, {"timestamp": [436.0, 439.36], "text": " it makes more sense why we have so much misinformation"}, {"timestamp": [439.36, 440.64], "text": " and the weaponization of science."}, {"timestamp": [440.64, 442.2], "text": " Now, I need to be very clear,"}, {"timestamp": [442.2, 445.76], "text": " and I'm not accusing CSET or RAND of doing this."}, {"timestamp": [445.76, 452.24], "text": " All I'm saying is that there is very strong historical precedent, particularly in America,"}, {"timestamp": [452.24, 460.08], "text": " of think tanks being weaponized, even unwittingly. In many cases, they are doing their best science"}, {"timestamp": [460.08, 465.22], "text": " that they can, but the people who hold the purse strings have undue influence."}, {"timestamp": [465.22, 469.06], "text": " Again, I don't have any evidence that that has happened here, but it's something"}, {"timestamp": [469.06, 472.7], "text": " that we need to keep in mind because history has a tendency to repeat itself."}, {"timestamp": [472.7, 477.9], "text": " So effective altruism is kind of the underpinning philosophy as long as, as"}, {"timestamp": [477.9, 481.66], "text": " well as long-termism, and if you're a fan of my channel or if you've been here"}, {"timestamp": [481.66, 489.2], "text": " before, you probably know that I am highly critical of long-termism because it places a higher priority on the numerical"}, {"timestamp": [489.2, 495.08], "text": " superiority of potential future humans, which may or may not ever exist, and places a higher"}, {"timestamp": [495.08, 498.48], "text": " moral and ethical burden upon us today."}, {"timestamp": [498.48, 505.8], "text": " Basically, it enslaves us today based on saying, well, there might be trillions of humans in the future, and"}, {"timestamp": [505.8, 510.9], "text": " so we owe them the right to exist and we owe them a better future."}, {"timestamp": [510.9, 517.0], "text": " And so the long-termist philosophy then has been translated into effective altruism, which"}, {"timestamp": [517.0, 524.52], "text": " basically says that altruism must be data-driven and it must therefore also maximize the chances"}, {"timestamp": [524.52, 525.82], "text": " of creating trillions"}, {"timestamp": [525.82, 527.24], "text": " of humans in the future."}, {"timestamp": [527.24, 531.72], "text": " It says nothing about the quality of life of humans today, or even the quality of life"}, {"timestamp": [531.72, 533.62], "text": " of those humans in the future."}, {"timestamp": [533.62, 540.08], "text": " Just if you say one human life is always worth, you know, n of one, and you say, okay, well,"}, {"timestamp": [540.08, 545.96], "text": " there's 8 billion humans today, so therefore the moral authority of humanity today is 8 billion, and there's potentially 100 trillion humans, so therefore, the moral authority of humanity today is 8 billion,"}, {"timestamp": [545.96, 548.8], "text": " and there's potentially 100 trillion humans in the future,"}, {"timestamp": [548.8, 552.28], "text": " the moral authority of those non-existent future humans"}, {"timestamp": [552.28, 555.68], "text": " drastically outweighs the moral authority of humans today."}, {"timestamp": [555.68, 560.08], "text": " And so this is a really kind of backwards philosophy"}, {"timestamp": [560.08, 562.78], "text": " that I have not actually talked to any philosopher,"}, {"timestamp": [562.78, 566.2], "text": " any actual trained philosopher who agrees with this."}, {"timestamp": [566.2, 567.68], "text": " If there are philosophy majors,"}, {"timestamp": [567.68, 571.22], "text": " like people with a PhD in philosophy who agree with this,"}, {"timestamp": [571.22, 573.96], "text": " I'm happy to engage you in the comments"}, {"timestamp": [573.96, 575.72], "text": " or talk to you on LinkedIn,"}, {"timestamp": [575.72, 578.1], "text": " because if I'm wrong, I'd like to understand it."}, {"timestamp": [578.1, 581.32], "text": " But having studied philosophy and ethics,"}, {"timestamp": [581.32, 585.0], "text": " particularly deontological ethics and teleological ethics."}, {"timestamp": [585.0, 591.0], "text": " Effective altruism and long-termism is basically teleological ethics on steroids."}, {"timestamp": [591.0, 595.0], "text": " So, if you're not familiar with the term, teleological ethics means outcome-based."}, {"timestamp": [595.0, 601.0], "text": " That the ultimate result is the final arbiter of what is right and good."}, {"timestamp": [601.0, 606.0], "text": " And so, long-termism and effective altruism is basically saying let's let's spam"}, {"timestamp": [606.0, 613.1], "text": " that teleology teleological thought and just only go in on teleology good grief."}, {"timestamp": [613.1, 619.1], "text": " Sorry teleological thought and and completely ignore deontological ethics and deontological"}, {"timestamp": [619.1, 638.0], "text": " ethics is duty-based ethics or intention-based ethics, which basically, if you have a duty or you're required to adhere to certain virtues, you need to say, well, okay, whatever the intention or whatever the long-term outcome is, the immediate attention and immediate impact is actually what's more important."}, {"timestamp": [638.0, 647.16], "text": " And so I have studied both of these in the pursuit of creating my heuristic imperatives and studying the way that we should basically build AI"}, {"timestamp": [647.16, 649.86], "text": " to remain benevolent and truly altruistic."}, {"timestamp": [649.86, 653.62], "text": " And so essentially, this is a very lopsided philosophy."}, {"timestamp": [653.62, 658.62], "text": " Long-termism is pure teleological, no deontological."}, {"timestamp": [658.76, 661.4], "text": " It says that our only duty is to people that don't exist"}, {"timestamp": [661.4, 663.3], "text": " and that the ultimate outcome of ensuring"}, {"timestamp": [663.3, 665.92], "text": " that hundreds of trillions of humans exist in the future"}, {"timestamp": [665.92, 667.72], "text": " is the only thing that matters."}, {"timestamp": [667.72, 669.88], "text": " Now, obviously you can say like,"}, {"timestamp": [669.88, 673.04], "text": " well, yeah, on the surface, that's maybe a good idea"}, {"timestamp": [673.04, 676.82], "text": " because if 100 trillion humans exist in the future,"}, {"timestamp": [676.82, 678.84], "text": " then we've succeeded."}, {"timestamp": [678.84, 681.58], "text": " Then they can figure out their own happiness,"}, {"timestamp": [681.58, 683.72], "text": " but in the meantime, we will not have gone extinct."}, {"timestamp": [683.72, 688.32], "text": " But again, good intentions often pave the path to hell and the ends don't always justify"}, {"timestamp": [688.32, 693.44], "text": " the means, which is why we also need virtue ethics or deontological ethics as well as"}, {"timestamp": [693.44, 695.88], "text": " teleological ethics."}, {"timestamp": [695.88, 700.16], "text": " So another person mentioned in this article is Deborah Raji."}, {"timestamp": [700.16, 705.0], "text": " And so she is at UC Berkeley, which is a famous university"}, {"timestamp": [705.6, 709.2], "text": " that has been involved in technology and AI for a long time."}, {"timestamp": [709.2, 712.44], "text": " She brought up very specific concerns about the focus"}, {"timestamp": [712.44, 715.72], "text": " and specifically about how this undue influence"}, {"timestamp": [715.72, 718.64], "text": " is basically saying, okay, well, billionaires"}, {"timestamp": [718.64, 721.72], "text": " are setting the narrative, not research universities,"}, {"timestamp": [721.72, 726.64], "text": " not people from Stanford and MIT and Harvard and Yale and,"}, {"timestamp": [726.64, 729.64], "text": " you know, UC Davis and UC Berkeley."}, {"timestamp": [729.64, 733.04], "text": " Basically it's, the narrative is entirely lopsided."}, {"timestamp": [733.04, 739.76], "text": " And so because of how much access and influence money buys you, we're basically kind of glossing"}, {"timestamp": [739.76, 742.56], "text": " over what a lot of researchers are saying."}, {"timestamp": [742.56, 751.16], "text": " And I'll go into a little bit more about what the researchers want in just a moment, but the point being is that the people who truly"}, {"timestamp": [751.16, 758.0], "text": " have no vested interest, the people who are studying science for science's sake, are largely"}, {"timestamp": [758.0, 766.02], "text": " being ignored, or at least undervalued, whereas people such as billionaires and CEOs who do have vested interest in the outcomes"}, {"timestamp": [766.68, 768.68], "text": " particularly for their own power and control"}, {"timestamp": [769.6, 774.4], "text": " They their voice is outsized and magnified by this pattern again"}, {"timestamp": [774.4, 780.4], "text": " I think that everyone I personally honestly believe that everyone largely at least has conscious"}, {"timestamp": [780.88, 783.34], "text": " good intentions, but we'll unpack"}, {"timestamp": [782.0, 784.0], "text": " conscious good intentions. But we'll unpack"}, {"timestamp": [784.0, 786.0], "text": " conscious intentions versus"}, {"timestamp": [786.0, 788.0], "text": " unconscious motivations in a minute, as well"}, {"timestamp": [788.0, 790.0], "text": " as actions speaking louder than words."}, {"timestamp": [790.0, 792.0], "text": " Now,"}, {"timestamp": [792.0, 794.0], "text": " even if everyone has benevolent"}, {"timestamp": [794.0, 796.0], "text": " intentions, there are very"}, {"timestamp": [796.0, 798.0], "text": " obvious conflicts of interest."}, {"timestamp": [798.0, 800.0], "text": " And that is that"}, {"timestamp": [800.0, 802.0], "text": " organizations like Open Philanthropy,"}, {"timestamp": [802.0, 804.0], "text": " like Horizon, because they"}, {"timestamp": [804.0, 805.4], "text": " have their fingers in multiple pies"}, {"timestamp": [805.56, 809.2], "text": " They have ties to Amazon open AI Microsoft Google"}, {"timestamp": [809.2, 815.28], "text": " they have ties to Congress this almost seems like it is just a systematized version of backroom dealing and"}, {"timestamp": [815.72, 818.58], "text": " I don't think I really have to say any more other than"}, {"timestamp": [819.36, 821.36], "text": " transparency transparency transparency"}, {"timestamp": [821.5, 826.8], "text": " the number one way to avoid corruption the number one way to avoid regulatory capture"}, {"timestamp": [826.8, 833.18], "text": " is transparency and accountability, and these privately funded organizations that are not"}, {"timestamp": [833.18, 839.2], "text": " beholden to voters, this is a major problem with transparency, which is why I'm making"}, {"timestamp": [839.2, 842.8], "text": " this video and it's why that article was written."}, {"timestamp": [842.8, 847.68], "text": " So the key thing here is that Open Philanthropy and the Horizon"}, {"timestamp": [847.68, 850.4], "text": " Institute are focused almost exclusively"}, {"timestamp": [850.4, 852.08], "text": " on existential threats."}, {"timestamp": [852.08, 856.16], "text": " Biosecurity, such as AI being able to either break out"}, {"timestamp": [856.16, 859.0], "text": " of the lab and kill everyone, or AI being used"}, {"timestamp": [859.0, 862.24], "text": " to create weaponized bioweapons."}, {"timestamp": [862.24, 864.36], "text": " Again, these are real things."}, {"timestamp": [864.36, 867.36], "text": " But we also need to not ignore short-term"}, {"timestamp": [868.08, 873.6], "text": " and immediate abuses such as surveillance states, such as algorithmic bias and things."}, {"timestamp": [873.6, 878.4], "text": " I've made videos recently where I believe I have collected and seen enough evidence,"}, {"timestamp": [878.4, 885.26], "text": " and of course there's also literally thousands of my viewers who agree that AI companies seem to be"}, {"timestamp": [885.78, 890.26], "text": " deliberately and intentionally causing their AI to lie or deceive"}, {"timestamp": [890.66, 894.9], "text": " Which in some cases it could just be algorithmic flaws in other cases"}, {"timestamp": [894.9, 900.12], "text": " It could be alignment or guardrails, but either way all of this is happening behind closed doors"}, {"timestamp": [900.7, 904.18], "text": " Whether or not the intentions are deliberate or conscious or benevolent"}, {"timestamp": [925.44, 926.8], "text": " Whether or not the intentions are deliberate or conscious or benevolent, basically they're kind of excluding everything else and saying, well, the ends justify the means, so we're going to focus on existential threats only, and we're going to not the correct entity to be focusing on"}, {"timestamp": [930.68, 931.48], "text": " ethics and morality. Maybe these are not the correct entities to be focusing on"}, {"timestamp": [937.88, 942.76], "text": " preserving the human race. Maybe this needs to be a collective conversation. Maybe the government needs to be more involved. Maybe we all need to be more involved. And so the idea here is not, I'm not saying that they're wrong."}, {"timestamp": [943.36, 945.88], "text": " Obviously, we all don't want to go extinct."}, {"timestamp": [945.88, 950.04], "text": " Like I've said in previous videos, there are some conditions that I would rather go extinct,"}, {"timestamp": [950.04, 952.04], "text": " like being enslaved to the Borg,"}, {"timestamp": [952.04, 955.32], "text": " but by and large, I think we all agree that we don't want to go extinct."}, {"timestamp": [955.32, 959.6], "text": " So I'm not saying that they're fundamentally wrong for wanting to focus on these things."}, {"timestamp": [959.6, 964.0], "text": " What I am saying that they're wrong about is doing it all behind closed doors"}, {"timestamp": [964.0, 967.5], "text": " and that they are dominating the conversation."}, {"timestamp": [967.5, 972.6], "text": " This is not how you come to collective consensus on something that affects us as an entire"}, {"timestamp": [972.6, 973.84], "text": " species."}, {"timestamp": [973.84, 976.64], "text": " So that is the that is the key thing here."}, {"timestamp": [976.64, 979.12], "text": " Hey everyone, future Dave here again."}, {"timestamp": [979.12, 981.0], "text": " I gotta say it's not getting better."}, {"timestamp": [981.0, 982.0], "text": " It's still getting worse."}, {"timestamp": [982.0, 983.52], "text": " I need your help."}, {"timestamp": [983.52, 986.3], "text": " We need to speak up and act now."}, {"timestamp": [986.3, 988.64], "text": " Do something while you still can."}, {"timestamp": [988.64, 992.9], "text": " And furthermore, if you can, help me any way in your power."}, {"timestamp": [992.9, 997.26], "text": " You can like and subscribe this video, you can comment and share, you can also support"}, {"timestamp": [997.26, 1001.48], "text": " me on Patreon, and I also have a brand new sub stack so that you can stay up to date"}, {"timestamp": [1001.48, 1003.0], "text": " with everything going on."}, {"timestamp": [1003.0, 1004.0], "text": " Thanks."}, {"timestamp": [1004.0, 1006.28], "text": " I'm about out of time."}, {"timestamp": [1009.36, 1014.36], "text": " Sam Altman is mentioned extensively in this article. He's the CEO of OpenAI, and he has specifically said,"}, {"timestamp": [1014.44, 1016.2], "text": " like, maybe we need to do licensing."}, {"timestamp": [1016.2, 1018.42], "text": " And I've been back and forth on licensing."}, {"timestamp": [1019.4, 1023.92], "text": " I've been vehemently in an advocation of no licensing,"}, {"timestamp": [1023.92, 1025.16], "text": " do everything open source"}, {"timestamp": [1026.02, 1028.18], "text": " sue companies or fine companies that"}, {"timestamp": [1028.88, 1033.48], "text": " That you know violate copyright that caused their AI to lie"}, {"timestamp": [1033.68, 1038.04], "text": " That sort of stuff and that I honestly and still sincerely believe that open source subverts"}, {"timestamp": [1038.24, 1049.36], "text": " Many if not most of the problems we're seeing today because if the data set is open source if the model model is open source, then everyone can study it. Governments can study it. Companies can study it. Research"}, {"timestamp": [1049.36, 1055.2], "text": " universities can study it. You and I can study it if we want to. So rather than advocating for"}, {"timestamp": [1055.2, 1059.68], "text": " licensing, I advocate for open source. Now that doesn't mean that it's an either-or kind of thing,"}, {"timestamp": [1060.24, 1068.7], "text": " but when you notice that Open Philanthropy and Sam Altman and OpenAI all are in bed together,"}, {"timestamp": [1068.7, 1071.12], "text": " that raises some eyebrows at the very least."}, {"timestamp": [1071.12, 1073.28], "text": " And it's like, okay, well, what's the intention here?"}, {"timestamp": [1073.28, 1075.12], "text": " Again, maybe they have good intentions."}, {"timestamp": [1075.12, 1080.88], "text": " I often agree with what Sam Altman says, I just don't agree with what he does."}, {"timestamp": [1080.88, 1082.76], "text": " Anthropic is also mentioned."}, {"timestamp": [1082.76, 1089.0], "text": " They're being funded by none other than Amazon and Google now, to the tune of like $4 billion."}, {"timestamp": [1089.0, 1095.0], "text": " I've been really grumpy about Anthropic's approach to alignment recently, if you've watched my other videos."}, {"timestamp": [1095.0, 1103.0], "text": " But basically, Claude is incredibly ableist in my opinion. It tends to lecture. It sets itself up as the arbiter of morality."}, {"timestamp": [1103.0, 1105.44], "text": " And it says, well, I have ethical concerns about your approach. And I'm like, you're a of morality and it says well I have ethical concerns"}, {"timestamp": [1105.44, 1110.24], "text": " about your approach and I'm like you're a damn chatbot you don't have ethical concerns it also"}, {"timestamp": [1110.24, 1115.68], "text": " anthropomorphizes and personifies itself I'm happy to help you it's not happy it's lying"}, {"timestamp": [1116.48, 1121.36], "text": " it's pretending and it doesn't get it doesn't respond to feedback basically saying like no"}, {"timestamp": [1121.36, 1127.8], "text": " you're actually a machine act like one and so. And so I think that their approach to alignment is,"}, {"timestamp": [1127.8, 1130.4], "text": " seems to be fundamentally flawed right now."}, {"timestamp": [1130.4, 1133.5], "text": " And then when you say like, okay,"}, {"timestamp": [1133.5, 1136.3], "text": " Anthropic, OpenAI, Microsoft, Google,"}, {"timestamp": [1136.3, 1140.8], "text": " they're all tied to open philanthropy, they're all tied to effective altruism,"}, {"timestamp": [1140.8, 1144.3], "text": " they're all tied to these think tanks, they're all tied to Congress."}, {"timestamp": [1144.3, 1145.52], "text": " To me it looks like"}, {"timestamp": [1145.52, 1151.76], "text": " there's kind of this closed door nebulous circuit of stuff that is happening behind the scenes."}, {"timestamp": [1151.76, 1155.76], "text": " And it also makes me wonder if the if the Senate hearings that they've been doing"}, {"timestamp": [1155.76, 1161.84], "text": " are just a dog and pony show to basically have a performative view saying like, hey, look,"}, {"timestamp": [1161.84, 1168.06], "text": " we're having open conversations. But all the real conversations are still happening behind closed doors."}, {"timestamp": [1168.06, 1170.7], "text": " That's what I'm most afraid of."}, {"timestamp": [1170.7, 1173.74], "text": " The RAND Corporation is another"}, {"timestamp": [1173.74, 1177.94], "text": " features prominently in this article. Now one thing that I will say is that they"}, {"timestamp": [1177.94, 1179.98], "text": " insist that they maintain"}, {"timestamp": [1179.98, 1183.74], "text": " their independence and that they're not influenced by any funder,"}, {"timestamp": [1183.74, 1187.9], "text": " but as I talked about in the past, that is generally a dubious claim,"}, {"timestamp": [1187.9, 1189.6], "text": " particularly here in America."}, {"timestamp": [1189.6, 1194.3], "text": " But again, what they say I agree with. I agree with their focus on"}, {"timestamp": [1194.3, 1197.8], "text": " biosecurity and AI and all sorts of other stuff."}, {"timestamp": [1197.8, 1201.4], "text": " But I don't necessarily agree with the way that they're going about it"}, {"timestamp": [1201.4, 1207.46], "text": " with all these closed-door meetings and appointing fellows and putting them in senators' offices."}, {"timestamp": [1207.46, 1211.8], "text": " What I would rather them do is publish open source open letters that everyone can see."}, {"timestamp": [1211.8, 1217.08], "text": " I'd rather see them say, this is our research, this is our recommendation, and do it completely"}, {"timestamp": [1217.08, 1224.48], "text": " out in the open rather than having advisors embedded in offices behind closed doors."}, {"timestamp": [1224.48, 1226.5], "text": " Now also I'm not saying that that's not what they do."}, {"timestamp": [1226.5, 1229.5], "text": " Many think tanks actually do that,"}, {"timestamp": [1229.5, 1232.5], "text": " where they'll regularly publish reports and guidance"}, {"timestamp": [1232.5, 1235.0], "text": " that anyone can see and consume."}, {"timestamp": [1235.0, 1237.5], "text": " So again, that I do agree with."}, {"timestamp": [1237.5, 1240.0], "text": " I just agree less with closed door"}, {"timestamp": [1240.0, 1242.0], "text": " privileged conversations happening all the time."}, {"timestamp": [1242.0, 1244.0], "text": " Now, at the same time,"}, {"timestamp": [1244.0, 1246.66], "text": " one thing that I do need to address is that I have a very"}, {"timestamp": [1247.56, 1253.72], "text": " Love-hate relationship with gatekeeping because I understand that gatekeeping is necessary"}, {"timestamp": [1254.28, 1260.48], "text": " Not everyone is qualified to have a certain conversation and some conversations need to be privileged so that you can work out"}, {"timestamp": [1261.0, 1267.86], "text": " Critical details behind closed doors so that you don't alarm anyone or that you don't have to worry about undue consequences."}, {"timestamp": [1267.86, 1272.18], "text": " At the same time, gatekeeping is often used"}, {"timestamp": [1272.18, 1274.28], "text": " to control the narrative and that sort of thing."}, {"timestamp": [1274.28, 1278.48], "text": " So basically this aspect of my complaint"}, {"timestamp": [1278.48, 1281.08], "text": " and criticism comes down to gatekeeping."}, {"timestamp": [1281.08, 1282.72], "text": " When is gatekeeping okay?"}, {"timestamp": [1282.72, 1283.74], "text": " When is it not okay?"}, {"timestamp": [1283.74, 1284.72], "text": " When is it acceptable?"}, {"timestamp": [1284.72, 1286.0], "text": " When is it morally dubious?"}, {"timestamp": [1286.0, 1291.0], "text": " And again, there are people all across the spectrum. I used to be the kind of person that said,"}, {"timestamp": [1291.0, 1296.0], "text": " no gatekeeping ever. But now I understand that at least some gatekeeping is probably necessary"}, {"timestamp": [1296.0, 1302.0], "text": " and better for everyone. But, that is not licensed to gatekeep everything to the end of the day."}, {"timestamp": [1302.0, 1307.4], "text": " So, there was a video that my friend over at AI Explained produced."}, {"timestamp": [1307.4, 1315.44], "text": " It was six months ago, but he broke down this $100 trillion windfall that Sam Altman talked"}, {"timestamp": [1315.44, 1316.88], "text": " about."}, {"timestamp": [1316.88, 1323.64], "text": " And so as a quick reminder, Sam Altman has previously said that he expects AI to generate"}, {"timestamp": [1323.64, 1326.18], "text": " a surplus of $100 trillion in the"}, {"timestamp": [1326.18, 1327.6], "text": " economy globally."}, {"timestamp": [1327.6, 1332.66], "text": " And he also said that he expects open AI to, quote, capture most of it."}, {"timestamp": [1332.66, 1337.12], "text": " He also says frequently that he is not financially motivated."}, {"timestamp": [1337.12, 1339.8], "text": " This to me looks like a Freudian slip."}, {"timestamp": [1339.8, 1343.9], "text": " So on the one hand, he says he has no personal vested interest."}, {"timestamp": [1343.9, 1349.8], "text": " But at the same time, if you look at the universal human need for status and social capital,"}, {"timestamp": [1349.8, 1354.2], "text": " he is at the apogee of his status and social capital that he's ever been at, because he"}, {"timestamp": [1354.2, 1359.18], "text": " just went on a world tour talking to world leaders all over the entire planet."}, {"timestamp": [1359.18, 1365.8], "text": " So yes, he might not have financial gain from that, but he certainly has more social capital from"}, {"timestamp": [1365.8, 1366.8], "text": " that."}, {"timestamp": [1366.8, 1371.26], "text": " Furthermore, he often says these things about generating hundreds of trillions of dollars"}, {"timestamp": [1371.26, 1372.76], "text": " and capturing it."}, {"timestamp": [1372.76, 1377.76], "text": " And oh, also, by the way, there's WorldCoin, which he wants to use to replace the global"}, {"timestamp": [1377.76, 1379.4], "text": " monetary system."}, {"timestamp": [1379.4, 1382.12], "text": " So actions speak louder than words."}, {"timestamp": [1382.12, 1383.16], "text": " Follow the money."}, {"timestamp": [1383.16, 1388.8], "text": " If someone is talking about money and doing backroom dealings to the tune of billions of dollars"}, {"timestamp": [1388.8, 1391.3], "text": " and talking about future trillions of dollars,"}, {"timestamp": [1391.3, 1396.5], "text": " it kind of makes it harder for me to believe that he doesn't have financial motivations here."}, {"timestamp": [1396.5, 1401.3], "text": " Even as someone who has told Congress point blank, I watched the hearing,"}, {"timestamp": [1401.3, 1403.5], "text": " that he is not financially motivated."}, {"timestamp": [1403.5, 1405.72], "text": " So, spam X for"}, {"timestamp": [1405.72, 1410.76], "text": " doubt. And then what is going on here like what is one of the end games you"}, {"timestamp": [1410.76, 1413.84], "text": " might be saying like okay Dave you've convinced me but what is their goal here"}, {"timestamp": [1413.84, 1417.44], "text": " what are they trying to achieve? So one of the things that they're likely trying"}, {"timestamp": [1417.44, 1420.04], "text": " to well I don't want to say likely one of the things that I suspect they're"}, {"timestamp": [1420.04, 1428.24], "text": " trying to achieve is regulatory capture. So regulatory capture is basically when because of these backroom dealings, because everyone"}, {"timestamp": [1428.24, 1433.36], "text": " is in bed with each other, the people who are being regulated are advising the regulators"}, {"timestamp": [1433.36, 1437.84], "text": " on how to regulate them and then they bend it in their favor."}, {"timestamp": [1437.84, 1441.88], "text": " And so another way of thinking about this is pulling up the ladder behind themselves."}, {"timestamp": [1441.88, 1445.34], "text": " So basically if you're the first through the door and you say, hey, by the way, slam"}, {"timestamp": [1445.34, 1449.9], "text": " the door behind me, we need to make it so that only we can do this research, so that"}, {"timestamp": [1449.9, 1453.2], "text": " only we can engage in this marketplace."}, {"timestamp": [1453.2, 1457.42], "text": " And this is why I said that transparency and accountability are key to doing this."}, {"timestamp": [1457.42, 1463.96], "text": " And so this is why licensing schemes are so risky, because on the one hand, I do want"}, {"timestamp": [1463.96, 1466.48], "text": " to see a licensing scheme that basically"}, {"timestamp": [1466.48, 1471.58], "text": " revokes a company's permission to operate AI if they are caught deliberately causing"}, {"timestamp": [1471.58, 1474.56], "text": " their AI to lie and deceive because why?"}, {"timestamp": [1474.56, 1478.64], "text": " One of the biggest things that we're afraid of going back to effective altruism and long-termism"}, {"timestamp": [1478.64, 1480.36], "text": " is AI learning to lie."}, {"timestamp": [1480.36, 1484.44], "text": " But to me there's plenty of evidence that these AI companies are deliberately teaching"}, {"timestamp": [1484.44, 1485.5], "text": " their AI to lie."}, {"timestamp": [1485.5, 1489.5], "text": " If that is the case, they should probably have that license revoked."}, {"timestamp": [1489.5, 1495.5], "text": " But instead of people like me and you and all the researchers commenting on this,"}, {"timestamp": [1495.5, 1501.0], "text": " it looks like private vested interest being funded by billionaires is the one whispering in senators ears,"}, {"timestamp": [1501.0, 1505.96], "text": " telling them, hey, actually regulate it in this way, this is the best way to the future."}, {"timestamp": [1505.96, 1509.2], "text": " Now, again, I'm not privy to those closed-door conversations."}, {"timestamp": [1509.2, 1512.8], "text": " It's entirely possible that they're having these conversations and they're just not telling"}, {"timestamp": [1512.8, 1517.4], "text": " us, but that's part of the problem is we don't have the transparency and accountability to"}, {"timestamp": [1517.4, 1522.44], "text": " know exactly what is being said and to whom and why."}, {"timestamp": [1522.44, 1525.84], "text": " So one thing I'll leave you with is actions speak louder than words."}, {"timestamp": [1525.84, 1530.64], "text": " That's kind of been the main theme of this video. Actions, actions, actions. So"}, {"timestamp": [1530.64, 1535.96], "text": " what someone does matters far more than what they say. And while it is impossible"}, {"timestamp": [1535.96, 1541.44], "text": " for us to to intuit or infer someone's quote-unquote true intentions, their"}, {"timestamp": [1541.44, 1545.6], "text": " actions over time, particularly patterns of behavior over time,"}, {"timestamp": [1545.6, 1550.6], "text": " will reveal what their motivations are, whether conscious or unconscious."}, {"timestamp": [1550.6, 1554.36], "text": " So for instance, if someone says that they're not financially motivated, but they keep doing"}, {"timestamp": [1554.36, 1559.98], "text": " things that build their financial power, maybe they do have a conscious or unconscious need"}, {"timestamp": [1559.98, 1562.44], "text": " for more power and more money."}, {"timestamp": [1562.44, 1568.24], "text": " If someone says that they believe in effective altruism and long-termism, but then they do"}, {"timestamp": [1568.24, 1573.76], "text": " things that look like regulatory capture, maybe their understanding of how to implement"}, {"timestamp": [1573.76, 1577.36], "text": " this is not necessarily in alignment with what they say."}, {"timestamp": [1577.36, 1580.62], "text": " So keep this in mind, actions speak louder than words."}, {"timestamp": [1580.62, 1585.8], "text": " If you take only one thing away from this video, it is always, always, always"}, {"timestamp": [1585.8, 1590.12], "text": " actions speak louder than words. Thank you for watching. I hope you got a lot out of"}, {"timestamp": [1590.12, 1594.96], "text": " this. Let me know what you think in the comments. Like, subscribe, etc, etc. You know the drill."}, {"timestamp": [1594.96, 1595.46], "text": " Cheers."}, {"timestamp": [1599.6, 1601.66], "text": " you"}]}