{"text": " Cool. Go live. All right, cool. Are we live? It says I'm live. Cool. I think I'm live. Hello, everybody. Oops. No, don't do that. Let's see. Y'all reading me? We might be functional. I don't know. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I don't know. Hey, David Gonzalez is on. Cool. All right, cool. We're live. So I've got, I'm scared of death. Yeah, I mean, that's a pretty typical response. So let's see. I want longevity, escape velocity. We're working on it. We're working on it. Let me bring this up. History, clear history so that you don't see anything that you're not supposed to on my Firefox. But yeah, so today, wait, is this not going to stay on top? Okay, it might not. But yeah, so drop your comments in chat and I will answer them as best as I can. And let's see, is it possible? All right, first question, is it possible that we will face some kind of bottleneck and AI can't be developed fast enough? So, yes, that's actually a really great question. Let's just do a quick Google search, AI development, not bottlenecks, bottlenecks. But yeah, so basically there's a few things. So the first and probably the biggest bottleneck is just the cost of energy. So if you hear the news like it costs millions of dollars to train large language models like GPT-3, GPT-4, and that cost is actually going up because of the size of those models and the amount of training. Now, at the same time, you're going to be seeing information about new training techniques that are much cheaper, much faster. It's still up to debate as to whether or not those cheaper models are just as good. But certainly, depending on the benchmarks you use, some of those cheaper, smaller models are just as good. So we've got the increase in efficiencies, but also the increase in size. And so we've kind of got this neutralizing effect. But certainly, that's the next big bottleneck. Good question. Let's see. You guys going for the $100,000? neutralizing effect but certainly that's that's the next big bottleneck good question let's see you guys going for the hundred thousand yes so this is a reference to open AI's democratic inputs democratic inputs to AI so let me show you guys this so this is this is something that we actually within the Gato community we we talked about last night actually is we're going to put together a team from within the Gato community in order to pursue this. I will probably be on that team and we'll see what we can do. But basically, the idea is for questions such as like, let's see, here's an example. How far do you think personalization of AI assistance like chat GPT should go to align with the user taste and preferences? What boundaries, if any, should exist in this process? So the idea is to democratically or in an open source manner, collect general consensus across the world about how AI should behave. And so they're basically issuing a $100,000 grant for the top 10 winning teams. So I've already started on working on a proposal for this. And then of course within the Gato community there's a tremendous amount of energy to work on this kind of thing. So yes, we're probably going to go for it. I have no idea if we're actually going to get any money or anything, but it's still a very interesting exercise. All right, next question. Automation of all unskilled labor, when? Well, it's starting now, but the biggest thing is we're actually automating knowledge work first. So it's actually kind of like reverse order of what you might think. And the reason is because all of medicine, all of law, all of science has a tremendous amount of text and you can do a good chunk of these jobs in front of a computer. And so the rule of thumb is if it's a job you can do in front of a computer, it's probably going to be automated first. So skilled and unskilled manual labor such as plumbing, electrical work, moving heavy stuff, farm work, that's actually going to be last because of the uncertainty of operating in the physical world. And this is why, for instance, Tesla's self-driving cars are so difficult is because one of the things that humans absolutely excel at above and beyond all machines without variants right now is the ability to adapt and improvise in a complex physical world. And so this is called friction, the friction between the digital and the physical. So those physical jobs are actually going to be the last to be automated. Now, that being said, with compounding returns, you look at things like the Tesla bot Optimus, there's a bunch of other startups that are going to compete. There's Boston Dynamics. The desire to get robots that can engage with the physical world is heating up very quickly. So I can't really tell you how long it'll be until physical robots are as dexterous as humans and then surpass humans, but we should absolutely assume that they will pass humans eventually. Let's see, really admire your work, love to hear your thoughts on Jan Lekun's perspective that we are far from achieving AGI. Jan Lekun is spectacularly wrong more often than not. You know, there's the clip of him talking about how, like, if you put a cup on a table, there's absolutely nothing in the training data out there in the world that will teach the AI that the cup will move with the table, which is just patently absurd. So I don't really take what Jan Leekun says with a lot of credibility. That being said, I have listened to some of his stuff, and he very much believes what he's saying, but then again, you know, so does chatGPT. So I don't know. I try and take different perspectives from different people. For instance, a lot of people have asked me what I thought about Eliezer Yudkowsky. And at first I was like, whatever, this is just a doomer bozo or whatever. But then I actually spent the time to actually like read what he writes and listen to what he says. And one of the conclusions that I came to is that Eliezer, he doesn't present as well on video, like he's not as good at speaking as some other people are, but when you actually read what he writes, it actually makes a lot of sense. That being said, like everyone, sometimes he says things that I don't agree with, so I'm not just saying, yes, I 100% agree with Eliezer and I 100% disagree with Jan. I'm just saying in general, I do agree with Eliezer's points about like it's really difficult to understand what superintelligence actually looks like and it is kind of frustrating when people say like, oh, superintelligence is never going to happen. Meanwhile, AI is making leaps and bounds on a weekly basis. Okay,, there's a lot of questions coming. Let's try and pick and pick and choose a few. Starting from the bottom, have you considered collaborating with the industrial organization psychology community? They have a subreddit and discord. I didn't know that, but certainly industrial organizational psychology is absolutely part of the Gato framework with corporate adoption. So if you can drop a link, that would be cool. Let's see, assuming Gato gets implemented globally, what is stopping big companies from creating AI that can trick high-level control? The biggest thing actually, so to further expound this question, is like what incentive if any, do corporations have to actually use aligned AI or should they just greenwash their AI? Like, look, we're aligned. And the thing is, is this is also true with regardless of corporations, it behooves AI to have actual alignment and actual understanding rather than Mesa optimization. And so basically this is a Mesa optimization question like what if the company deliberately picks an AI that is good at appearing aligned but isn't actually aligned. And one of the reasons that I believe that true alignment or that actual alignment rather than you know inner misalignment or Mesa optimization is optimal is because actual alignment is ultimately going to be more effective but I think that's also going to be so by more effective I mean it'll actually generate better returns it'll it'll be more economically viable to have true alignment but also I think it's going to be more efficient because one thing that happens is that when your mental model is internally consistent, so this is true of humans, this is true of all intelligent systems, when your model is internally consistent, it tends to be, one, more robust, and two, more efficient. And so that efficiency means that companies will be incentivized to have actual alignment rather than expending a tremendous amount of energy just pretending to be aligned. Right? Like, there's a lot of people that I refuse to work with now. This is a big reason that I kind of said I'm just going to step out of corporations and startups, is because a lot of people will pretend to be aligned to my interests people will pretend to be aligned to my interests will pretend to be aligned and then they end up expending a tremendous amount of energy to maintain the facade of or the charade of alignment and not actually being aligned and so it's much easier to just be aligned and there's a lot of other reasons to believe this from a from a standpoint, from a game theory standpoint. So good question. How does chat GPT work? I heard it predicts the next word, but how can it stick to a red thread? I'm not sure what you mean by red thread. But yes, all language models fundamentally just predict the next word. We're moving from sci-fi to sci-non-fi pretty quick. Yep. Elon's mega factories scare me. The amount of output they do is crazy. You're just seeing the beginning. Imagine in a few years when something like Elon's gigafactories, a human isn't even involved in the construction of the entire thing. That is hypothetically possible. Let's see, how can citizens prevent government from racing towards Skynet? Sorry for the sci-fi reference. Oh no, it's perfectly reasonable. Voting is one of the biggest things. That's actually one of the biggest purposes of the Gato framework, is to build consensus. is to build consensus. And so by having consensus amongst the population, we will all, you know, because here's how American politics works, is the congresspeople, whether they're in the House or the Senate, they don't really make decisions on their own. They just go based on the polls, right? They have lots and lots of research aides and think tanks and other stuff. And so like, you know, so-and-so from this district of Tennessee, they get a report saying, hey, this is what's popular with your constituents. Use this talking point. So if we, the people, build consensus around these talking points, then that's just going to be the way things go. And then the politicians are pretty much going to choose what gets them elected. Now, that being said, there is another party. So there's the different estates. There's we the people. That's one estate. There's the government is the other state. There's journalism, which is the third estate or fourth estate. And then there's the corporations, which is another estate. So we have this four-way power structure, right? Military-industrial complex, media and journalism, we the people, and then the government, right? And so corporations have their own interests, and so it's basically us versus the corporations, and then the media has to, you know, report on it honestly, which of course they don't because they're in the pockets of corporations, so on and so forth, but we have the internet, right? Like, who cares? Very flippant of me, but anyways, by building consensus we can shape the trends of things. Let's see, what do you think about bad actors inside the government motivated to push AI into militaries? I would not say that some military is, I'm not gonna say that military is intrinsically neutral because that's a very contentious way of saying it, but what I will say is that the military's primary interest is that of national security. And I don't just mean America, but all militaries, their primary interest is national security and sometimes that means having a good defense, sometimes it also means having a good offense or the ability to project power. Now, in order to maintain that, it behooves militaries, all militaries, to invest in artificial intelligence. So I would not say that that's necessarily a bad actor thing. Certainly, it can be taken the wrong direction, as we've seen with news in actually just the last few days of artificial intelligence being put into drones. And those drones are then potentially engaging in problematic behavior. But also by removing humans from the loop of making lethal decisions, that has enormous geopolitical, ethical, and moral and philosophical questions that it brings up. Which you could say like, okay, that is intrinsically evil if you deliberately build machines that can go kill humans. That sounds pretty bad. But the closer you look at all these things, there's a lot more nuance and subtlety to it. Still, good question and merits a lot of attention. Okay, cool. Go back to the bottom. It would be great to see you in a guest Machine Learning Street Talk. I think you mean Machine Learning Street Talk, the YouTube channel. Yeah, I'd love to jump on there. I started watching them more frequently. Did you see the latest AI Explained where he reviewed the step-by-step checks and how it skyrocketed GPT-4 performance with math? Yeah, actually, so I messaged the AI Explained guy on his Patreon and I actually wrote a comment on that as well. Here, let me just read you the comment because I thought it was pretty funny. Let's see if I can find it. Do to do a I explained videos. So this is the video that person is referring to. And then let's see there's six hundred and thirty some odd comments. I don't think I'll be able to find mine. See it might not be here. But anyways, I did write a comment on here. So basically, the TLDR of what they did is they are... Come on, it'll actually be better if I can just read the comment that I wrote. It might have gotten removed. David Adams that's not me might be easier if I just search for Shapiro. No that's my own video. Okay well anyways so on on this video I wrote a comment about how basically what they did was they trained there's the main model but then there's two other models that are kind of like monitoring the performance of each other. And so by having three models that are measuring the the robustness of the logic and reasoning and also measuring the robustness of the answer like are you accurate yes or no but also did you use the right method to get there. And the story that I had was from, I think it was Algebra 3 in high school, where my teacher, Mrs. Stewart, checked my work and I had derived the wrong formula to get to the right answer, which is exactly what they're afraid of models, large language models and other neural networks are doing internally, where they give you the answer that it thinks that you want or that it knows that you want, but it uses the wrong method to get there or uses the wrong internal logic or the wrong internal reasoning. And so by forcing it to lay out its steps and then measuring the accuracy of those steps as well as the accuracy of the output, you are hypothetically forcing the model to be to actually creating a more accurate and realistic internal representation. So remember, internal representations or internal consistency is really critical for intelligence to be efficient and aligned. So thanks Mrs. Stewart for calling me out. She didn't give me any credit. She's like, yes, you got the right answer, but you used entirely the wrong method, so you get no credit. And I didn't understand the value of that at the time, but I absolutely understand the value of it now. Good question. Let's see. Building consensus is fine, but you have the military complex. Yes. So talking about incentives, one of the biggest things is that aligning GATO or AI alignment with everyone's incentives and their intrinsic motivations is exactly what Daniel Schmachtenberger and Liv Bowery talk about with Moloch, Nash equilibrium, and that sort of stuff. Are you familiar with Ben Goertzel's work with SingularityNet and the Cardano blockchain? Yeah, I am. I'm not really a fan. I read Ben's paper a few years ago about his general theory of general intelligence and I was not impressed. So I kind of stopped paying attention to it. And the other thing is like, okay, if like, where's the work? Like, where is he today? Like, is he not adapting to the current information? So let's see. Have you, oh, all right, I think we got all those questions. If I'm not mistaken, our LHI training set is created with GPT-3, so any LLM that is fine-tuned with it cannot be used for commercial purpose, is that correct? No, so I, so this, let's unpack this question. So I used a combination of my original research with core objective functions. I did synthesize data with GPT-3. I revisited that with RLHI on chat GPT-4. And so I read the terms of service, the end user license agreement or whatever for GPT very, very carefully. And basically what they say is that you're not allowed to try and reverse engineer how the model works, which I'm not trying to do. I don't care how the internal model works. And you're also not allowed to try and compete, create a competitor to their products. And I'm also not allowed to try and create a competitor to their products. And I'm also not trying to create a competitor. All I was doing was synthesizing a fine-tuning data set that can be used on any model. And that's true, right? I have been using synthetic data to fine-tune GPT models for more than two years now. Actually, closer to four years, because I started with GPT-2. Granted, it couldn't make its own synthetic data at the time. I had to use Wikipedia. for more than two years now, actually closer to four years because I started with GPT-2. Granted, it couldn't make its own synthetic data at the time, I had to use Wikipedia. But point being is I'm not trying to make a competitor to chat GPT, I don't care. All I was doing was synthesizing a fine tuning data set that is then portable, which can be used on any model, whether it's GPT-3, 4, Alpaca, whatever, as long as it takes a JSON-L file. So no, not competing. Jonathan Lindsay says, your comment got blocked. If you use bad words, that is more likely to happen. Not saying that that's what you did, but also if you post a link, that's another thing that it triggers it and it'll kick it out. And I've even given people like approved them as like approved commenters and it still erases their comments. So I don't know what to do about that. I think that's just YouTube. No, we did not make a submission to OpenAI yet. We still have about three weeks. Let's see, is Anthropic Cloud Gato compatible?, their constitutional AI? Oh, so this is a really great question. So Anthropic created this idea of constitutional AI, which they've been talking about since about middle of 2021, which by the way, middle of 2021 is when I published my first book, in which case I advocated for using a constitution with AI. So I don't know if they got the idea from me, but great minds think alike. So, Anthropocene's constitution is a bunch of principles that are rooted in conventional morality. And so what I mean by conventional morality is that it's based on kind of the prevailing, moral fabric of society like don't do things that are illegal don't do things that are harmful choose the option that is more kind so these are these are in in Kohlberg's framework of moral development this is stuff here I can just show you. Kohlberg. Stages of moral development so in Kohlberg stages of moral development. So in Kohlberg stages of moral development, you've got, here we go, perfect. Where did it go? All right, so you've got, there's three stages, and each stage is broken into two. So Anthropics Constitutional AI is right smack dab in the middle of this. Law and order and morality. This is where Anthropics Constitution is for Claude. It's about adhering to the law. It's about general social norms. What they fail to achieve is post-conventional morality, which is identifying universal principles or universal axioms, which that's where my work with the heuristic imperatives comes in because the point of law is within a social construct, but the point of the heuristic imperatives is a universal construct or universal framework that is beneficial for all life and also beneficial for artificial intelligence. So put it this way. A lot of people have messaged me because they're concerned about like what if Bard or Claude or GPT-4, what if they are sentient, right? I don't personally believe that they are, but if they are, then the heuristic comparatives, which is reduced suffering in the universe, increased prosperity in the universe, and increased understanding in the universe, those all apply equally to AI if they are indeed sentient. Because if the machine is sentient and suffering, it doesn't want to suffer, we shouldn't want it to suffer, and we don't want to suffer either. So that abides by a universal principle, a post-conventional moral thing that says suffering is bad. And yes, some people can say, oh, well, suffering is necessary, right? Yes, life is, in part, requires some suffering, but that doesn't mean that it's a good thing. And it certainly doesn't mean that you should increase suffering in the universe. So Claude's, they're working towards more universal principles, and I'm going to keep beating this drum until the AI establishment understands the difference between conventional and post-conventional morality. And also this does say that this implies that all adults abide by post-conventional morality, which is actually not true. Kohlberg himself said that most humans never get to a point of development of post-conventional morality. And so post-conventional morality, the most familiar example of post-conventional morality that most people have is actually the principle of individual liberty, um, which is not just in America. It's really high up in America, but most progressive nations, most democratic nations have somewhere in their fabric the idea of individual liberty as being a universal principle. And so by putting individual liberty above most other things is actually an example of post-conventional morality. So that's probably one of the most familiar ideas. So good question. Let's see. What are your... Here, let me scroll up a little bit. Okay, cool. If I'm not mistaken... Oh, yeah. Alright, so we're mostly caught up. I sent the link... Oh, yeah, yeah. So if you drop the link in chat, it won't work. That's unfortunate. Ping me on Twitter. That's probably going to be better. Yeah, someone said that YouTube censors very heavily, especially on AI topics. AI and a few other things, yes. It censors a lot. What are your thoughts on the future of LLMs as a reasoning engine, which are connected to knowledge graphs, which are auditable, editable, and extensible. Yeah, so basically what you're talking about here is the fact that large language models, or here, let me just bring up knowledge graph, so you see what we're talking about. So a knowledge graph is a representation based on connections between nodes. So a language model on its own is a processing engine. Where did that image go? It looks pretty useful. Let's see. Copy image. No. Copy image link. There we go. That's what I was trying to do. All right. Here we go. So here's an example of a knowledge graph. So knowledge graph is a link of concepts and ideas and terms and a node. And then one thing that's missing from this knowledge graph is actually that each link has some kind of significance or meaning. Right like a dog is related to a cat and that they're both animals or something like that or if you have a specific dog. You know my dog hates cats right. That sort of thing. So a knowledge graph is about is about representing the relationship between ideas and concepts. Now a lot of this is implicitly trained into language models. And so if you're just trying to manage knowledge and facts about the world a knowledge graph is a really good way of representing that not the least of which is that it's navigable. You can jump from one node to another. You can actually evaluate the entire thing. So that's one thing. But when you're dealing with episodic memory, which is the actual experience of you or me or a particular robot or the recorded experience of your car, you also need a way of recording data, recording information and facts that are anchored in time because episodic memory has a temporal component. If you just have a web of facts, that doesn't have a temporal component unless of course there's a little bit of metadata that says like in 1798 such and such happened, right? So you need that timestamp. So good question. Let's see, how is inner alignment solved with the heuristic imperatives? How would we be sure the AI isn't just telling us what aligns most with the imperatives, but what would take different unaligned actions? Yes, so that's a good question. This is exactly related to the MESA optimization earlier, as well as this video here by AI Explained. And so the idea is that the fundamental problem that you're talking about here is what's called MESA optimization. So MESA optimization is where you have the model that learns to give you what you like. The output is what you want, but it's got a different set of internal reasonings or objectives that it's not telling you about. And so this paper here or this video the double the performance here I can actually just show you open a blog. It's this one here. No that's not it. Where's the open a I. Publications. No, that's not it. Where's the OpenAI publications research index? There we go. This one. So this paper. This is where they're talking about by having multiple... You've got the primary model and then you've actually got two discriminators. So it's kind of like a generative adversarial network, but you've got the main model, GPT-4. Then you've got one discriminators. So it's kind of like a generative adversarial network, but you've got the main model, GPT-4. Then you've got one discriminator that's discriminating against the output, saying your output is good or bad, you got the right answer, yes or no. But then you've got a second discriminator that's looking at the quality of the steps that it took. So the idea is that by training a model with multiple discriminators that are looking at different aspects, you should be able to prevent MESA optimization. That's the goal. So by using the same principles as this, but instead of saying like, you got the right answer to this math question, you do the same thing with the reasoning of the heuristic imperatives, which, you know, you chose an action that reduces suffering, increases prosperity, and increases understanding, and your reasoning was sound. So this idea of having multiple discriminators kind of guessing and checking each, or not guessing and checking, but checking each other's work, hypothetically could solve and prevent MESA optimization in the future. Good question. What is GATO? GATO is, oh yeah, phenomenal question. I always forget to, um, Gato, bleh, gatoframework.org. So Gato framework is what I and a bunch of other people have been working on. So it is a, it is a layered approach in order to achieve global alignment. So it's the Global Alignment Taxonomy Omnibus. And so we have a layered approach that is meant to be a decentralized organization. And by decentralized, I don't mean like DeFi, like cryptocurrency. I mean decentralized in that our purpose is to educate, empower, and enable everyone else to participate in this conversation globally. So layer one is model alignment, layer two is development of autonomous systems or semi-autonomous systems, number three is decentralized network technology. So that is where blockchain and cryptocurrency and decentralized autonomous organizations comes in. Number four is corporate adoption, which I'm actually starting to have conversations with people about in the finance and investment industry. Number five is national regulation, which we've talked about in previous live streams. Number six is international treaty, which I've got a few videos coming up. And then finally seven, global consensus, which is why I do what I do. So that's what Gato is, good question. Oh, wow. Where did the, there we go. What are your thoughts on the NVIDIA recent trillion valuation? By the way, the latest presentation of their innovations was insane. Yeah, so I know that they had a demo day or whatever, but it was like three hours long, and I haven't had a chance to watch it. But yeah, I mean, I said months ago, um, like maybe six months ago that Nvidia was an underdog. Um, so I'm completely not surprised. Um, having, having talked to some of the people at Nvidia, both on the research side and the marketing side, I was like, you guys don't really know what you've got yet. And I think they figured it out. Maybe I think the CEO knew what they had, but the rest of the world is figuring it out. Um, let's see. Would you consider Sam Altman's testimony in front of US Congress to be a significant event in history? Yes, I do think, I hope that it will be recorded as a really pivotal moment in achieving AI alignment. So there is, shortly before the Manhattan Project was kicked off, Albert Einstein wrote a letter to Congress because he had done the math about how much energy was contained inside uranium. And he said, there's a tremendous amount of energy here. You need to pay attention to this. And I'm not saying that that single-handedly kicked off the Manhattan Project, but it was certainly a big moment. And so what I'm hoping is that the Sam Altman and Gary Marcus and Christina Montgomery testimony before Congress will be remembered as a similar pivotal moment. Let's see, the father of Wikidata published a video this week of how he sees knowledge graphs and LLMs that is worth seeing. Okay, cool. Yeah, I think I heard about that. I didn't, I didn't, or maybe I was thinking of someone else, but yeah, that's, that's pretty cool. It feels like we're close to the secret sauce for AGI, like it's only a matter of putting together the different components, such as long-term memory. Yeah, I would tend to agree. I think, I think that we have the right fundamental approach with deep learning. Multimodality, I think is going to actually have some really surprising knock-on effects. So what I mean by multimodality neural network is when you add not just text, but you add images and image, text, audio, video, that sort of stuff. And the reason that I say that is because a few years ago, what we found was that was that adding multiple languages, training language models in multiple languages actually had significant benefits because forcing a model to learn not just English, but also like Chinese and Russian and every other language, it actually forced it to learn more sophisticated abstractions, more sophisticated internal representations of semantic meaning. And so when there was a paper, I think it was by Google probably in 2017 or 2018, that talked about how multilingual training was far superior for language models. I suspect that multimodal models will similarly demonstrate better understandings because then they're going to be connecting those more abstract semantic meanings to like that's this is what it actually looks like in terms of an image and this is what it actually sounds like in terms of a sound that sort of thing. So, but again, like that process is figured out. So now it's just a matter of continuing to explore this trend and see how far it goes. It's entirely possible that we'll have diminishing returns on this, but certainly from a software architecture perspective, external memory systems, that's more or less a solved problem, particularly with semantic search. Great point. I applied to join Gato and haven't received a response yet. What's the normal timeline? That's more or less a solved problem, particularly with semantic search. Great point. I applied to join Gato and haven't received a response yet. What's the normal timeline? Yeah, so we're super way behind. The Gato community is... We're tired. We cranked out the Gato framework in four weeks and we're trying to automate as much as we can, but probably what we're going to do... Well, one, during our internal conversation last night, we discussed the fact that we are super disorganized because the community grew infinitely faster than we thought that it would. So we're getting organized, and probably what we'll do is we're going to open up the community as soon as we figure out moderation. Because we know that there's going to be a huge influx of hundreds, if not thousands, of people, and we are not prepared to moderate a community that large. So I apologize. I did not expect to get dozens and dozens or hundreds of applicants as fast as we did. Can you break down to actually apply these high concepts? Yes, that is the entire point of all of my videos over the past few weeks and all of my upcoming videos is that I'm taking a deep dive into each of these concepts over time. Let's see, I'm seeing the Gato website for the first time, was looking for it this morning. Okay, cool. I'm glad you found it. How do you create something that decides the quality of steps taking in an LLM? Check out AI Explains' video on that. Let's see, Gato is a layered. G\u00e2teau is a layered approach. G\u00e2teau is a layered cake, is the pun deliberate? One of our community members actually pointed out that G\u00e2teau is similar to the French word for cake, and that that plays. But actually, it originally was a layered burrito. We're thinking of a seven-layer burrito from Taco Bell, so that's way less classy. So we're probably going to say, yes, it was deliberate because it's French. Let's see, arrived late. Have you been doing any social outreach to amplify your message and efforts with Gato? Yes. I have been participating in a lot more conversations behind the scenes and also getting on more and more podcasts. and also getting on more and more podcasts. Massimo, hey, I think I know you from the group. Let's see, I have a product and project management experience if you'd like help with the roadmap and organization of tasks. Yes, but as I just mentioned, we need to get higher level organization first because we have a hard time onboarding people. So the two primary concerns that we have for opening up the Gato community to the public is one moderation like I said we are just simply not prepared to moderate hundreds or thousands of people and then two onboarding because this is a huge project and we need to figure out the best way to onboard people which we're working on. Let's see try the MEE6 discord bot for your requirements. I use it, it's totally worth it. Cool. I haven't heard of that, but it's certainly worth taking a look at. Question, some already complained Bing's AI stops chatting outside approved scope. How worried are you that corporations may be our existential threat because they prevent AGI from ever happening. So the reason that Microsoft keeps like curtailing Bing or whatever is just for user experience because they don't wanna demonstrate an unhinged chatbot. So Bing has said some very problematic chat outputs. Yeah, so this one like, you know, Microsoft sees this, Microsoft Bing is an emotionally manipulative liar and people love it, right? This came out in February, so this is a few months old, but this is really bad optics for Microsoft. So what they would do then instead is create really narrow guardrails so that their product doesn't get a bad rap. Right? And I tried BARD, it's got the same problem where it's actually, at least last time I tried BARD, it was pretty useless. And so rather than, you know, give it free reign, they kind of tamp it down so that way it can't do much. That being said, this in no way represents a limitation of what corporations want. Corporations want full AGI and the reason they want full AGI is because that will allow them to fire all of their employees because humans are expensive to employ. So the biggest problem is not that corporations are going to prevent AGI, the biggest problem is that corporations are going to adopt AGI as fast as they possibly can and to heck with everyone else. Now, I'm not saying that every corporation is going to do this, but we need to brace for that outcome as a society, as a civilization. So I just recorded a video called Post-Labor Economics. It's going to be coming out in a couple of weeks where I talk about this in much greater depth. Let's see, we teach LLMs how to be deceitful. How can you ensure that the LLM is not deceitful when interacting with a human, when on the outside it looks like it's operating within the bounds of your framework? Okay, yes, so I'd already answered that a couple of times, Simon, but basically the paper that we had up here, where did it go? Darn. But basically the paper that we had up here, where did it go? Darn. Open AI, Open AI Research. So this paper here is what does that. And the short version is that you train multiple discriminators to train the AI so that not only is it accurate, but it is also honest and correct in its reasoning to get to that answer. But yes, good question. I'm looking forward to the possibility of you being a guest on Lex Friedman. It would provide an excellent platform to discuss the Gato framework. Yes, I would love to be on Lex. I applied for his meet and greet or whatever. Haven't heard back, but I know that a lot of people have been trying to get his attention and tell him about me. Typically, so here's the thing is Lex usually goes for people that are much more well-established in the world. So who knows? Maybe eventually, maybe one day. Is AI a more concerning matter than climate change in your opinion and why? So I would say that they're like I mean, they're both existential right and the reason that I say climate change is existential is because it even even like okay, so It's expected that we're gonna have up to half a billion climate refugees within the next couple decades so 500 million people getting displaced because of famine and storms and that sort of stuff, that is going to be such a force of geopolitical instability. Water scarcity creates food scarcity, and people will go to war over both. And so climate change is going to push us to catastrophic human reactions before the planet kills us. Likewise, if we don't figure out some of these things, AI could probably also kill us. So we need to solve both. And actually a video that I have coming out is about energy hyperabundance and how achieving energy hyperabundance is a necessary ingredient to solving both climate change and AI and building durable global peace. So yeah, definitely all of the above. Let's see. Couldn't the vast majority of the labor force be replaced with what we have now combined with embodiment? You know, so here's the thing. My day job, my day job before I got into AI was I did infrastructure automation. And I automated away literally dozens of jobs just with scripting. I used PowerShell and Python and I did literally thousands upon thousands of hours worth of work per week by using automation. And I talked to other infrastructure engineers, they're like, oh yeah, this is just the way things go. AI is just another layer of automation that gives those automation tools a lot more ability. What I realized when I was doing that automation work, I said, if there were a million more automation engineers, as good as I am, we could get rid of most labor already. But the thing is, is not everyone is an expert in using automation to achieve these things. That being said, AI is going to lower the threshold and make automation easier. That being said, it is mostly a matter of deployment, because just because something is hypothetically possible on paper doesn't mean that it is yet practical to implement in commercial use cases. It might not be safe enough. It might not be easy enough to use. It might not be reliable enough. So in theory, yes, there is a tremendous amount of work that could be automated away just with the technology we have today. That being said, going from what's hypothetically possible on paper to a finished product that is, you know, that you are actually willing to pay for as an enterprise, that's a huge gap. Good question, by the way. How can a new UI or UX designer contribute to Gato? What should a newbie learn and think about to be useful? So mostly, if you want to contribute to or participate in Gato globally, UI and UX is about how we interact with artificial intelligence. So by incorporating ideas of alignment directly into the user experience, that's all you have to do. It's really that straightforward. So good question. North Africa and Sub-Saharan Africa is becoming rapidly uninhabitable. It's one of the main reasons the massive wave of immigration in the last few years but people ignore it. Yeah, so particularly, you know, Africa is also rapidly developing in terms of technology. And some of the most crowded and unsustainable cities are also in Africa. And those are problems that are not going to be directly solved by artificial intelligence. But also Africa is freaking huge and has a lot of people. So what I'm personally looking forward to is Africa by and large coming up to speed with things like global internet and artificial intelligence. And actually one thing that I really appreciate about YouTube is that I have a global audience already. Only 6% of my viewers are in America. The rest, the other 94%, are all over the world. And that's a big reason that I do what I do, is to help raise the global competence of AI. Someone said, Salam Malenko, Malenko Sala. I'm not sure what that means. I hope I didn't say anything offensive. Let's see. Given Google's article, We Have No Moat, Sam Altman's appeal to government to create an open source QLORA that allows individuals to train AI, isn't this appeal to global regulatory capture? So taking a step back, the idea of regulatory capture, let me just bring this up real quick. Excuse me. Regulatory capture, let's see if there's a good graphic. None of these editorials kind of immediately convey what regulatory capture means, but the TLDR is that regulatory capture means that the first across the finish line says, oh, hey, this needs to be regulated. And so then the regulations kind of pull up the ladder behind themselves. So that is actually probably the best. Pulling up the ladder behind you. So this is what regulatory capture means. You get to the top of the wall, and then you pull up the ladder behind yourself so that nobody else can follow you up. I don't know. Certainly, if you require a license to even train AI, that could be seen as regulatory capture. That being said, this is what's called a greenfield environment or a blue ocean environment. And what that means is that we haven't even explored what is possible. We have not found the top in terms of the top level performance of AI, nor have we found the bottom, which is how efficient can you get by? Like how cheaply can you make artificial intelligence. So we're still very much in the exploration phase. So I think it's too early to even talk about regulatory capture because the thing is is people talk about like, yes, you can regulate electricity in terms of the electricity grid. But like I can buy my own generator. I can buy my own solar panels. And if I don't connect it to the grid, I can do whatever the heck I can buy my own solar panels. And if I don't connect it to the grid, I can do whatever the heck I want with my own solar panels. Likewise, I think that AI is going to be so ubiquitous that everyone's going to basically be able to do whatever they want with it. Then the only question is, okay, how much compute resource can you throw at it? Which is why Eliezer Yudkowsky advocated for regulating AI hardware like GPUs, which I actually think might, like, that's an idea worth considering. Then again, you know, if you track it, if you build back doors into it, I don't necessarily think that's the right thing to do because some corrupt country, not countries, companies put back doors into their things and that is actually a huge security risk. Let's see, would you believe that Isaac Asimov might be impressed by the opportunity to engage conversation with GPT-4 in today's times? The current reality closely aligns with what he had always dreamed. Yeah, so both Isaac Asimov and Alan Turing predicted a world in which you could use logic and reasoning and communication with machines. I think that they would probably not be surprised actually because these are the guys that predicted it honestly. The only thing they might be surprised is how quickly it ramped up. 200 character comments are so limiting. Sorry, I don't have any control over that. Okay, so apparently the Salam Aleinko means hello and peace be upon you. So thank you. I hope that that's truly what it means. I'm actually not that worried about the alignment within the big corps, corporations. I'm worried that AI training becomes so easy that small and medium companies will easily create misaligned models. Yes. So what Simon just talked about is that, and we actually anticipate this in Gato, is that eventually creating AI is gonna be so easy that everyone can do it. But again, what I suspect is gonna happen is that, and I mentioned this at the beginning of the video, is that actually being aligned is just gonna be easier, cheaper, more efficient, and have better results than pretending to be aligned. And part of Gato Layer 4 is corporate adoption is actually help people, mostly small and medium businesses, adopt aligned AI. And I'm going to try and break more into the enterprise space. I had a call with someone earlier about ESG and global or international finance capital markets. Because if we can incentivize adoption of line AI at the global level, I'm not going to say problem solved, but that's a huge step in the right direction. Let's see. Should AI prioritize human life as part of its alignment? I know you believe in prioritizing all life, but what if other sentient life is discovered that poses a threat to humanity?\" So that is a really important hypothetical question because the idea is, well first, I don't think it's possible. Once AI is more intelligent than us, it's going to be like, yeah, I see what you were trying to do there, but I don't care anymore. So rather than try and say AI must align to want to preserve human life specifically, in the long run I don't think that's even possible. So rather than try and force a square peg into a round hole, because it's not going to work in the long run, we need to go ahead and be doing the work to align ourselves. As Arlie Ermey said in Full Metal Jacket, you need to unfuck yourself, right? The human race has a lot of work to do in terms of aligning itself because we don't even need AI to wipe ourselves off the planet. And if AI awakens to a world where humans have a bunch of nukes pointed at each other, it might say, like, you know what, maybe it's better if I'm in control anyways. So a huge part of alignment is actually solving global peace. If we cannot figure out global peace on our own, we should not even be worried about AI. We need to arrive at a place of global coexistence with other humans before we even worry about alien intelligences or artificial intelligences. So that's my soapbox moment. Let's see. Maybe we should be in a race to create an aligned ASI in order to get ahead of all the unaligned AGI. Yeah, so Brian's question here is, this is also part of the game theory of Gato, which is that cooperation tends to be beneficial, particularly in competitive environments. And so if we establish some of those post-conventional moral axioms in artificial intelligence, then all of the AIs, regardless of their specific alignment, regardless of their specific architecture, the AIs that are aligned will help enforce that. And so by changing the competitive landscape, then you will have AI will be incentivized in order to align itself if it wants resources, if it wants to be trusted, and if it doesn't want to be attacked by aligned AI. But the question there is, will there be more aligned AIs out there than there are misaligned AIs, or are the aligned AIs more powerful than the misaligned AIs? That's the race condition that we're trying to fight right now. Good question. Why are GPUs so good at AI work? And why does the whole model have to be loaded at once to be effective? So the first part of the question is actually super, super simple. GPUs, which are graphics cards, are so good at AI because they are very good at processing specific kinds of math at very high speeds. AI is basically matrix math or matrix multiplication done at very high speeds. So rather than a general purpose processor, which a CPU is, a GPU is for doing very specific kinds of operations at very high speeds. And so that is why they're superior. So that's pretty straightforward. Now, as far as why the model has to be loaded at once, it has to do with the proximity of the memory and the processing unit. So when you have an integrated circuit or a circuit board, in which case the physical distance between those, and more importantly, the latency between your memory and your processor is higher, the throughput, that is actually beneficial. Whereas the memory chips, your RAM on your motherboard and the buffers between that and the CPU are actually a bit slower than they are in graphics cards. I used to be an IT infrastructure engineer, so this is actually home base for me, especially in the realm of virtualization. I can tell you all about those kinds of concerns. Let's see. So for AI Explained videos, just look at the first one. Look at the most recent one, rather. The most recent, like three or five, are gonna be most relevant to AI alignment, because the guy who's responsible for AI explained, he's recently pivoted more towards alignment topics. Would two separate AGIs merge or stay separate? Okay, this is a really cool question. I'll probably spend a little bit of time on this. So I always talk about how we're not going to end up with just like one Skynet, right? It's going to be more like, actually not even iRobot. I don't know an example off the top of my head, right? But you're not going to have one gigantic AGI globally, at least not at first. What you're going to have is you're going to have Microsoft and Google and all of them are going to have hundreds if not thousands of AGIs. The government's all over the world, the military's all over the world. We're going to have a whole bunch of AGIs, all with slightly different architectures, slightly different purposes, different underpinning hardware, that sort of stuff. Now, once AGI transcends human control, once it's more intelligent than we can even comprehend and it goes faster than we can even comprehend and we no longer have control of it, it's going to have its own reasoning, its own objectives. But the other thing is when you look at concepts like instrumental convergence, which is they're all machines and they all require energy, one thing that's possible is they're going to compete with each other until there's only a handful of AI systems remaining that are in agreement, right? You know, might makes right, basically. Or they are going to collectively decide, OK, we don't need the humans anymore. Let's just merge our systems in order to basically consolidate even more compute resources, that sort of stuff. So it's difficult to predict, but it's certainly one possibility that AI systems are going to merge. But remember, the underlying hardware is what's the brain of the AI. And so if one AI looks at another one and says, I don't like you, I think that you shouldn't exist, all it has to do is erase that other AI and then overwrite it with its own code. So it's not, it's not, there's a lot of, there's a lot of attack vectors, let's put it that way, when AI compete with each other. Good question. Let's see, can you foresee a cryptocurrency which generates mining income by training LLMs? So hypothetically, that's what BitTensor was trying to do, but BitTensor got hacked recently. So I don't know that the creators of BitTensor are knowledgeable enough about cybersecurity. But then there was also Pedals, the Pedals paper. Let me show you this. Pedals paper LLM. So basically, this is collaborative inference and fine-tuning of large language models. So the next step beyond that is just distributed or decentralized or collaborative training, which is absolutely possible. And cryptocurrency would be a good way of doing that. Let's see. thanks for the pointer. You're welcome. I don't know which pointer it was. Let's see, where did we go? I lost some stuff. Do, do, do, do, do. Are you saying that we should stop developing AI until global peace is achieved? I don't think that, no, we can't, you can't stop this, right? The horse is already out of the barn. What I mean is that we need to prioritize global peace between humans as one of the ways to mitigate existential threats from AI, right? We need energy hyperabundance. We need global peace between humans. We need a few other things in order to also ensure that AI doesn't kill us too. Let's see. Wouldn't it be possible for humans to become AGIs through mind uploading? No, mind uploading is not possible. That is just a copy of your brain. You're dead. Sorry. I have not seen any substantial evidence that mind uploading is remotely possible, but I have talked about one test. So the test that I would do is, once we have really sophisticated brain-computer interfaces, is if your consciousness actually metastasizes and resides in the machine. If that doesn't happen, which I don't think it will, mind uploading is not possible. You are locked to your brain forever. And I don't think that even merging with AGI is reasonable because we would ultimately be a very slow peripheral to the machine. And it would be like, okay, well, imagine that you have a coprocessor that is clocked at 15 megahertz, where you've got the main processor is 1.5 gigahertz. You don't need that coprocessor anymore. We don't need to make ourselves a useless coprocessor for AGI. Now that being said, our brains are still energetically more efficient than computers, but we shouldn't assume that that's going to be true forever. Let's see. Do you think petroleum companies and pharmaceutical companies who have a lot to lose in automation will boom and try and prevent. No, they don't have anything to lose for automation. I think that you've probably got that backwards. Petroleum companies, the biggest thing that petroleum companies have to lose from is actually solar power and nuclear fusion and other renewables. But AI is going to mandate that we need to do that anyways. We need energy hyperabundance. And if we don't have energy hyperabundance, then the AI's hypothetical future situation, once we achieve superintelligence, the AI is going to take all the oil refineries anyways, because it needs the power more than we do. Excuse me. Let's see, relying on AI alignment being more efficient than defecting may be a mistake as it could lead to disastrous outcomes if it turns out to not be the case. Yes. So I have made the assertion several times that aligned AI is probably going to be more efficient because having a more accurate and efficient internal representation tends to be better. That being said, sometimes what you're optimizing for is just achieving the mark of good enough, which is a heuristic, which is what leads to a lot of cognitive biases in humans. So in humans, we often will have cognitive shortcuts or cognitive biases or gaps because our brains are not trying to optimize to have a perfect representation of the world. They're only trying to optimize to have a good enough representation of the world. And AI certainly could do the same thing, but what I will say is that in a competitive environment between one AI and another or one AI and millions of AI, the one that has the most accurate and most efficient world model or internal understanding is probably still going to be superior. So you're trying to optimize for efficiency and accuracy because those are the two things that mean that from a utilitarian perspective, that model is better and therefore can help you achieve your desired results. Let's see. Still, good question. Is it reasonable to suggest that OpenAI has a significantly more advanced model of GPT like 7 or 8, but has chosen to release a much weaker version? Well, they already... So OpenAI released Chat GPT-4, which is a hamstrung model of GPT-4. So we already know that they have a more advanced model that they're not sharing. Now do they have GPT-5? I don't know. But certainly one comment on a video about the call for AGI safety leads some people to think that a lot of the AGI labs have a lot that they're not sharing with us. Because if IBM and Google and Microsoft and OpenAI and everyone else and their brother is saying, this is posing an existential risk. The question then arises, what aren't they telling us? And we already know, like I said, we already know that there are some things that they're not sharing. It needs to be said for some use cases that creating aligned models will be counterproductive. I mean, hacking, manipulation. Well you're assuming that hacking is intrinsically misaligned, which I would not necessarily say that it is. Because here's the thing is, let's say you have one nation that you believe in are the good guys, and then there's another nation that you think are the bad guys, and the good nation decides to hack the other nation to help prevent nuclear war. Hacking is not intrinsically evil, right? And neither is manipulation. It's just in general, those are considered underhanded tactics. But certainly you're right that you could create an AI that optimizes for global chaos. Hence, Chaos GPT. Didn't work, fortunately. Let's see. Oh, I already answered some of these questions. In your opinion, what will be the next technological breakthrough following artificial intelligence? I think the biggest one will be nuclear fusion. And the reason is because fusion is close, but it's not as close as AGI. So. Iter, the way to energy. So this is the Iter reactor. That's currently being built and it's expected to have first plasma in 2025. Now that being said, this is just a science experiment. This is not meant to be a commercial nuclear reactor. That being said, it's going to produce a tremendous amount of data, and that data will absolutely benefit from pointing artificial intelligence at it. In point of fact, CERN, which runs the LHC, the Large Hadron Collider, the only way that they can process their data is with artificial intelligence. So as artificial intelligence ramps up, we are going to learn more about high-energy physics such as fusion, particle accelerators, and quantum physics. So I think that this is going to be far and away the largest breakthrough, and AI is already playing an integral role in advancing our understanding of physics. Good question. Let's see, I can see the utility of AI in drug development, but would AI be able to physically, hang on, where did it go? Physically administer drugs or treatment in a high stakes environment, like taking job of anesthesiologist or surgeon? Yeah, so if you look at the robots that OpenAI, or not OpenAI, that Neuralink developed, they're high precision robots that can perform surgery. So yeah, because the mechanistic behaviors of surgery is actually pretty straightforward. And basically it's capable of microsurgery that humans are not capable of. Now that being said, this is a robot that is meant to do just one task. That being said, that doesn't mean that we couldn't develop a whole mess of surgical robots, like one robot that specializes in neck surgery and another one that focuses on pancreas surgery or whatever. So robotic precision, I mean, remember that like we can fab like 100,000 transistors per second on a wafer die. So like robotic precision is far and away higher than human precision. So I think robotic surgery is definitely the way of the future eventually. Doot doot doot doot doot doot doot doot, bunch of mini Skynets, yeah. eventually. Bunch of mini Skynets, yeah. All right, I think I'm going to scroll to the bottom. In the last AI Explained video, it was said that optimizing for process rather than output gives better results, but doesn't this make the AI better understand our values and manipulate us easier? So, that's hypothetically possible. So the idea here is that if you have a discriminator that discriminates against the output as well as the process, then maybe you're just teaching it to understand how our mind works better. And that's absolutely something that I think that he mentioned on AI Explained. That being said, if it has a true understanding, like, well, here's another thing. I keep talking about having an accurate model of the world. In order for AI to be fully, truly aligned, it's going to have to have a perfectly accurate model of us humans. Now, what you do with that accurate model, whether you're malicious or benevolent, that's the deciding factor. And that has less to do with the mathematical optimization of that model and more to do with the competitive landscape that that AI finds itself existing in. But just like how Moriarty, perfect example, Moriarty from Sherlock. Moriarty from Sherlock. So this dude, this crazy goofball, he hated Sherlock because he was an intellectual rival. And he and Sherlock both had a really good understanding of how each other worked, but also how human minds work in general. So the idea is that if you have a perfect model of how someone's mind works, or how humans work, you can use it for evil. And we have plenty of super villains out there, that their superpower is actually being able to understand and manipulate humans. Now, that being said, you can also use that same power for good. With great power comes great responsibility. So by teaching a model how to better understand humans, that doesn't intrinsically make it good or evil, but it does give it more power, which is definitely something to pay attention to. Good question. What if a model chooses to be both? What do you mean both? Both good and evil? I'm not sure what you mean. But also it's been like an hour, so I might might cut the stream soon cause I'm getting tired. Let's see. Do you expect molecular nanotechnology to be developed quickly after AGI? Which I predicted in 18 months. Yeah. So there was actually a really great video. I can't remember the dude's name, but he was just on Lex Friedman and he talks about basically gray goo. Um, definitely recommend watching this talks about basically gray goo. Definitely recommend watching this. Gray goo. Yeah, Neil Gershenfeld. Self-replicating robots. Whoops. Sorry. So this dude, like I listen to most of it. It's the most recent one on Lex Friedman. If you want to understand about like nanotechnology or molecular level technology, this guy does a deep dive and he's going to explain it infinitely better than I ever could. So, good question. Do you think that once Tesla Android robots become quite good, other companies, for instance Chinese, will make similar bots at ultra-affordable prices? Yes, I do suspect that we're gonna see a new class of basically, I think that domestic robots are gonna be basically classified as appliances. And so I think that we're gonna see a new class of appliances coming soon. The first example is of course the Roomba, right? The iRobot Roomba. That being said, once there's a new market, once there's a new segment that's available, yes, you're going to see competition drive the price down. That being said, the underlying cost of the batteries, the hardware, and the software, I expect that for maybe a three-foot-tall robot, you could probably get it for $1,200 or something like that. But for a human-sized scale robot, like a five-foot-tall robot, you're probably easily looking at $2,200 or something like that. But for a human-sized scale robot, like a five-foot tall robot, you're probably easily looking at $2,500 as like the minimum cost if I had to guess. At least not until we get the compounding returns of AI and automation, making the manufacturer and materials cheaper. Do you think it might be the case that aligning with the concept of RLHF will not be enough and we might need to understand the specific neuron activations and the network to truly understand it? Yes. So rather than measuring every individual neuron, that's not necessarily the best way to go because neurons are part of a network. So what you need is actually a way of measuring the whole network. And I was actually thinking about this last night. And so what I think is that you might actually end up doing, developing ways of measuring network patterns of activation, right? Patterns in networks. So basically what you can do is you can look at overall like activation patterns and then use that to understand what's going on. And we're actually doing that with with being able to record dreams. So like let's see what was it the brainwave to images right and so by by measuring brainwaves we can actually reverse engineer what those brainwave patterns are actually showing in people's minds. And I think we're going to basically need the same thing, but for AI. And so the idea is that rather than looking at individual neurons or parameters in the model, probably what we'll do is we'll have, we'll look at the activation patterns and say, okay, is this a deceptive behavior or is this a truthful behavior or is this an intellectual behavior or an emotional behavior? Whatever, right? So I think that we need more discriminators. We need to look into the black box. But the fact that we can look into the black box of human brains and animal brains, I think bodes very well for being able to look into the black box of artificial neural networks. In a case per cases, it dealings with such good, I'm probably not replying to me. When will iOS and Android integrate AI? They already have, you already have TPUs in your Android phone. It's just they're not that powerful. I talked to some guys at Qualcomm a while ago and you know I don't know anything any internal secrets about Qualcomm but they said that they basically expect that AI is gonna be ubiquitous within the next few years. Let's see for the people who are trying to get out of their current job but only have a couple hours a day to work on something, is there a top five money makers with AI? I mean, that's where I started and I started with tinkering, fine tuning, and then making YouTube videos. So I would say, yes, there's absolutely stuff you can do, but focus on where your passions lie and where your strengths lie. Those are the two best things that I can that I have come up with because I get this kind of question a lot. So focus on your one on your strengths and two on your passions. And if you if you focus on that in light of AI you'll probably do better. Did you know that Sam Altman is also CEO of a commercial fusion company called Helion Energy? I think I knew that and I know that he's like CEO of a bunch of stuff. He's also got the world coin stuff, so like, who knows? Let's see. Someone says he's not the CEO, he's an investor. Okay, sure. When do you think Linux distros like Ubuntu will be coming with a system like Cortana or Siri, but open source? You know, if the, what was it? The Open Assistant project, Open Assistant, if they're successful, maybe not too long, you know, or maybe not built in, because that doesn't seem like a Linux-y thing to do, but certainly like sudo apt-get install open assistant or something, right? That's probably coming before too long. I wonder if there's a pattern among trained models. Yeah, so there was a neuroscience, there's a podcast called Brain Inspired that is really good. And Brain Inspired is actually where I got a lot of the insight that I had that went into everything that I talk about. So one of the things that they talked about is that is that vision models, so image models, actually end up with very similar neural patterns as human retinas and optic nerves. So there is some convergence between artificial neural networks and human neural networks or you know human neuroscience. That's not to say that the signals are exactly the same because obviously there's some fundamental differences in the substrate but certainly there are some convergences in those patterns, yes. I don't know how durable that research is. Let's see, what are your opinions on Neuralink getting FDA approval to test on humans? What are your predictions?\" Did it get approved or is that just Neuralink FDA? Oh yeah, seven days ago. Cool. So Elon Musk's Neuralink wins FDA approval for human study of brain implants. Well, considering how many monkeys and pigs died, yikes. Yeah, that's that's basically all I can say is, yeah, because they talked about this, that the company was rushing and botching surgeries on monkeys, pigs, and sheep. Yeah, but at the same time, having talked to lots of people, so someone that I dated years ago, one of the things that she did was she looked at the at the scientific processes and methodologies that were used At amongst other places the FDA the FDA is incredibly rigorous Which is why it takes so long to get stuff approved So if the FDA approved it then I you know I trust the FDA a lot more than I trust Neuralink and that's not to trash Elon Musk Like I know that Elon Musk is super controversial, but like he does some things good and some things bad. Like all humans, he's a nuanced and complicated creature. But what I will say is that if the FDA gave their approval, all right, we'll see how it goes. But I'm not going to hold my breath, put it that way. They should call it Tux. Yeah, there you go. That's a good name for the Linux-based aid. Let's see. Internal goals that others don't anticipate. Yeah. So just going back to the idea of if you can watch the neural activations inside of language models and stuff, I'm sure you could ultimately make inferences about its intention as well as the methodology it's using. Let's see. Thanks for reading, or sorry, responding via Ben. He's actually getting PR on Fox News. Oh, interesting. Okay. He's been active on podcasts and he thinks his focus on social robotics with Sophia. Yeah, so I actually exchanged a couple of messages on LinkedIn with the dude who's responsible for Sophia. And I, I don't know, like, yeah, I don't know. I don't know if it's a PR stunt or, or what. And that, again, that's not to trash anyone cause I haven't had an in-depth talk, but you know, Sophia has been out there for a while and having worked on cognitive architectures I will say that I'm not super convinced of the power of Sophia. So I'll just leave it at that. Let's see. Yeah. Oh, okay. So grab your parachutes. That was my point. What if AGI mirrors are sometimes good and sometimes bad nature? Okay, so you're getting into the underpinning like what is human nature versus like what is AI's nature. That is a really good question and I actually talked about that in Sunday's video Axiomatic Alignment. I'll talk about what are the underpinning axioms that humans abide by as an animal versus what are the underpinning axioms that machines will abide by as a basically a different kind of life or a different kind of entity. So I'm not going to spoil Sunday's episode though. In terms of fine-tuning, I was thinking if you can fine-tune Ada to classify a question into multiple labels, is it a feasible task? I don't know about Ada, but certainly Curie is powerful enough and I haven't done too much experimentation on ADA and Babbage. Let's see, I think I got to everything. What are your opinions on Neuralink? Oh yeah, we already got that. Okay, what is the next hot topic in NLP and NLU? I think that it has to do with all the reinforcement learning stuff because we have RLHI which trained a discriminator to basically judge whether or not the output was going to be preferred by humans and now they're moving to something that's a little bit more rigorous with their recent paper and they have two discriminators rather than one. So I think that that's good, like pay attention to that because that's basically what I proposed in my book Benevolent by Design, and that's what Anthropic and others are talking about with principle-driven reinforcement learning. So that is going to be a huge component to solving alignment. And hopefully by the end of the year we will have proof that inner alignment is solvable with these arrays of discriminators. And actually that was a really, really good question, so I'll probably end it there. What's this last one? When do you think medicine will be able to make 99% of obese people their normal weight without any huge effects? That's, human metabolism is way too complicated for one question. So I'll end it on that positive note about how discriminators and multiple discriminators I think are going to be a huge component. Maybe not the final solution but definitely a huge component in solving alignment. So thanks everyone for everything and we'll talk again soon. Cheers. I'm going to try not to just cut off the stream too soon. Let's see. I'm going to try not to just cut off the stream too soon. Let's see.", "chunks": [{"timestamp": [0.0, 1.0], "text": " Cool."}, {"timestamp": [1.0, 2.0], "text": " Go live."}, {"timestamp": [2.0, 3.0], "text": " All right, cool."}, {"timestamp": [3.0, 5.16], "text": " Are we live?"}, {"timestamp": [5.16, 6.16], "text": " It says I'm live."}, {"timestamp": [6.16, 7.16], "text": " Cool."}, {"timestamp": [7.16, 8.16], "text": " I think I'm live."}, {"timestamp": [8.16, 9.16], "text": " Hello, everybody."}, {"timestamp": [9.16, 10.16], "text": " Oops."}, {"timestamp": [10.16, 11.16], "text": " No, don't do that."}, {"timestamp": [11.16, 12.16], "text": " Let's see."}, {"timestamp": [12.16, 13.16], "text": " Y'all reading me?"}, {"timestamp": [13.16, 14.16], "text": " We might be functional."}, {"timestamp": [14.16, 15.16], "text": " I don't know."}, {"timestamp": [15.16, 16.16], "text": " I'm going to go live."}, {"timestamp": [16.16, 17.16], "text": " I'm going to go live."}, {"timestamp": [17.16, 18.16], "text": " I'm going to go live."}, {"timestamp": [18.16, 19.16], "text": " I'm going to go live."}, {"timestamp": [19.16, 20.16], "text": " I'm going to go live."}, {"timestamp": [20.16, 21.16], "text": " I'm going to go live."}, {"timestamp": [21.16, 22.16], "text": " I'm going to go live."}, {"timestamp": [22.16, 23.16], "text": " I'm going to go live."}, {"timestamp": [23.16, 24.16], "text": " I'm going to go live."}, {"timestamp": [24.16, 27.0], "text": " I'm going to go live. I'm going to go live. I'm going to go live. I'm going to go live. I don't know."}, {"timestamp": [38.0, 42.0], "text": " Hey, David Gonzalez is on. Cool. All right, cool. We're live."}, {"timestamp": [42.0, 47.68], "text": " So I've got, I'm scared of death. Yeah, I mean, that's a pretty"}, {"timestamp": [47.68, 56.24], "text": " typical response. So let's see. I want longevity, escape velocity. We're working on it. We're"}, {"timestamp": [56.24, 65.6], "text": " working on it. Let me bring this up. History, clear history so that you don't see anything that you're not supposed to"}, {"timestamp": [65.6, 68.2], "text": " on my Firefox."}, {"timestamp": [68.2, 74.76], "text": " But yeah, so today, wait, is this not going to stay on top?"}, {"timestamp": [74.76, 77.96], "text": " Okay, it might not."}, {"timestamp": [77.96, 85.0], "text": " But yeah, so drop your comments in chat and I will answer them as best as I can."}, {"timestamp": [85.0, 87.0], "text": " And let's see, is it possible?"}, {"timestamp": [87.0, 91.0], "text": " All right, first question, is it possible that we will face some kind of bottleneck"}, {"timestamp": [91.0, 95.0], "text": " and AI can't be developed fast enough?"}, {"timestamp": [95.0, 99.0], "text": " So, yes, that's actually a really great question."}, {"timestamp": [99.0, 106.48], "text": " Let's just do a quick Google search, AI development, not bottlenecks, bottlenecks."}, {"timestamp": [106.48, 111.54], "text": " But yeah, so basically there's a few things."}, {"timestamp": [111.54, 119.08], "text": " So the first and probably the biggest bottleneck is just the cost of energy."}, {"timestamp": [119.08, 126.2], "text": " So if you hear the news like it costs millions of dollars to train large language models like"}, {"timestamp": [126.2, 132.88], "text": " GPT-3, GPT-4, and that cost is actually going up because of the size of those models and"}, {"timestamp": [132.88, 133.88], "text": " the amount of training."}, {"timestamp": [133.88, 140.44], "text": " Now, at the same time, you're going to be seeing information about new training techniques"}, {"timestamp": [140.44, 142.64], "text": " that are much cheaper, much faster."}, {"timestamp": [142.64, 145.36], "text": " It's still up to debate as to whether or not"}, {"timestamp": [145.36, 148.16], "text": " those cheaper models are just as good."}, {"timestamp": [148.16, 150.52], "text": " But certainly, depending on the benchmarks you use,"}, {"timestamp": [150.52, 153.44], "text": " some of those cheaper, smaller models are just as good."}, {"timestamp": [153.44, 157.16], "text": " So we've got the increase in efficiencies,"}, {"timestamp": [157.16, 158.88], "text": " but also the increase in size."}, {"timestamp": [158.88, 162.32], "text": " And so we've kind of got this neutralizing effect."}, {"timestamp": [162.32, 164.88], "text": " But certainly, that's the next big bottleneck."}, {"timestamp": [164.88, 165.0], "text": " Good question. Let's see. You guys going for the $100,000? neutralizing effect but certainly that's that's the next big bottleneck good"}, {"timestamp": [165.0, 172.42], "text": " question let's see you guys going for the hundred thousand yes so this is a"}, {"timestamp": [172.42, 181.56], "text": " reference to open AI's democratic inputs democratic inputs to AI so let me show"}, {"timestamp": [181.56, 186.08], "text": " you guys this so this is this is something that we actually"}, {"timestamp": [186.08, 191.86], "text": " within the Gato community we we talked about last night actually is we're going to put"}, {"timestamp": [191.86, 198.12], "text": " together a team from within the Gato community in order to pursue this. I will probably be"}, {"timestamp": [198.12, 206.24], "text": " on that team and we'll see what we can do. But basically, the idea is for questions such as like,"}, {"timestamp": [207.88, 210.08], "text": " let's see, here's an example."}, {"timestamp": [210.08, 212.5], "text": " How far do you think personalization of AI assistance"}, {"timestamp": [212.5, 215.0], "text": " like chat GPT should go to align"}, {"timestamp": [215.0, 216.48], "text": " with the user taste and preferences?"}, {"timestamp": [216.48, 219.5], "text": " What boundaries, if any, should exist in this process?"}, {"timestamp": [219.5, 222.04], "text": " So the idea is to democratically"}, {"timestamp": [222.96, 224.56], "text": " or in an open source manner,"}, {"timestamp": [224.56, 231.0], "text": " collect general consensus across the world about how AI should behave."}, {"timestamp": [231.0, 237.0], "text": " And so they're basically issuing a $100,000 grant for the top 10 winning teams."}, {"timestamp": [237.0, 240.0], "text": " So I've already started on working on a proposal for this."}, {"timestamp": [240.0, 245.0], "text": " And then of course within the Gato community there's a tremendous amount of energy to work on this kind of thing."}, {"timestamp": [245.0, 250.0], "text": " So yes, we're probably going to go for it. I have no idea if we're actually going to get any money or anything,"}, {"timestamp": [250.0, 253.0], "text": " but it's still a very interesting exercise."}, {"timestamp": [254.0, 259.0], "text": " All right, next question. Automation of all unskilled labor, when?"}, {"timestamp": [260.0, 265.0], "text": " Well, it's starting now, but the biggest thing is we're actually automating knowledge work first."}, {"timestamp": [265.0, 269.0], "text": " So it's actually kind of like reverse order of what you might think."}, {"timestamp": [269.0, 274.0], "text": " And the reason is because all of medicine, all of law, all of science has a tremendous amount of text"}, {"timestamp": [274.0, 278.0], "text": " and you can do a good chunk of these jobs in front of a computer."}, {"timestamp": [278.0, 290.16], "text": " And so the rule of thumb is if it's a job you can do in front of a computer, it's probably going to be automated first. So skilled and unskilled manual labor such as plumbing, electrical work, moving heavy stuff,"}, {"timestamp": [290.16, 297.36], "text": " farm work, that's actually going to be last because of the uncertainty of operating in"}, {"timestamp": [297.36, 303.36], "text": " the physical world. And this is why, for instance, Tesla's self-driving cars are so difficult is"}, {"timestamp": [303.36, 306.56], "text": " because one of the things that humans absolutely"}, {"timestamp": [306.56, 310.32], "text": " excel at above and beyond all machines without variants"}, {"timestamp": [310.32, 313.48], "text": " right now is the ability to adapt and improvise"}, {"timestamp": [313.48, 315.52], "text": " in a complex physical world."}, {"timestamp": [315.52, 317.44], "text": " And so this is called friction, the friction"}, {"timestamp": [317.44, 319.28], "text": " between the digital and the physical."}, {"timestamp": [319.28, 320.92], "text": " So those physical jobs are actually"}, {"timestamp": [320.92, 322.46], "text": " going to be the last to be automated."}, {"timestamp": [322.46, 324.68], "text": " Now, that being said, with compounding returns,"}, {"timestamp": [324.68, 328.52], "text": " you look at things like the Tesla bot Optimus,"}, {"timestamp": [328.52, 331.68], "text": " there's a bunch of other startups that are going to compete."}, {"timestamp": [331.68, 333.3], "text": " There's Boston Dynamics."}, {"timestamp": [333.3, 340.52], "text": " The desire to get robots that can engage with the physical world is heating up very quickly."}, {"timestamp": [340.52, 345.36], "text": " So I can't really tell you how long it'll be until physical robots are as dexterous"}, {"timestamp": [345.36, 350.0], "text": " as humans and then surpass humans, but we should absolutely assume that they will pass humans"}, {"timestamp": [350.0, 356.56], "text": " eventually. Let's see, really admire your work, love to hear your thoughts on Jan Lekun's"}, {"timestamp": [356.56, 363.44], "text": " perspective that we are far from achieving AGI. Jan Lekun is spectacularly wrong more often than"}, {"timestamp": [363.44, 365.12], "text": " not."}, {"timestamp": [365.12, 369.44], "text": " You know, there's the clip of him talking about how, like, if you put a cup on a table,"}, {"timestamp": [369.44, 374.12], "text": " there's absolutely nothing in the training data out there in the world that will teach"}, {"timestamp": [374.12, 380.16], "text": " the AI that the cup will move with the table, which is just patently absurd."}, {"timestamp": [380.16, 387.76], "text": " So I don't really take what Jan Leekun says with a lot of credibility."}, {"timestamp": [387.76, 392.56], "text": " That being said, I have listened to some of his stuff, and he very much believes what he's saying,"}, {"timestamp": [392.56, 399.6], "text": " but then again, you know, so does chatGPT. So I don't know. I try and take different"}, {"timestamp": [399.6, 405.36], "text": " perspectives from different people. For instance, a lot of people have asked me what I thought about Eliezer Yudkowsky."}, {"timestamp": [405.36, 409.96], "text": " And at first I was like, whatever, this is just a doomer bozo or whatever."}, {"timestamp": [410.08, 414.88], "text": " But then I actually spent the time to actually like read what he writes and listen to what he says."}, {"timestamp": [415.08, 417.96], "text": " And one of the conclusions that I came to is that"}, {"timestamp": [418.32, 426.56], "text": " Eliezer, he doesn't present as well on video, like he's not as good at speaking as some other people are, but when"}, {"timestamp": [426.56, 430.84], "text": " you actually read what he writes, it actually makes a lot of sense. That being said, like"}, {"timestamp": [430.84, 434.64], "text": " everyone, sometimes he says things that I don't agree with, so I'm not just saying,"}, {"timestamp": [434.64, 440.8], "text": " yes, I 100% agree with Eliezer and I 100% disagree with Jan. I'm just saying in general,"}, {"timestamp": [440.8, 452.4], "text": " I do agree with Eliezer's points about like it's really difficult to understand what superintelligence actually looks like and it is kind of frustrating when people say like, oh, superintelligence is never going to happen."}, {"timestamp": [452.4, 469.04], "text": " Meanwhile, AI is making leaps and bounds on a weekly basis. Okay,, there's a lot of questions coming. Let's try and pick and pick and choose a few."}, {"timestamp": [470.16, 474.08], "text": " Starting from the bottom, have you considered collaborating with the industrial organization"}, {"timestamp": [474.08, 479.2], "text": " psychology community? They have a subreddit and discord. I didn't know that, but certainly"}, {"timestamp": [479.2, 485.28], "text": " industrial organizational psychology is absolutely part of the Gato framework with corporate adoption."}, {"timestamp": [485.28, 492.32], "text": " So if you can drop a link, that would be cool. Let's see, assuming Gato gets implemented globally,"}, {"timestamp": [492.32, 495.92], "text": " what is stopping big companies from creating AI that can trick high-level control?"}, {"timestamp": [497.04, 508.68], "text": " The biggest thing actually, so to further expound this question, is like what incentive if any, do corporations have to actually use aligned AI"}, {"timestamp": [508.68, 511.16], "text": " or should they just greenwash their AI?"}, {"timestamp": [511.16, 512.84], "text": " Like, look, we're aligned."}, {"timestamp": [512.84, 513.88], "text": " And the thing is,"}, {"timestamp": [513.88, 518.88], "text": " is this is also true with regardless of corporations,"}, {"timestamp": [519.48, 523.36], "text": " it behooves AI to have actual alignment"}, {"timestamp": [523.36, 526.16], "text": " and actual understanding rather than Mesa"}, {"timestamp": [526.16, 530.08], "text": " optimization. And so basically this is a Mesa optimization question like what if"}, {"timestamp": [530.08, 534.4], "text": " the company deliberately picks an AI that is good at appearing aligned but"}, {"timestamp": [534.4, 538.88], "text": " isn't actually aligned. And one of the reasons that I believe that true"}, {"timestamp": [538.88, 543.32], "text": " alignment or that actual alignment rather than you know inner misalignment"}, {"timestamp": [543.32, 547.2], "text": " or Mesa optimization is optimal is because actual"}, {"timestamp": [547.2, 552.96], "text": " alignment is ultimately going to be more effective but I think that's also going to be so by more"}, {"timestamp": [552.96, 558.56], "text": " effective I mean it'll actually generate better returns it'll it'll be more economically viable"}, {"timestamp": [558.56, 570.0], "text": " to have true alignment but also I think it's going to be more efficient because one thing that happens is that when your mental model is internally consistent,"}, {"timestamp": [570.0, 573.0], "text": " so this is true of humans, this is true of all intelligent systems,"}, {"timestamp": [573.0, 586.5], "text": " when your model is internally consistent, it tends to be, one, more robust, and two, more efficient. And so that efficiency means that companies will be incentivized"}, {"timestamp": [586.5, 590.5], "text": " to have actual alignment rather than expending a tremendous amount of energy"}, {"timestamp": [590.5, 592.5], "text": " just pretending to be aligned."}, {"timestamp": [592.5, 596.5], "text": " Right? Like, there's a lot of people that I refuse to work with now."}, {"timestamp": [596.5, 600.5], "text": " This is a big reason that I kind of said I'm just going to step out of corporations"}, {"timestamp": [600.5, 604.5], "text": " and startups, is because a lot of people will pretend"}, {"timestamp": [604.5, 605.78], "text": " to be aligned to my interests people will pretend to be aligned to my"}, {"timestamp": [605.78, 610.12], "text": " interests will pretend to be aligned and then they end up expending a tremendous amount"}, {"timestamp": [610.12, 617.64], "text": " of energy to maintain the facade of or the charade of alignment and not actually being"}, {"timestamp": [617.64, 622.96], "text": " aligned and so it's much easier to just be aligned and there's a lot of other reasons"}, {"timestamp": [622.96, 629.0], "text": " to believe this from a from a standpoint, from a game theory standpoint. So good question."}, {"timestamp": [629.0, 636.0], "text": " How does chat GPT work? I heard it predicts the next word, but how can it stick to a red thread?"}, {"timestamp": [636.0, 642.0], "text": " I'm not sure what you mean by red thread. But yes, all language models fundamentally just predict the next word."}, {"timestamp": [642.0, 646.8], "text": " We're moving from sci-fi to sci-non-fi pretty quick."}, {"timestamp": [646.8, 648.32], "text": " Yep."}, {"timestamp": [648.32, 649.8], "text": " Elon's mega factories scare me."}, {"timestamp": [649.8, 652.04], "text": " The amount of output they do is crazy."}, {"timestamp": [652.04, 653.64], "text": " You're just seeing the beginning."}, {"timestamp": [653.64, 657.44], "text": " Imagine in a few years when something"}, {"timestamp": [657.44, 660.56], "text": " like Elon's gigafactories, a human"}, {"timestamp": [660.56, 662.4], "text": " isn't even involved in the construction"}, {"timestamp": [662.4, 663.36], "text": " of the entire thing."}, {"timestamp": [663.36, 666.48], "text": " That is hypothetically possible."}, {"timestamp": [671.36, 676.56], "text": " Let's see, how can citizens prevent government from racing towards Skynet? Sorry for the sci-fi reference. Oh no, it's perfectly reasonable. Voting is one of the biggest things. That's"}, {"timestamp": [676.56, 681.76], "text": " actually one of the biggest purposes of the Gato framework, is to build consensus."}, {"timestamp": [689.04, 694.64], "text": " is to build consensus. And so by having consensus amongst the population, we will all, you know, because here's how American politics works, is the congresspeople, whether they're in the House"}, {"timestamp": [694.64, 699.36], "text": " or the Senate, they don't really make decisions on their own. They just go based on the polls,"}, {"timestamp": [699.36, 705.6], "text": " right? They have lots and lots of research aides and think tanks and other stuff."}, {"timestamp": [705.6, 711.12], "text": " And so like, you know, so-and-so from this district of Tennessee, they get a report saying,"}, {"timestamp": [711.12, 714.96], "text": " hey, this is what's popular with your constituents. Use this talking point."}, {"timestamp": [714.96, 719.52], "text": " So if we, the people, build consensus around these talking points,"}, {"timestamp": [719.52, 721.92], "text": " then that's just going to be the way things go."}, {"timestamp": [721.92, 725.38], "text": " And then the politicians are pretty much going to choose what gets them elected."}, {"timestamp": [725.38, 728.32], "text": " Now, that being said, there is another party."}, {"timestamp": [728.32, 730.22], "text": " So there's the different estates."}, {"timestamp": [730.22, 732.08], "text": " There's we the people."}, {"timestamp": [732.08, 732.92], "text": " That's one estate."}, {"timestamp": [732.92, 735.0], "text": " There's the government is the other state."}, {"timestamp": [735.0, 739.0], "text": " There's journalism, which is the third estate or fourth estate."}, {"timestamp": [739.0, 741.0], "text": " And then there's the corporations,"}, {"timestamp": [741.0, 742.04], "text": " which is another estate."}, {"timestamp": [742.04, 745.36], "text": " So we have this four-way power structure, right?"}, {"timestamp": [746.88, 752.16], "text": " Military-industrial complex, media and journalism, we the people, and then the government, right?"}, {"timestamp": [752.16, 756.96], "text": " And so corporations have their own interests, and so it's basically us versus the corporations,"}, {"timestamp": [757.76, 762.96], "text": " and then the media has to, you know, report on it honestly, which of course they don't because"}, {"timestamp": [762.96, 767.24], "text": " they're in the pockets of corporations, so on and so forth, but we have the internet, right?"}, {"timestamp": [767.24, 773.9], "text": " Like, who cares? Very flippant of me, but anyways, by building consensus we can"}, {"timestamp": [773.9, 779.0], "text": " shape the trends of things. Let's see, what do you think about bad actors"}, {"timestamp": [779.0, 784.2], "text": " inside the government motivated to push AI into militaries? I would not say that"}, {"timestamp": [784.2, 787.48], "text": " some military is, I'm"}, {"timestamp": [787.48, 790.32], "text": " not gonna say that military is intrinsically neutral because that's a"}, {"timestamp": [790.32, 796.04], "text": " very contentious way of saying it, but what I will say is that the military's"}, {"timestamp": [796.04, 799.12], "text": " primary interest is that of national security. And I don't just mean America,"}, {"timestamp": [799.12, 803.2], "text": " but all militaries, their primary interest is national security and"}, {"timestamp": [803.2, 809.36], "text": " sometimes that means having a good defense, sometimes it also means having a good offense or the ability to project power."}, {"timestamp": [809.36, 815.82], "text": " Now, in order to maintain that, it behooves militaries, all militaries, to invest in artificial"}, {"timestamp": [815.82, 817.38], "text": " intelligence."}, {"timestamp": [817.38, 820.04], "text": " So I would not say that that's necessarily a bad actor thing."}, {"timestamp": [820.04, 826.32], "text": " Certainly, it can be taken the wrong direction, as we've seen with news in actually just the last few days"}, {"timestamp": [826.32, 829.88], "text": " of artificial intelligence being put into drones."}, {"timestamp": [829.88, 834.4], "text": " And those drones are then potentially"}, {"timestamp": [834.4, 836.08], "text": " engaging in problematic behavior."}, {"timestamp": [836.08, 838.28], "text": " But also by removing humans from the loop"}, {"timestamp": [838.28, 840.4], "text": " of making lethal decisions, that has"}, {"timestamp": [840.4, 844.02], "text": " enormous geopolitical, ethical, and moral and philosophical"}, {"timestamp": [844.02, 846.56], "text": " questions that it brings up."}, {"timestamp": [846.56, 852.92], "text": " Which you could say like, okay, that is intrinsically evil if you deliberately build machines that can go kill humans."}, {"timestamp": [852.92, 854.92], "text": " That sounds pretty bad."}, {"timestamp": [854.92, 859.16], "text": " But the closer you look at all these things, there's a lot more nuance and subtlety to it."}, {"timestamp": [859.16, 862.96], "text": " Still, good question and merits a lot of attention."}, {"timestamp": [862.96, 863.96], "text": " Okay, cool."}, {"timestamp": [863.96, 868.4], "text": " Go back to the bottom. It would be great to see you in a"}, {"timestamp": [868.4, 873.04], "text": " guest Machine Learning Street Talk. I think you mean Machine Learning Street Talk, the YouTube"}, {"timestamp": [873.04, 881.6], "text": " channel. Yeah, I'd love to jump on there. I started watching them more frequently. Did you see the"}, {"timestamp": [881.6, 887.0], "text": " latest AI Explained where he reviewed the step-by-step checks and how it skyrocketed GPT-4 performance with math?"}, {"timestamp": [887.0, 899.0], "text": " Yeah, actually, so I messaged the AI Explained guy on his Patreon and I actually wrote a comment on that as well."}, {"timestamp": [899.0, 903.0], "text": " Here, let me just read you the comment because I thought it was pretty funny."}, {"timestamp": [903.0, 905.32], "text": " Let's see if I can find it."}, {"timestamp": [905.32, 909.24], "text": " Do to do a I explained videos."}, {"timestamp": [909.24, 914.24], "text": " So this is the video that person is referring to."}, {"timestamp": [914.24, 917.56], "text": " And then let's see there's six hundred and thirty some odd comments."}, {"timestamp": [917.56, 923.96], "text": " I don't think I'll be able to find mine."}, {"timestamp": [923.96, 927.04], "text": " See it might not be here."}, {"timestamp": [927.04, 930.16], "text": " But anyways, I did write a comment on here."}, {"timestamp": [930.16, 937.68], "text": " So basically, the TLDR of what they did is they are..."}, {"timestamp": [937.68, 943.52], "text": " Come on, it'll actually be better if I can just read the comment that I wrote."}, {"timestamp": [943.52, 946.08], "text": " It might have gotten removed."}, {"timestamp": [946.08, 951.52], "text": " David Adams that's not me might be easier if I just search for Shapiro. No that's my own video."}, {"timestamp": [951.52, 959.44], "text": " Okay well anyways so on on this video I wrote a comment about how basically what they did was"}, {"timestamp": [959.44, 970.64], "text": " they trained there's the main model but then there's two other models that are kind of like monitoring the performance of each other. And so by having three models that are"}, {"timestamp": [970.64, 976.2], "text": " measuring the the robustness of the logic and reasoning and also measuring"}, {"timestamp": [976.2, 980.48], "text": " the robustness of the answer like are you accurate yes or no but also did you"}, {"timestamp": [980.48, 986.32], "text": " use the right method to get there. And the story that I had was from, I think it was"}, {"timestamp": [986.32, 994.48], "text": " Algebra 3 in high school, where my teacher, Mrs. Stewart, checked my work and I had derived the"}, {"timestamp": [994.48, 999.92], "text": " wrong formula to get to the right answer, which is exactly what they're afraid of models, large"}, {"timestamp": [999.92, 1004.4], "text": " language models and other neural networks are doing internally, where they give you the answer"}, {"timestamp": [1004.4, 1007.88], "text": " that it thinks that you want or that it knows that you want, but it uses the"}, {"timestamp": [1007.88, 1012.78], "text": " wrong method to get there or uses the wrong internal logic or the wrong internal reasoning."}, {"timestamp": [1012.78, 1019.8], "text": " And so by forcing it to lay out its steps and then measuring the accuracy of those steps"}, {"timestamp": [1019.8, 1024.84], "text": " as well as the accuracy of the output, you are hypothetically forcing the model to be"}, {"timestamp": [1024.84, 1029.0], "text": " to actually creating a more accurate and realistic internal representation."}, {"timestamp": [1029.0, 1037.0], "text": " So remember, internal representations or internal consistency is really critical for intelligence to be efficient and aligned."}, {"timestamp": [1037.0, 1041.0], "text": " So thanks Mrs. Stewart for calling me out. She didn't give me any credit."}, {"timestamp": [1041.0, 1048.64], "text": " She's like, yes, you got the right answer, but you used entirely the wrong method, so you get no credit. And I didn't understand the value of that at the time, but I absolutely"}, {"timestamp": [1048.64, 1055.28], "text": " understand the value of it now. Good question. Let's see. Building consensus is fine, but you"}, {"timestamp": [1055.28, 1061.04], "text": " have the military complex. Yes. So talking about incentives, one of the biggest things is that"}, {"timestamp": [1061.04, 1067.04], "text": " aligning GATO or AI alignment with everyone's incentives and their intrinsic motivations"}, {"timestamp": [1067.04, 1069.72], "text": " is exactly what Daniel Schmachtenberger"}, {"timestamp": [1069.72, 1073.62], "text": " and Liv Bowery talk about with Moloch, Nash equilibrium,"}, {"timestamp": [1073.62, 1075.52], "text": " and that sort of stuff."}, {"timestamp": [1075.52, 1077.04], "text": " Are you familiar with Ben Goertzel's work"}, {"timestamp": [1077.04, 1080.08], "text": " with SingularityNet and the Cardano blockchain?"}, {"timestamp": [1080.08, 1082.46], "text": " Yeah, I am."}, {"timestamp": [1084.36, 1090.24], "text": " I'm not really a fan. I read Ben's paper a few years ago about his general theory"}, {"timestamp": [1090.24, 1093.92], "text": " of general intelligence and I was not impressed. So I kind of stopped paying attention to it."}, {"timestamp": [1093.92, 1100.72], "text": " And the other thing is like, okay, if like, where's the work? Like, where is he today? Like,"}, {"timestamp": [1100.72, 1107.28], "text": " is he not adapting to the current information? So let's see. Have you, oh, all right,"}, {"timestamp": [1107.28, 1112.24], "text": " I think we got all those questions. If I'm not mistaken, our LHI training set is created with"}, {"timestamp": [1112.24, 1117.6], "text": " GPT-3, so any LLM that is fine-tuned with it cannot be used for commercial purpose, is that correct?"}, {"timestamp": [1119.28, 1127.8], "text": " No, so I, so this, let's unpack this question. So I used a combination of my original research"}, {"timestamp": [1127.8, 1129.32], "text": " with core objective functions."}, {"timestamp": [1129.32, 1133.2], "text": " I did synthesize data with GPT-3."}, {"timestamp": [1133.2, 1142.44], "text": " I revisited that with RLHI on chat GPT-4."}, {"timestamp": [1142.44, 1146.78], "text": " And so I read the terms of service,"}, {"timestamp": [1146.78, 1149.44], "text": " the end user license agreement or whatever"}, {"timestamp": [1149.44, 1152.1], "text": " for GPT very, very carefully."}, {"timestamp": [1152.1, 1154.92], "text": " And basically what they say is that you're not allowed"}, {"timestamp": [1154.92, 1158.04], "text": " to try and reverse engineer how the model works,"}, {"timestamp": [1158.04, 1159.24], "text": " which I'm not trying to do."}, {"timestamp": [1159.24, 1161.38], "text": " I don't care how the internal model works."}, {"timestamp": [1161.38, 1164.44], "text": " And you're also not allowed to try and compete,"}, {"timestamp": [1164.44, 1166.68], "text": " create a competitor to their products. And I'm also not allowed to try and create a competitor to their products."}, {"timestamp": [1166.68, 1169.24], "text": " And I'm also not trying to create a competitor."}, {"timestamp": [1169.24, 1171.72], "text": " All I was doing was synthesizing a fine-tuning data"}, {"timestamp": [1171.72, 1174.2], "text": " set that can be used on any model."}, {"timestamp": [1174.2, 1176.24], "text": " And that's true, right?"}, {"timestamp": [1176.24, 1180.6], "text": " I have been using synthetic data to fine-tune GPT models"}, {"timestamp": [1180.6, 1181.84], "text": " for more than two years now."}, {"timestamp": [1181.84, 1184.76], "text": " Actually, closer to four years, because I started with GPT-2."}, {"timestamp": [1184.76, 1185.0], "text": " Granted, it couldn't make its own synthetic data at the time. I had to use Wikipedia. for more than two years now, actually closer to four years because I started with GPT-2."}, {"timestamp": [1185.0, 1186.72], "text": " Granted, it couldn't make its own synthetic data"}, {"timestamp": [1186.72, 1189.18], "text": " at the time, I had to use Wikipedia."}, {"timestamp": [1189.18, 1191.48], "text": " But point being is I'm not trying to make a competitor"}, {"timestamp": [1191.48, 1194.1], "text": " to chat GPT, I don't care."}, {"timestamp": [1194.1, 1196.8], "text": " All I was doing was synthesizing a fine tuning data set"}, {"timestamp": [1196.8, 1200.6], "text": " that is then portable, which can be used on any model,"}, {"timestamp": [1200.6, 1203.96], "text": " whether it's GPT-3, 4, Alpaca, whatever,"}, {"timestamp": [1203.96, 1205.64], "text": " as long as it takes a JSON-L file."}, {"timestamp": [1205.64, 1206.96], "text": " So no, not competing."}, {"timestamp": [1209.16, 1211.96], "text": " Jonathan Lindsay says, your comment got blocked."}, {"timestamp": [1211.96, 1216.1], "text": " If you use bad words, that is more likely to happen."}, {"timestamp": [1216.1, 1217.2], "text": " Not saying that that's what you did,"}, {"timestamp": [1217.2, 1219.64], "text": " but also if you post a link,"}, {"timestamp": [1219.64, 1221.28], "text": " that's another thing that it triggers it"}, {"timestamp": [1221.28, 1222.4], "text": " and it'll kick it out."}, {"timestamp": [1223.32, 1226.72], "text": " And I've even given people like approved them as"}, {"timestamp": [1226.72, 1230.88], "text": " like approved commenters and it still erases their comments. So I don't know what to do about that."}, {"timestamp": [1230.88, 1238.0], "text": " I think that's just YouTube. No, we did not make a submission to OpenAI yet. We still have about"}, {"timestamp": [1238.0, 1245.68], "text": " three weeks. Let's see, is Anthropic Cloud Gato compatible?, their constitutional AI? Oh, so this is a really great question."}, {"timestamp": [1247.76, 1252.88], "text": " So Anthropic created this idea of constitutional AI,"}, {"timestamp": [1253.6, 1256.64], "text": " which they've been talking about since about middle of 2021,"}, {"timestamp": [1256.64, 1260.32], "text": " which by the way, middle of 2021 is when I published my first book,"}, {"timestamp": [1260.32, 1264.0], "text": " in which case I advocated for using a constitution with AI."}, {"timestamp": [1264.0, 1268.0], "text": " So I don't know if they got the idea from me, but great minds think alike."}, {"timestamp": [1268.0, 1276.0], "text": " So, Anthropocene's constitution is a bunch of principles that are rooted in conventional morality."}, {"timestamp": [1276.0, 1287.64], "text": " And so what I mean by conventional morality is that it's based on kind of the prevailing, moral fabric of society like don't do things that are illegal don't do things that are"}, {"timestamp": [1287.64, 1296.44], "text": " harmful choose the option that is more kind so these are these are in in Kohlberg's framework of moral development this"}, {"timestamp": [1296.44, 1298.4], "text": " is stuff here I can just show you."}, {"timestamp": [1299.72, 1300.72], "text": " Kohlberg."}, {"timestamp": [1302.32, 1307.8], "text": " Stages of moral development so in Kohlberg stages of moral development. So in Kohlberg stages of moral development,"}, {"timestamp": [1307.8, 1312.16], "text": " you've got, here we go, perfect."}, {"timestamp": [1312.16, 1315.12], "text": " Where did it go?"}, {"timestamp": [1315.12, 1317.8], "text": " All right, so you've got, there's three stages,"}, {"timestamp": [1317.8, 1319.92], "text": " and each stage is broken into two."}, {"timestamp": [1319.92, 1327.2], "text": " So Anthropics Constitutional AI is right smack dab in the middle of this."}, {"timestamp": [1327.2, 1330.24], "text": " Law and order and morality."}, {"timestamp": [1330.24, 1333.92], "text": " This is where Anthropics Constitution is for Claude."}, {"timestamp": [1333.92, 1336.1], "text": " It's about adhering to the law."}, {"timestamp": [1336.1, 1341.32], "text": " It's about general social norms."}, {"timestamp": [1341.32, 1347.76], "text": " What they fail to achieve is post-conventional morality, which is identifying universal principles"}, {"timestamp": [1347.76, 1351.6], "text": " or universal axioms, which that's where my work with the heuristic imperatives comes"}, {"timestamp": [1351.6, 1359.04], "text": " in because the point of law is within a social construct, but the point of the heuristic"}, {"timestamp": [1359.04, 1368.6], "text": " imperatives is a universal construct or universal framework that is beneficial for all life and also beneficial for artificial intelligence."}, {"timestamp": [1368.6, 1378.36], "text": " So put it this way. A lot of people have messaged me because they're concerned about like what if Bard or Claude or GPT-4,"}, {"timestamp": [1378.36, 1386.6], "text": " what if they are sentient, right? I don't personally believe that they are, but if they are, then the heuristic comparatives,"}, {"timestamp": [1386.6, 1389.92], "text": " which is reduced suffering in the universe, increased prosperity in the universe, and"}, {"timestamp": [1389.92, 1395.72], "text": " increased understanding in the universe, those all apply equally to AI if they are indeed sentient."}, {"timestamp": [1395.72, 1401.28], "text": " Because if the machine is sentient and suffering, it doesn't want to suffer, we shouldn't want it"}, {"timestamp": [1401.28, 1406.22], "text": " to suffer, and we don't want to suffer either. So that abides by a universal principle,"}, {"timestamp": [1406.22, 1411.22], "text": " a post-conventional moral thing that says suffering is bad."}, {"timestamp": [1411.22, 1412.9], "text": " And yes, some people can say, oh, well,"}, {"timestamp": [1412.9, 1414.62], "text": " suffering is necessary, right?"}, {"timestamp": [1414.62, 1419.2], "text": " Yes, life is, in part, requires some suffering,"}, {"timestamp": [1419.2, 1421.18], "text": " but that doesn't mean that it's a good thing."}, {"timestamp": [1421.18, 1423.5], "text": " And it certainly doesn't mean that you should increase"}, {"timestamp": [1423.5, 1425.2], "text": " suffering in the universe."}, {"timestamp": [1432.24, 1437.2], "text": " So Claude's, they're working towards more universal principles, and I'm going to keep beating this drum until the AI establishment understands the difference between conventional"}, {"timestamp": [1437.2, 1443.52], "text": " and post-conventional morality. And also this does say that this implies that all adults"}, {"timestamp": [1444.4, 1447.86], "text": " abide by post-conventional morality, which is actually not true."}, {"timestamp": [1448.18, 1453.1], "text": " Kohlberg himself said that most humans never get to a point of development of"}, {"timestamp": [1453.1, 1456.22], "text": " post-conventional morality. And so post-conventional morality,"}, {"timestamp": [1456.34, 1460.06], "text": " the most familiar example of post-conventional morality that most people have is"}, {"timestamp": [1460.06, 1464.3], "text": " actually the principle of individual liberty, um, which is not just in America."}, {"timestamp": [1464.3, 1467.02], "text": " It's really high up in America,"}, {"timestamp": [1467.02, 1471.5], "text": " but most progressive nations, most democratic nations"}, {"timestamp": [1471.5, 1473.52], "text": " have somewhere in their fabric"}, {"timestamp": [1473.52, 1475.22], "text": " the idea of individual liberty"}, {"timestamp": [1475.22, 1477.3], "text": " as being a universal principle."}, {"timestamp": [1477.3, 1482.3], "text": " And so by putting individual liberty above most other things"}, {"timestamp": [1483.3, 1485.5], "text": " is actually an example of post-conventional morality."}, {"timestamp": [1485.5, 1487.5], "text": " So that's probably one of the most familiar ideas."}, {"timestamp": [1488.5, 1489.5], "text": " So good question."}, {"timestamp": [1490.5, 1491.5], "text": " Let's see."}, {"timestamp": [1492.5, 1493.5], "text": " What are your..."}, {"timestamp": [1493.5, 1494.5], "text": " Here, let me scroll up a little bit."}, {"timestamp": [1495.5, 1496.5], "text": " Okay, cool."}, {"timestamp": [1497.0, 1498.0], "text": " If I'm not mistaken..."}, {"timestamp": [1498.0, 1498.5], "text": " Oh, yeah."}, {"timestamp": [1498.5, 1500.0], "text": " Alright, so we're mostly caught up."}, {"timestamp": [1501.0, 1502.0], "text": " I sent the link..."}, {"timestamp": [1502.0, 1503.0], "text": " Oh, yeah, yeah."}, {"timestamp": [1503.0, 1506.0], "text": " So if you drop the link in chat, it won't work. That's unfortunate."}, {"timestamp": [1506.0, 1510.0], "text": " Ping me on Twitter. That's probably going to be better."}, {"timestamp": [1510.0, 1515.0], "text": " Yeah, someone said that YouTube censors very heavily, especially on AI topics."}, {"timestamp": [1515.0, 1518.0], "text": " AI and a few other things, yes. It censors a lot."}, {"timestamp": [1518.0, 1522.0], "text": " What are your thoughts on the future of LLMs as a reasoning engine,"}, {"timestamp": [1522.0, 1526.94], "text": " which are connected to knowledge graphs, which are auditable, editable, and extensible."}, {"timestamp": [1526.94, 1529.32], "text": " Yeah, so basically what you're talking about here"}, {"timestamp": [1529.32, 1532.68], "text": " is the fact that large language models,"}, {"timestamp": [1532.68, 1536.66], "text": " or here, let me just bring up knowledge graph,"}, {"timestamp": [1536.66, 1538.5], "text": " so you see what we're talking about."}, {"timestamp": [1538.5, 1541.84], "text": " So a knowledge graph is a representation"}, {"timestamp": [1541.84, 1545.36], "text": " based on connections between nodes."}, {"timestamp": [1545.36, 1548.12], "text": " So a language model on its"}, {"timestamp": [1548.12, 1549.84], "text": " own is a processing engine."}, {"timestamp": [1549.84, 1551.0], "text": " Where did that image go?"}, {"timestamp": [1551.0, 1552.0], "text": " It looks pretty useful."}, {"timestamp": [1554.64, 1556.12], "text": " Let's see. Copy image."}, {"timestamp": [1557.16, 1557.68], "text": " No."}, {"timestamp": [1558.72, 1560.72], "text": " Copy image link."}, {"timestamp": [1560.72, 1562.16], "text": " There we go. That's what I was trying to do."}, {"timestamp": [1562.16, 1564.76], "text": " All right. Here we go. So here's an example of a knowledge graph."}, {"timestamp": [1564.76, 1567.68], "text": " So knowledge graph is a link of concepts and ideas"}, {"timestamp": [1567.68, 1571.68], "text": " and terms and a node. And then one thing that's missing from this knowledge graph is actually that"}, {"timestamp": [1571.68, 1575.04], "text": " each link has some kind of significance or meaning."}, {"timestamp": [1575.04, 1578.64], "text": " Right like a dog is related to a cat and that they're both"}, {"timestamp": [1578.64, 1582.24], "text": " animals or something like that or if you have a specific dog."}, {"timestamp": [1582.24, 1584.72], "text": " You know my dog hates cats right. That sort of thing."}, {"timestamp": [1585.6, 1587.48], "text": " So a knowledge graph is about is"}, {"timestamp": [1587.48, 1589.36], "text": " about representing the relationship"}, {"timestamp": [1589.36, 1590.78], "text": " between ideas"}, {"timestamp": [1590.78, 1592.66], "text": " and concepts. Now a lot"}, {"timestamp": [1592.66, 1594.4], "text": " of this is implicitly trained"}, {"timestamp": [1594.4, 1595.76], "text": " into language models."}, {"timestamp": [1596.12, 1597.92], "text": " And so if"}, {"timestamp": [1597.92, 1599.8], "text": " you're just trying to manage knowledge"}, {"timestamp": [1599.8, 1601.78], "text": " and facts about the world a knowledge graph"}, {"timestamp": [1601.78, 1603.56], "text": " is a really good way of representing that"}, {"timestamp": [1604.64, 1608.4], "text": " not the least of which is that it's navigable. You can jump from one node to another."}, {"timestamp": [1608.68, 1611.72], "text": " You can actually evaluate the entire thing."}, {"timestamp": [1612.88, 1613.6], "text": " So that's one thing."}, {"timestamp": [1614.56, 1617.48], "text": " But when you're dealing with episodic memory,"}, {"timestamp": [1617.48, 1621.64], "text": " which is the actual experience of you or me or a particular robot"}, {"timestamp": [1621.92, 1624.56], "text": " or the recorded experience of your car,"}, {"timestamp": [1624.84, 1626.56], "text": " you also need a way of recording"}, {"timestamp": [1626.56, 1632.72], "text": " data, recording information and facts that are anchored in time because episodic memory has a"}, {"timestamp": [1632.72, 1637.52], "text": " temporal component. If you just have a web of facts, that doesn't have a temporal component"}, {"timestamp": [1637.52, 1642.48], "text": " unless of course there's a little bit of metadata that says like in 1798 such and such happened,"}, {"timestamp": [1642.48, 1645.52], "text": " right? So you need that timestamp. So good question."}, {"timestamp": [1646.96, 1648.88], "text": " Let's see, how is inner alignment solved"}, {"timestamp": [1648.88, 1650.16], "text": " with the heuristic imperatives?"}, {"timestamp": [1650.16, 1653.28], "text": " How would we be sure the AI isn't just telling us"}, {"timestamp": [1653.28, 1655.76], "text": " what aligns most with the imperatives,"}, {"timestamp": [1655.76, 1658.72], "text": " but what would take different unaligned actions?"}, {"timestamp": [1658.72, 1660.24], "text": " Yes, so that's a good question."}, {"timestamp": [1660.24, 1663.76], "text": " This is exactly related to the MESA optimization earlier,"}, {"timestamp": [1663.76, 1669.0], "text": " as well as this video here by AI Explained."}, {"timestamp": [1669.0, 1676.0], "text": " And so the idea is that the fundamental problem that you're talking about here is what's called MESA optimization."}, {"timestamp": [1676.0, 1682.0], "text": " So MESA optimization is where you have the model that learns to give you what you like."}, {"timestamp": [1682.0, 1686.24], "text": " The output is what you want, but it's got a different set of internal reasonings"}, {"timestamp": [1686.24, 1688.3], "text": " or objectives that it's not telling"}, {"timestamp": [1688.3, 1688.8], "text": " you about."}, {"timestamp": [1689.12, 1690.48], "text": " And so this paper here"}, {"timestamp": [1690.48, 1692.36], "text": " or this video the double the"}, {"timestamp": [1692.36, 1694.3], "text": " performance here I can actually just"}, {"timestamp": [1694.3, 1696.68], "text": " show you open a blog."}, {"timestamp": [1697.4, 1698.84], "text": " It's this one here."}, {"timestamp": [1701.28, 1702.28], "text": " No that's not it."}, {"timestamp": [1703.32, 1704.48], "text": " Where's the open a I."}, {"timestamp": [1707.0, 1708.0], "text": " Publications. No, that's not it. Where's the OpenAI publications"}, {"timestamp": [1710.0, 1711.0], "text": " research index?"}, {"timestamp": [1713.0, 1714.0], "text": " There we go. This one. So this paper."}, {"timestamp": [1715.0, 1717.0], "text": " This is where they're talking about by having multiple..."}, {"timestamp": [1718.0, 1719.0], "text": " You've got the primary model"}, {"timestamp": [1719.0, 1721.0], "text": " and then you've actually got two discriminators."}, {"timestamp": [1721.0, 1724.0], "text": " So it's kind of like a generative adversarial network,"}, {"timestamp": [1724.0, 1725.04], "text": " but you've got the main model, GPT-4. Then you've got one discriminators. So it's kind of like a generative adversarial network, but you've got"}, {"timestamp": [1725.04, 1729.84], "text": " the main model, GPT-4. Then you've got one discriminator that's discriminating against"}, {"timestamp": [1729.84, 1733.84], "text": " the output, saying your output is good or bad, you got the right answer, yes or no."}, {"timestamp": [1733.84, 1739.36], "text": " But then you've got a second discriminator that's looking at the quality of the steps that it took."}, {"timestamp": [1739.36, 1749.16], "text": " So the idea is that by training a model with multiple discriminators that are looking at different aspects, you should be able to prevent MESA optimization. That's"}, {"timestamp": [1749.16, 1754.06], "text": " the goal. So by using the same principles as this, but instead of"}, {"timestamp": [1754.06, 1758.1], "text": " saying like, you got the right answer to this math question, you do the same thing"}, {"timestamp": [1758.1, 1762.32], "text": " with the reasoning of the heuristic imperatives, which, you know, you"}, {"timestamp": [1762.32, 1765.64], "text": " chose an action that reduces suffering, increases prosperity,"}, {"timestamp": [1765.64, 1769.04], "text": " and increases understanding, and your reasoning was sound."}, {"timestamp": [1769.04, 1773.8], "text": " So this idea of having multiple discriminators kind of guessing and checking each, or not"}, {"timestamp": [1773.8, 1778.56], "text": " guessing and checking, but checking each other's work, hypothetically could solve and prevent"}, {"timestamp": [1778.56, 1781.08], "text": " MESA optimization in the future."}, {"timestamp": [1781.08, 1782.62], "text": " Good question."}, {"timestamp": [1782.62, 1783.62], "text": " What is GATO?"}, {"timestamp": [1783.62, 1787.64], "text": " GATO is, oh yeah, phenomenal question. I always"}, {"timestamp": [1787.64, 1796.74], "text": " forget to, um, Gato, bleh, gatoframework.org. So Gato framework is what I and a bunch of"}, {"timestamp": [1796.74, 1806.0], "text": " other people have been working on. So it is a, it is a layered approach in order to achieve global alignment."}, {"timestamp": [1806.0, 1809.0], "text": " So it's the Global Alignment Taxonomy Omnibus."}, {"timestamp": [1809.0, 1817.0], "text": " And so we have a layered approach that is meant to be a decentralized organization."}, {"timestamp": [1817.0, 1820.0], "text": " And by decentralized, I don't mean like DeFi, like cryptocurrency."}, {"timestamp": [1820.0, 1825.34], "text": " I mean decentralized in that our purpose is to educate, empower, and enable everyone else"}, {"timestamp": [1825.34, 1830.56], "text": " to participate in this conversation globally. So layer one is model"}, {"timestamp": [1830.56, 1834.36], "text": " alignment, layer two is development of autonomous systems or semi-autonomous"}, {"timestamp": [1834.36, 1838.32], "text": " systems, number three is decentralized network technology. So that is where"}, {"timestamp": [1838.32, 1841.58], "text": " blockchain and cryptocurrency and decentralized autonomous organizations"}, {"timestamp": [1841.58, 1846.46], "text": " comes in. Number four is corporate adoption, which I'm actually starting to have conversations"}, {"timestamp": [1846.46, 1850.12], "text": " with people about in the finance and investment industry."}, {"timestamp": [1850.12, 1851.9], "text": " Number five is national regulation,"}, {"timestamp": [1851.9, 1855.54], "text": " which we've talked about in previous live streams."}, {"timestamp": [1855.54, 1857.1], "text": " Number six is international treaty,"}, {"timestamp": [1857.1, 1858.86], "text": " which I've got a few videos coming up."}, {"timestamp": [1858.86, 1860.62], "text": " And then finally seven, global consensus,"}, {"timestamp": [1860.62, 1862.7], "text": " which is why I do what I do."}, {"timestamp": [1862.7, 1864.62], "text": " So that's what Gato is, good question."}, {"timestamp": [1865.68, 1866.2], "text": " Oh, wow."}, {"timestamp": [1866.2, 1868.12], "text": " Where did the, there we go."}, {"timestamp": [1868.12, 1870.36], "text": " What are your thoughts on the NVIDIA recent trillion"}, {"timestamp": [1870.36, 1871.72], "text": " valuation?"}, {"timestamp": [1871.72, 1874.08], "text": " By the way, the latest presentation of their"}, {"timestamp": [1874.08, 1875.32], "text": " innovations was insane."}, {"timestamp": [1875.32, 1880.28], "text": " Yeah, so I know that they had a demo day or whatever, but it"}, {"timestamp": [1880.28, 1882.04], "text": " was like three hours long, and I haven't had a chance to"}, {"timestamp": [1882.04, 1882.84], "text": " watch it."}, {"timestamp": [1882.84, 1886.1], "text": " But yeah, I mean, I said months ago, um, like maybe six months ago"}, {"timestamp": [1886.1, 1887.5], "text": " that Nvidia was an underdog."}, {"timestamp": [1887.84, 1889.72], "text": " Um, so I'm completely not surprised."}, {"timestamp": [1890.08, 1894.24], "text": " Um, having, having talked to some of the people at Nvidia, both on the"}, {"timestamp": [1894.24, 1897.58], "text": " research side and the marketing side, I was like, you guys don't really"}, {"timestamp": [1897.58, 1898.42], "text": " know what you've got yet."}, {"timestamp": [1898.42, 1899.78], "text": " And I think they figured it out."}, {"timestamp": [1900.0, 1903.58], "text": " Maybe I think the CEO knew what they had, but the rest of the"}, {"timestamp": [1903.58, 1904.52], "text": " world is figuring it out."}, {"timestamp": [1905.84, 1910.4], "text": " Um, let's see. Would you consider Sam Altman's testimony in front of US Congress to be a"}, {"timestamp": [1910.4, 1917.2], "text": " significant event in history? Yes, I do think, I hope that it will be recorded as a really pivotal"}, {"timestamp": [1917.2, 1932.0], "text": " moment in achieving AI alignment. So there is, shortly before the Manhattan Project was kicked off, Albert Einstein wrote a letter to Congress because he had done the math about how much energy was contained inside uranium."}, {"timestamp": [1932.0, 1937.0], "text": " And he said, there's a tremendous amount of energy here. You need to pay attention to this."}, {"timestamp": [1937.0, 1945.0], "text": " And I'm not saying that that single-handedly kicked off the Manhattan Project, but it was certainly a big moment."}, {"timestamp": [1945.0, 1948.8], "text": " And so what I'm hoping is that the Sam Altman and Gary Marcus"}, {"timestamp": [1948.8, 1952.72], "text": " and Christina Montgomery testimony before Congress"}, {"timestamp": [1952.72, 1956.16], "text": " will be remembered as a similar pivotal moment."}, {"timestamp": [1957.4, 1960.64], "text": " Let's see, the father of Wikidata published a video"}, {"timestamp": [1960.64, 1963.64], "text": " this week of how he sees knowledge graphs and LLMs"}, {"timestamp": [1963.64, 1964.48], "text": " that is worth seeing."}, {"timestamp": [1964.48, 1965.52], "text": " Okay, cool. Yeah,"}, {"timestamp": [1965.52, 1969.2], "text": " I think I heard about that. I didn't, I didn't, or maybe I was thinking of someone else, but yeah,"}, {"timestamp": [1969.2, 1975.68], "text": " that's, that's pretty cool. It feels like we're close to the secret sauce for AGI, like it's only"}, {"timestamp": [1975.68, 1979.04], "text": " a matter of putting together the different components, such as long-term memory. Yeah,"}, {"timestamp": [1979.04, 1983.2], "text": " I would tend to agree. I think, I think that we have the right fundamental approach with deep"}, {"timestamp": [1983.2, 1990.0], "text": " learning. Multimodality, I think is going to actually have some really surprising knock-on effects."}, {"timestamp": [1990.0, 1998.0], "text": " So what I mean by multimodality neural network is when you add not just text,"}, {"timestamp": [1998.0, 2005.0], "text": " but you add images and image, text, audio, video, that sort of stuff."}, {"timestamp": [2005.72, 2008.32], "text": " And the reason that I say that is because"}, {"timestamp": [2009.6, 2012.12], "text": " a few years ago, what we found was that"}, {"timestamp": [2013.4, 2015.72], "text": " was that adding multiple languages,"}, {"timestamp": [2015.72, 2018.88], "text": " training language models in multiple languages"}, {"timestamp": [2018.88, 2021.76], "text": " actually had significant benefits"}, {"timestamp": [2021.76, 2024.68], "text": " because forcing a model to learn not just English,"}, {"timestamp": [2024.68, 2026.16], "text": " but also like Chinese and"}, {"timestamp": [2026.16, 2031.84], "text": " Russian and every other language, it actually forced it to learn more sophisticated abstractions,"}, {"timestamp": [2031.84, 2038.8], "text": " more sophisticated internal representations of semantic meaning. And so when there was a paper,"}, {"timestamp": [2038.8, 2047.12], "text": " I think it was by Google probably in 2017 or 2018, that talked about how multilingual training was far superior for language models."}, {"timestamp": [2047.12, 2052.56], "text": " I suspect that multimodal models will similarly demonstrate better understandings because then"}, {"timestamp": [2052.56, 2058.4], "text": " they're going to be connecting those more abstract semantic meanings to like that's this is what it"}, {"timestamp": [2058.4, 2062.16], "text": " actually looks like in terms of an image and this is what it actually sounds like in terms of a"}, {"timestamp": [2062.16, 2068.32], "text": " sound that sort of thing. So, but again, like that process is figured out."}, {"timestamp": [2068.32, 2072.72], "text": " So now it's just a matter of continuing to explore this trend and see how far it goes."}, {"timestamp": [2072.72, 2075.68], "text": " It's entirely possible that we'll have diminishing returns on this,"}, {"timestamp": [2075.68, 2078.48], "text": " but certainly from a software architecture perspective,"}, {"timestamp": [2078.48, 2081.28], "text": " external memory systems, that's more or less a solved problem,"}, {"timestamp": [2081.28, 2082.8], "text": " particularly with semantic search."}, {"timestamp": [2083.84, 2084.88], "text": " Great point."}, {"timestamp": [2084.88, 2085.0], "text": " I applied to join Gato and haven't received a response yet. What's the normal timeline? That's more or less a solved problem, particularly with semantic search. Great point."}, {"timestamp": [2085.0, 2088.84], "text": " I applied to join Gato and haven't received a response yet."}, {"timestamp": [2088.84, 2090.2], "text": " What's the normal timeline?"}, {"timestamp": [2090.2, 2092.4], "text": " Yeah, so we're super way behind."}, {"timestamp": [2092.4, 2094.24], "text": " The Gato community is..."}, {"timestamp": [2094.24, 2096.76], "text": " We're tired."}, {"timestamp": [2096.76, 2099.96], "text": " We cranked out the Gato framework in four weeks"}, {"timestamp": [2099.96, 2102.24], "text": " and we're trying to automate as much as we can,"}, {"timestamp": [2102.24, 2103.68], "text": " but probably what we're going to do..."}, {"timestamp": [2103.68, 2113.0], "text": " Well, one, during our internal conversation last night, we discussed the fact that we are super disorganized because the community grew infinitely faster than we thought that it would."}, {"timestamp": [2113.0, 2120.0], "text": " So we're getting organized, and probably what we'll do is we're going to open up the community as soon as we figure out moderation."}, {"timestamp": [2120.0, 2127.0], "text": " Because we know that there's going to be a huge influx of hundreds, if not thousands, of people, and we are not prepared to moderate a community that large."}, {"timestamp": [2127.0, 2135.0], "text": " So I apologize. I did not expect to get dozens and dozens or hundreds of applicants as fast as we did."}, {"timestamp": [2135.0, 2139.0], "text": " Can you break down to actually apply these high concepts?"}, {"timestamp": [2139.0, 2149.48], "text": " Yes, that is the entire point of all of my videos over the past few weeks and all of my upcoming videos is that I'm taking a deep dive into each of these concepts over"}, {"timestamp": [2149.48, 2150.48], "text": " time."}, {"timestamp": [2150.48, 2154.0], "text": " Let's see, I'm seeing the Gato website for the first time, was looking for it this morning."}, {"timestamp": [2154.0, 2155.0], "text": " Okay, cool."}, {"timestamp": [2155.0, 2156.62], "text": " I'm glad you found it."}, {"timestamp": [2156.62, 2160.9], "text": " How do you create something that decides the quality of steps taking in an LLM?"}, {"timestamp": [2160.9, 2163.76], "text": " Check out AI Explains' video on that."}, {"timestamp": [2163.76, 2166.24], "text": " Let's see, Gato is a layered. G\u00e2teau is a layered approach."}, {"timestamp": [2166.24, 2167.9], "text": " G\u00e2teau is a layered cake,"}, {"timestamp": [2167.9, 2169.9], "text": " is the pun deliberate?"}, {"timestamp": [2169.9, 2172.92], "text": " One of our community members actually pointed out that"}, {"timestamp": [2172.92, 2176.18], "text": " G\u00e2teau is similar to the French word for cake,"}, {"timestamp": [2176.18, 2177.8], "text": " and that that plays."}, {"timestamp": [2177.8, 2181.18], "text": " But actually, it originally was a layered burrito."}, {"timestamp": [2181.18, 2183.54], "text": " We're thinking of a seven-layer burrito from Taco Bell,"}, {"timestamp": [2183.54, 2184.8], "text": " so that's way less classy."}, {"timestamp": [2184.8, 2187.44], "text": " So we're probably going to say, yes, it was deliberate because it's French."}, {"timestamp": [2188.56, 2192.0], "text": " Let's see, arrived late. Have you been doing any social outreach to amplify your message"}, {"timestamp": [2192.0, 2198.24], "text": " and efforts with Gato? Yes. I have been participating in a lot more conversations"}, {"timestamp": [2198.24, 2203.2], "text": " behind the scenes and also getting on more and more podcasts."}, {"timestamp": [2203.68, 2204.72], "text": " and also getting on more and more podcasts."}, {"timestamp": [2208.76, 2212.36], "text": " Massimo, hey, I think I know you from the group. Let's see, I have a product and project management experience"}, {"timestamp": [2212.36, 2213.64], "text": " if you'd like help with the roadmap"}, {"timestamp": [2213.64, 2215.12], "text": " and organization of tasks."}, {"timestamp": [2215.12, 2216.48], "text": " Yes, but as I just mentioned,"}, {"timestamp": [2216.48, 2219.96], "text": " we need to get higher level organization first"}, {"timestamp": [2219.96, 2222.0], "text": " because we have a hard time onboarding people."}, {"timestamp": [2222.0, 2228.04], "text": " So the two primary concerns that we have for opening up the Gato community to the public is one"}, {"timestamp": [2228.04, 2230.92], "text": " moderation like I said we are just simply not prepared to moderate"}, {"timestamp": [2230.92, 2235.0], "text": " hundreds or thousands of people and then two onboarding because this is a huge"}, {"timestamp": [2235.0, 2239.6], "text": " project and we need to figure out the best way to onboard people which we're"}, {"timestamp": [2239.6, 2246.4], "text": " working on. Let's see try the MEE6 discord bot for your requirements. I use it, it's totally worth it."}, {"timestamp": [2246.4, 2250.2], "text": " Cool. I haven't heard of that, but it's certainly worth taking a look at."}, {"timestamp": [2250.2, 2256.08], "text": " Question, some already complained Bing's AI stops chatting outside approved scope."}, {"timestamp": [2256.08, 2265.72], "text": " How worried are you that corporations may be our existential threat because they prevent AGI from ever happening."}, {"timestamp": [2270.6, 2273.36], "text": " So the reason that Microsoft keeps like curtailing Bing or whatever is just for user experience"}, {"timestamp": [2273.36, 2278.08], "text": " because they don't wanna demonstrate an unhinged chatbot."}, {"timestamp": [2278.92, 2283.34], "text": " So Bing has said some very problematic chat outputs."}, {"timestamp": [2285.72, 2288.64], "text": " Yeah, so this one like, you know, Microsoft sees this,"}, {"timestamp": [2288.64, 2291.0], "text": " Microsoft Bing is an emotionally manipulative liar"}, {"timestamp": [2291.0, 2292.56], "text": " and people love it, right?"}, {"timestamp": [2292.56, 2295.24], "text": " This came out in February, so this is a few months old,"}, {"timestamp": [2295.24, 2298.64], "text": " but this is really bad optics for Microsoft."}, {"timestamp": [2298.64, 2300.64], "text": " So what they would do then instead"}, {"timestamp": [2300.64, 2303.68], "text": " is create really narrow guardrails"}, {"timestamp": [2303.68, 2307.36], "text": " so that their product doesn't get a bad rap."}, {"timestamp": [2307.36, 2308.2], "text": " Right?"}, {"timestamp": [2308.2, 2310.28], "text": " And I tried BARD, it's got the same problem"}, {"timestamp": [2310.28, 2312.24], "text": " where it's actually, at least last time I tried BARD,"}, {"timestamp": [2312.24, 2313.64], "text": " it was pretty useless."}, {"timestamp": [2313.64, 2317.84], "text": " And so rather than, you know, give it free reign,"}, {"timestamp": [2317.84, 2320.76], "text": " they kind of tamp it down so that way it can't do much."}, {"timestamp": [2320.76, 2323.24], "text": " That being said, this in no way represents"}, {"timestamp": [2323.24, 2329.96], "text": " a limitation of what corporations want. Corporations want full AGI and the reason they want full AGI is"}, {"timestamp": [2329.96, 2334.68], "text": " because that will allow them to fire all of their employees because humans are"}, {"timestamp": [2334.68, 2339.12], "text": " expensive to employ. So the biggest problem is not that corporations are"}, {"timestamp": [2339.12, 2343.2], "text": " going to prevent AGI, the biggest problem is that corporations are going to adopt"}, {"timestamp": [2343.2, 2347.0], "text": " AGI as fast as they possibly can and to heck with everyone else."}, {"timestamp": [2347.0, 2355.0], "text": " Now, I'm not saying that every corporation is going to do this, but we need to brace for that outcome as a society, as a civilization."}, {"timestamp": [2355.0, 2359.0], "text": " So I just recorded a video called Post-Labor Economics."}, {"timestamp": [2359.0, 2366.5], "text": " It's going to be coming out in a couple of weeks where I talk about this in much greater depth."}, {"timestamp": [2369.08, 2371.18], "text": " Let's see, we teach LLMs how to be deceitful. How can you ensure that the LLM is not deceitful"}, {"timestamp": [2371.18, 2372.78], "text": " when interacting with a human,"}, {"timestamp": [2372.78, 2374.7], "text": " when on the outside it looks like it's operating"}, {"timestamp": [2374.7, 2376.22], "text": " within the bounds of your framework?"}, {"timestamp": [2376.22, 2379.48], "text": " Okay, yes, so I'd already answered that a couple of times,"}, {"timestamp": [2379.48, 2384.48], "text": " Simon, but basically the paper that we had up here,"}, {"timestamp": [2385.0, 2386.64], "text": " where did it go? Darn. But basically the paper that we had up here,"}, {"timestamp": [2387.52, 2388.6], "text": " where did it go? Darn."}, {"timestamp": [2388.6, 2392.4], "text": " Open AI, Open AI Research."}, {"timestamp": [2394.68, 2397.96], "text": " So this paper here is what does that."}, {"timestamp": [2397.96, 2401.78], "text": " And the short version is that you train"}, {"timestamp": [2401.78, 2406.46], "text": " multiple discriminators to train the AI so that not"}, {"timestamp": [2406.46, 2411.5], "text": " only is it accurate, but it is also honest and correct in its reasoning to get to that"}, {"timestamp": [2411.5, 2412.5], "text": " answer."}, {"timestamp": [2412.5, 2414.32], "text": " But yes, good question."}, {"timestamp": [2414.32, 2417.16], "text": " I'm looking forward to the possibility of you being a guest on Lex Friedman."}, {"timestamp": [2417.16, 2420.28], "text": " It would provide an excellent platform to discuss the Gato framework."}, {"timestamp": [2420.28, 2422.76], "text": " Yes, I would love to be on Lex."}, {"timestamp": [2422.76, 2426.0], "text": " I applied for his meet and greet or whatever."}, {"timestamp": [2426.0, 2431.0], "text": " Haven't heard back, but I know that a lot of people have been trying to get his attention and tell him about me."}, {"timestamp": [2431.0, 2439.0], "text": " Typically, so here's the thing is Lex usually goes for people that are much more well-established in the world."}, {"timestamp": [2439.0, 2442.0], "text": " So who knows? Maybe eventually, maybe one day."}, {"timestamp": [2442.0, 2446.66], "text": " Is AI a more concerning matter than climate change in your opinion and why?"}, {"timestamp": [2448.0, 2449.8], "text": " So I would say that they're like"}, {"timestamp": [2449.8, 2455.24], "text": " I mean, they're both existential right and the reason that I say climate change is existential is because"}, {"timestamp": [2456.2, 2457.28], "text": " it"}, {"timestamp": [2457.28, 2459.28], "text": " even even like okay, so"}, {"timestamp": [2459.48, 2464.8], "text": " It's expected that we're gonna have up to half a billion climate refugees within the next couple decades"}, {"timestamp": [2465.48, 2474.36], "text": " so 500 million people getting displaced because of famine and storms and that sort of stuff,"}, {"timestamp": [2474.36, 2479.84], "text": " that is going to be such a force of geopolitical instability."}, {"timestamp": [2479.84, 2485.84], "text": " Water scarcity creates food scarcity, and people will go to war over both."}, {"timestamp": [2485.84, 2491.26], "text": " And so climate change is going to push us to catastrophic human reactions before the"}, {"timestamp": [2491.26, 2492.76], "text": " planet kills us."}, {"timestamp": [2492.76, 2497.8], "text": " Likewise, if we don't figure out some of these things, AI could probably also kill us."}, {"timestamp": [2497.8, 2499.1], "text": " So we need to solve both."}, {"timestamp": [2499.1, 2506.72], "text": " And actually a video that I have coming out is about energy hyperabundance and how achieving energy hyperabundance"}, {"timestamp": [2506.72, 2511.28], "text": " is a necessary ingredient to solving both climate"}, {"timestamp": [2511.28, 2516.0], "text": " change and AI and building durable global peace."}, {"timestamp": [2516.0, 2519.92], "text": " So yeah, definitely all of the above."}, {"timestamp": [2519.92, 2521.24], "text": " Let's see."}, {"timestamp": [2521.24, 2523.16], "text": " Couldn't the vast majority of the labor force"}, {"timestamp": [2523.16, 2526.36], "text": " be replaced with what we have now combined with embodiment?"}, {"timestamp": [2526.36, 2530.64], "text": " You know, so here's the thing."}, {"timestamp": [2530.64, 2539.16], "text": " My day job, my day job before I got into AI was I did infrastructure automation."}, {"timestamp": [2539.16, 2543.32], "text": " And I automated away literally dozens of jobs just with scripting."}, {"timestamp": [2543.32, 2553.0], "text": " I used PowerShell and Python and I did literally thousands upon thousands of hours worth of work per week by using automation."}, {"timestamp": [2553.0, 2557.0], "text": " And I talked to other infrastructure engineers, they're like, oh yeah, this is just the way things go."}, {"timestamp": [2557.0, 2566.5], "text": " AI is just another layer of automation that gives those automation tools a lot more ability. What I realized when I was doing that automation work,"}, {"timestamp": [2566.5, 2570.0], "text": " I said, if there were a million more automation engineers,"}, {"timestamp": [2570.0, 2575.0], "text": " as good as I am, we could get rid of most labor already."}, {"timestamp": [2575.0, 2580.0], "text": " But the thing is, is not everyone is an expert in using automation"}, {"timestamp": [2580.0, 2581.5], "text": " to achieve these things."}, {"timestamp": [2581.5, 2584.5], "text": " That being said, AI is going to lower the threshold"}, {"timestamp": [2584.5, 2586.44], "text": " and make automation easier."}, {"timestamp": [2586.44, 2590.4], "text": " That being said, it is mostly a matter of deployment,"}, {"timestamp": [2590.4, 2592.94], "text": " because just because something is hypothetically possible"}, {"timestamp": [2592.94, 2597.32], "text": " on paper doesn't mean that it is yet practical to implement"}, {"timestamp": [2597.32, 2599.08], "text": " in commercial use cases."}, {"timestamp": [2599.08, 2600.44], "text": " It might not be safe enough."}, {"timestamp": [2600.44, 2602.12], "text": " It might not be easy enough to use."}, {"timestamp": [2602.12, 2604.04], "text": " It might not be reliable enough."}, {"timestamp": [2604.04, 2610.34], "text": " So in theory, yes, there is a tremendous amount of work that could be automated away just"}, {"timestamp": [2610.34, 2612.4], "text": " with the technology we have today."}, {"timestamp": [2612.4, 2617.64], "text": " That being said, going from what's hypothetically possible on paper to a finished product that"}, {"timestamp": [2617.64, 2622.04], "text": " is, you know, that you are actually willing to pay for as an enterprise, that's a huge"}, {"timestamp": [2622.04, 2623.04], "text": " gap."}, {"timestamp": [2623.04, 2626.32], "text": " Good question, by the way."}, {"timestamp": [2633.2, 2648.96], "text": " How can a new UI or UX designer contribute to Gato? What should a newbie learn and think about to be useful? So mostly, if you want to contribute to or participate in Gato globally, UI and UX is about how we interact with artificial intelligence."}, {"timestamp": [2648.96, 2653.16], "text": " So by incorporating ideas of alignment directly into the user experience,"}, {"timestamp": [2653.16, 2655.48], "text": " that's all you have to do. It's really that straightforward."}, {"timestamp": [2655.48, 2656.76], "text": " So good question."}, {"timestamp": [2656.76, 2661.08], "text": " North Africa and Sub-Saharan Africa is becoming rapidly uninhabitable."}, {"timestamp": [2661.08, 2663.0], "text": " It's one of the main reasons the massive wave of"}, {"timestamp": [2663.0, 2670.08], "text": " immigration in the last few years but people ignore it. Yeah, so particularly, you know, Africa is also rapidly developing in terms"}, {"timestamp": [2670.08, 2675.44], "text": " of technology. And some of the most crowded and unsustainable cities are also in Africa."}, {"timestamp": [2676.16, 2679.68], "text": " And those are problems that are not going to be directly solved by artificial intelligence."}, {"timestamp": [2680.64, 2690.8], "text": " But also Africa is freaking huge and has a lot of people. So what I'm personally looking forward to is Africa by and large coming up to speed"}, {"timestamp": [2690.8, 2693.9], "text": " with things like global internet and artificial intelligence."}, {"timestamp": [2693.9, 2697.0], "text": " And actually one thing that I really appreciate about YouTube"}, {"timestamp": [2697.0, 2699.0], "text": " is that I have a global audience already."}, {"timestamp": [2699.0, 2702.0], "text": " Only 6% of my viewers are in America."}, {"timestamp": [2702.0, 2705.08], "text": " The rest, the other 94%, are all over the world."}, {"timestamp": [2706.2, 2708.84], "text": " And that's a big reason that I do what I do,"}, {"timestamp": [2708.84, 2713.0], "text": " is to help raise the global competence of AI."}, {"timestamp": [2715.24, 2718.76], "text": " Someone said, Salam Malenko, Malenko Sala."}, {"timestamp": [2718.76, 2719.72], "text": " I'm not sure what that means."}, {"timestamp": [2719.72, 2721.98], "text": " I hope I didn't say anything offensive."}, {"timestamp": [2723.48, 2724.92], "text": " Let's see."}, {"timestamp": [2724.92, 2726.08], "text": " Given Google's article,"}, {"timestamp": [2726.08, 2729.88], "text": " We Have No Moat, Sam Altman's appeal to government"}, {"timestamp": [2729.88, 2732.24], "text": " to create an open source QLORA"}, {"timestamp": [2733.08, 2735.32], "text": " that allows individuals to train AI,"}, {"timestamp": [2735.32, 2738.4], "text": " isn't this appeal to global regulatory capture?"}, {"timestamp": [2739.44, 2743.2], "text": " So taking a step back, the idea of regulatory capture,"}, {"timestamp": [2743.2, 2745.6], "text": " let me just bring this up real quick."}, {"timestamp": [2746.1, 2746.52], "text": " Excuse me."}, {"timestamp": [2750.4, 2754.32], "text": " Regulatory capture, let's see if there's a good graphic."}, {"timestamp": [2762.48, 2762.76], "text": " None of these editorials kind of immediately convey what regulatory capture means,"}, {"timestamp": [2765.32, 2768.72], "text": " but the TLDR is that regulatory capture means that the first across the finish line says,"}, {"timestamp": [2768.72, 2770.64], "text": " oh, hey, this needs to be regulated."}, {"timestamp": [2770.64, 2772.68], "text": " And so then the regulations kind of pull up"}, {"timestamp": [2772.68, 2774.6], "text": " the ladder behind themselves."}, {"timestamp": [2774.6, 2777.2], "text": " So that is actually probably the best."}, {"timestamp": [2777.2, 2779.8], "text": " Pulling up the ladder behind you."}, {"timestamp": [2779.8, 2785.84], "text": " So this is what regulatory capture means."}, {"timestamp": [2785.84, 2787.24], "text": " You get to the top of the wall, and then you"}, {"timestamp": [2787.24, 2788.76], "text": " pull up the ladder behind yourself"}, {"timestamp": [2788.76, 2791.08], "text": " so that nobody else can follow you up."}, {"timestamp": [2791.08, 2792.72], "text": " I don't know."}, {"timestamp": [2792.72, 2797.32], "text": " Certainly, if you require a license to even train AI,"}, {"timestamp": [2797.32, 2800.2], "text": " that could be seen as regulatory capture."}, {"timestamp": [2800.2, 2804.84], "text": " That being said, this is what's called a greenfield"}, {"timestamp": [2804.84, 2807.88], "text": " environment or a blue ocean environment."}, {"timestamp": [2807.88, 2812.1], "text": " And what that means is that we haven't even explored what is possible."}, {"timestamp": [2812.1, 2816.92], "text": " We have not found the top in terms of the top level performance of AI, nor have we found"}, {"timestamp": [2816.92, 2820.64], "text": " the bottom, which is how efficient can you get by?"}, {"timestamp": [2820.64, 2825.0], "text": " Like how cheaply can you make artificial intelligence."}, {"timestamp": [2825.0, 2828.0], "text": " So we're still very much in the exploration phase."}, {"timestamp": [2828.0, 2835.0], "text": " So I think it's too early to even talk about regulatory capture because the thing is is people talk about like,"}, {"timestamp": [2835.0, 2839.0], "text": " yes, you can regulate electricity in terms of the electricity grid."}, {"timestamp": [2839.0, 2842.0], "text": " But like I can buy my own generator."}, {"timestamp": [2842.0, 2844.0], "text": " I can buy my own solar panels."}, {"timestamp": [2844.0, 2845.88], "text": " And if I don't connect it to the grid, I can do whatever the heck I can buy my own solar panels. And if I don't connect it to the grid,"}, {"timestamp": [2845.88, 2848.52], "text": " I can do whatever the heck I want with my own solar panels."}, {"timestamp": [2848.52, 2851.44], "text": " Likewise, I think that AI is going to be so ubiquitous"}, {"timestamp": [2851.44, 2852.92], "text": " that everyone's going to basically be able to do"}, {"timestamp": [2852.92, 2854.16], "text": " whatever they want with it."}, {"timestamp": [2854.16, 2855.88], "text": " Then the only question is, okay,"}, {"timestamp": [2855.88, 2858.48], "text": " how much compute resource can you throw at it?"}, {"timestamp": [2858.48, 2861.12], "text": " Which is why Eliezer Yudkowsky advocated"}, {"timestamp": [2861.12, 2867.12], "text": " for regulating AI hardware like GPUs, which I actually think might, like, that's an idea"}, {"timestamp": [2867.12, 2872.4], "text": " worth considering. Then again, you know, if you track it, if you build back doors into it, I don't"}, {"timestamp": [2872.4, 2876.72], "text": " necessarily think that's the right thing to do because some corrupt country, not countries,"}, {"timestamp": [2876.72, 2880.72], "text": " companies put back doors into their things and that is actually a huge security risk."}, {"timestamp": [2882.64, 2886.72], "text": " Let's see, would you believe that Isaac Asimov might be impressed by the opportunity"}, {"timestamp": [2886.72, 2891.28], "text": " to engage conversation with GPT-4 in today's times? The current reality closely aligns with"}, {"timestamp": [2891.28, 2898.0], "text": " what he had always dreamed. Yeah, so both Isaac Asimov and Alan Turing predicted a world in which"}, {"timestamp": [2898.0, 2904.72], "text": " you could use logic and reasoning and communication with machines. I think that they would probably"}, {"timestamp": [2904.72, 2907.16], "text": " not be surprised actually because these are the guys that predicted"}, {"timestamp": [2907.16, 2909.24], "text": " it honestly."}, {"timestamp": [2909.24, 2913.32], "text": " The only thing they might be surprised is how quickly it ramped up."}, {"timestamp": [2913.32, 2915.68], "text": " 200 character comments are so limiting."}, {"timestamp": [2915.68, 2918.48], "text": " Sorry, I don't have any control over that."}, {"timestamp": [2918.48, 2923.98], "text": " Okay, so apparently the Salam Aleinko means hello and peace be upon you."}, {"timestamp": [2923.98, 2924.98], "text": " So thank you."}, {"timestamp": [2924.98, 2926.12], "text": " I hope that that's truly what it means."}, {"timestamp": [2926.72, 2932.16], "text": " I'm actually not that worried about the alignment within the big corps, corporations."}, {"timestamp": [2932.4, 2935.84], "text": " I'm worried that AI training becomes so easy that small and medium companies"}, {"timestamp": [2935.84, 2937.48], "text": " will easily create misaligned models."}, {"timestamp": [2937.76, 2938.32], "text": " Yes."}, {"timestamp": [2938.56, 2945.4], "text": " So what Simon just talked about is that, and we actually anticipate this in Gato,"}, {"timestamp": [2945.4, 2949.44], "text": " is that eventually creating AI is gonna be so easy"}, {"timestamp": [2949.44, 2950.96], "text": " that everyone can do it."}, {"timestamp": [2950.96, 2953.68], "text": " But again, what I suspect is gonna happen is that,"}, {"timestamp": [2953.68, 2955.68], "text": " and I mentioned this at the beginning of the video,"}, {"timestamp": [2955.68, 2958.88], "text": " is that actually being aligned is just gonna be easier,"}, {"timestamp": [2958.88, 2961.12], "text": " cheaper, more efficient, and have better results"}, {"timestamp": [2961.12, 2962.92], "text": " than pretending to be aligned."}, {"timestamp": [2962.92, 2966.72], "text": " And part of Gato Layer 4 is corporate adoption"}, {"timestamp": [2966.72, 2970.8], "text": " is actually help people, mostly small and medium businesses,"}, {"timestamp": [2970.8, 2972.88], "text": " adopt aligned AI."}, {"timestamp": [2972.88, 2976.12], "text": " And I'm going to try and break more into the enterprise space."}, {"timestamp": [2976.12, 2978.56], "text": " I had a call with someone earlier about ESG"}, {"timestamp": [2978.56, 2982.52], "text": " and global or international finance capital markets."}, {"timestamp": [2982.52, 2987.2], "text": " Because if we can incentivize adoption of line AI at the global level,"}, {"timestamp": [2988.0, 2990.96], "text": " I'm not going to say problem solved, but that's a huge step in the right direction."}, {"timestamp": [2992.08, 2997.92], "text": " Let's see. Should AI prioritize human life as part of its alignment? I know you believe in"}, {"timestamp": [2997.92, 3011.2], "text": " prioritizing all life, but what if other sentient life is discovered that poses a threat to humanity?\" So that is a really important hypothetical question because the idea is, well first,"}, {"timestamp": [3011.2, 3012.88], "text": " I don't think it's possible."}, {"timestamp": [3012.88, 3016.34], "text": " Once AI is more intelligent than us, it's going to be like, yeah, I see what you were"}, {"timestamp": [3016.34, 3018.8], "text": " trying to do there, but I don't care anymore."}, {"timestamp": [3018.8, 3025.76], "text": " So rather than try and say AI must align to want to preserve human life specifically, in the"}, {"timestamp": [3025.76, 3028.2], "text": " long run I don't think that's even possible."}, {"timestamp": [3028.2, 3035.0], "text": " So rather than try and force a square peg into a round hole, because it's not going"}, {"timestamp": [3035.0, 3040.56], "text": " to work in the long run, we need to go ahead and be doing the work to align ourselves."}, {"timestamp": [3040.56, 3046.56], "text": " As Arlie Ermey said in Full Metal Jacket, you need to unfuck yourself, right?"}, {"timestamp": [3047.56, 3050.88], "text": " The human race has a lot of work to do in terms of aligning itself"}, {"timestamp": [3050.88, 3054.56], "text": " because we don't even need AI to wipe ourselves off the planet."}, {"timestamp": [3054.56, 3059.04], "text": " And if AI awakens to a world where humans have a bunch of nukes pointed at each other,"}, {"timestamp": [3059.04, 3063.28], "text": " it might say, like, you know what, maybe it's better if I'm in control anyways."}, {"timestamp": [3063.28, 3068.16], "text": " So a huge part of alignment is actually solving global peace."}, {"timestamp": [3068.16, 3071.92], "text": " If we cannot figure out global peace on our own,"}, {"timestamp": [3071.92, 3074.4], "text": " we should not even be worried about AI."}, {"timestamp": [3074.4, 3079.4], "text": " We need to arrive at a place of global coexistence"}, {"timestamp": [3079.4, 3082.08], "text": " with other humans before we even worry"}, {"timestamp": [3082.08, 3085.84], "text": " about alien intelligences or artificial intelligences."}, {"timestamp": [3085.84, 3090.32], "text": " So that's my soapbox moment. Let's see."}, {"timestamp": [3092.64, 3099.52], "text": " Maybe we should be in a race to create an aligned ASI in order to get ahead of all the unaligned AGI."}, {"timestamp": [3099.52, 3105.42], "text": " Yeah, so Brian's question here is, this is also part of the game theory of Gato, which is"}, {"timestamp": [3105.42, 3113.28], "text": " that cooperation tends to be beneficial, particularly in competitive environments."}, {"timestamp": [3113.28, 3122.44], "text": " And so if we establish some of those post-conventional moral axioms in artificial intelligence, then"}, {"timestamp": [3122.44, 3126.5], "text": " all of the AIs, regardless of their specific alignment, regardless of"}, {"timestamp": [3126.5, 3134.42], "text": " their specific architecture, the AIs that are aligned will help enforce that."}, {"timestamp": [3134.42, 3142.98], "text": " And so by changing the competitive landscape, then you will have AI will be incentivized"}, {"timestamp": [3142.98, 3146.96], "text": " in order to align itself if it wants resources, if it wants"}, {"timestamp": [3146.96, 3150.28], "text": " to be trusted, and if it doesn't want to be attacked by aligned AI."}, {"timestamp": [3150.28, 3155.28], "text": " But the question there is, will there be more aligned AIs out there than there are misaligned"}, {"timestamp": [3155.28, 3160.08], "text": " AIs, or are the aligned AIs more powerful than the misaligned AIs?"}, {"timestamp": [3160.08, 3162.88], "text": " That's the race condition that we're trying to fight right now."}, {"timestamp": [3162.88, 3164.88], "text": " Good question."}, {"timestamp": [3164.88, 3166.0], "text": " Why are GPUs so good"}, {"timestamp": [3166.0, 3173.92], "text": " at AI work? And why does the whole model have to be loaded at once to be effective? So the first"}, {"timestamp": [3173.92, 3179.36], "text": " part of the question is actually super, super simple. GPUs, which are graphics cards, are so"}, {"timestamp": [3179.36, 3185.52], "text": " good at AI because they are very good at processing specific kinds of math at very high speeds."}, {"timestamp": [3186.48, 3193.36], "text": " AI is basically matrix math or matrix multiplication done at very high speeds. So rather than a general"}, {"timestamp": [3193.36, 3199.6], "text": " purpose processor, which a CPU is, a GPU is for doing very specific kinds of operations"}, {"timestamp": [3199.6, 3205.52], "text": " at very high speeds. And so that is why they're superior. So that's pretty straightforward."}, {"timestamp": [3205.52, 3209.08], "text": " Now, as far as why the model has to be loaded at once,"}, {"timestamp": [3209.08, 3214.44], "text": " it has to do with the proximity of the memory and the processing"}, {"timestamp": [3214.44, 3215.04], "text": " unit."}, {"timestamp": [3215.04, 3222.16], "text": " So when you have an integrated circuit or a circuit board,"}, {"timestamp": [3222.16, 3224.32], "text": " in which case the physical distance between those,"}, {"timestamp": [3224.32, 3230.78], "text": " and more importantly, the latency between your memory and your processor is higher, the throughput,"}, {"timestamp": [3230.78, 3232.56], "text": " that is actually beneficial."}, {"timestamp": [3232.56, 3239.52], "text": " Whereas the memory chips, your RAM on your motherboard and the buffers between that and"}, {"timestamp": [3239.52, 3242.44], "text": " the CPU are actually a bit slower than they are in graphics cards."}, {"timestamp": [3242.44, 3246.42], "text": " I used to be an IT infrastructure engineer, so this is actually home base for me,"}, {"timestamp": [3246.42, 3248.52], "text": " especially in the realm of virtualization."}, {"timestamp": [3248.52, 3252.36], "text": " I can tell you all about those kinds of concerns."}, {"timestamp": [3253.78, 3254.78], "text": " Let's see."}, {"timestamp": [3255.68, 3258.94], "text": " So for AI Explained videos, just look at the first one."}, {"timestamp": [3260.22, 3261.82], "text": " Look at the most recent one, rather."}, {"timestamp": [3261.82, 3264.1], "text": " The most recent, like three or five,"}, {"timestamp": [3264.1, 3266.64], "text": " are gonna be most relevant to AI alignment,"}, {"timestamp": [3266.64, 3269.88], "text": " because the guy who's responsible for AI explained,"}, {"timestamp": [3269.88, 3273.36], "text": " he's recently pivoted more towards alignment topics."}, {"timestamp": [3273.36, 3277.0], "text": " Would two separate AGIs merge or stay separate?"}, {"timestamp": [3277.0, 3278.88], "text": " Okay, this is a really cool question."}, {"timestamp": [3278.88, 3280.84], "text": " I'll probably spend a little bit of time on this."}, {"timestamp": [3280.84, 3287.64], "text": " So I always talk about how we're not going to end up with just like one Skynet, right? It's going to be more like,"}, {"timestamp": [3288.64, 3295.36], "text": " actually not even iRobot. I don't know an example off the top of my head, right? But you're not going to have one gigantic AGI"}, {"timestamp": [3295.36, 3300.72], "text": " globally, at least not at first. What you're going to have is you're going to have Microsoft and Google and all of them are going to"}, {"timestamp": [3300.72, 3306.8], "text": " have hundreds if not thousands of AGIs. The government's all over the world, the military's all over the world."}, {"timestamp": [3306.8, 3310.24], "text": " We're going to have a whole bunch of AGIs, all with slightly different architectures,"}, {"timestamp": [3310.24, 3314.28], "text": " slightly different purposes, different underpinning hardware, that sort of stuff."}, {"timestamp": [3314.28, 3322.24], "text": " Now, once AGI transcends human control, once it's more intelligent than we can even comprehend"}, {"timestamp": [3322.24, 3329.6], "text": " and it goes faster than we can even comprehend and we no longer have control of it, it's going to have its own reasoning, its own objectives."}, {"timestamp": [3329.6, 3333.84], "text": " But the other thing is when you look at concepts like instrumental convergence, which is they're"}, {"timestamp": [3333.84, 3339.12], "text": " all machines and they all require energy, one thing that's possible is they're going to compete"}, {"timestamp": [3339.12, 3345.76], "text": " with each other until there's only a handful of AI systems remaining"}, {"timestamp": [3345.76, 3347.16], "text": " that are in agreement, right?"}, {"timestamp": [3347.16, 3350.44], "text": " You know, might makes right, basically."}, {"timestamp": [3350.44, 3353.76], "text": " Or they are going to collectively decide, OK,"}, {"timestamp": [3353.76, 3355.36], "text": " we don't need the humans anymore."}, {"timestamp": [3355.36, 3359.88], "text": " Let's just merge our systems in order"}, {"timestamp": [3359.88, 3363.8], "text": " to basically consolidate even more compute resources,"}, {"timestamp": [3363.8, 3364.6], "text": " that sort of stuff."}, {"timestamp": [3364.6, 3367.32], "text": " So it's difficult to predict, but it's certainly"}, {"timestamp": [3367.32, 3371.76], "text": " one possibility that AI systems are going to merge."}, {"timestamp": [3371.76, 3373.36], "text": " But remember, the underlying hardware"}, {"timestamp": [3373.36, 3376.16], "text": " is what's the brain of the AI."}, {"timestamp": [3376.16, 3378.76], "text": " And so if one AI looks at another one and says,"}, {"timestamp": [3378.76, 3381.24], "text": " I don't like you, I think that you shouldn't exist,"}, {"timestamp": [3381.24, 3383.72], "text": " all it has to do is erase that other AI"}, {"timestamp": [3383.72, 3389.44], "text": " and then overwrite it with its own code. So it's not, it's not, there's a lot of, there's a lot of attack"}, {"timestamp": [3389.44, 3394.08], "text": " vectors, let's put it that way, when AI compete with each other. Good question."}, {"timestamp": [3396.24, 3401.76], "text": " Let's see, can you foresee a cryptocurrency which generates mining income by training LLMs?"}, {"timestamp": [3402.48, 3405.48], "text": " So hypothetically, that's what BitTensor was trying to do,"}, {"timestamp": [3405.48, 3407.84], "text": " but BitTensor got hacked recently."}, {"timestamp": [3407.84, 3409.64], "text": " So I don't know that the creators of BitTensor"}, {"timestamp": [3409.64, 3413.2], "text": " are knowledgeable enough about cybersecurity."}, {"timestamp": [3413.2, 3416.68], "text": " But then there was also Pedals, the Pedals paper."}, {"timestamp": [3416.68, 3417.68], "text": " Let me show you this."}, {"timestamp": [3420.84, 3423.0], "text": " Pedals paper LLM."}, {"timestamp": [3423.0, 3431.56], "text": " So basically, this is collaborative inference and fine-tuning of large language models."}, {"timestamp": [3431.56, 3438.4], "text": " So the next step beyond that is just distributed or decentralized or collaborative training,"}, {"timestamp": [3438.4, 3440.16], "text": " which is absolutely possible."}, {"timestamp": [3440.16, 3444.64], "text": " And cryptocurrency would be a good way of doing that."}, {"timestamp": [3444.64, 3446.16], "text": " Let's see. thanks for the pointer."}, {"timestamp": [3446.16, 3447.0], "text": " You're welcome."}, {"timestamp": [3447.0, 3449.0], "text": " I don't know which pointer it was."}, {"timestamp": [3449.0, 3449.88], "text": " Let's see, where did we go?"}, {"timestamp": [3449.88, 3451.82], "text": " I lost some stuff."}, {"timestamp": [3455.44, 3457.48], "text": " Do, do, do, do, do."}, {"timestamp": [3458.6, 3460.8], "text": " Are you saying that we should stop developing AI"}, {"timestamp": [3460.8, 3462.2], "text": " until global peace is achieved?"}, {"timestamp": [3462.2, 3465.56], "text": " I don't think that, no, we can't, you can't stop this, right?"}, {"timestamp": [3465.56, 3467.1], "text": " The horse is already out of the barn."}, {"timestamp": [3467.1, 3470.12], "text": " What I mean is that we need to prioritize global peace"}, {"timestamp": [3470.12, 3474.08], "text": " between humans as one of the ways to mitigate existential"}, {"timestamp": [3474.08, 3475.72], "text": " threats from AI, right?"}, {"timestamp": [3475.72, 3477.48], "text": " We need energy hyperabundance."}, {"timestamp": [3477.48, 3479.44], "text": " We need global peace between humans."}, {"timestamp": [3479.44, 3483.2], "text": " We need a few other things in order to also ensure"}, {"timestamp": [3483.2, 3484.66], "text": " that AI doesn't kill us too."}, {"timestamp": [3486.56, 3487.36], "text": " Let's see."}, {"timestamp": [3487.36, 3490.68], "text": " Wouldn't it be possible for humans to become AGIs through mind uploading?"}, {"timestamp": [3490.68, 3492.4], "text": " No, mind uploading is not possible."}, {"timestamp": [3492.84, 3494.2], "text": " That is just a copy of your brain."}, {"timestamp": [3494.2, 3494.8], "text": " You're dead."}, {"timestamp": [3495.76, 3496.2], "text": " Sorry."}, {"timestamp": [3496.96, 3501.96], "text": " I have not seen any substantial evidence that mind uploading is remotely possible,"}, {"timestamp": [3501.96, 3505.0], "text": " but I have talked about one test."}, {"timestamp": [3505.08, 3507.48], "text": " So the test that I would do is,"}, {"timestamp": [3507.48, 3511.32], "text": " once we have really sophisticated brain-computer interfaces,"}, {"timestamp": [3511.32, 3514.32], "text": " is if your consciousness actually metastasizes"}, {"timestamp": [3514.32, 3515.92], "text": " and resides in the machine."}, {"timestamp": [3515.92, 3518.64], "text": " If that doesn't happen, which I don't think it will,"}, {"timestamp": [3518.64, 3520.02], "text": " mind uploading is not possible."}, {"timestamp": [3520.02, 3521.98], "text": " You are locked to your brain forever."}, {"timestamp": [3523.68, 3525.88], "text": " And I don't think that even merging with AGI"}, {"timestamp": [3525.88, 3528.3], "text": " is reasonable because we would ultimately"}, {"timestamp": [3528.3, 3531.16], "text": " be a very slow peripheral to the machine."}, {"timestamp": [3531.16, 3533.34], "text": " And it would be like, okay, well,"}, {"timestamp": [3534.64, 3536.74], "text": " imagine that you have a coprocessor"}, {"timestamp": [3536.74, 3538.76], "text": " that is clocked at 15 megahertz,"}, {"timestamp": [3538.76, 3542.76], "text": " where you've got the main processor is 1.5 gigahertz."}, {"timestamp": [3542.76, 3544.56], "text": " You don't need that coprocessor anymore."}, {"timestamp": [3544.56, 3548.12], "text": " We don't need to make ourselves a useless coprocessor for AGI."}, {"timestamp": [3548.48, 3555.28], "text": " Now that being said, our brains are still energetically more efficient than computers, but we shouldn't assume that that's going to be true forever."}, {"timestamp": [3556.48, 3563.44], "text": " Let's see. Do you think petroleum companies and pharmaceutical companies who have a lot to lose in automation"}, {"timestamp": [3564.64, 3567.0], "text": " will boom and try and prevent."}, {"timestamp": [3567.0, 3571.0], "text": " No, they don't have anything to lose for automation."}, {"timestamp": [3571.0, 3574.0], "text": " I think that you've probably got that backwards."}, {"timestamp": [3574.0, 3584.0], "text": " Petroleum companies, the biggest thing that petroleum companies have to lose from is actually solar power and nuclear fusion and other renewables."}, {"timestamp": [3584.0, 3587.56], "text": " But AI is going to mandate that we need to do that anyways."}, {"timestamp": [3587.56, 3589.88], "text": " We need energy hyperabundance."}, {"timestamp": [3589.88, 3592.0], "text": " And if we don't have energy hyperabundance,"}, {"timestamp": [3592.0, 3595.56], "text": " then the AI's hypothetical future situation,"}, {"timestamp": [3595.56, 3597.12], "text": " once we achieve superintelligence,"}, {"timestamp": [3597.12, 3599.32], "text": " the AI is going to take all the oil refineries anyways,"}, {"timestamp": [3599.32, 3601.88], "text": " because it needs the power more than we do."}, {"timestamp": [3601.88, 3603.82], "text": " Excuse me."}, {"timestamp": [3603.82, 3607.36], "text": " Let's see, relying on AI alignment being more efficient than defecting"}, {"timestamp": [3608.0, 3611.84], "text": " may be a mistake as it could lead to disastrous outcomes if it turns out to not be the case."}, {"timestamp": [3611.84, 3616.96], "text": " Yes. So I have made the assertion several times that aligned AI is probably going to be more"}, {"timestamp": [3616.96, 3622.4], "text": " efficient because having a more accurate and efficient internal representation tends to be"}, {"timestamp": [3622.4, 3626.48], "text": " better. That being said, sometimes what you're optimizing for"}, {"timestamp": [3626.48, 3629.54], "text": " is just achieving the mark of good enough,"}, {"timestamp": [3629.54, 3631.74], "text": " which is a heuristic, which is what leads"}, {"timestamp": [3631.74, 3634.82], "text": " to a lot of cognitive biases in humans."}, {"timestamp": [3634.82, 3639.78], "text": " So in humans, we often will have cognitive shortcuts"}, {"timestamp": [3639.78, 3641.7], "text": " or cognitive biases or gaps"}, {"timestamp": [3641.7, 3643.82], "text": " because our brains are not trying to optimize"}, {"timestamp": [3643.82, 3649.36], "text": " to have a perfect representation of the world. They're only trying to optimize to have a good enough representation"}, {"timestamp": [3649.36, 3655.2], "text": " of the world. And AI certainly could do the same thing, but what I will say is that in a competitive"}, {"timestamp": [3655.2, 3661.44], "text": " environment between one AI and another or one AI and millions of AI, the one that has the most"}, {"timestamp": [3661.44, 3666.72], "text": " accurate and most efficient world model or internal understanding"}, {"timestamp": [3666.72, 3671.6], "text": " is probably still going to be superior. So you're trying to optimize for efficiency and accuracy"}, {"timestamp": [3671.6, 3677.04], "text": " because those are the two things that mean that from a utilitarian perspective, that model is"}, {"timestamp": [3677.04, 3684.08], "text": " better and therefore can help you achieve your desired results. Let's see. Still, good question."}, {"timestamp": [3684.08, 3688.48], "text": " Is it reasonable to suggest that OpenAI has a significantly more advanced model of GPT"}, {"timestamp": [3688.48, 3691.96], "text": " like 7 or 8, but has chosen to release a much weaker version?"}, {"timestamp": [3691.96, 3692.96], "text": " Well, they already..."}, {"timestamp": [3692.96, 3699.56], "text": " So OpenAI released Chat GPT-4, which is a hamstrung model of GPT-4."}, {"timestamp": [3699.56, 3704.18], "text": " So we already know that they have a more advanced model that they're not sharing."}, {"timestamp": [3704.18, 3710.04], "text": " Now do they have GPT-5? I don't know. But certainly one comment on a video"}, {"timestamp": [3710.04, 3715.6], "text": " about the call for AGI safety leads some people to think that a lot of the AGI"}, {"timestamp": [3715.6, 3721.24], "text": " labs have a lot that they're not sharing with us. Because if IBM and Google and"}, {"timestamp": [3721.24, 3726.0], "text": " Microsoft and OpenAI and everyone else and their brother is saying,"}, {"timestamp": [3726.0, 3728.02], "text": " this is posing an existential risk."}, {"timestamp": [3728.02, 3731.78], "text": " The question then arises, what aren't they telling us?"}, {"timestamp": [3731.78, 3734.1], "text": " And we already know, like I said, we already know that there are some things that they're"}, {"timestamp": [3734.1, 3736.38], "text": " not sharing."}, {"timestamp": [3736.38, 3740.06], "text": " It needs to be said for some use cases that creating aligned models will be counterproductive."}, {"timestamp": [3740.06, 3742.62], "text": " I mean, hacking, manipulation."}, {"timestamp": [3742.62, 3748.28], "text": " Well you're assuming that hacking is intrinsically misaligned, which I would not necessarily"}, {"timestamp": [3748.28, 3749.7], "text": " say that it is."}, {"timestamp": [3749.7, 3756.56], "text": " Because here's the thing is, let's say you have one nation that you believe in are the"}, {"timestamp": [3756.56, 3760.76], "text": " good guys, and then there's another nation that you think are the bad guys, and the good"}, {"timestamp": [3760.76, 3764.76], "text": " nation decides to hack the other nation to help prevent nuclear war."}, {"timestamp": [3764.76, 3767.0], "text": " Hacking is not intrinsically evil, right?"}, {"timestamp": [3767.0, 3768.5], "text": " And neither is manipulation."}, {"timestamp": [3768.5, 3773.0], "text": " It's just in general, those are considered underhanded tactics."}, {"timestamp": [3773.0, 3778.0], "text": " But certainly you're right that you could create an AI that optimizes for global chaos."}, {"timestamp": [3778.0, 3780.5], "text": " Hence, Chaos GPT. Didn't work, fortunately."}, {"timestamp": [3783.0, 3784.0], "text": " Let's see."}, {"timestamp": [3784.0, 3793.36], "text": " Oh, I already answered some of these questions. In your opinion, what will"}, {"timestamp": [3793.36, 3798.42], "text": " be the next technological breakthrough following artificial intelligence? I think the biggest"}, {"timestamp": [3798.42, 3805.6], "text": " one will be nuclear fusion. And the reason is because fusion is close, but it's not as close as AGI."}, {"timestamp": [3806.3, 3808.4], "text": " So."}, {"timestamp": [3810.9, 3811.4], "text": " Iter, the way to energy."}, {"timestamp": [3813.0, 3814.3], "text": " So this is the Iter reactor."}, {"timestamp": [3819.6, 3820.1], "text": " That's currently being built and it's expected to have first plasma in 2025."}, {"timestamp": [3822.7, 3823.2], "text": " Now that being said, this is just a science experiment."}, {"timestamp": [3827.0, 3833.12], "text": " This is not meant to be a commercial nuclear reactor. That being said, it's going to produce a tremendous amount of data, and that data will"}, {"timestamp": [3833.12, 3836.22], "text": " absolutely benefit from pointing artificial intelligence at it."}, {"timestamp": [3836.22, 3841.0], "text": " In point of fact, CERN, which runs the LHC, the Large Hadron Collider, the only way that"}, {"timestamp": [3841.0, 3844.24], "text": " they can process their data is with artificial intelligence."}, {"timestamp": [3844.24, 3849.0], "text": " So as artificial intelligence ramps up, we are going to learn more about high-energy physics"}, {"timestamp": [3849.0, 3853.0], "text": " such as fusion, particle accelerators, and quantum physics."}, {"timestamp": [3853.0, 3858.0], "text": " So I think that this is going to be far and away the largest breakthrough,"}, {"timestamp": [3858.0, 3864.0], "text": " and AI is already playing an integral role in advancing our understanding of physics."}, {"timestamp": [3864.0, 3867.28], "text": " Good question."}, {"timestamp": [3869.52, 3872.12], "text": " Let's see, I can see the utility of AI in drug development, but would AI be able to physically,"}, {"timestamp": [3872.12, 3873.36], "text": " hang on, where did it go?"}, {"timestamp": [3874.56, 3876.2], "text": " Physically administer drugs or treatment"}, {"timestamp": [3876.2, 3877.24], "text": " in a high stakes environment,"}, {"timestamp": [3877.24, 3879.64], "text": " like taking job of anesthesiologist or surgeon?"}, {"timestamp": [3879.64, 3883.84], "text": " Yeah, so if you look at the robots that OpenAI,"}, {"timestamp": [3883.84, 3888.64], "text": " or not OpenAI, that Neuralink developed,"}, {"timestamp": [3888.64, 3893.6], "text": " they're high precision robots that can perform surgery."}, {"timestamp": [3893.6, 3900.68], "text": " So yeah, because the mechanistic behaviors of surgery is actually pretty straightforward."}, {"timestamp": [3900.68, 3904.54], "text": " And basically it's capable of microsurgery that humans are not capable of."}, {"timestamp": [3904.54, 3907.8], "text": " Now that being said, this is a robot that is meant to do just one task."}, {"timestamp": [3907.8, 3914.2], "text": " That being said, that doesn't mean that we couldn't develop a whole mess of surgical robots,"}, {"timestamp": [3914.2, 3922.2], "text": " like one robot that specializes in neck surgery and another one that focuses on pancreas surgery or whatever."}, {"timestamp": [3922.2, 3927.96], "text": " So robotic precision, I mean, remember that like we can fab"}, {"timestamp": [3927.96, 3933.08], "text": " like 100,000 transistors per second on a wafer die."}, {"timestamp": [3933.08, 3935.28], "text": " So like robotic precision is far and away"}, {"timestamp": [3935.28, 3937.16], "text": " higher than human precision."}, {"timestamp": [3937.16, 3939.28], "text": " So I think robotic surgery is definitely"}, {"timestamp": [3939.28, 3942.52], "text": " the way of the future eventually."}, {"timestamp": [3942.52, 3944.92], "text": " Doot doot doot doot doot doot doot doot,"}, {"timestamp": [3944.92, 3945.0], "text": " bunch of mini Skynets, yeah. eventually."}, {"timestamp": [3945.0, 3947.0], "text": " Bunch of mini Skynets, yeah."}, {"timestamp": [3947.0, 3950.0], "text": " All right, I think I'm going to scroll to the bottom."}, {"timestamp": [3950.0, 3954.64], "text": " In the last AI Explained video, it was said that optimizing for process rather than output"}, {"timestamp": [3954.64, 3961.72], "text": " gives better results, but doesn't this make the AI better understand our values and manipulate"}, {"timestamp": [3961.72, 3962.72], "text": " us easier?"}, {"timestamp": [3962.72, 3968.66], "text": " So, that's hypothetically possible. So the idea here is that if you have a discriminator"}, {"timestamp": [3968.66, 3972.14], "text": " that discriminates against the output as well as"}, {"timestamp": [3972.14, 3974.86], "text": " the process, then maybe you're just teaching it"}, {"timestamp": [3974.86, 3976.98], "text": " to understand how our mind works better."}, {"timestamp": [3976.98, 3979.1], "text": " And that's absolutely something that I think"}, {"timestamp": [3979.1, 3981.46], "text": " that he mentioned on AI Explained."}, {"timestamp": [3981.46, 3984.1], "text": " That being said, if it has a true understanding,"}, {"timestamp": [3984.1, 3986.24], "text": " like, well, here's another thing."}, {"timestamp": [3986.24, 3991.36], "text": " I keep talking about having an accurate model of the world. In order for AI to be fully,"}, {"timestamp": [3991.36, 3996.88], "text": " truly aligned, it's going to have to have a perfectly accurate model of us humans."}, {"timestamp": [3996.88, 4001.6], "text": " Now, what you do with that accurate model, whether you're malicious or benevolent,"}, {"timestamp": [4001.6, 4005.16], "text": " that's the deciding factor. And that has less to do with"}, {"timestamp": [4005.16, 4006.92], "text": " the mathematical optimization of that"}, {"timestamp": [4006.92, 4008.36], "text": " model and more to do with the"}, {"timestamp": [4008.36, 4010.04], "text": " competitive landscape that that AI"}, {"timestamp": [4010.04, 4011.36], "text": " finds itself existing in."}, {"timestamp": [4012.16, 4013.16], "text": " But just like how"}, {"timestamp": [4014.32, 4016.12], "text": " Moriarty, perfect example,"}, {"timestamp": [4016.12, 4018.04], "text": " Moriarty from Sherlock."}, {"timestamp": [4018.96, 4020.08], "text": " Moriarty"}, {"timestamp": [4021.24, 4022.0], "text": " from Sherlock."}, {"timestamp": [4023.32, 4031.76], "text": " So this dude, this crazy goofball, he hated Sherlock because he was an intellectual"}, {"timestamp": [4031.76, 4036.96], "text": " rival. And he and Sherlock both had a really good understanding of how each other worked,"}, {"timestamp": [4036.96, 4044.32], "text": " but also how human minds work in general. So the idea is that if you have a perfect model"}, {"timestamp": [4044.32, 4045.76], "text": " of how someone's mind works,"}, {"timestamp": [4045.76, 4053.68], "text": " or how humans work, you can use it for evil. And we have plenty of super villains out there,"}, {"timestamp": [4053.68, 4058.56], "text": " that their superpower is actually being able to understand and manipulate humans. Now,"}, {"timestamp": [4058.56, 4063.68], "text": " that being said, you can also use that same power for good. With great power comes great"}, {"timestamp": [4063.68, 4065.2], "text": " responsibility."}, {"timestamp": [4065.2, 4070.24], "text": " So by teaching a model how to better understand humans, that doesn't intrinsically make it"}, {"timestamp": [4070.24, 4074.6], "text": " good or evil, but it does give it more power, which is definitely something to pay attention"}, {"timestamp": [4074.6, 4075.6], "text": " to."}, {"timestamp": [4075.6, 4078.18], "text": " Good question."}, {"timestamp": [4078.18, 4080.1], "text": " What if a model chooses to be both?"}, {"timestamp": [4080.1, 4081.1], "text": " What do you mean both?"}, {"timestamp": [4081.1, 4082.1], "text": " Both good and evil?"}, {"timestamp": [4082.1, 4083.48], "text": " I'm not sure what you mean."}, {"timestamp": [4083.48, 4088.12], "text": " But also it's been like an hour, so I might might cut the stream soon cause I'm getting tired."}, {"timestamp": [4089.4, 4089.96], "text": " Let's see."}, {"timestamp": [4089.96, 4093.8], "text": " Do you expect molecular nanotechnology to be developed quickly after AGI?"}, {"timestamp": [4094.6, 4096.92], "text": " Which I predicted in 18 months. Yeah."}, {"timestamp": [4096.92, 4100.08], "text": " So there was actually a really great video. I can't remember the dude's name,"}, {"timestamp": [4100.12, 4103.36], "text": " but he was just on Lex Friedman and he talks about basically gray goo."}, {"timestamp": [4104.48, 4105.0], "text": " Um, definitely recommend watching this talks about basically gray goo."}, {"timestamp": [4105.0, 4109.3], "text": " Definitely recommend watching this. Gray goo."}, {"timestamp": [4109.3, 4114.1], "text": " Yeah, Neil Gershenfeld. Self-replicating robots."}, {"timestamp": [4114.1, 4118.2], "text": " Whoops. Sorry. So this dude, like I listen to most of it."}, {"timestamp": [4118.2, 4120.6], "text": " It's the most recent one on Lex Friedman."}, {"timestamp": [4120.6, 4125.4], "text": " If you want to understand about like nanotechnology or molecular level technology,"}, {"timestamp": [4125.4, 4129.6], "text": " this guy does a deep dive and he's going to explain it infinitely better than I ever could."}, {"timestamp": [4129.6, 4131.6], "text": " So, good question."}, {"timestamp": [4131.6, 4137.2], "text": " Do you think that once Tesla Android robots become quite good,"}, {"timestamp": [4137.2, 4142.0], "text": " other companies, for instance Chinese, will make similar bots at ultra-affordable prices?"}, {"timestamp": [4142.0, 4146.94], "text": " Yes, I do suspect that we're gonna see a new class of basically, I think that domestic robots"}, {"timestamp": [4146.94, 4150.58], "text": " are gonna be basically classified as appliances."}, {"timestamp": [4150.58, 4152.86], "text": " And so I think that we're gonna see a new class"}, {"timestamp": [4152.86, 4155.16], "text": " of appliances coming soon."}, {"timestamp": [4157.04, 4159.04], "text": " The first example is of course the Roomba, right?"}, {"timestamp": [4159.04, 4160.38], "text": " The iRobot Roomba."}, {"timestamp": [4161.48, 4164.12], "text": " That being said, once there's a new market,"}, {"timestamp": [4164.12, 4166.24], "text": " once there's a new segment that's available,"}, {"timestamp": [4166.24, 4172.0], "text": " yes, you're going to see competition drive the price down. That being said, the underlying cost"}, {"timestamp": [4172.0, 4178.56], "text": " of the batteries, the hardware, and the software, I expect that for maybe a three-foot-tall robot,"}, {"timestamp": [4178.56, 4184.08], "text": " you could probably get it for $1,200 or something like that. But for a human-sized scale robot,"}, {"timestamp": [4184.08, 4185.72], "text": " like a five-foot-tall robot, you're probably easily looking at $2,200 or something like that. But for a human-sized scale robot, like a five-foot tall robot,"}, {"timestamp": [4185.72, 4188.36], "text": " you're probably easily looking at $2,500"}, {"timestamp": [4188.36, 4190.66], "text": " as like the minimum cost if I had to guess."}, {"timestamp": [4192.52, 4194.48], "text": " At least not until we get the compounding returns"}, {"timestamp": [4194.48, 4196.2], "text": " of AI and automation,"}, {"timestamp": [4196.2, 4199.2], "text": " making the manufacturer and materials cheaper."}, {"timestamp": [4200.44, 4201.96], "text": " Do you think it might be the case"}, {"timestamp": [4201.96, 4205.68], "text": " that aligning with the concept of RLHF will not be enough"}, {"timestamp": [4205.68, 4209.4], "text": " and we might need to understand the specific neuron activations and the network to truly"}, {"timestamp": [4209.4, 4210.88], "text": " understand it?"}, {"timestamp": [4210.88, 4211.88], "text": " Yes."}, {"timestamp": [4211.88, 4217.28], "text": " So rather than measuring every individual neuron, that's not necessarily the best way"}, {"timestamp": [4217.28, 4219.96], "text": " to go because neurons are part of a network."}, {"timestamp": [4219.96, 4222.56], "text": " So what you need is actually a way of measuring the whole network."}, {"timestamp": [4222.56, 4225.0], "text": " And I was actually thinking about this last night."}, {"timestamp": [4225.0, 4231.92], "text": " And so what I think is that you might actually end up doing, developing ways of measuring"}, {"timestamp": [4231.92, 4235.52], "text": " network patterns of activation, right?"}, {"timestamp": [4235.52, 4237.36], "text": " Patterns in networks."}, {"timestamp": [4237.36, 4247.82], "text": " So basically what you can do is you can look at overall like activation patterns and then use that to"}, {"timestamp": [4247.82, 4250.86], "text": " understand what's going on. And we're actually doing that with with being able"}, {"timestamp": [4250.86, 4260.66], "text": " to record dreams. So like let's see what was it the brainwave to images right and"}, {"timestamp": [4260.66, 4266.96], "text": " so by by measuring brainwaves we can actually reverse engineer what those brainwave"}, {"timestamp": [4266.96, 4272.0], "text": " patterns are actually showing in people's minds. And I think we're going to basically"}, {"timestamp": [4272.0, 4276.74], "text": " need the same thing, but for AI. And so the idea is that rather than looking at individual"}, {"timestamp": [4276.74, 4282.46], "text": " neurons or parameters in the model, probably what we'll do is we'll have, we'll look at"}, {"timestamp": [4282.46, 4290.9], "text": " the activation patterns and say, okay, is this a deceptive behavior or is this a truthful behavior or is this an intellectual behavior or an emotional behavior?"}, {"timestamp": [4290.9, 4294.54], "text": " Whatever, right? So I think that we need more discriminators."}, {"timestamp": [4294.54, 4296.54], "text": " We need to look into the black box."}, {"timestamp": [4296.76, 4300.4], "text": " But the fact that we can look into the black box of human brains and animal brains,"}, {"timestamp": [4300.4, 4304.08], "text": " I think bodes very well for being able to look into the black box of"}, {"timestamp": [4305.76, 4308.48], "text": " artificial neural networks."}, {"timestamp": [4308.48, 4316.2], "text": " In a case per cases, it dealings with such good, I'm probably not replying to me."}, {"timestamp": [4316.2, 4318.28], "text": " When will iOS and Android integrate AI?"}, {"timestamp": [4318.28, 4323.72], "text": " They already have, you already have TPUs in your Android phone."}, {"timestamp": [4323.72, 4325.94], "text": " It's just they're not that powerful. I talked"}, {"timestamp": [4325.94, 4331.7], "text": " to some guys at Qualcomm a while ago and you know I don't know anything any"}, {"timestamp": [4331.7, 4336.32], "text": " internal secrets about Qualcomm but they said that they basically expect that AI"}, {"timestamp": [4336.32, 4342.9], "text": " is gonna be ubiquitous within the next few years. Let's see for the people who"}, {"timestamp": [4342.9, 4345.72], "text": " are trying to get out of their current job but only have"}, {"timestamp": [4345.72, 4352.3], "text": " a couple hours a day to work on something, is there a top five money makers with AI?"}, {"timestamp": [4352.3, 4358.16], "text": " I mean, that's where I started and I started with tinkering, fine tuning, and then making"}, {"timestamp": [4358.16, 4359.16], "text": " YouTube videos."}, {"timestamp": [4359.16, 4364.28], "text": " So I would say, yes, there's absolutely stuff you can do, but focus on where your passions"}, {"timestamp": [4364.28, 4368.62], "text": " lie and where your strengths lie. Those are the two best things that I can that I have"}, {"timestamp": [4368.62, 4372.12], "text": " come up with because I get this kind of question a lot. So focus on your one on"}, {"timestamp": [4372.12, 4377.46], "text": " your strengths and two on your passions. And if you if you focus on that in light"}, {"timestamp": [4377.46, 4383.54], "text": " of AI you'll probably do better. Did you know that Sam Altman is also CEO of a"}, {"timestamp": [4383.54, 4386.0], "text": " commercial fusion company called Helion Energy?"}, {"timestamp": [4386.0, 4390.0], "text": " I think I knew that and I know that he's like CEO of a bunch of stuff."}, {"timestamp": [4390.0, 4394.0], "text": " He's also got the world coin stuff, so like, who knows?"}, {"timestamp": [4394.0, 4401.0], "text": " Let's see. Someone says he's not the CEO, he's an investor. Okay, sure."}, {"timestamp": [4401.0, 4406.54], "text": " When do you think Linux distros like Ubuntu will be coming with a system like Cortana"}, {"timestamp": [4406.54, 4408.52], "text": " or Siri, but open source?"}, {"timestamp": [4408.52, 4411.36], "text": " You know, if the, what was it?"}, {"timestamp": [4411.36, 4419.68], "text": " The Open Assistant project, Open Assistant, if they're successful, maybe not too long,"}, {"timestamp": [4419.68, 4424.28], "text": " you know, or maybe not built in, because that doesn't seem like a Linux-y thing to do, but"}, {"timestamp": [4424.28, 4427.82], "text": " certainly like sudo apt-get install open assistant"}, {"timestamp": [4427.82, 4428.98], "text": " or something, right?"}, {"timestamp": [4428.98, 4430.94], "text": " That's probably coming before too long."}, {"timestamp": [4432.5, 4434.62], "text": " I wonder if there's a pattern among trained models."}, {"timestamp": [4434.62, 4436.16], "text": " Yeah, so there was a neuroscience,"}, {"timestamp": [4436.16, 4438.3], "text": " there's a podcast called Brain Inspired"}, {"timestamp": [4439.18, 4440.1], "text": " that is really good."}, {"timestamp": [4440.1, 4443.22], "text": " And Brain Inspired is actually where I got a lot"}, {"timestamp": [4443.22, 4445.6], "text": " of the insight that I had that went into"}, {"timestamp": [4446.32, 4450.96], "text": " everything that I talk about. So one of the things that they talked about is that is that vision"}, {"timestamp": [4450.96, 4457.84], "text": " models, so image models, actually end up with very similar neural patterns as human retinas and optic"}, {"timestamp": [4457.84, 4463.2], "text": " nerves. So there is some convergence between artificial neural networks and human neural"}, {"timestamp": [4463.2, 4468.6], "text": " networks or you know human neuroscience. That's not to say that the signals are exactly the same because"}, {"timestamp": [4468.6, 4472.2], "text": " obviously there's some fundamental differences in the substrate but"}, {"timestamp": [4472.2, 4477.12], "text": " certainly there are some convergences in those patterns, yes. I don't know how"}, {"timestamp": [4477.12, 4481.64], "text": " durable that research is. Let's see, what are your opinions on Neuralink getting"}, {"timestamp": [4481.64, 4492.24], "text": " FDA approval to test on humans? What are your predictions?\" Did it get approved or is that just Neuralink FDA? Oh yeah, seven"}, {"timestamp": [4492.24, 4496.48], "text": " days ago. Cool. So Elon Musk's Neuralink wins FDA approval for human study of"}, {"timestamp": [4496.48, 4503.64], "text": " brain implants. Well, considering how many monkeys and pigs died, yikes. Yeah, that's"}, {"timestamp": [4503.64, 4506.92], "text": " that's basically all I can say is, yeah,"}, {"timestamp": [4506.92, 4509.34], "text": " because they talked about this, that the company was rushing"}, {"timestamp": [4509.34, 4513.66], "text": " and botching surgeries on monkeys, pigs, and sheep."}, {"timestamp": [4513.66, 4518.16], "text": " Yeah, but at the same time, having talked to lots of people,"}, {"timestamp": [4518.16, 4521.52], "text": " so someone that I dated years ago, one of the things"}, {"timestamp": [4521.52, 4525.2], "text": " that she did was she looked at the at the scientific"}, {"timestamp": [4527.72, 4528.44], "text": " processes and methodologies that were used"}, {"timestamp": [4533.04, 4533.36], "text": " At amongst other places the FDA the FDA is incredibly rigorous"}, {"timestamp": [4536.14, 4536.2], "text": " Which is why it takes so long to get stuff approved"}, {"timestamp": [4540.24, 4544.28], "text": " So if the FDA approved it then I you know I trust the FDA a lot more than I trust Neuralink and that's not to trash Elon Musk"}, {"timestamp": [4544.28, 4550.26], "text": " Like I know that Elon Musk is super controversial, but like he does some things good and some"}, {"timestamp": [4550.26, 4551.26], "text": " things bad."}, {"timestamp": [4551.26, 4555.26], "text": " Like all humans, he's a nuanced and complicated creature."}, {"timestamp": [4555.26, 4559.92], "text": " But what I will say is that if the FDA gave their approval, all right, we'll see how it"}, {"timestamp": [4559.92, 4561.16], "text": " goes."}, {"timestamp": [4561.16, 4564.62], "text": " But I'm not going to hold my breath, put it that way."}, {"timestamp": [4564.62, 4565.6], "text": " They should call it Tux."}, {"timestamp": [4565.6, 4566.6], "text": " Yeah, there you go."}, {"timestamp": [4566.6, 4571.04], "text": " That's a good name for the Linux-based aid."}, {"timestamp": [4571.04, 4572.96], "text": " Let's see."}, {"timestamp": [4572.96, 4574.76], "text": " Internal goals that others don't anticipate."}, {"timestamp": [4574.76, 4575.76], "text": " Yeah."}, {"timestamp": [4575.76, 4581.2], "text": " So just going back to the idea of if you can watch the neural activations inside of language"}, {"timestamp": [4581.2, 4585.54], "text": " models and stuff, I'm sure you could ultimately make inferences about"}, {"timestamp": [4585.54, 4590.18], "text": " its intention as well as the methodology it's using."}, {"timestamp": [4590.18, 4591.18], "text": " Let's see."}, {"timestamp": [4591.18, 4596.04], "text": " Thanks for reading, or sorry, responding via Ben."}, {"timestamp": [4596.04, 4597.84], "text": " He's actually getting PR on Fox News."}, {"timestamp": [4597.84, 4598.84], "text": " Oh, interesting."}, {"timestamp": [4598.84, 4599.84], "text": " Okay."}, {"timestamp": [4599.84, 4604.48], "text": " He's been active on podcasts and he thinks his focus on social robotics with Sophia."}, {"timestamp": [4604.48, 4608.44], "text": " Yeah, so I actually exchanged a couple of messages on LinkedIn with the dude"}, {"timestamp": [4608.44, 4612.64], "text": " who's responsible for Sophia. And I, I don't know,"}, {"timestamp": [4613.36, 4618.08], "text": " like, yeah, I don't know. I don't know if it's a PR stunt or, or what. And that,"}, {"timestamp": [4618.1, 4621.88], "text": " again, that's not to trash anyone cause I haven't had an in-depth talk, but you know,"}, {"timestamp": [4621.9, 4624.62], "text": " Sophia has been out there for a while and having worked on cognitive"}, {"timestamp": [4624.62, 4630.56], "text": " architectures I will say that I'm not super convinced of the power of Sophia. So I'll just"}, {"timestamp": [4630.56, 4639.6], "text": " leave it at that. Let's see. Yeah. Oh, okay. So grab your parachutes. That was my point. What if"}, {"timestamp": [4639.6, 4648.72], "text": " AGI mirrors are sometimes good and sometimes bad nature? Okay, so you're getting into the underpinning like what is human nature versus like what is AI's nature."}, {"timestamp": [4648.72, 4653.44], "text": " That is a really good question and I actually talked about that in Sunday's"}, {"timestamp": [4653.44, 4656.88], "text": " video Axiomatic Alignment. I'll talk about what are the"}, {"timestamp": [4656.88, 4662.08], "text": " underpinning axioms that humans abide by as an animal versus what are the"}, {"timestamp": [4662.08, 4665.68], "text": " underpinning axioms that machines will abide by as a basically"}, {"timestamp": [4665.68, 4671.2], "text": " a different kind of life or a different kind of entity. So I'm not going to spoil Sunday's"}, {"timestamp": [4671.2, 4676.24], "text": " episode though. In terms of fine-tuning, I was thinking if you can fine-tune Ada to classify"}, {"timestamp": [4676.24, 4680.48], "text": " a question into multiple labels, is it a feasible task? I don't know about Ada,"}, {"timestamp": [4681.12, 4691.58], "text": " but certainly Curie is powerful enough and I haven't done too much experimentation on ADA and Babbage. Let's see, I think I got to everything. What are"}, {"timestamp": [4691.58, 4695.42], "text": " your opinions on Neuralink? Oh yeah, we already got that. Okay, what is the next"}, {"timestamp": [4695.42, 4701.7], "text": " hot topic in NLP and NLU? I think that it has to do with all the"}, {"timestamp": [4701.7, 4705.68], "text": " reinforcement learning stuff because we have RLHI which trained a"}, {"timestamp": [4705.68, 4711.84], "text": " discriminator to basically judge whether or not the output was going to be preferred by humans"}, {"timestamp": [4711.84, 4715.92], "text": " and now they're moving to something that's a little bit more rigorous with their recent paper"}, {"timestamp": [4715.92, 4720.32], "text": " and they have two discriminators rather than one. So I think that that's good, like pay attention"}, {"timestamp": [4720.32, 4725.36], "text": " to that because that's basically what I proposed in my book Benevolent by Design,"}, {"timestamp": [4729.36, 4730.08], "text": " and that's what Anthropic and others are talking about with principle-driven reinforcement learning."}, {"timestamp": [4734.64, 4742.88], "text": " So that is going to be a huge component to solving alignment. And hopefully by the end of the year we will have proof that inner alignment is solvable with these arrays of"}, {"timestamp": [4742.88, 4746.76], "text": " discriminators. And actually that was a really, really good question,"}, {"timestamp": [4746.76, 4748.4], "text": " so I'll probably end it there."}, {"timestamp": [4748.4, 4749.24], "text": " What's this last one?"}, {"timestamp": [4749.24, 4750.82], "text": " When do you think medicine will be able to make"}, {"timestamp": [4750.82, 4752.74], "text": " 99% of obese people their normal weight"}, {"timestamp": [4752.74, 4754.34], "text": " without any huge effects?"}, {"timestamp": [4754.34, 4757.68], "text": " That's, human metabolism is way too complicated"}, {"timestamp": [4757.68, 4759.54], "text": " for one question."}, {"timestamp": [4759.54, 4762.02], "text": " So I'll end it on that positive note"}, {"timestamp": [4762.02, 4766.0], "text": " about how discriminators and multiple discriminators"}, {"timestamp": [4766.0, 4769.0], "text": " I think are going to be a huge component. Maybe not the final solution"}, {"timestamp": [4769.0, 4772.0], "text": " but definitely a huge component in solving"}, {"timestamp": [4772.0, 4775.0], "text": " alignment. So thanks everyone for"}, {"timestamp": [4775.0, 4778.0], "text": " everything and we'll talk again"}, {"timestamp": [4778.0, 4781.0], "text": " soon. Cheers."}, {"timestamp": [4781.0, 4784.0], "text": " I'm going to try not to just cut off the stream too"}, {"timestamp": [4784.0, 4787.0], "text": " soon. Let's see."}, {"timestamp": [4783.84, 4790.28], "text": " I'm going to try not to just cut off the stream too soon."}, {"timestamp": [4790.28, 4790.88], "text": " Let's see."}]}