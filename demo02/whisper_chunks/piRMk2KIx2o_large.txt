{"text": " Alright, so I know that MemGPT and a few other things are all the rage right now, and I've gotten a lot of comments of people wanting me to talk about it, and I haven't talked about it, and the reason is because I figured this out months ago and I posted about it and people seem to have forgotten, so I am here to remind you that this is a much easier solution than MemGPT, it is much more powerful and you can use it right now, and it is called Sparse Priming Representations. So you can see and it is called Sparse Priming Representations. So you can see that I first recorded Sparse Priming Representations seven months ago. You can come out and read the repo, it's all free, it's under daveshapp.com slash sparseprimingrepresentations. The TLDR, and I'll go into a little bit more detail and examples in just a moment, but the TLDR is that language models work in a very similar way to human brains in this one particular way, and that is that they models work in a very similar way to human brains in this one particular way, and that is that they are associative. So semantic associations mean that all I need to do is tell you, let's say, the golden age of Rome. So three words, golden age of Rome, and what does that do? That conjures up millions of other ideas, facts, and images for your brain because those three words are associated with a lot of other stuff in your head. This is what we call a mental model. Now, large language models have lots of mental models as has been proven by the scientific literature. They have theory of mind, they have planning, they have reasoning, they have logic, and yeah I know that there are some flaws with the logic, but you know what? Humans have flawed logic too, so that's an arbitrary standard. Anyways so because large language models have semantic associations just like human brains all you need to do is give it shorthand notes in order to remind it of stuff that it already knows and so with stuff like memGPT where you're constantly doing these really complex loops to try and distill stuff and figure out what you need and blah blah blah like no that's so dumb what you're constantly doing these really complex loops to try and distill stuff and figure out what you need and blah blah blah. Like, no! That's so dumb! What you're trying to do with memory, with retrieval, is you're trying to create an internal state in the language model that is good enough to be useful in the future. And so the idea is, well, here, let me just show you the custom instruction that I got. So I'll just read this to you real quick because this is kind of the depth of the theory. So sparse priming representations. There are only a handful of ways to teach large language models and all have limitations and strengths. First is initial bulk training. This is ludicrously expensive and not practical. Number two is fine tuning. Fine tuning is not necessarily useful for knowledge retrieval. Maybe that changes in the future as fine tuning techniques advance, buttuning is not necessarily useful for knowledge retrieval. Maybe that changes in the future as fine-tuning techniques advance, but you don't use fine-tuning for knowledge retrieval. People have, by now it seems like they've accepted that because now the focus is on retrieval augmented generation, RAG. Online learning. So online learning is the idea that maybe the language model will continuously learn based on what you feed it. We'll see if that pans out. As far as I know, there's no commercially viable options for that yet. And then finally, in-context learning. In-context learning is literally the only way to teach models anything that is outside of its training distribution. Everything else is just window dressing. So because of all this rag, retrieval augmented generation is all the rage right now. Tools like vector databases and knowledge graphs are being used, but you quickly fill up the context window with dumb retrieval augmented generation, is all the rage right now. Tools like vector databases and knowledge graphs are being used, but you quickly fill up the context window with dumb retrieval. And so what I mean by dumb retrieval is you just try and match whatever is in the context to an arbitrary number of nodes in your knowledge graph or your vector database, and then you just fill it all in. Now, in the long run, I suspect that we're going to have language models that are big enough that you can just say like, here's 10,000 KB articles, read them all and tell me what's relevant. You know, you look at LM Infinite, you look at Microsoft LongNet, eventually we're gonna have that, but there's still always gonna be costs associated with those gigantic context windows. So you're always going to need to figure out how to distill knowledge. So one of the most common questions I get is, Dave, how do you overcome the context window limitations? The short answer is you don't stop wasting your time trying to do it. It is a algorithmic limitation. You have, you know, 20,000 tokens or 100,000 tokens, you have those tokens, that's it, stop trying to get around it. It's like trying to stuff 10 pounds of stuff in a 5 pound bag, you know the saying. It's just not going to work. Now, okay, there is one giant asterisk here though. Most of the techniques out there do not make use of the best superpower that language models have, latent space. No one else seems to understand that there is one huge way that LLMs work similar to human minds\u2014associative learning. Here's the story. I realized a long time ago that with just a few words you could prime the LLMs to think in a certain way. I did a bunch of experiments and I found that you can prime models to even understand complex novel ideas that were outside its training distribution. For instance, I taught the models some of my concepts like heuristic imperatives, the ACE framework, terminal race condition, and a bunch of other stuff that I made up that is absolutely outside of the training data. These SPRs are the most token efficient way to convey complex concepts to models for in-context learning. What you do is you compress huge blocks of information, be it company data, chat logs, specific events, or whatever, into SPRs, and then you store the SPR in the metadata of your knowledge graph node or in your vector store or whatever. And then the SPR is what you inject at inference time, not the human readable data. So then I created a system window that you can use to compress pretty much anything into an SPR. Now the, let's see, where is it? So you can read the original theory under sparse priming representations. I did add the system message here as well, so you can just copy paste this into the chat GPT, either the custom instructions or the system window in the API. So mission, you are a sparse priming representation writer and SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks. So again, this is itself a sparse priming representation. I'm giving it a term and I'm labeling it. I'm saying SPR, so now it knows what an SPR is, and I'm associating SPR, this novel term, with NLP, NLU, and NLG. So it understands, okay, cool, we're very deliberately activating certain associations inside of GPT. Particularly useful for the latest generation of large language models. Again, I'm giving it an association. You will be given information by the user which you are to render as an SPR. Theory. LLMs are a kind of deep neural network. GPT often gets LLMs wrong because for whatever reason OpenAI decided not to teach it about AI because, you know, maybe it'll escape the lab, whatever. They have been demonstrated to embed knowledge, abilities, and concepts ranging from reasoning to planning and even to theory of mind. So I'm giving it an unequivocal assertion, this is what an LLM is. I'm not telling it, you're an LLM, because then it'll get all bent out of shape and say, well that's an AI language model. I'm like, no, this is what LLMs are and this is what LLMs are capable of. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like humans, LLMs are associative. Again, very, very specific use of words. By giving it the word associative, and human minds, and all these other things, it's now thinking in a certain way internally. Meaning you only need to use the correct associations to prime another model to think in the same way. Methodology. Render the input as a distilled list of succinct statements. So this is, again, very, very specific verbs and adjectives. Render, distilled, succinct. Statements, assertions, associations, concepts, analogies, and metaphors. The idea is to capture as much conceptually as possible, but with as few words as possible. Write it in a way that makes sense to you, as the future audience will be another language model, not a human. Okay, so I just gave a lot to you. Here's an example. So I took that mission statement on GPT-4, temperature zero, and I gave it about half of my ACE framework. So you can see here's a tremendous amount of text, and then what it spits out is just a list of statements. And this list of statements, if you read it, you're like, okay, cool, you know, Ace Framework, Blueprint for Architects, Task Prosecution Layer. So then what you do is you take this SPR, which took, you know, you can get, in this case it looks like you get about 20 to 1 compression, and so then you take the SPR and you say, okay, cool, let's clear all this out. And then you say, let's see, the following is an SPR, sparse priming representation. And what I'll do is I'll actually add a second system message by the time you see this so that you can actually have a system message to unpack SPRs. Unpack this into its full content. And so you can see what it's doing is it's basically a form of semantic compression. So because large language models are associative all I needed to do was give it enough details about something and now I can reconstruct the original idea. This is far and away the best token efficient way of conveying things. Now obviously this is not quite as long but you can also then ask questions. Let's see, how does it handle ethical decision-making? Ethical decision-making is primarily handled by the aspirational layer. This layer provides a blah blah blah. Now there are a few details that are that are missing from this thing, but again when you get when you get a representation that is that efficient, you can play around with it and you can also have multiple SPRs associated with various concepts or nested in the metadata. So anyways, thanks for watching. I hope you got a lot out of this. Now stop asking me about MemGBD. Come on. This is kid stuff. SPRs are the way to go. Bye. SPRs are the way to go. Bye.", "chunks": [{"timestamp": [0.0, 5.6], "text": " Alright, so I know that MemGPT and a few other things are all the rage right now, and I've"}, {"timestamp": [5.6, 9.46], "text": " gotten a lot of comments of people wanting me to talk about it, and I haven't talked"}, {"timestamp": [9.46, 13.76], "text": " about it, and the reason is because I figured this out months ago and I posted about it"}, {"timestamp": [13.76, 18.0], "text": " and people seem to have forgotten, so I am here to remind you that this is a much easier"}, {"timestamp": [18.0, 22.86], "text": " solution than MemGPT, it is much more powerful and you can use it right now, and it is called"}, {"timestamp": [22.86, 25.0], "text": " Sparse Priming Representations. So you can see and it is called Sparse Priming Representations."}, {"timestamp": [25.0, 27.06], "text": " So you can see that I first recorded"}, {"timestamp": [27.06, 29.62], "text": " Sparse Priming Representations seven months ago."}, {"timestamp": [29.62, 31.62], "text": " You can come out and read the repo, it's all free,"}, {"timestamp": [31.62, 34.96], "text": " it's under daveshapp.com slash sparseprimingrepresentations."}, {"timestamp": [34.96, 37.94], "text": " The TLDR, and I'll go into a little bit more detail"}, {"timestamp": [37.94, 39.06], "text": " and examples in just a moment,"}, {"timestamp": [39.06, 41.72], "text": " but the TLDR is that language models work"}, {"timestamp": [41.72, 44.26], "text": " in a very similar way to human brains"}, {"timestamp": [44.26, 45.88], "text": " in this one particular way, and that is that they models work in a very similar way to human brains in this one particular way,"}, {"timestamp": [45.88, 48.84], "text": " and that is that they are associative."}, {"timestamp": [48.84, 52.04], "text": " So semantic associations mean that all I need to do"}, {"timestamp": [52.04, 57.04], "text": " is tell you, let's say, the golden age of Rome."}, {"timestamp": [57.38, 59.18], "text": " So three words, golden age of Rome,"}, {"timestamp": [59.18, 60.12], "text": " and what does that do?"}, {"timestamp": [60.12, 63.14], "text": " That conjures up millions of other ideas,"}, {"timestamp": [63.14, 64.48], "text": " facts, and images for your brain"}, {"timestamp": [64.48, 69.44], "text": " because those three words are associated with a lot of other stuff in your head."}, {"timestamp": [69.44, 74.12], "text": " This is what we call a mental model. Now, large language models have lots of mental"}, {"timestamp": [74.12, 77.2], "text": " models as has been proven by the scientific literature. They have theory"}, {"timestamp": [77.2, 80.96], "text": " of mind, they have planning, they have reasoning, they have logic, and yeah I"}, {"timestamp": [80.96, 84.14], "text": " know that there are some flaws with the logic, but you know what? Humans have"}, {"timestamp": [84.14, 87.92], "text": " flawed logic too, so that's an arbitrary standard. Anyways so"}, {"timestamp": [87.92, 92.16], "text": " because large language models have semantic associations just like human"}, {"timestamp": [92.16, 96.34], "text": " brains all you need to do is give it shorthand notes in order to remind it of"}, {"timestamp": [96.34, 100.36], "text": " stuff that it already knows and so with stuff like memGPT where you're"}, {"timestamp": [100.36, 103.72], "text": " constantly doing these really complex loops to try and distill stuff and"}, {"timestamp": [103.72, 108.08], "text": " figure out what you need and blah blah blah like no that's so dumb what you're constantly doing these really complex loops to try and distill stuff and figure out what you need and blah blah blah. Like, no! That's so dumb! What you're trying"}, {"timestamp": [108.08, 113.6], "text": " to do with memory, with retrieval, is you're trying to create an internal state in the"}, {"timestamp": [113.6, 119.92], "text": " language model that is good enough to be useful in the future. And so the idea is, well, here,"}, {"timestamp": [119.92, 129.64], "text": " let me just show you the custom instruction that I got. So I'll just read this to you real quick because this is kind of the depth of the theory."}, {"timestamp": [129.64, 131.2], "text": " So sparse priming representations."}, {"timestamp": [131.2, 135.96], "text": " There are only a handful of ways to teach large language models and all have limitations and strengths."}, {"timestamp": [135.96, 137.28], "text": " First is initial bulk training."}, {"timestamp": [137.28, 139.8], "text": " This is ludicrously expensive and not practical."}, {"timestamp": [139.8, 141.0], "text": " Number two is fine tuning."}, {"timestamp": [141.0, 144.04], "text": " Fine tuning is not necessarily useful for knowledge retrieval."}, {"timestamp": [144.04, 148.58], "text": " Maybe that changes in the future as fine tuning techniques advance, buttuning is not necessarily useful for knowledge retrieval. Maybe that changes in the future as fine-tuning techniques advance, but you don't use fine-tuning"}, {"timestamp": [148.58, 149.7], "text": " for knowledge retrieval."}, {"timestamp": [149.7, 153.42], "text": " People have, by now it seems like they've accepted that because now the focus is on"}, {"timestamp": [153.42, 156.6], "text": " retrieval augmented generation, RAG."}, {"timestamp": [156.6, 157.6], "text": " Online learning."}, {"timestamp": [157.6, 161.06], "text": " So online learning is the idea that maybe the language model will continuously learn"}, {"timestamp": [161.06, 162.7], "text": " based on what you feed it."}, {"timestamp": [162.7, 163.82], "text": " We'll see if that pans out."}, {"timestamp": [163.82, 166.6], "text": " As far as I know, there's no commercially viable options"}, {"timestamp": [166.6, 167.4], "text": " for that yet."}, {"timestamp": [167.4, 169.72], "text": " And then finally, in-context learning."}, {"timestamp": [169.72, 171.92], "text": " In-context learning is literally the only way"}, {"timestamp": [171.92, 174.48], "text": " to teach models anything that is outside of its training"}, {"timestamp": [174.48, 175.48], "text": " distribution."}, {"timestamp": [175.48, 178.26], "text": " Everything else is just window dressing."}, {"timestamp": [178.26, 181.8], "text": " So because of all this rag, retrieval augmented generation"}, {"timestamp": [181.8, 183.12], "text": " is all the rage right now."}, {"timestamp": [183.12, 185.6], "text": " Tools like vector databases and knowledge graphs are being used, but you quickly fill up the context window with dumb retrieval augmented generation, is all the rage right now. Tools like vector databases and knowledge graphs are being used,"}, {"timestamp": [185.6, 188.6], "text": " but you quickly fill up the context window"}, {"timestamp": [188.6, 191.8], "text": " with dumb retrieval. And so what I mean by dumb retrieval is you just"}, {"timestamp": [191.8, 194.6], "text": " try and match whatever is in the context"}, {"timestamp": [194.6, 197.0], "text": " to an arbitrary number of"}, {"timestamp": [197.0, 200.0], "text": " nodes in your knowledge graph or your vector database,"}, {"timestamp": [200.0, 201.8], "text": " and then you just fill it all in."}, {"timestamp": [201.8, 204.6], "text": " Now, in the long run, I suspect that we're going to have"}, {"timestamp": [204.6, 207.84], "text": " language models that are big enough"}, {"timestamp": [207.84, 211.46], "text": " that you can just say like, here's 10,000 KB articles,"}, {"timestamp": [211.46, 213.36], "text": " read them all and tell me what's relevant."}, {"timestamp": [213.36, 215.02], "text": " You know, you look at LM Infinite,"}, {"timestamp": [215.02, 217.52], "text": " you look at Microsoft LongNet,"}, {"timestamp": [217.52, 218.68], "text": " eventually we're gonna have that,"}, {"timestamp": [218.68, 221.12], "text": " but there's still always gonna be costs associated"}, {"timestamp": [221.12, 223.24], "text": " with those gigantic context windows."}, {"timestamp": [223.24, 224.96], "text": " So you're always going to need to figure out"}, {"timestamp": [224.96, 226.68], "text": " how to distill knowledge."}, {"timestamp": [226.68, 228.68], "text": " So one of the most common questions I get is,"}, {"timestamp": [228.68, 230.96], "text": " Dave, how do you overcome the context window limitations?"}, {"timestamp": [230.96, 232.88], "text": " The short answer is you don't stop wasting your time"}, {"timestamp": [232.88, 234.2], "text": " trying to do it."}, {"timestamp": [234.2, 236.12], "text": " It is a algorithmic limitation."}, {"timestamp": [236.12, 240.16], "text": " You have, you know, 20,000 tokens or 100,000 tokens,"}, {"timestamp": [240.16, 241.88], "text": " you have those tokens, that's it,"}, {"timestamp": [241.88, 244.04], "text": " stop trying to get around it."}, {"timestamp": [244.04, 249.64], "text": " It's like trying to stuff 10 pounds of stuff in a 5 pound bag, you know the saying."}, {"timestamp": [249.64, 251.64], "text": " It's just not going to work."}, {"timestamp": [251.64, 254.94], "text": " Now, okay, there is one giant asterisk here though."}, {"timestamp": [254.94, 260.8], "text": " Most of the techniques out there do not make use of the best superpower that language models have, latent space."}, {"timestamp": [260.8, 268.68], "text": " No one else seems to understand that there is one huge way that LLMs work similar to human minds\u2014associative learning. Here's the story. I realized a long time"}, {"timestamp": [268.68, 272.88], "text": " ago that with just a few words you could prime the LLMs to think in a certain way. I did"}, {"timestamp": [272.88, 276.42], "text": " a bunch of experiments and I found that you can prime models to even understand complex"}, {"timestamp": [276.42, 280.96], "text": " novel ideas that were outside its training distribution. For instance, I taught the models"}, {"timestamp": [280.96, 285.12], "text": " some of my concepts like heuristic imperatives, the ACE framework, terminal race condition,"}, {"timestamp": [285.12, 287.76], "text": " and a bunch of other stuff that I made up that is absolutely"}, {"timestamp": [287.76, 290.28], "text": " outside of the training data."}, {"timestamp": [290.28, 292.12], "text": " These SPRs are the most token efficient way"}, {"timestamp": [292.12, 296.0], "text": " to convey complex concepts to models for in-context learning."}, {"timestamp": [296.0, 298.36], "text": " What you do is you compress huge blocks of information,"}, {"timestamp": [298.36, 301.56], "text": " be it company data, chat logs, specific events, or whatever,"}, {"timestamp": [301.56, 303.56], "text": " into SPRs, and then you store the SPR"}, {"timestamp": [303.56, 308.76], "text": " in the metadata of your knowledge graph node or in your vector store or whatever."}, {"timestamp": [308.76, 313.16], "text": " And then the SPR is what you inject at inference time, not the human readable data."}, {"timestamp": [313.16, 319.36], "text": " So then I created a system window that you can use to compress pretty much anything into"}, {"timestamp": [319.36, 320.68], "text": " an SPR."}, {"timestamp": [320.68, 323.64], "text": " Now the, let's see, where is it?"}, {"timestamp": [323.64, 325.0], "text": " So you can read the original theory"}, {"timestamp": [325.0, 327.3], "text": " under sparse priming representations."}, {"timestamp": [327.3, 329.22], "text": " I did add the system message here as well,"}, {"timestamp": [329.22, 333.4], "text": " so you can just copy paste this into the chat GPT,"}, {"timestamp": [333.4, 335.02], "text": " either the custom instructions"}, {"timestamp": [335.02, 337.58], "text": " or the system window in the API."}, {"timestamp": [337.58, 340.44], "text": " So mission, you are a sparse priming representation writer"}, {"timestamp": [340.44, 349.0], "text": " and SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks."}, {"timestamp": [349.0, 352.0], "text": " So again, this is itself a sparse priming representation."}, {"timestamp": [352.0, 357.0], "text": " I'm giving it a term and I'm labeling it. I'm saying SPR, so now it knows what an SPR is,"}, {"timestamp": [357.0, 366.48], "text": " and I'm associating SPR, this novel term, with NLP, NLU, and NLG. So it understands, okay, cool, we're very deliberately activating certain"}, {"timestamp": [366.48, 373.28], "text": " associations inside of GPT. Particularly useful for the latest generation of large language models."}, {"timestamp": [373.28, 377.52], "text": " Again, I'm giving it an association. You will be given information by the user which you are"}, {"timestamp": [377.52, 388.64], "text": " to render as an SPR. Theory. LLMs are a kind of deep neural network. GPT often gets LLMs wrong because for whatever reason OpenAI decided not to teach it about"}, {"timestamp": [388.64, 392.0], "text": " AI because, you know, maybe it'll escape the lab, whatever."}, {"timestamp": [392.0, 395.64], "text": " They have been demonstrated to embed knowledge, abilities, and concepts ranging from reasoning"}, {"timestamp": [395.64, 397.52], "text": " to planning and even to theory of mind."}, {"timestamp": [397.52, 402.8], "text": " So I'm giving it an unequivocal assertion, this is what an LLM is."}, {"timestamp": [402.8, 407.94], "text": " I'm not telling it, you're an LLM, because then it'll get all bent out of shape and say, well that's an AI language model."}, {"timestamp": [407.94, 412.02], "text": " I'm like, no, this is what LLMs are and this is what LLMs are capable of. These are"}, {"timestamp": [412.02, 415.1], "text": " called latent abilities and latent content, collectively referred to as"}, {"timestamp": [415.1, 419.3], "text": " latent space. The latent space of an LLM can be activated with the correct series"}, {"timestamp": [419.3, 423.04], "text": " of words as inputs, which will create a useful internal state of the neural"}, {"timestamp": [423.04, 428.32], "text": " network. This is not unlike how the right shorthand cues can prime a human mind to think in a"}, {"timestamp": [428.32, 429.4], "text": " certain way."}, {"timestamp": [429.4, 431.24], "text": " Like humans, LLMs are associative."}, {"timestamp": [431.24, 434.4], "text": " Again, very, very specific use of words."}, {"timestamp": [434.4, 439.12], "text": " By giving it the word associative, and human minds, and all these other things, it's now"}, {"timestamp": [439.12, 442.48], "text": " thinking in a certain way internally."}, {"timestamp": [442.48, 447.0], "text": " Meaning you only need to use the correct associations to prime another model to think in the same way."}, {"timestamp": [447.0, 451.0], "text": " Methodology. Render the input as a distilled list of succinct statements."}, {"timestamp": [451.0, 455.0], "text": " So this is, again, very, very specific verbs and adjectives."}, {"timestamp": [455.0, 457.0], "text": " Render, distilled, succinct."}, {"timestamp": [457.0, 461.0], "text": " Statements, assertions, associations, concepts, analogies, and metaphors."}, {"timestamp": [461.0, 465.2], "text": " The idea is to capture as much conceptually as possible, but with as few words as possible."}, {"timestamp": [465.2, 468.76], "text": " Write it in a way that makes sense to you, as the future audience will be another language"}, {"timestamp": [468.76, 470.4], "text": " model, not a human."}, {"timestamp": [470.4, 473.24], "text": " Okay, so I just gave a lot to you."}, {"timestamp": [473.24, 474.24], "text": " Here's an example."}, {"timestamp": [474.24, 479.08], "text": " So I took that mission statement on GPT-4, temperature zero, and I gave it about half"}, {"timestamp": [479.08, 480.28], "text": " of my ACE framework."}, {"timestamp": [480.28, 486.0], "text": " So you can see here's a tremendous amount of text, and then what it spits out is just a list of statements."}, {"timestamp": [486.0, 489.0], "text": " And this list of statements, if you read it, you're like,"}, {"timestamp": [489.0, 492.0], "text": " okay, cool, you know, Ace Framework, Blueprint for Architects,"}, {"timestamp": [492.0, 493.0], "text": " Task Prosecution Layer."}, {"timestamp": [493.0, 496.0], "text": " So then what you do is you take this SPR, which took, you know,"}, {"timestamp": [496.0, 501.0], "text": " you can get, in this case it looks like you get about 20 to 1 compression,"}, {"timestamp": [501.0, 504.0], "text": " and so then you take the SPR and you say, okay, cool,"}, {"timestamp": [504.0, 506.0], "text": " let's clear"}, {"timestamp": [506.0, 508.0], "text": " all this out."}, {"timestamp": [508.0, 510.0], "text": " And then you say,"}, {"timestamp": [510.0, 512.0], "text": " let's see,"}, {"timestamp": [512.0, 514.0], "text": " the following is an"}, {"timestamp": [514.0, 516.0], "text": " SPR, sparse"}, {"timestamp": [516.0, 518.0], "text": " priming"}, {"timestamp": [518.0, 520.0], "text": " representation."}, {"timestamp": [520.0, 522.0], "text": " And what I'll do"}, {"timestamp": [522.0, 524.0], "text": " is I'll actually add a second system"}, {"timestamp": [524.0, 528.0], "text": " message by the time you see this so that you can actually have a system message"}, {"timestamp": [528.0, 532.0], "text": " to unpack SPRs. Unpack this into its"}, {"timestamp": [532.0, 536.0], "text": " full content."}, {"timestamp": [536.0, 540.0], "text": " And so you can see what it's doing is it's basically a form"}, {"timestamp": [540.0, 544.0], "text": " of semantic compression. So because"}, {"timestamp": [544.0, 546.0], "text": " large language models are associative"}, {"timestamp": [546.0, 548.0], "text": " all I needed to do was give it"}, {"timestamp": [548.0, 550.0], "text": " enough details about something and now I can"}, {"timestamp": [550.0, 552.0], "text": " reconstruct the original idea."}, {"timestamp": [552.0, 554.0], "text": " This is far and away"}, {"timestamp": [554.0, 556.0], "text": " the best token efficient way"}, {"timestamp": [556.0, 558.0], "text": " of conveying"}, {"timestamp": [558.0, 560.0], "text": " things. Now obviously this is not quite as long"}, {"timestamp": [560.0, 562.0], "text": " but you can also then ask questions."}, {"timestamp": [562.0, 564.0], "text": " Let's see, how does"}, {"timestamp": [564.0, 567.76], "text": " it handle ethical decision-making?"}, {"timestamp": [569.48, 574.88], "text": " Ethical decision-making is primarily handled by the aspirational layer. This layer provides a blah blah blah. Now"}, {"timestamp": [574.88, 579.64], "text": " there are a few details that are that are missing from this thing, but again"}, {"timestamp": [580.84, 582.84], "text": " when you get when you get a"}, {"timestamp": [583.4, 585.0], "text": " representation that is that efficient,"}, {"timestamp": [585.0, 589.0], "text": " you can play around with it and you can also have multiple SPRs associated"}, {"timestamp": [589.0, 593.0], "text": " with various concepts or nested in the metadata."}, {"timestamp": [593.0, 596.0], "text": " So anyways, thanks for watching. I hope you got a lot out of this."}, {"timestamp": [596.0, 599.0], "text": " Now stop asking me about MemGBD. Come on."}, {"timestamp": [599.0, 602.0], "text": " This is kid stuff. SPRs are the way to go. Bye."}, {"timestamp": [597.45, 601.45], "text": " SPRs are the way to go. Bye."}]}