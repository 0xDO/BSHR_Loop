{"text": " Morning everybody, David Shapiro here for my third video in the 0 to Python and GPT-3 bootcamp. What the heck are embeddings? I get this question all the time. It is by far the biggest, hottest topic, so this is why I'm doing it as episode 3. But before we get started, I'm going to ask that you consider liking and subscribing this video, and also jump over to my Patreon to support me there. If I get to enough support, who knows, maybe I can do this full-time one day. Anyways, let's go ahead and jump into the video. What are embeddings? First, I need to give you a little bit of background. So basically what an embedding is, it's a vector. So what is a vector? Let's just start from scratch. A vector is any string of numbers in an array. This is a vector. Here, let me make this a little bit bigger so that you can see it. Okay, so we'll do vector. So hashtag vector, let me set the language to Python so that it looks right. Ta-da, okay. This is a vector. Oops. This is a vector. So the mathematical definition of a vector, it is a one-dimensional matrix. So I'll do aka one-dimensional matrix or a list. Okay, so that's what a vector is. Great. So what then is an embedding? So the difference between a vector and an embedding is mathematically they're the same, but an embedding has semantic meaning. And if you want to take a deeper dive I'll have this link in the description. This is tensorflow.org which is made by Google. They really advanced this technology a few years ago with Universal Sentence Encoder. This is the progenitor technology that allowed GPT-3 to exist, and they did this starting in I think about 2014. Anyways, so if you want a deep dive, follow this link, but just for the sake of this video, I'll show you kind of a short version. So embedding equals vector with semantic meaning. So older NLP stuff like NLTK would do like word webs and stuff where like semantic meaning was always relevant to other words. So like cat is a type of mammal for instance, it's also a type of animal, it's associated with pet. But that was not very efficient. Those technologies are really old and they thought it was going to revolutionize NLP. It certainly did a lot, but it was not as flexible as neural networks are today. So an embedding is just a vector with semantic meaning. So how do we, like, what does that mean? Okay, so let's say each position, each index in our vector has a meaning. So we'll say, we'll do X and Y. So we're gonna populate this vector with just two values and we'll say that position x equals social power right so max of 1.0 min of negative 1.0 so we're basically going to make our own embedding. And then this is based on one of the original examples that Google used to use. Position y equals gender. Gender or sex. So we'll say max is 1.0 min of negative 1.0 min of negative 1.0. So we'll say like one equals ultra masculine and negative one equals ultra feminine. Okay, so we have this one by two matrix and we're gonna use it to represent a person. So, or semantic meaning. So if we have a semantic meaning that is one by one, and we say that social power is maximum of one and 1.0 is ultra masculine, right? So what would this be? we can we know that okay so what who has a maximum social power theoretical so that sounds like an emperor so we'll say this is the Padishah Emperor of the known universe known universe Dune reference there So maximum social power possible and also maximum masculinity. I actually don't know if the emperor was maximum masculinity. He might actually be closer to like 0.5. So we'll just say that because like you think ultra masculine, you think of like what if the emperor was like the rock. So then let's duplicate that we'll say 1.1 punish a ember if he was Dwayne Johnson. And so there we have we have our first two embeddings. Now okay so what if we do the other one? What if we go the opposite way? So like negative 1.0 that would be like a peasant, right? Or actually probably someone who doesn't have free will. So like a prisoner or something. And then we say like zero. So this is like someone with no free will or agency and also gender neutral. Right, so that's what a semantic meaning is. Now, with GPT-3, the smallest one, if you go to their embeddings, the smallest one, Ada, has 1,024 dimensions. So what happens is these models are trained to break down semantic meaning into many,24 dimensions. So what happens is these models are trained to break down semantic meaning into many, many dimensions. And DaVinci has 12,000 dimensions. So here we're just doing it with one, or sorry, two vectors with two dimensions each. So, you know, this is like super, super simple. All right, so now you know the basic of what I mean when I say vector or embedding. So again, the only difference between a vector and an embedding is that an embedding has semantic meaning. And each of those positions is somewhat abstract. Okay, so then what do you do with it though? Like how do you compare one of these to another? I'm glad you asked. So we're gonna do a basic similarity search. We're gonna do a classification problem. So let's start with, oh, first let me introduce. I've added import NumPy as np. So this is standard math package, I'll say module for Python. And so when you do import something as something else, this allows you to refer to it as shorthand. So if I double click on that, you see that NumPy is used down here. So the way to use these vectors is to compare them with a dot product. And then that dot product, the higher the dot product, the more similar the vectors are. That's it. It's that simple. So I've added this function. It's super simple. All it does is return the dot product between these two vectors. And then I've added the GPT-3 embedding, where you just pass it a string. And I've got the engine already set to text similarity ADA. This will suffice for many things, especially if you're just doing like a single sentence. If you look at like the original universal sentence encoder, they did like, I think like 124 and 256 and 512 vectors or dimensions. The newest, newer ones are probably bigger than that. All these links will be in the description if you'd need them. All right. If name equals main. So what are we gonna do? What do we wanna do with this? Well, first, let's just do a really simple dot product of like, where did it go? These. So let's say like, okay, what's the difference between like the emperor of the known universe if he was Dwayne Johnson versus if it was just, you know, the normal super masculine one. So we'll go here and we'll say, we'll say v1 for vector one equals 1.0, 1.0, v2 equals 1.0 and then 0.5. So then let's do print similarity. So then this will be Emperor Duane and then this will just be Padishah Emperor. So then we'll return similarity of v1, v2. All right, so let's run this real quick. CD Python GPD 3 tutorial, Python embedding. All right, so the dot product is 1.5. So that is the level of similarity. Now, what happens if we do a third one? We'll do the androgynous. So we'll do negative 1.0 for social power and then 0.0 for... so this is like the gender neutral prisoner. All right, so then we'll duplicate this and we'll swap out v2 for v3. So we'll get two outputs in this one. So I'll just do hashtag or pound sign 1.5. So that's the score that it returned. Oh, whoops, I need to save it. There we go. Okay, so then you see like,ops, I need to save it. There we go. Okay, so then you see like, oh, wait, hold on now. This is way different. So this one is negative 1.0. Okay, so now you're starting to get the idea of what it means to compare these. Now, when you have, you know, 1, a thousand or twelve thousand dimensions, these numbers get much more nuanced. Okay, so how do we use this? How do we use this to do search or comparison or clustering? So I had this idea. So we're gonna do categories equals and we'll do plant, reptile, mammal, fish. So we're just gonna do, we're gonna do four categories. And okay, so then what, what do we do with that? We'll do a while loop, while true, and we'll do a equals input, enter a life form here. So for Python, you're supposed, like the default is that you use single quotes. And I can't remember off the top of my head right now when you're supposed to switch between single quotes and double quotes, but like PowerShell, for instance, the default is you're supposed to use double quotes. Okay, so we get a live form and then what do we do with that? So then we'll do vector equals GPT-3 embedding of our input. So we'll get an embedding back. And just so that you can see what this looks like, let's just do a quick printout of this. So we saved it. CLS, Python embedding. Enter a life form here. We'll do bald eagle because this is America. And it spits out a huge number. You see how big this number is? Well it's actually a list of numbers but you get the idea. So this is the Ada version semantic embedding of bald eagle. It is 1,024 floating point numbers between negative 1.0 and 1.0. We can do this with DaVinci and it'll be even longer. Why did this get messed up? Oh, there we go. So there you have it. That's what an embedding looks like. So it just, it shoots it back real quick. All right, but what are we gonna do with this? So what we wanna do is we wanna find, we wanna classify. What did I put in and we wanna match it to one of these categories. So how do we do that? Well, the first thing we can do is, and this will be really inefficient because what I'm gonna do is I'm gonna get a vector for these each time. But really what we should do in the long run, in a longer video or in a future video, we'll store these. We'll store the embeddings for each of these. So let's do a function. So we'll match, let's see, result equals match class. And so what we'll do is we'll pass our initial vector and our categories. And so this is the vector. So we're gonna get the vector for whatever life form we put in was, and then we're gonna pass these categories. So then we'll do def match class and then we'll do vector and our categories. And I know in a previous video I said that it's best practices not to reuse the same variable names, but since I'm not modifying these I'm just reading them it's not that big a deal, but it's still poor practice so just keep that in mind. I don't always follow the rules. Okay, so match class. So the first thing that we need to do is for each of these categories. So we'll do classes equals list. And then for C in categories, we'll do, we'll just copy this real quick. Vector equals GPT-3 embedding, but instead of A we'll do C. So that's this, so that, well, this is a for loop, which means that we're gonna iterate through each of these items. And we're gonna say, okay, what is my vector? And actually, you know what? If I got to write this, I might as well put this down below. So we declare our categories. So we'll do for C in categories. And we'll do this here. So let's run this once. So this is another rule of thumb. If you're going to run a piece of code more than once, you put it in a place that one it'll only run once if you only need to. But also if you need to call it multiple times, then you then you declare it as a function. So let's just clean this up here first. We'll do a little bit better. All right, so we get the vector for the category. And so then what do we do? Well, we want to save it in this new variable called classes. And so we'll say info equals, and we'll declare this as a dictionary. So this is an explicit definition. So info equals dict, and then we'll say info. Actually, that's not the way that I prefer to declare these. So you can implicitly declare with the curly brackets so that says this is a dictionary. And so we'll say the dictionary, so we'll say category equals C and then we'll say vector equals vector. And so basically what this does is it creates what's also called a hash table, where it's like, okay, every instance of info is going to have an item named category. And so if we need to get the category, we just call that. And then we also can call the vector. And so then we'll say class, oops, classes.appendinfo. class is dot append info. And let me show you what this looks like. So then we'll do print classes. And this is not gonna be pretty cause it's gonna be pretty big. And then we'll just add a quick exit zero so that we won't even dive into this. But this will just show you what we're doing. And let me do a quick time check We're already at 17 minutes. Okay So, let's do Python embedding. Whoops. What did I do? All right Okay, so what this is gonna do is it's gonna Create a list of dictionaries and each of those dictionaries is going to contain a couple pieces of information. So here's what it looks like, category mammal and then here's the vector that declares that it's a mammal. Okay, cool. All right, and we're almost done, I promise. So let's go ahead and we can get rid of that. Just comment those out. All right, so result equals match class vector, and then we'll just say classes. So this'll just be a really quick search. So we've already done all the embeddings that we need to. So then let's just match it. Okay. Classes. All right, so what we'll do then is we'll say results equals list and then for C in classes we will want to get the dot product. So we'll say that to the brain. I need more coffee. That's what I need. So for C in classes, we will do score equals and we'll do similarity cause that's the function that we declared right here. So we'll do similarity of our vector and then our class, our C vector. Cause remember we added, and then our class, our C vector. Because remember, we have a dictionary that has the vector for each category here. So then we get the score. And then what we do is we'll do info equals category equals C category. Oops. And then we'll do, oops, score equals score. And then results.append info. And there you have it. That is pretty much it. Then we'll do return results. And so then we'll print the result at the end. And that should be it. Let me do a quick test. Python embedding. So it's going to get the thing in the background. So let's say bald eagle. Oh that doesn't quite look right. So let me show you another trick. From pprint, import pprint. So pprint is called pretty print. And so we're going to change this to pretty print, which will, instead of it kind of being all in one line here, it'll make it a little bit prettier. So let's, whoops, do this again. Enter a life form here, bald eagle. Okay, so then we can say the category plant, 0.77, not so good. Category reptile, 0.82. Category mammal, 0.82, category mammal 0.80, category fish 0.80. Now you might notice that I did something wrong here. I didn't include fish or a bird as a category. So interestingly though, the bald eagle, which is a descendant of the dinosaurs, the common ancestor, it's closest to reptile. So let's run this again because I'm silly and we'll add bird as a category. Let's try that again. Bald eagle. And then we say bird, there we go, 0.87. So bald eagle is semantically closest to bird ta-da all right what else do we have let's do a Komodo dragon and 0.86 as we'd expect that's the highest score so it's a reptile what else do we have let's do a shark so a shark it is a 0.91 similarity to a fish. Now there was a question, I think it was on my Discord server. We were talking about like, okay, how do you do semantic search for memories and stuff? What if we add another category that is not an animal kingdom? Let's say if we do pet versus wild animal, right? So let's do that. Do a clear screen real quick. Let's say if we do pet versus wild animal, right? So let's do that. Do a clear screen real quick. Python embedding. Okay, so let's do a cat. So if we put in a cat, it's closer to a pet of score of 0.93 than it is to wild animal of 0.85. Right, and it's also a mammal, although this says it's very close to also being a fish. That's kind of funny. Oh, wait, nope. It says it's closer to bird than it is to mammal. Interesting. Okay, so I have seen some people complain about these embeddings and I'm beginning to see what they mean. Let's do domestic cat, see if that clears it up. So domestic cat, .83 to mammal. Okay, that's a little bit better. So we're a little bit more specific and then it's just slightly more pet than wild animal. Let's do a golden retriever. So that's not even the name of a species, that's a breed of dog. Okay, so golden retriever is a wild animal according to this, and it is also a reptile. Okay, so this is pretty funny. I'm wondering what happens. Let's see if this gets better if we do a different embedding engine. Okay, so we're doing Ada and we're doing text similarity, but there's also text search. So this one I don't think will apply, but let's do this. Let's upgrade to Babbage. And just see what happens. This is actually kind of funny, I did not expect this to happen. All right, so let's change our engine to text similarity Babbage. This is kind of funny. CLS. So it works in some cases, but not in others. funny. So it works in some cases but not in others. It'll take a little bit longer so let's start with bald eagle. So bald eagle, let's see, reptile, mammal, fish, bird. So bald eagle is just slightly more reptile than anything else. That's interesting It is a wild animal though. So we got we got bald eagle is more associated with the term wild animal than it is with pet Let's see Then what was it golden retriever? Let's see if we get the right Let's see. It is also a reptile interesting And it's still a wild animal. Okay, so going up to Babbage didn't help Let's see. It is also a reptile, interesting. And it's still a wild animal. Okay, so going up to Babbage didn't help. I wonder if I'm using this wrong. Anyways, you get the idea. You can do this for text search and classification. This is actually kind of funny. I will need to do some research and figure out what I've done wrong here. I'm sure someone will let me know. But anyways, thanks for watching. to do some research and figure out what I've done wrong here. I'm sure someone will let me know. But anyways, thanks for watching.", "chunks": [{"timestamp": [0.0, 8.0], "text": " Morning everybody, David Shapiro here for my third video in the 0 to Python and GPT-3 bootcamp."}, {"timestamp": [8.0, 17.0], "text": " What the heck are embeddings? I get this question all the time. It is by far the biggest, hottest topic, so this is why I'm doing it as episode 3."}, {"timestamp": [17.0, 27.76], "text": " But before we get started, I'm going to ask that you consider liking and subscribing this video, and also jump over to my Patreon to support me there."}, {"timestamp": [27.92, 29.48], "text": " If I get to enough support, who knows,"}, {"timestamp": [29.48, 31.68], "text": " maybe I can do this full-time one day."}, {"timestamp": [31.68, 33.6], "text": " Anyways, let's go ahead and jump into the video."}, {"timestamp": [33.6, 35.5], "text": " What are embeddings?"}, {"timestamp": [35.5, 40.08], "text": " First, I need to give you a little bit of background."}, {"timestamp": [40.08, 44.92], "text": " So basically what an embedding is, it's a vector."}, {"timestamp": [44.92, 46.08], "text": " So what is a vector?"}, {"timestamp": [46.08, 47.12], "text": " Let's just start from scratch."}, {"timestamp": [47.12, 51.64], "text": " A vector is any string of numbers in an array."}, {"timestamp": [51.64, 52.6], "text": " This is a vector."}, {"timestamp": [52.6, 53.88], "text": " Here, let me make this a little bit bigger"}, {"timestamp": [53.88, 55.54], "text": " so that you can see it."}, {"timestamp": [55.54, 58.52], "text": " Okay, so we'll do vector."}, {"timestamp": [60.32, 63.04], "text": " So hashtag vector, let me set the language to Python"}, {"timestamp": [63.04, 64.78], "text": " so that it looks right."}, {"timestamp": [64.78, 66.04], "text": " Ta-da, okay."}, {"timestamp": [66.04, 66.96], "text": " This is a vector."}, {"timestamp": [69.04, 69.88], "text": " Oops."}, {"timestamp": [71.28, 72.74], "text": " This is a vector."}, {"timestamp": [72.74, 75.04], "text": " So the mathematical definition of a vector,"}, {"timestamp": [75.04, 78.14], "text": " it is a one-dimensional matrix."}, {"timestamp": [78.14, 83.14], "text": " So I'll do aka one-dimensional matrix or a list."}, {"timestamp": [84.92, 87.56], "text": " Okay, so that's what a vector is. Great. So what"}, {"timestamp": [87.56, 91.8], "text": " then is an embedding? So the difference between a vector and an embedding is"}, {"timestamp": [91.8, 97.84], "text": " mathematically they're the same, but an embedding has semantic meaning. And if"}, {"timestamp": [97.84, 102.92], "text": " you want to take a deeper dive I'll have this link in the description. This is"}, {"timestamp": [102.92, 106.04], "text": " tensorflow.org which is made by Google."}, {"timestamp": [106.04, 112.0], "text": " They really advanced this technology a few years ago with Universal Sentence"}, {"timestamp": [112.0, 116.32], "text": " Encoder. This is the progenitor technology that allowed GPT-3 to"}, {"timestamp": [116.32, 122.2], "text": " exist, and they did this starting in I think about 2014. Anyways, so if you want"}, {"timestamp": [122.2, 125.0], "text": " a deep dive, follow this link,"}, {"timestamp": [125.58, 127.64], "text": " but just for the sake of this video,"}, {"timestamp": [127.64, 129.56], "text": " I'll show you kind of a short version."}, {"timestamp": [129.56, 134.26], "text": " So embedding equals vector with semantic meaning."}, {"timestamp": [135.46, 140.08], "text": " So older NLP stuff like NLTK would do like word webs"}, {"timestamp": [140.08, 142.16], "text": " and stuff where like semantic meaning"}, {"timestamp": [142.16, 144.64], "text": " was always relevant to other words."}, {"timestamp": [144.64, 149.6], "text": " So like cat is a type of mammal for instance, it's also a type of animal, it's associated"}, {"timestamp": [149.6, 151.44], "text": " with pet."}, {"timestamp": [151.44, 153.84], "text": " But that was not very efficient."}, {"timestamp": [153.84, 158.56], "text": " Those technologies are really old and they thought it was going to revolutionize NLP."}, {"timestamp": [158.56, 161.92], "text": " It certainly did a lot, but it was not as flexible as neural networks are today."}, {"timestamp": [161.92, 166.8], "text": " So an embedding is just a vector with semantic meaning."}, {"timestamp": [167.2, 169.5], "text": " So how do we, like, what does that mean?"}, {"timestamp": [169.5, 172.72], "text": " Okay, so let's say each position,"}, {"timestamp": [172.72, 177.38], "text": " each index in our vector has a meaning."}, {"timestamp": [177.38, 182.12], "text": " So we'll say, we'll do X and Y."}, {"timestamp": [182.12, 185.76], "text": " So we're gonna populate this vector with just two values and"}, {"timestamp": [185.76, 198.96], "text": " we'll say that position x equals social power right so max of 1.0 min of"}, {"timestamp": [198.96, 211.0], "text": " negative 1.0 so we're basically going to make our own embedding. And then this is based on one of the original examples that Google used to use."}, {"timestamp": [211.0, 215.0], "text": " Position y equals gender."}, {"timestamp": [215.0, 217.0], "text": " Gender or sex."}, {"timestamp": [217.0, 221.0], "text": " So we'll say max is 1.0"}, {"timestamp": [221.0, 225.0], "text": " min of negative 1.0 min of negative 1.0."}, {"timestamp": [225.54, 230.54], "text": " So we'll say like one equals ultra masculine"}, {"timestamp": [230.68, 235.68], "text": " and negative one equals ultra feminine."}, {"timestamp": [237.9, 242.9], "text": " Okay, so we have this one by two matrix"}, {"timestamp": [243.9, 247.0], "text": " and we're gonna use it to represent a person."}, {"timestamp": [247.0, 254.0], "text": " So, or semantic meaning. So if we have a semantic meaning that is one by one,"}, {"timestamp": [254.0, 262.0], "text": " and we say that social power is maximum of one and 1.0 is ultra masculine, right?"}, {"timestamp": [262.0, 265.8], "text": " So what would this be? we can we know that okay so"}, {"timestamp": [265.8, 270.88], "text": " what who has a maximum social power theoretical so that sounds like an"}, {"timestamp": [270.88, 278.76], "text": " emperor so we'll say this is the Padishah Emperor of the known universe"}, {"timestamp": [278.76, 287.82], "text": " known universe Dune reference there So maximum social power possible and also maximum masculinity."}, {"timestamp": [287.82, 291.08], "text": " I actually don't know if the emperor was maximum masculinity."}, {"timestamp": [291.08, 294.4], "text": " He might actually be closer to like 0.5."}, {"timestamp": [294.4, 299.12], "text": " So we'll just say that because like you think ultra masculine, you think of like what if"}, {"timestamp": [299.12, 303.7], "text": " the emperor was like the rock."}, {"timestamp": [303.7, 310.66], "text": " So then let's duplicate that we'll say 1.1 punish a ember if he"}, {"timestamp": [310.66, 319.68], "text": " was Dwayne Johnson. And so there we have we have our first two embeddings. Now"}, {"timestamp": [319.68, 323.38], "text": " okay so what if we do the other one? What if we go the opposite way? So like"}, {"timestamp": [323.38, 327.16], "text": " negative 1.0 that would be like a peasant, right?"}, {"timestamp": [327.16, 330.0], "text": " Or actually probably someone who doesn't have free will."}, {"timestamp": [330.0, 332.0], "text": " So like a prisoner or something."}, {"timestamp": [332.0, 334.4], "text": " And then we say like zero."}, {"timestamp": [334.4, 339.4], "text": " So this is like someone with no free will or agency"}, {"timestamp": [343.94, 345.0], "text": " and also gender neutral."}, {"timestamp": [348.32, 350.6], "text": " Right, so that's what a semantic meaning is."}, {"timestamp": [350.6, 354.94], "text": " Now, with GPT-3, the smallest one,"}, {"timestamp": [356.28, 358.34], "text": " if you go to their embeddings,"}, {"timestamp": [359.22, 363.08], "text": " the smallest one, Ada, has 1,024 dimensions."}, {"timestamp": [363.08, 365.5], "text": " So what happens is these models are trained to break down semantic meaning into many,24 dimensions. So what happens is these models are trained"}, {"timestamp": [365.5, 367.12], "text": " to break down semantic meaning"}, {"timestamp": [367.12, 369.86], "text": " into many, many dimensions."}, {"timestamp": [369.86, 372.46], "text": " And DaVinci has 12,000 dimensions."}, {"timestamp": [372.46, 375.92], "text": " So here we're just doing it with one,"}, {"timestamp": [375.92, 380.92], "text": " or sorry, two vectors with two dimensions each."}, {"timestamp": [381.16, 386.0], "text": " So, you know, this is like super, super simple."}, {"timestamp": [386.0, 389.72], "text": " All right, so now you know the basic of what I mean"}, {"timestamp": [389.72, 392.44], "text": " when I say vector or embedding."}, {"timestamp": [392.44, 394.52], "text": " So again, the only difference between a vector"}, {"timestamp": [394.52, 399.52], "text": " and an embedding is that an embedding has semantic meaning."}, {"timestamp": [399.6, 403.04], "text": " And each of those positions is somewhat abstract."}, {"timestamp": [403.04, 405.04], "text": " Okay, so then what do you do with it though?"}, {"timestamp": [405.04, 408.44], "text": " Like how do you compare one of these to another?"}, {"timestamp": [408.44, 409.6], "text": " I'm glad you asked."}, {"timestamp": [409.6, 414.6], "text": " So we're gonna do a basic similarity search."}, {"timestamp": [416.24, 419.12], "text": " We're gonna do a classification problem."}, {"timestamp": [419.12, 422.2], "text": " So let's start with, oh, first let me introduce."}, {"timestamp": [423.5, 425.0], "text": " I've added import NumPy as np."}, {"timestamp": [426.14, 431.14], "text": " So this is standard math package,"}, {"timestamp": [431.7, 433.9], "text": " I'll say module for Python."}, {"timestamp": [433.9, 437.06], "text": " And so when you do import something as something else,"}, {"timestamp": [437.06, 439.32], "text": " this allows you to refer to it as shorthand."}, {"timestamp": [439.32, 440.44], "text": " So if I double click on that,"}, {"timestamp": [440.44, 442.78], "text": " you see that NumPy is used down here."}, {"timestamp": [442.78, 446.48], "text": " So the way to use these vectors is to compare them"}, {"timestamp": [446.48, 451.84], "text": " with a dot product. And then that dot product, the higher the dot product, the more similar the"}, {"timestamp": [451.84, 457.84], "text": " vectors are. That's it. It's that simple. So I've added this function. It's super simple. All it"}, {"timestamp": [457.84, 466.32], "text": " does is return the dot product between these two vectors. And then I've added the GPT-3 embedding,"}, {"timestamp": [466.32, 468.32], "text": " where you just pass it a string."}, {"timestamp": [468.32, 472.26], "text": " And I've got the engine already set to text similarity ADA."}, {"timestamp": [472.26, 474.76], "text": " This will suffice for many things,"}, {"timestamp": [474.76, 477.9], "text": " especially if you're just doing like a single sentence."}, {"timestamp": [477.9, 479.92], "text": " If you look at like the original"}, {"timestamp": [479.92, 482.12], "text": " universal sentence encoder,"}, {"timestamp": [482.12, 485.88], "text": " they did like, I think like 124 and 256"}, {"timestamp": [485.88, 488.54], "text": " and 512 vectors or dimensions."}, {"timestamp": [489.4, 492.6], "text": " The newest, newer ones are probably bigger than that."}, {"timestamp": [492.6, 494.36], "text": " All these links will be in the description"}, {"timestamp": [494.36, 495.48], "text": " if you'd need them."}, {"timestamp": [495.48, 496.72], "text": " All right."}, {"timestamp": [496.72, 498.88], "text": " If name equals"}, {"timestamp": [502.6, 503.44], "text": " main."}, {"timestamp": [505.08, 505.92], "text": " So what are we gonna do?"}, {"timestamp": [505.92, 507.16], "text": " What do we wanna do with this?"}, {"timestamp": [507.16, 511.06], "text": " Well, first, let's just do a really simple dot product"}, {"timestamp": [511.06, 515.88], "text": " of like, where did it go?"}, {"timestamp": [515.88, 516.8], "text": " These."}, {"timestamp": [516.8, 519.02], "text": " So let's say like, okay, what's the difference"}, {"timestamp": [519.02, 521.76], "text": " between like the emperor of the known universe"}, {"timestamp": [521.76, 524.48], "text": " if he was Dwayne Johnson versus if it was just,"}, {"timestamp": [524.48, 527.3], "text": " you know, the normal super masculine one."}, {"timestamp": [529.18, 532.06], "text": " So we'll go here and we'll say,"}, {"timestamp": [534.96, 539.96], "text": " we'll say v1 for vector one equals 1.0, 1.0,"}, {"timestamp": [540.68, 545.0], "text": " v2 equals 1.0 and then 0.5."}, {"timestamp": [547.32, 551.52], "text": " So then let's do print similarity."}, {"timestamp": [551.52, 556.52], "text": " So then this will be Emperor Duane"}, {"timestamp": [556.56, 561.56], "text": " and then this will just be Padishah Emperor."}, {"timestamp": [564.24, 567.0], "text": " So then we'll return similarity of v1, v2."}, {"timestamp": [567.0, 571.0], "text": " All right, so let's run this real quick."}, {"timestamp": [571.0, 577.0], "text": " CD Python GPD 3 tutorial, Python embedding."}, {"timestamp": [577.0, 582.0], "text": " All right, so the dot product is 1.5."}, {"timestamp": [582.0, 592.4], "text": " So that is the level of similarity. Now, what happens if we do a third one? We'll do the"}, {"timestamp": [592.4, 606.84], "text": " androgynous. So we'll do negative 1.0 for social power and then 0.0 for... so this is like the gender neutral prisoner."}, {"timestamp": [608.28, 612.74], "text": " All right, so then we'll duplicate this and we'll swap out v2 for v3."}, {"timestamp": [612.74, 615.44], "text": " So we'll get two outputs in this one."}, {"timestamp": [615.44, 619.6], "text": " So I'll just do hashtag or pound sign 1.5."}, {"timestamp": [619.6, 621.44], "text": " So that's the score that it returned."}, {"timestamp": [623.44, 624.56], "text": " Oh, whoops, I need to save it."}, {"timestamp": [624.56, 625.4], "text": " There we go. Okay, so then you see like,ops, I need to save it. There we go."}, {"timestamp": [626.46, 629.98], "text": " Okay, so then you see like, oh, wait, hold on now."}, {"timestamp": [629.98, 632.08], "text": " This is way different."}, {"timestamp": [632.08, 635.82], "text": " So this one is negative 1.0."}, {"timestamp": [636.82, 639.22], "text": " Okay, so now you're starting to get the idea"}, {"timestamp": [639.22, 641.1], "text": " of what it means to compare these."}, {"timestamp": [641.1, 646.14], "text": " Now, when you have, you know, 1, a thousand or twelve thousand dimensions, these numbers"}, {"timestamp": [646.14, 657.12], "text": " get much more nuanced. Okay, so how do we use this? How do we use this to do search"}, {"timestamp": [657.12, 666.0], "text": " or comparison or clustering? So I had this idea. So we're gonna do categories equals"}, {"timestamp": [666.36, 671.36], "text": " and we'll do plant, reptile, mammal, fish."}, {"timestamp": [677.64, 680.98], "text": " So we're just gonna do, we're gonna do four categories."}, {"timestamp": [680.98, 683.8], "text": " And okay, so then what, what do we do with that?"}, {"timestamp": [686.68, 688.7], "text": " We'll do a while loop, while true,"}, {"timestamp": [689.68, 691.78], "text": " and we'll do a equals input,"}, {"timestamp": [694.04, 697.3], "text": " enter a life form here."}, {"timestamp": [701.96, 703.52], "text": " So for Python, you're supposed,"}, {"timestamp": [703.52, 706.82], "text": " like the default is that you use single quotes."}, {"timestamp": [706.82, 709.06], "text": " And I can't remember off the top of my head right now"}, {"timestamp": [709.06, 710.88], "text": " when you're supposed to switch between single quotes"}, {"timestamp": [710.88, 713.96], "text": " and double quotes, but like PowerShell, for instance,"}, {"timestamp": [713.96, 716.98], "text": " the default is you're supposed to use double quotes."}, {"timestamp": [716.98, 719.62], "text": " Okay, so we get a live form"}, {"timestamp": [719.62, 721.74], "text": " and then what do we do with that?"}, {"timestamp": [721.74, 730.76], "text": " So then we'll do vector equals GPT-3 embedding of our input."}, {"timestamp": [730.76, 732.76], "text": " So we'll get an embedding back."}, {"timestamp": [732.76, 736.2], "text": " And just so that you can see what this looks like,"}, {"timestamp": [736.2, 740.8], "text": " let's just do a quick printout of this."}, {"timestamp": [740.8, 742.8], "text": " So we saved it."}, {"timestamp": [742.8, 747.56], "text": " CLS, Python embedding. Enter a life form here. We'll do bald"}, {"timestamp": [747.56, 753.52], "text": " eagle because this is America. And it spits out a huge number. You see how big"}, {"timestamp": [753.52, 758.52], "text": " this number is? Well it's actually a list of numbers but you get the idea. So this"}, {"timestamp": [758.52, 766.42], "text": " is the Ada version semantic embedding of bald eagle. It is 1,024 floating point numbers"}, {"timestamp": [766.42, 771.16], "text": " between negative 1.0 and 1.0."}, {"timestamp": [771.16, 773.8], "text": " We can do this with DaVinci and it'll be even longer."}, {"timestamp": [775.68, 777.4], "text": " Why did this get messed up?"}, {"timestamp": [777.4, 778.24], "text": " Oh, there we go."}, {"timestamp": [779.88, 782.04], "text": " So there you have it."}, {"timestamp": [782.04, 783.92], "text": " That's what an embedding looks like."}, {"timestamp": [783.92, 786.24], "text": " So it just, it shoots it back real quick."}, {"timestamp": [787.3, 789.08], "text": " All right, but what are we gonna do with this?"}, {"timestamp": [789.08, 792.28], "text": " So what we wanna do is we wanna find, we wanna classify."}, {"timestamp": [792.28, 794.44], "text": " What did I put in and we wanna match it"}, {"timestamp": [794.44, 796.08], "text": " to one of these categories."}, {"timestamp": [796.08, 797.42], "text": " So how do we do that?"}, {"timestamp": [797.42, 799.82], "text": " Well, the first thing we can do is,"}, {"timestamp": [799.82, 801.16], "text": " and this will be really inefficient"}, {"timestamp": [801.16, 803.92], "text": " because what I'm gonna do is I'm gonna get a vector"}, {"timestamp": [803.92, 805.82], "text": " for these each time."}, {"timestamp": [805.82, 810.4], "text": " But really what we should do in the long run, in a longer video or in a future video, we'll"}, {"timestamp": [810.4, 811.4], "text": " store these."}, {"timestamp": [811.4, 815.54], "text": " We'll store the embeddings for each of these."}, {"timestamp": [815.54, 818.28], "text": " So let's do a function."}, {"timestamp": [818.28, 825.0], "text": " So we'll match, let's see, result equals match class."}, {"timestamp": [826.78, 830.56], "text": " And so what we'll do is we'll pass our initial vector"}, {"timestamp": [830.56, 832.84], "text": " and our categories."}, {"timestamp": [832.84, 834.9], "text": " And so this is the vector."}, {"timestamp": [834.9, 836.16], "text": " So we're gonna get the vector"}, {"timestamp": [836.16, 838.52], "text": " for whatever life form we put in was,"}, {"timestamp": [838.52, 840.68], "text": " and then we're gonna pass these categories."}, {"timestamp": [842.16, 850.8], "text": " So then we'll do def match class and then we'll do vector and our categories."}, {"timestamp": [850.8, 855.28], "text": " And I know in a previous video I said that it's best practices not to reuse"}, {"timestamp": [855.28, 859.8], "text": " the same variable names, but since I'm not modifying these I'm just reading"}, {"timestamp": [859.8, 864.1], "text": " them it's not that big a deal, but it's still poor practice so just keep that in"}, {"timestamp": [864.1, 866.44], "text": " mind. I don't always follow the rules."}, {"timestamp": [867.72, 868.78], "text": " Okay, so match class."}, {"timestamp": [868.78, 870.64], "text": " So the first thing that we need to do"}, {"timestamp": [872.48, 874.88], "text": " is for each of these categories."}, {"timestamp": [876.36, 879.72], "text": " So we'll do classes equals list."}, {"timestamp": [880.8, 885.0], "text": " And then for C in categories,"}, {"timestamp": [885.48, 889.8], "text": " we'll do, we'll just copy this real quick."}, {"timestamp": [889.8, 891.5], "text": " Vector equals GPT-3 embedding,"}, {"timestamp": [891.5, 893.32], "text": " but instead of A we'll do C."}, {"timestamp": [893.32, 895.4], "text": " So that's this, so that, well, this is a for loop,"}, {"timestamp": [895.4, 896.64], "text": " which means that we're gonna iterate"}, {"timestamp": [896.64, 899.38], "text": " through each of these items."}, {"timestamp": [899.38, 904.28], "text": " And we're gonna say, okay, what is my vector?"}, {"timestamp": [904.28, 905.76], "text": " And actually, you know what?"}, {"timestamp": [905.76, 908.34], "text": " If I got to write this, I might as well put this down below."}, {"timestamp": [912.6, 914.44], "text": " So we declare our categories."}, {"timestamp": [914.44, 916.44], "text": " So we'll do for C in categories."}, {"timestamp": [919.4, 920.52], "text": " And we'll do this here."}, {"timestamp": [920.52, 921.66], "text": " So let's run this once."}, {"timestamp": [921.66, 923.28], "text": " So this is another rule of thumb."}, {"timestamp": [923.28, 929.04], "text": " If you're going to run a piece of code more than once, you put it in a place that one it'll only run once if you only"}, {"timestamp": [929.04, 934.72], "text": " need to. But also if you need to call it multiple times, then you then you declare it as a function."}, {"timestamp": [935.28, 940.4], "text": " So let's just clean this up here first. We'll do a little bit better. All right, so we get the"}, {"timestamp": [940.4, 949.08], "text": " vector for the category. And so then what do we do? Well, we want to save it in this new variable called classes."}, {"timestamp": [949.08, 954.8], "text": " And so we'll say info equals, and we'll"}, {"timestamp": [954.8, 956.12], "text": " declare this as a dictionary."}, {"timestamp": [956.12, 959.48], "text": " So this is an explicit definition."}, {"timestamp": [959.48, 963.84], "text": " So info equals dict, and then we'll say info."}, {"timestamp": [963.84, 968.48], "text": " Actually, that's not the way that I prefer to declare these. So you can implicitly declare with the curly"}, {"timestamp": [968.48, 972.94], "text": " brackets so that says this is a dictionary. And so we'll say the"}, {"timestamp": [972.94, 982.56], "text": " dictionary, so we'll say category equals C and then we'll say vector equals vector."}, {"timestamp": [982.56, 985.28], "text": " And so basically what this does is it creates"}, {"timestamp": [985.28, 987.68], "text": " what's also called a hash table,"}, {"timestamp": [987.68, 990.32], "text": " where it's like, okay, every instance of info"}, {"timestamp": [990.32, 993.66], "text": " is going to have an item named category."}, {"timestamp": [993.66, 996.96], "text": " And so if we need to get the category, we just call that."}, {"timestamp": [996.96, 999.04], "text": " And then we also can call the vector."}, {"timestamp": [999.04, 1004.04], "text": " And so then we'll say class, oops, classes.appendinfo."}, {"timestamp": [1005.0, 1008.6], "text": " class is dot append info."}, {"timestamp": [1010.76, 1014.28], "text": " And let me show you what this looks like. So then we'll do print classes."}, {"timestamp": [1014.28, 1015.44], "text": " And this is not gonna be pretty"}, {"timestamp": [1015.44, 1017.18], "text": " cause it's gonna be pretty big."}, {"timestamp": [1017.18, 1020.48], "text": " And then we'll just add a quick exit zero"}, {"timestamp": [1020.48, 1022.4], "text": " so that we won't even dive into this."}, {"timestamp": [1022.4, 1023.96], "text": " But this will just show you what we're doing."}, {"timestamp": [1023.96, 1026.12], "text": " And let me do a quick time check"}, {"timestamp": [1028.28, 1030.48], "text": " We're already at 17 minutes. Okay"}, {"timestamp": [1035.54, 1037.74], "text": " So, let's do Python embedding. Whoops. What did I do? All right"}, {"timestamp": [1040.0, 1041.2], "text": " Okay, so what this is gonna do is it's gonna"}, {"timestamp": [1047.08, 1049.68], "text": " Create a list of dictionaries and each of those dictionaries is going to contain a couple pieces of information."}, {"timestamp": [1049.68, 1053.8], "text": " So here's what it looks like, category mammal and then here's the vector that declares that"}, {"timestamp": [1053.8, 1054.8], "text": " it's a mammal."}, {"timestamp": [1054.8, 1056.8], "text": " Okay, cool."}, {"timestamp": [1056.8, 1060.4], "text": " All right, and we're almost done, I promise."}, {"timestamp": [1060.4, 1065.08], "text": " So let's go ahead and we can get rid of that. Just comment those out."}, {"timestamp": [1067.48, 1070.72], "text": " All right, so result equals match class vector, and then we'll just say classes."}, {"timestamp": [1070.72, 1072.56], "text": " So this'll just be a really quick search."}, {"timestamp": [1072.56, 1075.66], "text": " So we've already done all the embeddings that we need to."}, {"timestamp": [1075.66, 1077.8], "text": " So then let's just match it."}, {"timestamp": [1077.8, 1078.62], "text": " Okay."}, {"timestamp": [1080.2, 1081.14], "text": " Classes."}, {"timestamp": [1082.04, 1084.24], "text": " All right, so what we'll do then"}, {"timestamp": [1084.24, 1085.0], "text": " is we'll say"}, {"timestamp": [1085.0, 1089.0], "text": " results equals list and then for"}, {"timestamp": [1090.0, 1094.0], "text": " C in classes we will want to get the dot product."}, {"timestamp": [1097.0, 1101.0], "text": " So we'll say that to the brain. I need more coffee."}, {"timestamp": [1101.0, 1102.0], "text": " That's what I need."}, {"timestamp": [1105.68, 1109.84], "text": " So for C in classes, we will do score equals"}, {"timestamp": [1110.84, 1113.48], "text": " and we'll do similarity"}, {"timestamp": [1113.48, 1115.8], "text": " cause that's the function that we declared right here."}, {"timestamp": [1115.8, 1118.66], "text": " So we'll do similarity of our vector"}, {"timestamp": [1120.0, 1124.24], "text": " and then our class, our C vector."}, {"timestamp": [1125.52, 1128.72], "text": " Cause remember we added, and then our class, our C vector. Because remember, we have a dictionary"}, {"timestamp": [1128.72, 1133.72], "text": " that has the vector for each category here."}, {"timestamp": [1133.76, 1135.56], "text": " So then we get the score."}, {"timestamp": [1135.56, 1138.88], "text": " And then what we do is we'll do info equals"}, {"timestamp": [1141.72, 1145.0], "text": " category equals C category."}, {"timestamp": [1146.46, 1147.3], "text": " Oops."}, {"timestamp": [1148.18, 1153.18], "text": " And then we'll do, oops, score equals score."}, {"timestamp": [1155.9, 1160.46], "text": " And then results.append info."}, {"timestamp": [1162.58, 1163.94], "text": " And there you have it."}, {"timestamp": [1163.94, 1165.0], "text": " That is pretty much it."}, {"timestamp": [1165.04, 1167.16], "text": " Then we'll do return results."}, {"timestamp": [1169.6, 1172.2], "text": " And so then we'll print the result at the end."}, {"timestamp": [1174.36, 1176.64], "text": " And that should be it. Let me do a quick test."}, {"timestamp": [1177.68, 1180.2], "text": " Python embedding. So it's going to get the thing in the background."}, {"timestamp": [1180.24, 1181.68], "text": " So let's say bald eagle."}, {"timestamp": [1183.4, 1185.12], "text": " Oh that doesn't quite look right."}, {"timestamp": [1185.12, 1189.56], "text": " So let me show you another trick."}, {"timestamp": [1189.56, 1192.68], "text": " From pprint, import pprint."}, {"timestamp": [1192.68, 1195.56], "text": " So pprint is called pretty print."}, {"timestamp": [1195.56, 1197.92], "text": " And so we're going to change this"}, {"timestamp": [1197.92, 1200.36], "text": " to pretty print, which will, instead"}, {"timestamp": [1200.36, 1202.28], "text": " of it kind of being all in one line here,"}, {"timestamp": [1202.28, 1206.2], "text": " it'll make it a little bit prettier. So let's, whoops,"}, {"timestamp": [1209.84, 1210.68], "text": " do this again."}, {"timestamp": [1211.88, 1213.98], "text": " Enter a life form here, bald eagle."}, {"timestamp": [1214.96, 1219.96], "text": " Okay, so then we can say the category plant, 0.77,"}, {"timestamp": [1220.72, 1222.04], "text": " not so good."}, {"timestamp": [1222.04, 1224.88], "text": " Category reptile, 0.82."}, {"timestamp": [1224.88, 1231.5], "text": " Category mammal, 0.82, category mammal 0.80, category fish 0.80."}, {"timestamp": [1231.5, 1233.56], "text": " Now you might notice that I did something wrong here."}, {"timestamp": [1233.56, 1237.78], "text": " I didn't include fish or a bird as a category."}, {"timestamp": [1237.78, 1243.7], "text": " So interestingly though, the bald eagle, which is a descendant of the dinosaurs, the common"}, {"timestamp": [1243.7, 1247.04], "text": " ancestor, it's closest to reptile."}, {"timestamp": [1247.04, 1249.04], "text": " So let's run this again because I'm silly"}, {"timestamp": [1249.04, 1252.96], "text": " and we'll add bird as a category."}, {"timestamp": [1254.92, 1255.96], "text": " Let's try that again."}, {"timestamp": [1257.4, 1258.62], "text": " Bald eagle."}, {"timestamp": [1259.76, 1263.44], "text": " And then we say bird, there we go, 0.87."}, {"timestamp": [1263.44, 1266.56], "text": " So bald eagle is semantically closest to bird"}, {"timestamp": [1266.56, 1276.04], "text": " ta-da all right what else do we have let's do a Komodo dragon and 0.86 as"}, {"timestamp": [1276.04, 1280.92], "text": " we'd expect that's the highest score so it's a reptile what else do we have"}, {"timestamp": [1280.92, 1288.14], "text": " let's do a shark so a shark it is a 0.91 similarity to a fish."}, {"timestamp": [1289.44, 1290.54], "text": " Now there was a question,"}, {"timestamp": [1290.54, 1291.98], "text": " I think it was on my Discord server."}, {"timestamp": [1291.98, 1294.1], "text": " We were talking about like,"}, {"timestamp": [1294.1, 1297.32], "text": " okay, how do you do semantic search for memories and stuff?"}, {"timestamp": [1297.32, 1298.82], "text": " What if we add another category"}, {"timestamp": [1298.82, 1300.36], "text": " that is not an animal kingdom?"}, {"timestamp": [1300.36, 1304.96], "text": " Let's say if we do pet versus wild animal, right?"}, {"timestamp": [1304.96, 1305.0], "text": " So let's do that. Do a clear screen real quick. Let's say if we do pet versus wild animal, right?"}, {"timestamp": [1305.0, 1305.9], "text": " So let's do that."}, {"timestamp": [1307.22, 1309.3], "text": " Do a clear screen real quick."}, {"timestamp": [1309.3, 1311.24], "text": " Python embedding."}, {"timestamp": [1311.24, 1313.32], "text": " Okay, so let's do a cat."}, {"timestamp": [1314.64, 1319.64], "text": " So if we put in a cat, it's closer to a pet"}, {"timestamp": [1320.46, 1325.0], "text": " of score of 0.93 than it is to wild animal of 0.85."}, {"timestamp": [1325.5, 1327.46], "text": " Right, and it's also a mammal,"}, {"timestamp": [1327.46, 1330.76], "text": " although this says it's very close to also being a fish."}, {"timestamp": [1330.76, 1333.46], "text": " That's kind of funny."}, {"timestamp": [1333.46, 1334.82], "text": " Oh, wait, nope."}, {"timestamp": [1334.82, 1338.92], "text": " It says it's closer to bird than it is to mammal."}, {"timestamp": [1338.92, 1340.1], "text": " Interesting."}, {"timestamp": [1340.1, 1342.14], "text": " Okay, so I have seen some people complain"}, {"timestamp": [1342.14, 1343.02], "text": " about these embeddings"}, {"timestamp": [1343.02, 1346.5], "text": " and I'm beginning to see what they mean."}, {"timestamp": [1346.5, 1351.56], "text": " Let's do domestic cat, see if that clears it up."}, {"timestamp": [1351.56, 1354.8], "text": " So domestic cat, .83 to mammal."}, {"timestamp": [1354.8, 1356.44], "text": " Okay, that's a little bit better."}, {"timestamp": [1356.44, 1363.32], "text": " So we're a little bit more specific and then it's just slightly more pet than wild animal."}, {"timestamp": [1363.32, 1372.0], "text": " Let's do a golden retriever. So that's not even the name of a species, that's a breed of dog."}, {"timestamp": [1372.0, 1386.52], "text": " Okay, so golden retriever is a wild animal according to this, and it is also a reptile. Okay, so this is pretty funny. I'm wondering what happens. Let's see if this gets better"}, {"timestamp": [1386.52, 1390.76], "text": " if we do a different embedding engine."}, {"timestamp": [1390.76, 1395.56], "text": " Okay, so we're doing Ada and we're doing text similarity,"}, {"timestamp": [1395.56, 1398.9], "text": " but there's also text search."}, {"timestamp": [1398.9, 1401.02], "text": " So this one I don't think will apply,"}, {"timestamp": [1402.12, 1404.6], "text": " but let's do this."}, {"timestamp": [1404.6, 1406.26], "text": " Let's upgrade to Babbage."}, {"timestamp": [1407.26, 1408.5], "text": " And just see what happens."}, {"timestamp": [1408.5, 1409.5], "text": " This is actually kind of funny,"}, {"timestamp": [1409.5, 1411.1], "text": " I did not expect this to happen."}, {"timestamp": [1411.94, 1414.58], "text": " All right, so let's change our engine"}, {"timestamp": [1414.58, 1418.2], "text": " to text similarity Babbage."}, {"timestamp": [1419.62, 1420.72], "text": " This is kind of funny."}, {"timestamp": [1421.62, 1422.46], "text": " CLS."}, {"timestamp": [1422.46, 1424.42], "text": " So it works in some cases, but not in others."}, {"timestamp": [1430.4, 1438.4], "text": " funny. So it works in some cases but not in others. It'll take a little bit longer so let's start with bald eagle. So bald eagle, let's see, reptile, mammal, fish, bird. So"}, {"timestamp": [1438.4, 1451.76], "text": " bald eagle is just slightly more reptile than anything else. That's interesting It is a wild animal though. So we got we got bald eagle is more associated with the term wild animal than it is with pet"}, {"timestamp": [1453.4, 1455.28], "text": " Let's see"}, {"timestamp": [1455.28, 1459.02], "text": " Then what was it golden retriever? Let's see if we get the right"}, {"timestamp": [1460.12, 1463.76], "text": " Let's see. It is also a reptile interesting"}, {"timestamp": [1464.52, 1465.44], "text": " And it's still a wild animal. Okay, so going up to Babbage didn't help Let's see. It is also a reptile, interesting."}, {"timestamp": [1465.44, 1466.44], "text": " And it's still a wild animal."}, {"timestamp": [1466.44, 1470.36], "text": " Okay, so going up to Babbage didn't help."}, {"timestamp": [1470.36, 1472.0], "text": " I wonder if I'm using this wrong."}, {"timestamp": [1472.0, 1473.54], "text": " Anyways, you get the idea."}, {"timestamp": [1473.54, 1478.08], "text": " You can do this for text search and classification."}, {"timestamp": [1478.08, 1479.16], "text": " This is actually kind of funny."}, {"timestamp": [1479.16, 1482.12], "text": " I will need to do some research and figure out what I've done wrong here."}, {"timestamp": [1482.12, 1483.84], "text": " I'm sure someone will let me know."}, {"timestamp": [1483.84, 1485.04], "text": " But anyways, thanks for watching."}, {"timestamp": [1482.57, 1484.67], "text": " to do some research and figure out what I've done wrong here."}, {"timestamp": [1484.67, 1486.27], "text": " I'm sure someone will let me know."}, {"timestamp": [1486.27, 1488.79], "text": " But anyways, thanks for watching."}]}