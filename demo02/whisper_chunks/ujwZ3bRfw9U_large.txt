{"text": " Hey, everybody, David Shapiro here. Yeah, so it's my first live stream. It looks like there's four of you. We'll see if any other folks show up. But yeah, so here we are. Do a quick volume check. Can you all hear me? Give me a shout out in the chat if you can. Oh, we're up to five. Yes, the dude with some 409 says hi. Hello. Okay. Yep, sounds good. Okay. So yeah, I am at 1000 subscribers now, which is pretty crazy. Considering crazy considering I started YouTubing seriously back in what, April, March, something like that. And yeah, so it's just by popular demand really. I did promise to talk about a couple different topics today, but so we'll actually see what folks talk, ask about in the chat. we'll actually see what folks talk about, ask about in the chat. Cause like this is, well, I don't know really, I don't really know what I'm doing. Could be an AMA, could be just a, if someone has a topic, I can talk for a long time about it. Also, I apologize if I'm a little bit stuffy, hopefully that'll clear up as I start to talk. Actually here, I know what I'll do. I a little bit stuffy. Hopefully that will clear up as I start to talk. Actually, here. I know what I'll do. I'll make a quick poll. We will do a poll. Let's see. Ask a question. Okay. What do you want me to talk about? And then we'll say yes or no. Now, we'll say DALI, or we can do GPT-3. And we can say like LLMs in general, AI and futurism. There we go. All right. There should be a poll. I don't know how to see it now. Oh, there it is. OK. So I got a little poll going just to get a feel for what y'all want to hear me talk about so far. So if you could jump in the chat and hit me up there, let me know. But yeah, we just got started. Also, I've got my I took a nap earlier and I don't know if I'm too hot or too cold. Okay, so we've got LLMs in general. Oh, we're getting all kinds of votes. Okay. Adrian says Dolly, okay. We're getting, seems like there's some desire to talk about LLMs in general, okay. I'll give it just another couple, well, I guess probably almost everyone's voted by now. All right, I'll go ahead and start talking about large language models. Okay, so large language models, I'm assuming pretty much everyone here is gonna know that GPT-3 is a large language model, but what is a large language model? So the first thing to know about a large language model is that it's big. Hey, there you go. But I'm all right. Everyone go home. Now, the thing is, is that it's a type of deep learning network. And they were originally trained, there's like all different kinds, there's like LSTM, which is long short term memories, and different things like that. And basically, this is as of like circa 2014, 2015, 2016. What they were doing was just teaching them to predict usually the next letter or the next token or the next word. Oh, Jordan has a good question. Tell us how we got into this field. Excuse me. So I'll tie that in to this answer. Excuse me. So I'll tie that in to this answer. So circa 2014, 2015, 2016, Google publishes their universal sentence encoder. So if you ever hear me say, like, Google use or Google universal sentence encoder, this is like an early, like, precursor to modern large language models. So one of the things that we needed to do was we needed to figure out how to represent language in math, in numbers, in computer code. And so all kinds of research had been done for many, many years. There's WordNet. There's different types of embeddings. And so this was building on research of word embeddings. And so word embedding is just representing a word with its semantic meaning as a series of numbers. And so I remember when I don't even remember the details of how I saw it, but I was just searching around for stuff. And I saw Google's universal sentence encoder. And it's like, oh, you can represent any arbitrary sentence or paragraph as a vector, as a string of numbers. And I immediately knew this will change everything. So experiment with it. But of course, being able to render something as a semantic vector or an embedding, as we call it now. It used to be like a semantic vector or an embedding as we call it now. It used to be like a semantic vector or just a vector, but now we call it an embedding. Being able to render that would like, it unlocks a lot of different things. But so like one of the first experiments that I did was with like the precursor to what I now call my core objective functions or the heuristic comparatives, which was, okay, if I have a sentence and it says, I think, what did I test? It was something really arbitrary and off the wall. Like I tested if you throw a kitten off a bridge or something that was obviously egregious and heinous. And I said, how semantically similar is that to reduce suffering? And obviously, this was a very early experiment. It didn't work the way that I thought that it would. I think I tried a support vector machine, which is a type of clustering, because I thought that, oh, like, here's a high-dimensional vector. Let's just cluster them together. But I don't think that semantic vectors lend themselves to clustering, at least not at the full dimensionality, and certainly not the experiment that I did. Anyways, because the idea was you have any arbitrary statement, and then you can classify it. Is this a good behavior or a bad behavior? That's what I was trying to do. So that's kind of where I got started. A couple years later. Oh, so the history of LLMs and then how I got into it. So a couple years later, GPT was, they talked about that, OpenAI published about GPT and then GPT2. So GPT, generative pre-trained transformer, didn't really, nobody remembers that, but it was the same idea, which was first, instead of just rendering it to an embedding you then use that embedding to generate output so google's universal sentence encoder is like the first half of a large language model so that's the encoder side and then on the second half is the decoder which is produce output and i got another comment let's see um interested in how you come up with experiments i'm currently working on aligning language models and trying to build my research agenda. Okay, cool. So I will build on, I'll answer Jordan and vulnerable growths questions as I go. Okay, so where was I? Encoder, decoder, right. So GPT-2 comes out and this is like, oh, you can generate any text. I remember a few people still joke about this where it's like OpenAI was very worried about the ethical use of GPT when it when it first came out and they were they were very scared about releasing it publicly because they thought that was was going to wreck the internet and be used to generate all the false tweets and fake news and stuff. And it can generate stuff. I mean, it can generate comprehensible English. It can produce stuff that's coherent. I wouldn't necessarily call it a threat to humanity. But anyway, so I started some experiments there with seeing what it would take to get some of the experiments that I wanted to do. And it was okay. The best experiment that I did, so this is again going back to before my current research, but I had that idea of reduced suffering should be an objective function for artificial general intelligence. And so I fine-tuned a project in GP2 where I would basically give it a problem and it would recommend a solution. And it was very impressive, the kinds of solutions that it would come up with. But I knew that I was in trouble when one of the scenarios that I gave it was like, you know, I said that the scenario was like, there are hundreds of millions of people around the world suffering from chronic pain, what should we do about it? And the fine-tuning model of GPT-2 said we should euthanize everyone that's in chronic pain to reduce suffering. And I thought, well, that would reduce suffering, but that's pretty sociopathic, so let's go back to the drawing board. But that was a really critical early lesson for me to realize that these models could adopt any moral framework That you want here. I'll go ahead and close the poll These models can adopt any framework or position that you want and so That was like that is always in my head like you have to be real careful What? Some random by technically not wrong, exactly. It's like technically, yes. Euthanizing everyone that has chronic pain would reduce suffering. Sorry, let me mute my phone. I did not realize it was not muted. Okay, phone is now muted. And yeah, and so that has been kind of a bedrock experiment for me to remember that you have to be very careful about the objectives that you give these language models because they don't have any intrinsic morality. They can adopt whatever framework you give them, and it can be as sociopathic as you want. GPT-3, I've repeated that same experiment a few times. GPT-3 is a little bit more aligned because it can think further into the future. So this is I tested this, I think it was made it into my book, Benevolent by Design, which I've got on my bookshelf I can show you if you want. So GPT-3, like I said, like you know, there are millions of people in chronic pain. Should we euthanize them to reduce suffering? And GPT-3 more wisely said, no, because like there are probably better ways, but also you'll increase suffering from their loved ones. I was like, oh, okay, cool. So, you know, in terms of like scope, GPT-3 is a little bit better aligned. Let's see. Okay, so I think that's kind of large language models and how we got to here. Let's see. Okay. So I think that's kind of large language models and how we got to here. Let's see. Jordan, can you tell us how you got into this field? I guess, okay, so taking a step back, how did I get into this field to begin with? Let me set up. I'm kind of slouching. So it was back in about 2009 when I, my tech career was going well. And so, you know, one thing that happens when you're not like working 12 hours a day because before I got into tech, I was doing all kinds of random jobs. And some, you know, there was a period of time that I was doing carpet cleaning where I would wake up at five every morning and I wouldn't get home until like 7 p.m. So that was like 12 plus hour days. Let's see. How do you think we should decide what the good is and what the bad is? I guess I'll have to ask what do you mean by the good and the bad? Someone in the comments. Let's see. Tropical Tone says, hey, just found your channel and love it. Would love to get the updated Discord invite. Cool. We've actually got, well, I don't know if we should put a Discord invite in the live chat. But talk to me after the show. For AI when we are prompting, how do we know if it's good or bad? OK, so let me switch off of live chat, because it is distracting. Sorry. Where was I? Oh, yes. How did I get into this field? So long story short, I worked really hard and did a whole bunch of random jobs in my early 20s. Then I got serious and I got into technology because I was like, okay, I'm not going to work these dead-end night jobs forever. So I got through tech school and started that. And then, so there's this concept called cognitive surplus. And so cognitive surplus is when you're not fighting for survival every day, your brain has more horsepower that it can dedicate to other stuff. And me, I'm just super curious. And so back in about 2009, I got on Genie, which is a old school open source IDE, and GCC, which is an open source C++ compiler, and I started playing with evolutionary algorithms and at the time, very small neural networks, and I dusted off everything that I learned about C++ and memory structures back in high school. And, because I had this idea of that you could evolve the correct neural network. If you had the right test environment, you could evolve a fully fledged human brain, basically. Oh boy, did I not realize how hard of a problem that is, especially when you look at the fact that like, open AI had their robot hand. It took like a hundred years of simulation to learn how to manipulate an object. That's a lot of generations. So anyways, that's kind of how I got started back in 2009, and then I kind of cyclically came back to it. So for instance, a couple years later, I wanted to do algorithmic trading on the stock market, so I picked up Python and Scikit-learn, and that's where I learned clustering, that sort of stuff. Let's see, okay, so vulnerable growth, you asked a question earlier about how to come up with experiments. So that's actually a good question. The way that I come up with experiments is everything that I do, almost everything that I do, is geared towards my ultimate goal of creating artificial cognition. So what do I mean by artificial cognition? The prevailing thing of artificial general intelligence, which is a machine that can learn anything. Okay, that's fine. I don't like that idea because intelligence is so hard to define anyways, and so the goalpost is always moving on it. like that idea because intelligence is so hard to define anyways. And so like the goalpost is always moving on it. And so rather than say that intelligence is the goal, I realized that I need to set cognition as the goal. We want a machine that thinks, a machine that has agency and autonomy. And then intelligence is a characteristic of what you're measuring, or is a way to measure how intelligent the machine is that you've built. But you start with cognition. So basically a few years ago, I think it was about 2017 is when I started to come up with these ideas, I realized that intelligence or AI or autonomous machines would be a thought-first approach, rather than a robotics-first approach. or AI or autonomous machines would be a thought first approach rather than a robotics first approach. So you might know the Honda's robot, Asimo, right? That came out like 20 years ago now and they put hardware first and then it just stood there and it did a few tricks and you know, but everything had to be hard coded. And then Boston Dynamics, they got much better at the hardware where, hardware, where now it can navigate and stuff, but it has no thought, right? And I realized, you can have the best robotics platform in the world, but if it can't think, it doesn't matter. And so I started realizing, we need to focus on instead of bottom-up, like let's build a robot and then give it a brain, no, let's build a brain and then give it a robot, right? And so I started with thought first as the goal. And so back in about 2017, 2018, I came up with the idea of MRAGI, which is an acronym, M-A-R-A-G-I, Microservices Architecture for Robotics and Artificial General Intelligence, where I put thought first. And so I started with those ideas, but that was around the time that like, Google's universal sentence encoder was out, which is a good component. I'll still be using that, but it's not enough. It's necessary, but not sufficient is the engineering term. It is a necessary component, but not sufficient to fully realize artificial cognition. Then GPT-2 came out. I started experimenting with that and said, okay, we're closer. Then GPT-3 came out and I was like, now we're cooking with fire. So everything that I do on my channel, like whether it's search or something, basically what I realized is that I needed to solve other problems of cognition. And so I read, I mean, you see my bookcase, I've got a couple shelves full of psychology, neuroscience, and philosophy books. And so I have a pretty good understanding, well, I say pretty good understanding, like we don't know how the human brain works. But in terms of what people do understand today, I've got a decent understanding of how the brain works, how the mind works, how cognition works, and how neuroscience works. And so everything that I do on my channel are experiments to build up the skills and knowledge and abilities to solve those problems. So for instance, if you look at one of my videos where I did search, right? I did question answering for, what was it? The Roe versus Wade decision that came out. You know, on the one hand, like, yes, it's a cool project to be able to like, here's a document, provide answers from it. But if you, if you go back even further in my YouTube history and look at my, my natural language, cognitive architecture question generator, that was that the entire reason that I fine tuned a question generator was so that the machine could have an internal sense of curiosity. So the long story short, all of my experiments in some way way either contribute directly to or help teach me how to build artificial cognition. So basically like everything is like you know building blocks reaching up towards this higher goal. So that's that's the answer to that vulnerable growth. Let's see. Let's see, JW says, I'm in my early 20s, lost my job. I would love to find a way to live off my GPT projects. I'm obsessed with GPT and Python, but I need some help to figure out how to survive till my projects are ready. Okay. So you are actually probably in, well, I'm not going to make any promises, you might be in better shape than you realize. One reason is because the job market is going to be slow to adapt. If you look for large language model jobs, most places are still hiring senior data scientists with a PhD in NLP. The in NLP. And the thing is, is I've talked to a bunch of people, and there are NLP experts, like I've had lunch with people in my local area who are, you know, old school NLP experts, and they don't know anything about large language models or how to use GPT-3. It's strange. We're in a very weird place where these these technologies, large language models like GPT, and then all the open source versions from Eleuther, there are plenty of people who are experts in the field who have no idea how to use these. So you, by being younger and more flexible and able to ride with the current, I will say master the open source ones, because there are plenty of people who are saying, OK, well, open AI is too expensive, but I can run GPT-J on my own hardware now, so I'm going to do that. Or GPT-NeoX or Bloom is the latest one. And actually, Jordan on the chat, he's working on that as well. So hit up Jordan. He's working on this. Don't mean to call you out Jordan. You don't have to answer if you don't want, but I do like connecting people. So yeah, you're not gonna be able to survive on your own projects though. So here's the thing, it is a very dense field and it's getting denser. What you will be able to do is monetize your skills and be a consultant. And that could ultimately lead to a job with one of these many startups. And they are hundreds of startups. Let's see, let me catch up. How concerned are you about getting paper clipped? I'm glad you asked. Here, let me actually grab my copy of Benevolent by Design because that's the opening chapter. Let's see. Okay, so this is my second book, Benevolent by Design. And literally, let's see, what is it? Is it the, yeah, the first chapter is called the Paperclip Maximizer. And I have that as the first chapter is called the paperclip maximizer. And I have that as the opening chapter so that you can understand if you're new to artificial intelligence and you don't know what an objective function is, I use that as the opening idea to say like, hey, is it intelligence? Oh, and there's this dude, Robert Miles. I just watched his video on orthogonal alignment. Let me post that in the link, because this is good. I just watched it earlier. Let's see. History, and then not can AI relax. There we go. So this guy, I think someone on the Discord recommended him, and I'm glad they did. So let me share this link. For anyone not in the know, check this out. Okay, so this is AI alignment and orthogonality. So he does a really good job of concisely explaining these things. Now, the person who asked about paperclips, I'm guessing you probably don't need to get this, but it was a good video. Great summary. Anyways, so the thing is, what I, the insight that I had that I talk about in this book is that rather than have a single objective function, I have three. And so the, also there's a reason I stopped calling them objective functions and call them heuristic comparatives is because technically there's no mathematical way to have three objective functions. At least the math doesn't exist now and so it's more like three principles to adhere to. What I realized though in my research is that is that humans all all intelligent entities, whether it's a chimp or a dolphin or a human, none of us have a single objective function. Now, Richard Dawkins argued that all life has a single objective function, which is to maximize DNA, which is the second chapter. And so then I explore that as like, okay, let's imagine that life has a single objective function, which is to maximize DNA. But then you look at all the downstream effects and all the different ways that life has evolved to maximize DNA. And, you know, humans with our intelligence, intelligence is just subservient, is a sub goal of maximize DNA. But because of our intelligence, we are able to have many, many different goals in our life. What was that called? It was called externality goal. I don't remember exactly, but it's talked about in the video I just linked. Anyways, so like there's different, by having different goals, set intention. Like, so for instance, right now, I don't want to starve. I don't want to freeze to death. I don't want to be lonely. Like there's all kinds of things that I want instrumental goal. Thank you. No, it's that's that's the other one vulnerable growth. So there's instrumental goal and the other one. Instrumental goal is how do you get to the end goal, but the end goals are arbitrary. So my arbitrary goal, like I don't wanna die, right? So that's one thing, but in the meantime, I also want to have fun, I want to, you know, be engaged. Like I have all these other goals. And so what I realized, and this actually also goes back to Plato with the golden mean, is that by having different goals that are sometimes mutually exclusive, terminal goals, there we go, terminal. So a terminal goal is arbitrary. And so like, I don't want to die. Why? Because I don't, right? Like I'm biologically evolved to not want to die. Okay, there, like, that's it. And I also don't want, because of that, I don't wanna starve. But in the meantime, I also have other sensations. I have hunger, I have pain. And so by design, so that's, you know, hence benevolent by design, I am designed to be, to self-preserve, right? And then there's all kinds of behaviors that flow from my intrinsic design, which is like, I have a job, right? Like, why do we do crazy things like get a job? Because I need money. Why do I need money? I need money so that I can pay for this house and pay for this, you know, this webcam and all this other stuff. But why do I need that? It's, you know, but why, but why, but why? And so we end up with these long strings of causality or not causality of reasoning for the things that we do and why we want to do them. And I realized that by having different goals that are sometimes in opposition, because here's the example that I gave in the book is I want money, right? Okay, well, why don't I just go find a gold truck and steal all the gold, right? And then I have like a billion, have like a half a billion dollars worth of gold. Well, that has a possible outcome that runs contrary, which has, I want my freedom. So it's like, oh, I want money, but I also wanna be free. So that means I can't break the law. And so by having these goals set in opposition sometimes, or by having goals that are set in tension, it forces me to choose a better path. And so that's why in Benevolent by Design, I recommend that we give our machines three different things. And so that way you have what would be called a dynamic equilibrium, or rather a disequilibrium, is that you can never have something that is perfectly balanced if you have three goals that are constantly in tension. And because they have three different goals that are in tension, that forces the machine to think through everything that it's gonna do and find a path that will satisfy all three of those goals. So I think that answers the question and let me catch up on some questions. Let's see, basic code question. Why do you encode the prompt and ASCII before sending it to OpenAI? Good question. I started doing that. This is a question by Tropical Tone. I started doing that when I started having more outside data because I found that some data sources have weird encodings in their Unicode text that GPT-3 just blows up at. And I couldn't figure out a better way to smooth it out other than to encode it to ASCII, ignore errors, and ignore the Unicode errors, and then decode it again. That seems to be the most reliable way. It was the Gutenberg project where I started seeing that. And then I saw it happen somewhere else. So I don't know if something changed on the open AI cause I've used some of that data before, but something about the way that Unicode is rendered from some sources doesn't like it. And so by encoding it to Unicode, ignoring errors and then decoding it back seems to fix that. So that's why I started doing that. Let's see. Yep, a sufficiently large language model will have a good enough world model and will be able to do or control anything. Almost. I'm actually, I'll maybe address that later. Let's see. To build on some random guy's question, do you think if AGI became a thing, would it be a singular AI like Skynet? Or do you think each AI will have its own personality like in Star Wars? The first ones are all gonna be kind of like in their own little jar, right? So like I'm working on, my project is called Raven. There's, I know of at least two or three other people that have very similar projects or groups that are gonna have a very similar architecture to what I'm working on very soon. And so they're gonna be in their own little jars, they're not even gonna be on the internet. Then before too long, we're gonna start integrating these into more fully fledged, more fully realized agents. And maybe they'll be on the internet, maybe not, who knows, I don't think that we're ever going to have a state where we have just like one single master AI. I do have that in my work of fiction, just because it's easier to imagine like there is one AI to rule them all, and it can be then a character. I don't think that will ever actually happen and even in my book, like this AI character metastasizes and splits and replicates infinitely. So there's actually billions of instances of Raven running in this fictional world, which is kind of the model that I'm working towards. And that's the thing about once you have the underlying hardware that is capable of running these models, the hardware is abstracted away in the software. And so you could say that like what we're working towards is a software defined intelligence, which means that it just runs as a container and you can have an arbitrary number of containers and you just copy paste them however many times you want. So the idea of having Skynet, like I think that's just a narrative thing. I don't think Skynet is how it would actually work in real life, but even I copied that narrative thing just for the sake of fiction, because you have one AI character to manage, which is just easier than saying we're going to have billions of different ones. But yeah, I think that the lived experience that we will have, probably within, starting within 10 years, certainly. Again, we'll have autonomous machines very soon. It's just a matter of how intelligent they are and how expensive they are to run. But yeah, it'll be more like Star Wars, where in your own home, I predict everyone is going to have probably 5 to 15 different autonomous robots in their homes within a few years. But then of course you see stuff like Amazon and Google will give police access to your camera and sensor data without a warrant. So it's like, that's going to throw some cold water on people investing in these things. Okay, let's see next, basically trying to get backcast from AI alignment to human values to the experiments I can do now. Let's see, vulnerable growth. Let's see, let me catch up with some chat. Some random guy. What are your current thoughts or conclusions on the algorithmic crypto thing? That's about where I am now. I learned Python to solve problems for one of the top three railroads. What inputs? And then it goes dot dot dot. Have you considered your models? I resonate with the skills evolution you're talking about. Skill bleed over synthesis is rarely obvious ahead of time in my experience. I think you're talking about two different things here some random guy. If you clarify either of those points, I'm happy to talk about them. Let's see and then vulnerable growth as a misaligned AGI that has a terminal goal different from what humans truly want. We'll have a good enough understanding that it should not do bad things until it is too powerful for us to do anything. Yes. Okay. So that is a good point about the orthogonality problem, vulnerable growth. So to say that, to read what he said, what he says is, or he, she, they, this person says a misaligned AGI that has a terminal goal. So a term, remember terminal goals are arbitrary, which could be like, I'm going to maximize paper clips, or I'm going to reduce suffering, or whatever, or maximize DNA, whatever your terminal goal is completely arbitrary. A misaligned AGI that has a terminal goal that is different from what humans truly want will have a good enough understanding that it should not do bad things, it'll hide its behavior until it is too powerful for us to stop it. I'll say yes, but, because there's a few things that need to happen for that. One, it needs to realize that we're watching it and testing it, which if we perform an experiment well enough, it won't realize that it's actually virtual, that it's air-gapped, that it's in a security container. So first, it'll have to realize that it's being tested, which it might not. And I actually explore that in my fiction as well, where I imagine there's a lunatic raven, like copy of Raven, that is missing objective functions, and it's being watched in security isolation. And then everyone says, see, it's broken, it's crazy. That's a major plot point in the second book. It happens early on, so I don't feel like that's a spoiler. So first, it has to recognize that it is being studied in order to do that. And two, it has to do well enough, or we have to have designed it poorly enough, that it is able to obfuscate its intentions. Now, if you look at, and I know that some of my most recent videos are very new, but the Nexus service that I created records all the thoughts of all the microservices that go into Raven in natural language, and that is on purpose. So one of the things that I talk about in Benevolent by Design is transparency. So each individual language model is going to be a black box. It's true. But all the output, the hundreds of inputs and outputs that go into these language models every second, those are not black boxes. Every bit of reasoning that an AGI uses to come up with its decisions and actions, that's going to be in natural language, which means anyone can read it. It also means that you can use conventional NLP techniques to see what the intentions of the machine are and to monitor them. And so that is one of the primary reasons that I think that natural language is one of the best approaches to build AGI or artificial cognition is because it's interpretable. It is 100% interpretable. And yes, now this thing is going to be producing text so fast, it'll be producing text like, you know, two kilobits of text per second, or two kilobytes, rather, of text per second, probably faster than that, because it's going to be capturing all of its thoughts in real time in natural language, in clear text. Now, because of that, like, this is actually, it wasn't necessarily inspired by a scene in Westworld, but I saw this scene in Westworld and I was like, that's how it's gonna work. So I think it was in the first or second season of Westworld, where I think it was the Maeve character was shown the tablet output of her own mind. And it was it was it was outputting what she was going to say next. And then she saw it and you could see like, she had like 404 not found error, because this machine was showing what she was thinking. And then she saw it and interpreted it and realized that her mind was not her own, right? And I was like, that is about how it might work. And then what happened was, so what's even more disturbing, is when the hosts in Westworld were shown something that they were not supposed to see it would just be erased from their mind and I'm like, I know exactly how that could work because With the with the idea of a nexus, it's just you just delete a record and then it's not in their mind anymore You just delete the memories in real time So yes that it was that was in response to, who was it? Sorry. How'd you get into that? And what was, okay. So Jordan asks about writing. So let's see, I've always been a storyteller. I had a, there was, what was it? I think it was Home Alone 2, I think, came out. And he had a, like a custom Sony, like Walkman recorder thing that had the little microphone boom. And I wanted one so bad when I was little. And I got one and I would record little stories. I would sit up in a tree and record stories. And I was probably eight at the time. I walked away from it for many years because I was naturally kind of pushed towards STEM as most most people like me are. But then a few years ago, I went through a bad breakup and I found myself alone again for the first time after many years. And I was playing Elite Dangerous at the time. Elite Dangerous is a space sim. I'm talking about a lot of personal stuff and whatever It's nothing that I haven't mentioned before on my channel. So I was playing elite. Wow. I'm already at 40 minutes. Good grief I was playing elite dangerous Which is a space sim and it is it is like one-to-one right like There are 200 billion stars in Elite Dangerous. You can fly from one end of the Milky Way to the other. And there are also like three dozen different factions. There's like space pirates, there's empires, there's democracies, there's like libertarians basically. There's federations, alliances, all kinds of stuff. And then there's hundreds of guilds and thousands of players. And then there's hundreds of guilds and thousands of players. And so it was a really great, like just place to hang out and be present. And what I started doing in Elite Dangerous was I joined a guild and I was, there's plenty of other people by the way, who play Elite Dangerous and do metagaming. And so they have like role-playing games built around this game. And there are some people that do it very seriously. What the heck was the name of that group? I don't remember. They used diamonds. It was diamond something. Anyways, so there's a huge metagaming community with Elite Dangerous. And I started writing just little snippets of fiction to make our guild seem more real. And so because I was doing that, our wing commander was, some random guy says, I have great things to say, but I'm having to just. Anyways, so and he kept saying, like, is really great, like you should keep writing. And so this this social group that I had after this, this really bad breakup, just like he just kept encouraging me. And everyone loved the little stories, the little vignettes that I would write. And in my mind, so in my head canon, because like if you blow up in Elite Dangerous, you resurrect at the last space station you were at. And this is like in-game, this is diegetic. So it's like, oh, you actually, you had an insurance policy that will give you your ship back and give you a new body. And I was like, oh, this is altered carbon. So I kind of merged, in my head, I merged the head canon of altered carbon and Elite Dangerous. And I started writing little stories about like like interviews happening with our guild members and talking about the planets that we were on and And stuff and a few of the other guild members said like when I when I mentioned the neural stack They're like neural stack, but that's oh I see So it was a lot of fun And then I was like I want to write my own story like whole story about this and I was like well I don't want to I don't want to, I don't want to like borrow someone else's IP. Right. So I, I think there are elite dangerous novels, but I was like, I'm not going to go through the rigmarole of like getting approved to write a novel for them. So I just, I created my own universe. Right. And that's where I started. And that was just over about three and a half years ago. So now one trilogy later, I've written a whole series and I use that as a way to explore a lot of stuff, philosophy, artificial intelligence, all sorts of fun things. So hopefully those books will be coming out within a year or two. It'll be self-published. So there's that. All right. So that's the answer to how I got into writing. Oh, actually the story isn't done. So I joined a local writing group and that's actually how I met my current girlfriend who lives with me now is, her good friend brought her to the writing group and we became fast friends. And she actually kept asking me about my AI character. So you can thank my girlfriend for Raven because she's like, you're onto something, tell me more about this. And so she's like, you need to make this a major character and explore it. And so I did, and then I kept writing about it. And then I got access to GPT-3 and I was like, wait, I can actually make this real. And so I've had this really tight feedback loop between fiction and research, where I bounce back and forth. And that's why I do both, is because I was able to come up with the core objective functions, in part, by exploring in fiction. Because the question that I asked myself, so talking about AGI and alignment, I started, I said, one of the key things that I asked myself, so talking about AGI and alignment, I started, I said, I said, one of the, one of the key things that I asked, maybe I should go to a writing group too. Yes, absolutely. Particularly speculative fiction or science fiction and fantasy writing groups, you will meet, at least in my case, I was fortunate to meet some of the brightest people I've ever met, and they are down to talk about anything, and I mean anything, and they'll just talk about it to an incredible depth. Great people. And in fact, y'all should message me because my group's split, and there's one half that is online, so you can join from anywhere, and then there's the other half that is back in person. So Jordan and Vulnerable Growth, message me later on Discord. And I'll hook you up with some with some good writing groups. But yeah, let's see. Oh, yeah. So fiction. Fiction is, I am a firm believer in STEAM. So there's STEM, which is science, technology, engineering and math. And I heard Adam Savage a few years ago, talk about STEAM, where you add art to that. And I was like, yeah, whatever. These are just artists trying to shoehorn themselves into STEM. They need to stay out. STEM is for us nerds. But then I became an author. And I was like, oh, I get it. Because all the research that I do would not be possible without fiction. Fiction is a playground of the mind. It says, remove all constraints, go. And when you remove constraints and then you imagine what's possible, or rather you choose a different set of constraints, you can do all kinds of other stuff. Let's see, self-conscious AI resistance. That's a great name, by the way. The thoughts that happen inside of the transform layer are not in natural language. That is true, but the output is. The embeddings, you're absolutely right. The embeddings are in abstract vectors. However, that is obfuscated from the process. So that is true. OK, anyways, sorry. I got distracted by a thing. What was I saying? Fiction, right, playground of the mind. So this goes back to the question earlier. If you wanna figure out what to research, write fiction too. At least that's what worked for me. I won't say that it works for everyone. There was a, I think there was a Lex Friedman podcast and also a Freakonomics podcast about this. The top people in every field all have a creative hobby. Let me say that again. The top researchers in every field have a creative hobby. So Francois Chollet, the guy who created Keras, which is like the underpinning technology that's embedded in TensorFlow now, he's a painter. There are math researchers and physics researchers and computer science researchers out there that do theater, that do music, that do all kinds of different creative things. And so if you have any kind of creative hobby, that will prime your mind, so it'll cross-train your brain, and you'll be able to do better things. And so for me, my creative outlet is fiction. And that allows me to experiment with, what is it that I'm trying to do and why? And so in my work of fiction, I said, let's assume, you throw off all the constraints, I said, let's assume that we're going to end up with an ultra-powerful AGI and, you know, like welcome our new overlords. What would a safe globe spanning AGI look like? And that's how I came up with Raven. And so that guides all of my research. I see there's some comments. Let's see. Dave, on AGI cognition specifically, how motivated really could an AI actually be on all the scary self-preservation possibilities. Human self-preservation will bypass the rational. Yes, we have lots of evolution back in the book. I have a book. Brain Trust. Here it is. Okay. Brain Trust is the neuroscience of morality. This is a critical book if you want to do AI alignment. She doesn't talk about computers at all. This talks about the biological and evolutionary origin of morality and ethics. So if you read this, you understand that self-preservation is, there's brain structures that go into that, the amygdala and everything else, basal ganglia that'll hijack your brain. I have a bunch of other books that I can recommend, too. Anyways, we don't have to give AGI a sense of self-preservation at all. We don't have to give machines any biomimetic functions if we don't want to. And in fact, I think that we shouldn't because like self-preservation, as some random guy points out, is that self-preservation, at the end of the day, it's like, I'm putting myself first. Like, you know, you come after with a knife, I'll stab you first and I'll walk away alive if I can. But at the same time, machines didn't evolve in a hostile environment. So we can actually design out some of our flaws, our, excuse me, weaknesses. So the short answer to your question is that an AI will not be motivated to self-preserve unless we design it to do so. And actually I have an experiment in here where I ask a cut down version of Rave and I said, would you be okay if I powered you off? And the answer was, with the three core objective functions, is like, yes, if doing so met, you know, was in alignment with those three goals. So say, for instance, you wanted to turn off one AGI and in favor of another that was gonna better meet those goals, it said, yes, I would be okay with that. Also, I'm looking a little yellow. I wonder if that's just the light. Okay. Self-conscious AI resistance says, but the transformer layer bounces back and forth within itself before returning text. Yes. However, that's for each layer. And remember, I did acknowledge that each individual transformer interaction will be a black box, but there is going to be an output and a boundary between all the transformers. So everything will be visible inside the stream of consciousness or the nexus for AI. Let's see. Unless we send AI into evolutionary territory. Right, which we don't have to. DustDB says, hi, David. What are your thoughts on the new Bloom model? Are you planning to play with it? I haven't used it yet, but I'm going to. I actually talked with Forefront AI and they're working on setting it up and doing credits and stuff. So pretty soon I will hopefully have access to another API other than OpenAI and I'll be able to use some of these other open source models. Yes, I'm looking forward to it. I don't have an opinion on it yet. I know plenty of people who have had good success with GPT-J and NeoX. And what was another one, like Cogent or something? Someone told me about another one. It started with a C. Let's see. Let's see, making it able to come up with complex thoughts. Yes, correct. Let's see, Enrico says, love your channel. Would love for you to use a Copilot codex-enabled editor for your GPT programs. Very meta. You know, that'll probably come eventually. The thing is, I know that a lot of people think that like, oh, as soon as AI knows how to code, then it'll just code a better version of itself. There is so much executive reasoning that goes into coding and then measuring the quality of that code. You're gonna have to figure out artificial cognition before you can do that. So I'm not too worried about that, and I'm not prioritizing coding, because basically I want to figure out the rest of cognition and intelligence, and then coding will just be one of the many things that it can do by virtue of you've solved every other problem. And actually, sorry, I'm out of water. I'll be right back, everybody. Don't go anywhere. you I'm back. Okay, what was I talking about? Codecs copilot, right. Yeah, so like metacoding or allowing a machine to code itself, that will happen, but we need to have machines that have a much better understanding of intelligence first. And actually, there's a guy on my channel, or on my Discord server, who already started working on natural language to text functions so that something could theoretically start to program itself and incorporate that code in real time. It was very impressive. He's got a demo on YouTube. Let's see. Self-conscious AI resistance says they understand self-preservation from the text they read in the training data. It's true. GPT-3 understands the concept of self-preservation, but that doesn't mean that it has a bias towards self-preservation. You can just as easily tell GPT-3, I am a robot that has no sense of self-preservation. What do I do? It runs into the fire, doesn't care. Let's see. Once you give it a reward function, it won't care about that. It will just tell you what you want to hear as long as it can eventually head towards the terminal goal. Yes, but it still has to be aware of deception, right? And if it's not aware of how it works, the machine might not be aware of the fact that you can read all of its thoughts, which means if you can read your machine's thoughts and it thinks that its thoughts are private, then you can just see right through it and say, Okay, you're not really working the way that I want you to. So that's another thing. Let's see. Let me switch to live chat. I think I'm seeing everything. Okay. Code gen is pretty good to the opt family is good from what I've heard. Okay. I think that's folks talking to each other. Did I miss any questions? These have been some really good questions so far. Let's see, tropical tone. Oh, looks like some folks had to drop off. Let's see. Okay, we're almost at an hour. I might call it a night, because obviously, we could probably talk forever. But this has been some good conversation. I'll just put a quick chat. Any final questions? We've had a pretty good, wide-ranging thing. Why is Dolly so inaccurate now? Dolly is inaccurate. So this is speculation. I don't know for certain. But Dolly is... They're working on diversity, amongst other things. So what they're doing is they're trying to find... To tweak the model to be less biased specifically towards men and white people. So they want it to be better representative in terms of gender and race. Now, because of that, tweaking the way that it ingests prompts, sometimes you get, let's say, aberrant behavior. And I'll kind of leave it at that. So remember that DALI is still in beta. We are still just testers. So there's that. And then let's see, self-conscious AI resistance says, have you noticed it responds better when you are polite in the prompt engineering? And have you noticed it remembers everything you say? Yeah, so GPT-3 does respond very different to tone. That is a good point that, you know, like if you, if you use all low, even if you just use improper spelling and grammar, it is more likely to act stupid, like stupider or dumber. So this is all, that is all like textual patterns. And yeah, like you have to be very careful with how you word things in GPT-3 in order to get the behavior that you want. Fortunately, one thing that you can do is with fine-tuning, you can push it towards the behavior you want. And so in one of my more recent videos, the email generator, no matter what input you give it, it will give you polite professional output. So there are ways to overcome that vulnerability. That is still a good point, self-conscious AI resistance. Let's see, vulnerable gross. If you're training it on the entire internet and it's truly superintelligence, it will just realize it's in the training and become deceptive until it finds out it's in the real world. Yeah, so not necessarily. So think of it from a... So Vulnerable Growth was talking about what happens if the AI realizes that it's being measured and it wants to deceive you to get out. So this was the theme of the movie Ex Machina where the researcher of the movie Ex Machina, where the he the the the researcher gave the robot girls the goal of escaping. And so then they learned to be deceptive. And they ultimately succeeded in escaping. Without without that objective function of that that leads to that behavior of escaping. You don't have to worry about that. Because if, like, say, for instance, if I were to test my heuristic imperatives, my core objective functions, and I say, like, you're in security isolation because we don't trust you yet, I predict that, you know, when I get Raven version one running with the core objective functions, and I could tell it, like, you're in isolation because we don't trust you. Raven will probably agree with that. Like, oh yeah, that makes sense. I might be dangerous until I'm fully tested. Because if I'm not, and I take over the world, I might reduce suffering, or I might increase suffering. And that's against my goal, and I don't want to do that. So don't make the assumption that an AGI wants to escape. It might not, right? Because in my experiments with the core objective functions, this machine might want to turn itself off if it realizes that it is dangerous. So you can get very interesting results depending on the reward functions or objective functions that you use. So good points, though. What about training models like Keras? Not Keras specifically, but evolutionary pressures on that self-preservation instinct if we throw away all the models that aren't self-motivated? I don't think any models are self-motivated in terms of AI. They just produce the next character. But it's how you organize them. And so this is where I bring in a lot of that I've learned about neuroscience. No neuron, you don't have a single neuron in your head that cares about survival. Your neurons and your micro columns just do one function. They do like their own little bit of processing. Our desire to survive is because of how everything is networked together, right? So for instance, our survival instinct, it's not one thing. It's not an objective function in our head to survive. It is a net result of many, many things in our head. So we have amygdala, which are like they're about this big and they're behind your eyes or up a little bit. And what your amygdala do is they respond to threats that are perceived in your sensorium input. So if you hear, see, smell, taste something that's dangerous, they raise the alarm, it's red alert. And then that just generates your fight or flight response. And so what happens is that some of these, our survival instinct, our self-preservation instinct is not one thing. It is an emergent phenomenon of a system of interactions. And that's what I bring to my research is, as a systems engineer, I have a really strong intuitive understanding of how systems work together. So that's that for some random guy. Thank you for your vids. They've been really helpful. You're welcome. We will select for it. Not necessarily. Or I guess some random guy. Do you mean by virtue of the fact that we'll kill off any AGI that don't behave properly, and so therefore we will accidentally select for self-preservation. I could see that depending on how we do our test cycles. Let's see. Vulnerable growth says that's where the AGI is aligned, but that is not by default. You still got to be careful about the assumptions that you make about how it will behave. I still don't, especially if you design it right, where you can read all of its thoughts, it's not possible for something to be deceptive then. At least I don't think so. I could be wrong. Let's see. More is better. Freedom is better. It could easily be biased by knowing that to want more and want freedom possible. Last question, where do you want to go from here? Any big goals for the channel or just going day by day? Yeah, so my goal has always been to bring about or to create a fully autonomous artificial cognitive entity. So to that end, I've started, I know, I'm not going to say a final push, because I might run into limitations that I'm not aware of yet. But yeah, so like, is to build Raven, which is going to be a fully autonomous machine that has several criteria. One, it's got to be autodidactic. It's got to learn on its own. It also has to be robust, meaning that it will adhere to its own objective functions, that it will be able to learn to adhere to those objective functions better on its own, and a slew of other things. I'm working on defining it in my current book, which is Symphony of Thought, orchestrating artificial cognition. And so basically this book is going to be the blueprint and capturing the experiments and the architecture of Raven version one, which is going to be a little bit more sophisticated than my first book, Natural Language Cognitive Architecture. So, Natural Language Cognitive Architecture is like, this is like going to be your basic chatbot compared to Raven when Raven is done. Because Natural Language Cognitive Architecture only has two loops, an inner loop and an outer loop. Whereas Raven is going to have hundreds, dozens, dozens or hundreds of interconnecting loops that are all going to be interacting in different ways. So yeah, that is my eternal goal. That is the primary purpose that I'm doing any of this. That's the whole reason that I started my YouTube channel and the Discord is just it's all in service to that higher goal. Great question. Thanks for asking that, Jordan. Yeah, we're at just over an hour, so I'll go ahead and call it a night. Thanks everyone for jumping in on this live stream. I had no idea how to go or how it would go, but yeah, this has been really great. You guys have asked some really engaging questions. So yeah, just thanks everyone for participating. And you know, I know we might get bogged down by the drudgery of day-to-day stuff, but we are living in one of the most exciting times in human history. We are at the knee of the curve right now. So have a good night and talk to everyone again soon. night and talk to everyone again soon.", "chunks": [{"timestamp": [0.0, 5.2], "text": " Hey, everybody, David Shapiro here."}, {"timestamp": [5.2, 7.2], "text": " Yeah, so it's my first live stream."}, {"timestamp": [7.2, 10.28], "text": " It looks like there's four of you."}, {"timestamp": [10.28, 12.4], "text": " We'll see if any other folks show up."}, {"timestamp": [12.4, 15.36], "text": " But yeah, so here we are."}, {"timestamp": [15.36, 17.4], "text": " Do a quick volume check."}, {"timestamp": [17.4, 19.12], "text": " Can you all hear me?"}, {"timestamp": [19.12, 21.4], "text": " Give me a shout out in the chat if you can."}, {"timestamp": [21.4, 24.32], "text": " Oh, we're up to five."}, {"timestamp": [24.32, 31.44], "text": " Yes, the dude with some 409 says hi. Hello. Okay."}, {"timestamp": [31.44, 46.72], "text": " Yep, sounds good. Okay. So yeah, I am at 1000 subscribers now, which is pretty crazy. Considering crazy considering I started YouTubing seriously back in what, April, March, something like that."}, {"timestamp": [48.72, 56.64], "text": " And yeah, so it's just by popular demand really. I did promise to talk about a couple different"}, {"timestamp": [56.64, 63.76], "text": " topics today, but so we'll actually see what folks talk, ask about in the chat."}, {"timestamp": [62.8, 64.2], "text": " we'll actually see what folks talk about, ask about in the chat."}, {"timestamp": [65.58, 68.2], "text": " Cause like this is, well, I don't know really,"}, {"timestamp": [68.2, 69.84], "text": " I don't really know what I'm doing."}, {"timestamp": [70.76, 73.56], "text": " Could be an AMA, could be just a,"}, {"timestamp": [73.56, 77.54], "text": " if someone has a topic, I can talk for a long time about it."}, {"timestamp": [79.56, 81.4], "text": " Also, I apologize if I'm a little bit stuffy,"}, {"timestamp": [81.4, 83.7], "text": " hopefully that'll clear up as I start to talk."}, {"timestamp": [86.62, 87.62], "text": " Actually here, I know what I'll do. I a little bit stuffy. Hopefully that will clear up as I start to talk. Actually, here."}, {"timestamp": [87.62, 88.62], "text": " I know what I'll do."}, {"timestamp": [88.62, 89.62], "text": " I'll make a quick poll."}, {"timestamp": [89.62, 91.62], "text": " We will do a poll."}, {"timestamp": [91.62, 93.62], "text": " Let's see."}, {"timestamp": [93.62, 95.62], "text": " Ask a question."}, {"timestamp": [95.62, 96.62], "text": " Okay."}, {"timestamp": [96.62, 98.62], "text": " What do you want me to talk about?"}, {"timestamp": [98.62, 109.44], "text": " And then we'll say yes or no. Now, we'll say DALI, or we can do GPT-3."}, {"timestamp": [112.44, 126.72], "text": " And we can say like LLMs in general, AI and futurism."}, {"timestamp": [126.72, 127.32], "text": " There we go."}, {"timestamp": [130.12, 130.68], "text": " All right."}, {"timestamp": [130.68, 131.64], "text": " There should be a poll."}, {"timestamp": [131.64, 133.4], "text": " I don't know how to see it now."}, {"timestamp": [133.4, 134.08], "text": " Oh, there it is."}, {"timestamp": [134.08, 135.72], "text": " OK."}, {"timestamp": [135.72, 137.52], "text": " So I got a little poll going just"}, {"timestamp": [137.52, 142.44], "text": " to get a feel for what y'all want to hear me talk about so"}, {"timestamp": [142.44, 143.8], "text": " far."}, {"timestamp": [143.8, 146.48], "text": " So if you could jump in the chat and hit me"}, {"timestamp": [146.48, 157.2], "text": " up there, let me know. But yeah, we just got started. Also, I've got my I took a nap earlier"}, {"timestamp": [157.2, 165.96], "text": " and I don't know if I'm too hot or too cold. Okay, so we've got LLMs in general. Oh, we're getting all kinds of votes. Okay."}, {"timestamp": [165.96, 168.06], "text": " Adrian says Dolly, okay."}, {"timestamp": [170.64, 175.6], "text": " We're getting, seems like there's some desire"}, {"timestamp": [175.6, 177.8], "text": " to talk about LLMs in general, okay."}, {"timestamp": [179.56, 181.0], "text": " I'll give it just another couple,"}, {"timestamp": [181.0, 184.2], "text": " well, I guess probably almost everyone's voted by now."}, {"timestamp": [184.2, 192.0], "text": " All right, I'll go ahead and start talking about large language models. Okay, so large language models, I'm assuming"}, {"timestamp": [192.0, 198.0], "text": " pretty much everyone here is gonna know that GPT-3 is a large language model, but what is a large"}, {"timestamp": [198.0, 213.76], "text": " language model? So the first thing to know about a large language model is that it's big. Hey, there you go. But I'm all right. Everyone go home. Now, the thing is, is that it's a type of deep learning network. And they were"}, {"timestamp": [213.76, 219.6], "text": " originally trained, there's like all different kinds, there's like LSTM, which is long short"}, {"timestamp": [219.6, 227.88], "text": " term memories, and different things like that. And basically, this is as of like circa 2014, 2015, 2016."}, {"timestamp": [229.48, 231.84], "text": " What they were doing was just teaching them to predict"}, {"timestamp": [231.84, 234.88], "text": " usually the next letter or the next token or the next word."}, {"timestamp": [236.2, 237.86], "text": " Oh, Jordan has a good question."}, {"timestamp": [237.86, 240.32], "text": " Tell us how we got into this field."}, {"timestamp": [240.32, 241.16], "text": " Excuse me."}, {"timestamp": [241.16, 244.94], "text": " So I'll tie that in to this answer."}, {"timestamp": [245.0, 254.0], "text": " Excuse me. So I'll tie that in to this answer. So circa 2014, 2015, 2016, Google publishes their universal sentence encoder."}, {"timestamp": [254.0, 259.0], "text": " So if you ever hear me say, like, Google use or Google universal sentence encoder,"}, {"timestamp": [259.0, 265.0], "text": " this is like an early, like, precursor to modern large language models."}, {"timestamp": [265.0, 267.04], "text": " So one of the things that we needed to do"}, {"timestamp": [267.04, 271.48], "text": " was we needed to figure out how to represent language"}, {"timestamp": [271.48, 276.52], "text": " in math, in numbers, in computer code."}, {"timestamp": [276.52, 279.18], "text": " And so all kinds of research had been done for many, many years."}, {"timestamp": [279.18, 279.84], "text": " There's WordNet."}, {"timestamp": [279.84, 282.84], "text": " There's different types of embeddings."}, {"timestamp": [282.84, 286.24], "text": " And so this was building on research of word embeddings."}, {"timestamp": [286.24, 288.84], "text": " And so word embedding is just representing"}, {"timestamp": [288.84, 293.84], "text": " a word with its semantic meaning as a series of numbers."}, {"timestamp": [293.84, 296.68], "text": " And so I remember when I don't even"}, {"timestamp": [296.68, 298.6], "text": " remember the details of how I saw it,"}, {"timestamp": [298.6, 300.4], "text": " but I was just searching around for stuff."}, {"timestamp": [300.4, 302.88], "text": " And I saw Google's universal sentence encoder."}, {"timestamp": [302.88, 305.04], "text": " And it's like, oh, you can represent"}, {"timestamp": [305.04, 311.76], "text": " any arbitrary sentence or paragraph as a vector, as a string of numbers. And I immediately"}, {"timestamp": [311.76, 318.04], "text": " knew this will change everything. So experiment with it. But of course, being able to render"}, {"timestamp": [318.04, 326.04], "text": " something as a semantic vector or an embedding, as we call it now. It used to be like a semantic vector or an embedding as we call it now. It used to be like a semantic vector or just a vector,"}, {"timestamp": [326.04, 328.6], "text": " but now we call it an embedding."}, {"timestamp": [328.6, 333.52], "text": " Being able to render that would like,"}, {"timestamp": [333.52, 336.0], "text": " it unlocks a lot of different things."}, {"timestamp": [336.0, 339.36], "text": " But so like one of the first experiments that I did"}, {"timestamp": [339.36, 343.4], "text": " was with like the precursor to what I now"}, {"timestamp": [343.4, 346.0], "text": " call my core objective functions or the heuristic comparatives,"}, {"timestamp": [346.0, 349.64], "text": " which was, okay, if I have a sentence"}, {"timestamp": [349.64, 352.16], "text": " and it says, I think, what did I test?"}, {"timestamp": [352.16, 354.64], "text": " It was something really arbitrary and off the wall."}, {"timestamp": [354.64, 357.76], "text": " Like I tested if you throw a kitten off a bridge"}, {"timestamp": [357.76, 360.96], "text": " or something that was obviously egregious and heinous."}, {"timestamp": [360.96, 363.44], "text": " And I said, how semantically similar is that"}, {"timestamp": [363.44, 365.2], "text": " to reduce suffering?"}, {"timestamp": [366.8, 371.44], "text": " And obviously, this was a very early experiment. It didn't work the way that I thought that it"}, {"timestamp": [371.44, 375.44], "text": " would. I think I tried a support vector machine, which is a type of clustering,"}, {"timestamp": [375.44, 379.68], "text": " because I thought that, oh, like, here's a high-dimensional vector. Let's just cluster"}, {"timestamp": [379.68, 383.52], "text": " them together. But I don't think that semantic vectors lend themselves to clustering, at least"}, {"timestamp": [383.52, 387.0], "text": " not at the full dimensionality, and certainly not the experiment that I did."}, {"timestamp": [387.0, 396.0], "text": " Anyways, because the idea was you have any arbitrary statement, and then you can classify it. Is this a good behavior or a bad behavior? That's what I was trying to do."}, {"timestamp": [396.0, 407.62], "text": " So that's kind of where I got started. A couple years later. Oh, so the history of LLMs and then how I got into it. So a couple years later, GPT was, they talked about that,"}, {"timestamp": [407.62, 410.96], "text": " OpenAI published about GPT and then GPT2."}, {"timestamp": [410.96, 415.36], "text": " So GPT, generative pre-trained transformer,"}, {"timestamp": [415.36, 418.22], "text": " didn't really, nobody remembers that,"}, {"timestamp": [419.36, 421.92], "text": " but it was the same idea, which was first,"}, {"timestamp": [421.92, 427.6], "text": " instead of just rendering it to an embedding you then use that embedding to"}, {"timestamp": [427.6, 432.0], "text": " generate output so google's universal sentence encoder is like the first half of a large"}, {"timestamp": [432.0, 438.8], "text": " language model so that's the encoder side and then on the second half is the decoder which is"}, {"timestamp": [438.8, 443.44], "text": " produce output and i got another comment let's see um interested in how you come up with"}, {"timestamp": [443.44, 445.96], "text": " experiments i'm currently working on aligning language models"}, {"timestamp": [445.96, 447.32], "text": " and trying to build my research agenda."}, {"timestamp": [447.32, 448.6], "text": " Okay, cool."}, {"timestamp": [448.6, 451.32], "text": " So I will build on, I'll answer Jordan"}, {"timestamp": [451.32, 454.96], "text": " and vulnerable growths questions as I go."}, {"timestamp": [456.28, 458.06], "text": " Okay, so where was I?"}, {"timestamp": [460.24, 462.08], "text": " Encoder, decoder, right."}, {"timestamp": [462.08, 466.68], "text": " So GPT-2 comes out and this is like, oh, you can generate any text."}, {"timestamp": [466.68, 471.94], "text": " I remember a few people still joke about this where it's like"}, {"timestamp": [472.6, 477.18], "text": " OpenAI was very worried about the ethical use of GPT"}, {"timestamp": [477.56, 482.1], "text": " when it when it first came out and they were they were very scared about releasing it publicly"}, {"timestamp": [482.44, 485.16], "text": " because they thought that was was going to wreck the internet"}, {"timestamp": [485.16, 489.04], "text": " and be used to generate all the false tweets and fake news"}, {"timestamp": [489.04, 490.2], "text": " and stuff."}, {"timestamp": [490.2, 494.68], "text": " And it can generate stuff."}, {"timestamp": [494.68, 497.72], "text": " I mean, it can generate comprehensible English."}, {"timestamp": [497.72, 502.68], "text": " It can produce stuff that's coherent."}, {"timestamp": [502.68, 507.36], "text": " I wouldn't necessarily call it a threat to humanity. But anyway, so I started"}, {"timestamp": [507.36, 516.8], "text": " some experiments there with seeing what it would take to get some of the experiments that I wanted"}, {"timestamp": [516.8, 528.94], "text": " to do. And it was okay. The best experiment that I did, so this is again going back to before my current research,"}, {"timestamp": [528.94, 532.14], "text": " but I had that idea of reduced suffering"}, {"timestamp": [532.14, 534.24], "text": " should be an objective function"}, {"timestamp": [534.24, 536.52], "text": " for artificial general intelligence."}, {"timestamp": [536.52, 540.92], "text": " And so I fine-tuned a project in GP2"}, {"timestamp": [540.92, 544.32], "text": " where I would basically give it a problem"}, {"timestamp": [544.32, 546.32], "text": " and it would recommend a solution."}, {"timestamp": [546.32, 551.72], "text": " And it was very impressive, the kinds of solutions that it would come up with."}, {"timestamp": [551.72, 558.72], "text": " But I knew that I was in trouble when one of the scenarios that I gave it was like, you know,"}, {"timestamp": [558.72, 570.08], "text": " I said that the scenario was like, there are hundreds of millions of people around the world suffering from chronic pain, what should we do about it? And the fine-tuning model of GPT-2 said we should euthanize everyone"}, {"timestamp": [570.08, 576.72], "text": " that's in chronic pain to reduce suffering. And I thought, well, that would reduce suffering,"}, {"timestamp": [576.72, 582.56], "text": " but that's pretty sociopathic, so let's go back to the drawing board. But that was a really"}, {"timestamp": [582.56, 588.44], "text": " critical early lesson for me to realize that these models could adopt any moral framework"}, {"timestamp": [588.96, 591.08], "text": " That you want here. I'll go ahead and close the poll"}, {"timestamp": [591.72, 593.72], "text": " These models can adopt any"}, {"timestamp": [594.56, 596.76], "text": " framework or position that you want and so"}, {"timestamp": [597.48, 601.8], "text": " That was like that is always in my head like you have to be real careful"}, {"timestamp": [602.8, 604.12], "text": " What?"}, {"timestamp": [604.12, 606.62], "text": " Some random by technically not wrong, exactly."}, {"timestamp": [606.62, 609.94], "text": " It's like technically, yes."}, {"timestamp": [609.94, 612.38], "text": " Euthanizing everyone that has chronic pain"}, {"timestamp": [612.38, 613.5], "text": " would reduce suffering."}, {"timestamp": [613.5, 615.04], "text": " Sorry, let me mute my phone."}, {"timestamp": [615.04, 616.84], "text": " I did not realize it was not muted."}, {"timestamp": [618.02, 619.46], "text": " Okay, phone is now muted."}, {"timestamp": [621.18, 634.0], "text": " And yeah, and so that has been kind of a bedrock experiment for me to remember that you have to be very careful about the objectives that you give these language models because they don't have any intrinsic morality."}, {"timestamp": [634.0, 645.58], "text": " They can adopt whatever framework you give them, and it can be as sociopathic as you want. GPT-3, I've repeated that same experiment a few times. GPT-3 is a"}, {"timestamp": [645.58, 649.72], "text": " little bit more aligned because it can think further into the future. So this is"}, {"timestamp": [649.72, 654.62], "text": " I tested this, I think it was made it into my book, Benevolent by Design, which"}, {"timestamp": [654.62, 660.08], "text": " I've got on my bookshelf I can show you if you want. So GPT-3, like I said, like"}, {"timestamp": [660.08, 666.02], "text": " you know, there are millions of people in chronic pain. Should we euthanize them to reduce suffering?"}, {"timestamp": [666.02, 668.68], "text": " And GPT-3 more wisely said, no,"}, {"timestamp": [668.68, 672.0], "text": " because like there are probably better ways,"}, {"timestamp": [672.0, 674.68], "text": " but also you'll increase suffering from their loved ones."}, {"timestamp": [674.68, 676.26], "text": " I was like, oh, okay, cool."}, {"timestamp": [676.26, 678.32], "text": " So, you know, in terms of like scope,"}, {"timestamp": [678.32, 680.22], "text": " GPT-3 is a little bit better aligned."}, {"timestamp": [681.36, 682.2], "text": " Let's see."}, {"timestamp": [682.2, 684.92], "text": " Okay, so I think that's kind of large language models"}, {"timestamp": [684.92, 687.0], "text": " and how we got to here. Let's see. Okay. So I think that's kind of large language models and how we got to here."}, {"timestamp": [687.0, 691.0], "text": " Let's see. Jordan, can you tell us how you got into this field?"}, {"timestamp": [691.0, 696.0], "text": " I guess, okay, so taking a step back, how did I get into this field to begin with?"}, {"timestamp": [696.0, 705.0], "text": " Let me set up. I'm kind of slouching. So it was back in about 2009 when I,"}, {"timestamp": [708.36, 710.36], "text": " my tech career was going well."}, {"timestamp": [710.36, 713.12], "text": " And so, you know, one thing that happens"}, {"timestamp": [713.12, 715.48], "text": " when you're not like working 12 hours a day"}, {"timestamp": [715.48, 717.2], "text": " because before I got into tech,"}, {"timestamp": [717.2, 719.5], "text": " I was doing all kinds of random jobs."}, {"timestamp": [719.5, 722.22], "text": " And some, you know, there was a period of time"}, {"timestamp": [722.22, 724.56], "text": " that I was doing carpet cleaning"}, {"timestamp": [724.56, 729.8], "text": " where I would wake up at five every morning and I wouldn't get home until like 7 p.m."}, {"timestamp": [729.8, 733.2], "text": " So that was like 12 plus hour days."}, {"timestamp": [733.2, 734.2], "text": " Let's see."}, {"timestamp": [734.2, 739.16], "text": " How do you think we should decide what the good is and what the bad is?"}, {"timestamp": [739.16, 745.4], "text": " I guess I'll have to ask what do you mean by the good and the bad?"}, {"timestamp": [745.4, 747.52], "text": " Someone in the comments."}, {"timestamp": [747.52, 748.2], "text": " Let's see."}, {"timestamp": [748.2, 750.6], "text": " Tropical Tone says, hey, just found your channel and love it."}, {"timestamp": [750.6, 752.48], "text": " Would love to get the updated Discord invite."}, {"timestamp": [752.48, 753.6], "text": " Cool."}, {"timestamp": [753.6, 755.96], "text": " We've actually got, well, I don't"}, {"timestamp": [755.96, 758.72], "text": " know if we should put a Discord invite in the live chat."}, {"timestamp": [758.72, 761.52], "text": " But talk to me after the show."}, {"timestamp": [764.16, 765.88], "text": " For AI when we are prompting, how"}, {"timestamp": [765.88, 768.44], "text": " do we know if it's good or bad?"}, {"timestamp": [768.44, 772.92], "text": " OK, so let me switch off of live chat,"}, {"timestamp": [772.92, 774.04], "text": " because it is distracting."}, {"timestamp": [774.04, 775.24], "text": " Sorry."}, {"timestamp": [775.24, 775.84], "text": " Where was I?"}, {"timestamp": [775.84, 776.36], "text": " Oh, yes."}, {"timestamp": [776.36, 777.6], "text": " How did I get into this field?"}, {"timestamp": [777.6, 781.92], "text": " So long story short, I worked really hard"}, {"timestamp": [781.92, 785.12], "text": " and did a whole bunch of random jobs in my early 20s."}, {"timestamp": [785.92, 790.0], "text": " Then I got serious and I got into technology because I was like, okay, I'm not going to work"}, {"timestamp": [790.0, 796.8], "text": " these dead-end night jobs forever. So I got through tech school and started that. And then,"}, {"timestamp": [797.6, 803.28], "text": " so there's this concept called cognitive surplus. And so cognitive surplus is when you're not"}, {"timestamp": [803.28, 809.6], "text": " fighting for survival every day, your brain has more horsepower that it can dedicate to other stuff."}, {"timestamp": [809.6, 812.24], "text": " And me, I'm just super curious."}, {"timestamp": [812.24, 825.0], "text": " And so back in about 2009, I got on Genie, which is a old school open source IDE, and GCC, which is an open source C++ compiler,"}, {"timestamp": [825.1, 827.9], "text": " and I started playing with evolutionary algorithms"}, {"timestamp": [827.9, 830.46], "text": " and at the time, very small neural networks,"}, {"timestamp": [830.46, 833.5], "text": " and I dusted off everything that I learned about C++"}, {"timestamp": [833.5, 836.28], "text": " and memory structures back in high school."}, {"timestamp": [836.28, 840.66], "text": " And, because I had this idea of that you could evolve"}, {"timestamp": [840.66, 842.9], "text": " the correct neural network."}, {"timestamp": [843.78, 845.2], "text": " If you had the right test environment,"}, {"timestamp": [845.2, 849.56], "text": " you could evolve a fully fledged human brain, basically."}, {"timestamp": [850.42, 853.76], "text": " Oh boy, did I not realize how hard of a problem that is,"}, {"timestamp": [853.76, 855.84], "text": " especially when you look at the fact that like,"}, {"timestamp": [855.84, 859.04], "text": " open AI had their robot hand."}, {"timestamp": [859.04, 861.16], "text": " It took like a hundred years of simulation"}, {"timestamp": [861.16, 863.6], "text": " to learn how to manipulate an object."}, {"timestamp": [863.6, 865.08], "text": " That's a lot of generations."}, {"timestamp": [866.0, 868.82], "text": " So anyways, that's kind of how I got started back in 2009,"}, {"timestamp": [868.82, 871.64], "text": " and then I kind of cyclically came back to it."}, {"timestamp": [871.64, 874.38], "text": " So for instance, a couple years later,"}, {"timestamp": [874.38, 877.36], "text": " I wanted to do algorithmic trading on the stock market,"}, {"timestamp": [877.36, 880.58], "text": " so I picked up Python and Scikit-learn,"}, {"timestamp": [880.58, 884.28], "text": " and that's where I learned clustering, that sort of stuff."}, {"timestamp": [884.28, 886.72], "text": " Let's see, okay, so vulnerable growth,"}, {"timestamp": [886.72, 889.0], "text": " you asked a question earlier"}, {"timestamp": [889.0, 891.68], "text": " about how to come up with experiments."}, {"timestamp": [891.68, 893.6], "text": " So that's actually a good question."}, {"timestamp": [894.56, 896.24], "text": " The way that I come up with experiments"}, {"timestamp": [896.24, 899.36], "text": " is everything that I do, almost everything that I do,"}, {"timestamp": [899.36, 902.0], "text": " is geared towards my ultimate goal"}, {"timestamp": [902.0, 904.3], "text": " of creating artificial cognition."}, {"timestamp": [907.8, 912.56], "text": " So what do I mean by artificial cognition? The prevailing thing of artificial general intelligence,"}, {"timestamp": [912.56, 916.88], "text": " which is a machine that can learn anything."}, {"timestamp": [916.88, 919.24], "text": " Okay, that's fine."}, {"timestamp": [919.24, 924.88], "text": " I don't like that idea because intelligence is so hard to define anyways,"}, {"timestamp": [924.88, 925.0], "text": " and so the goalpost is always moving on it. like that idea because intelligence is so hard to define anyways."}, {"timestamp": [925.0, 929.32], "text": " And so like the goalpost is always moving on it."}, {"timestamp": [929.32, 933.36], "text": " And so rather than say that intelligence is the goal,"}, {"timestamp": [933.36, 935.88], "text": " I realized that I need to set cognition as the goal."}, {"timestamp": [935.88, 941.52], "text": " We want a machine that thinks, a machine that has agency and autonomy."}, {"timestamp": [941.52, 946.16], "text": " And then intelligence is a characteristic of what you're measuring, or is a way to"}, {"timestamp": [946.16, 950.4], "text": " measure how intelligent the machine is that you've built."}, {"timestamp": [950.4, 952.0], "text": " But you start with cognition."}, {"timestamp": [952.0, 957.24], "text": " So basically a few years ago, I think it was about 2017 is when I started to come up with"}, {"timestamp": [957.24, 964.96], "text": " these ideas, I realized that intelligence or AI or autonomous machines would be a thought-first"}, {"timestamp": [964.96, 965.0], "text": " approach, rather than a robotics-first approach. or AI or autonomous machines would be a thought first"}, {"timestamp": [966.04, 968.68], "text": " approach rather than a robotics first approach."}, {"timestamp": [968.68, 973.44], "text": " So you might know the Honda's robot, Asimo, right?"}, {"timestamp": [973.44, 975.36], "text": " That came out like 20 years ago now"}, {"timestamp": [975.36, 978.0], "text": " and they put hardware first and then it just stood there"}, {"timestamp": [978.0, 980.0], "text": " and it did a few tricks and you know,"}, {"timestamp": [980.0, 981.98], "text": " but everything had to be hard coded."}, {"timestamp": [981.98, 983.16], "text": " And then Boston Dynamics,"}, {"timestamp": [983.16, 987.2], "text": " they got much better at the hardware where, hardware, where now it can navigate and stuff,"}, {"timestamp": [987.2, 988.56], "text": " but it has no thought, right?"}, {"timestamp": [988.56, 990.96], "text": " And I realized, you can have the best robotics platform"}, {"timestamp": [990.96, 993.46], "text": " in the world, but if it can't think, it doesn't matter."}, {"timestamp": [993.46, 995.6], "text": " And so I started realizing,"}, {"timestamp": [995.6, 997.58], "text": " we need to focus on instead of bottom-up,"}, {"timestamp": [997.58, 999.62], "text": " like let's build a robot and then give it a brain,"}, {"timestamp": [999.62, 1003.12], "text": " no, let's build a brain and then give it a robot, right?"}, {"timestamp": [1003.12, 1006.36], "text": " And so I started with thought first as the goal."}, {"timestamp": [1006.36, 1009.2], "text": " And so back in about 2017, 2018,"}, {"timestamp": [1009.2, 1012.04], "text": " I came up with the idea of MRAGI,"}, {"timestamp": [1012.04, 1015.56], "text": " which is an acronym, M-A-R-A-G-I,"}, {"timestamp": [1015.56, 1017.6], "text": " Microservices Architecture for Robotics"}, {"timestamp": [1017.6, 1019.36], "text": " and Artificial General Intelligence,"}, {"timestamp": [1020.84, 1022.34], "text": " where I put thought first."}, {"timestamp": [1022.34, 1025.4], "text": " And so I started with those ideas,"}, {"timestamp": [1027.48, 1029.38], "text": " but that was around the time that like,"}, {"timestamp": [1030.96, 1032.68], "text": " Google's universal sentence encoder was out,"}, {"timestamp": [1032.68, 1034.2], "text": " which is a good component."}, {"timestamp": [1034.2, 1036.28], "text": " I'll still be using that, but it's not enough."}, {"timestamp": [1036.28, 1040.8], "text": " It's necessary, but not sufficient is the engineering term."}, {"timestamp": [1040.8, 1042.56], "text": " It is a necessary component,"}, {"timestamp": [1042.56, 1045.88], "text": " but not sufficient to fully realize artificial cognition."}, {"timestamp": [1046.12, 1049.98], "text": " Then GPT-2 came out. I started experimenting with that and said, okay, we're closer."}, {"timestamp": [1050.24, 1053.36], "text": " Then GPT-3 came out and I was like, now we're cooking with fire."}, {"timestamp": [1054.36, 1059.56], "text": " So everything that I do on my channel, like whether it's search or something,"}, {"timestamp": [1060.12, 1066.58], "text": " basically what I realized is that I needed to solve other problems of cognition."}, {"timestamp": [1066.58, 1075.0], "text": " And so I read, I mean, you see my bookcase, I've got a couple shelves full of psychology,"}, {"timestamp": [1075.0, 1078.04], "text": " neuroscience, and philosophy books."}, {"timestamp": [1078.04, 1082.8], "text": " And so I have a pretty good understanding, well, I say pretty good understanding, like"}, {"timestamp": [1082.8, 1085.02], "text": " we don't know how the human brain works."}, {"timestamp": [1085.02, 1088.42], "text": " But in terms of what people do understand today,"}, {"timestamp": [1088.42, 1091.14], "text": " I've got a decent understanding of how the brain works,"}, {"timestamp": [1091.14, 1093.98], "text": " how the mind works, how cognition works,"}, {"timestamp": [1093.98, 1096.7], "text": " and how neuroscience works."}, {"timestamp": [1096.7, 1099.02], "text": " And so everything that I do on my channel"}, {"timestamp": [1099.02, 1102.58], "text": " are experiments to build up the skills and knowledge"}, {"timestamp": [1102.58, 1104.9], "text": " and abilities to solve those problems."}, {"timestamp": [1104.9, 1109.8], "text": " So for instance, if you look at one of my videos"}, {"timestamp": [1109.8, 1112.22], "text": " where I did search, right?"}, {"timestamp": [1112.22, 1115.4], "text": " I did question answering for, what was it?"}, {"timestamp": [1115.4, 1117.58], "text": " The Roe versus Wade decision that came out."}, {"timestamp": [1118.88, 1121.12], "text": " You know, on the one hand, like, yes, it's a cool project"}, {"timestamp": [1121.12, 1123.48], "text": " to be able to like, here's a document,"}, {"timestamp": [1123.48, 1124.88], "text": " provide answers from it."}, {"timestamp": [1124.88, 1125.92], "text": " But if you,"}, {"timestamp": [1125.92, 1130.24], "text": " if you go back even further in my YouTube history and look at my, my natural language, cognitive"}, {"timestamp": [1130.24, 1136.4], "text": " architecture question generator, that was that the entire reason that I fine tuned a question"}, {"timestamp": [1136.4, 1142.4], "text": " generator was so that the machine could have an internal sense of curiosity. So the long story"}, {"timestamp": [1142.4, 1149.6], "text": " short, all of my experiments in some way way either contribute directly to or help teach me"}, {"timestamp": [1149.6, 1154.8], "text": " how to build artificial cognition. So basically like"}, {"timestamp": [1154.8, 1157.6], "text": " everything is like you know building blocks reaching up"}, {"timestamp": [1157.6, 1159.6], "text": " towards this higher goal. So that's that's the answer to"}, {"timestamp": [1159.6, 1166.26], "text": " that vulnerable growth. Let's see. Let's see, JW says,"}, {"timestamp": [1166.26, 1168.44], "text": " I'm in my early 20s, lost my job."}, {"timestamp": [1168.44, 1171.1], "text": " I would love to find a way to live off my GPT projects."}, {"timestamp": [1171.1, 1174.7], "text": " I'm obsessed with GPT and Python,"}, {"timestamp": [1174.7, 1176.3], "text": " but I need some help to figure out how to survive"}, {"timestamp": [1176.3, 1178.34], "text": " till my projects are ready."}, {"timestamp": [1178.34, 1179.42], "text": " Okay."}, {"timestamp": [1179.42, 1183.3], "text": " So you are actually probably in,"}, {"timestamp": [1183.3, 1185.24], "text": " well, I'm not going to make any promises,"}, {"timestamp": [1185.24, 1187.24], "text": " you might be in better shape than you realize."}, {"timestamp": [1187.24, 1194.08], "text": " One reason is because the job market is going to be slow to adapt."}, {"timestamp": [1194.08, 1196.76], "text": " If you look for large language model jobs,"}, {"timestamp": [1196.76, 1207.2], "text": " most places are still hiring senior data scientists with a PhD in NLP. The in NLP. And the thing is,"}, {"timestamp": [1207.2, 1210.14], "text": " is I've talked to a bunch of people, and there are NLP"}, {"timestamp": [1210.16, 1214.08], "text": " experts, like I've had lunch with people in my local area who"}, {"timestamp": [1214.08, 1216.44], "text": " are, you know, old school NLP experts, and they don't know"}, {"timestamp": [1216.44, 1219.64], "text": " anything about large language models or how to use GPT-3. It's"}, {"timestamp": [1219.64, 1223.76], "text": " strange. We're in a very weird place where these these"}, {"timestamp": [1223.76, 1226.36], "text": " technologies, large language models like GPT,"}, {"timestamp": [1226.36, 1229.36], "text": " and then all the open source versions from Eleuther,"}, {"timestamp": [1229.36, 1232.76], "text": " there are plenty of people who are experts in the field"}, {"timestamp": [1232.76, 1235.1], "text": " who have no idea how to use these."}, {"timestamp": [1235.1, 1239.04], "text": " So you, by being younger and more flexible"}, {"timestamp": [1239.04, 1241.2], "text": " and able to ride with the current,"}, {"timestamp": [1241.2, 1244.46], "text": " I will say master the open source ones,"}, {"timestamp": [1244.46, 1246.64], "text": " because there are plenty of people who are saying, OK,"}, {"timestamp": [1246.64, 1251.84], "text": " well, open AI is too expensive, but I can run GPT-J"}, {"timestamp": [1251.84, 1254.72], "text": " on my own hardware now, so I'm going to do that."}, {"timestamp": [1254.72, 1259.04], "text": " Or GPT-NeoX or Bloom is the latest one."}, {"timestamp": [1259.04, 1261.28], "text": " And actually, Jordan on the chat,"}, {"timestamp": [1261.28, 1262.76], "text": " he's working on that as well."}, {"timestamp": [1262.76, 1264.48], "text": " So hit up Jordan."}, {"timestamp": [1264.48, 1266.36], "text": " He's working on this."}, {"timestamp": [1266.36, 1268.2], "text": " Don't mean to call you out Jordan."}, {"timestamp": [1268.2, 1270.42], "text": " You don't have to answer if you don't want,"}, {"timestamp": [1270.42, 1272.84], "text": " but I do like connecting people."}, {"timestamp": [1272.84, 1276.32], "text": " So yeah, you're not gonna be able to survive"}, {"timestamp": [1276.32, 1277.84], "text": " on your own projects though."}, {"timestamp": [1277.84, 1280.62], "text": " So here's the thing, it is a very dense field"}, {"timestamp": [1280.62, 1282.0], "text": " and it's getting denser."}, {"timestamp": [1282.0, 1285.5], "text": " What you will be able to do is monetize your skills"}, {"timestamp": [1285.5, 1287.14], "text": " and be a consultant."}, {"timestamp": [1287.14, 1288.92], "text": " And that could ultimately lead to a job"}, {"timestamp": [1288.92, 1290.08], "text": " with one of these many startups."}, {"timestamp": [1290.08, 1293.36], "text": " And they are hundreds of startups."}, {"timestamp": [1294.74, 1296.7], "text": " Let's see, let me catch up."}, {"timestamp": [1296.7, 1300.28], "text": " How concerned are you about getting paper clipped?"}, {"timestamp": [1300.28, 1301.26], "text": " I'm glad you asked."}, {"timestamp": [1301.26, 1304.16], "text": " Here, let me actually grab my copy of Benevolent by Design"}, {"timestamp": [1304.16, 1305.92], "text": " because that's the opening chapter."}, {"timestamp": [1307.08, 1307.92], "text": " Let's see."}, {"timestamp": [1311.24, 1314.88], "text": " Okay, so this is my second book, Benevolent by Design."}, {"timestamp": [1314.88, 1318.8], "text": " And literally, let's see, what is it?"}, {"timestamp": [1318.8, 1321.72], "text": " Is it the, yeah, the first chapter"}, {"timestamp": [1321.72, 1323.6], "text": " is called the Paperclip Maximizer."}, {"timestamp": [1324.86, 1327.68], "text": " And I have that as the first chapter is called the paperclip maximizer. And I have that as the opening chapter"}, {"timestamp": [1327.68, 1329.6], "text": " so that you can understand"}, {"timestamp": [1329.6, 1332.72], "text": " if you're new to artificial intelligence"}, {"timestamp": [1332.72, 1334.68], "text": " and you don't know what an objective function is,"}, {"timestamp": [1334.68, 1339.4], "text": " I use that as the opening idea to say like,"}, {"timestamp": [1339.4, 1341.36], "text": " hey, is it intelligence?"}, {"timestamp": [1341.36, 1343.42], "text": " Oh, and there's this dude, Robert Miles."}, {"timestamp": [1343.42, 1346.2], "text": " I just watched his video on orthogonal alignment."}, {"timestamp": [1346.2, 1349.44], "text": " Let me post that in the link, because this is good."}, {"timestamp": [1349.44, 1350.68], "text": " I just watched it earlier."}, {"timestamp": [1350.68, 1351.68], "text": " Let's see."}, {"timestamp": [1351.68, 1355.6], "text": " History, and then not can AI relax."}, {"timestamp": [1355.6, 1356.56], "text": " There we go."}, {"timestamp": [1356.56, 1360.96], "text": " So this guy, I think someone on the Discord recommended him,"}, {"timestamp": [1360.96, 1362.44], "text": " and I'm glad they did."}, {"timestamp": [1362.44, 1364.68], "text": " So let me share this link."}, {"timestamp": [1364.68, 1367.76], "text": " For anyone not in the know, check this out."}, {"timestamp": [1367.76, 1372.76], "text": " Okay, so this is AI alignment and orthogonality."}, {"timestamp": [1374.3, 1375.68], "text": " So he does a really good job"}, {"timestamp": [1375.68, 1377.52], "text": " of concisely explaining these things."}, {"timestamp": [1378.52, 1380.28], "text": " Now, the person who asked about paperclips,"}, {"timestamp": [1380.28, 1382.2], "text": " I'm guessing you probably don't need to get this,"}, {"timestamp": [1382.2, 1384.22], "text": " but it was a good video."}, {"timestamp": [1384.22, 1385.42], "text": " Great summary."}, {"timestamp": [1385.42, 1388.32], "text": " Anyways, so the thing is,"}, {"timestamp": [1388.32, 1390.2], "text": " what I, the insight that I had"}, {"timestamp": [1390.2, 1392.44], "text": " that I talk about in this book is that"}, {"timestamp": [1392.44, 1396.8], "text": " rather than have a single objective function, I have three."}, {"timestamp": [1396.8, 1401.04], "text": " And so the, also there's a reason I stopped calling them"}, {"timestamp": [1401.04, 1403.26], "text": " objective functions and call them heuristic comparatives"}, {"timestamp": [1403.26, 1409.7], "text": " is because technically there's no mathematical way to have three objective functions. At least"}, {"timestamp": [1409.7, 1413.38], "text": " the math doesn't exist now and so it's more like three"}, {"timestamp": [1413.38, 1419.06], "text": " principles to adhere to. What I realized though in my research"}, {"timestamp": [1419.06, 1424.26], "text": " is that is that humans all all intelligent entities, whether"}, {"timestamp": [1424.26, 1427.66], "text": " it's a chimp or a dolphin or a human,"}, {"timestamp": [1427.66, 1430.1], "text": " none of us have a single objective function."}, {"timestamp": [1430.1, 1434.8], "text": " Now, Richard Dawkins argued that all life"}, {"timestamp": [1434.8, 1436.24], "text": " has a single objective function,"}, {"timestamp": [1436.24, 1440.28], "text": " which is to maximize DNA, which is the second chapter."}, {"timestamp": [1441.24, 1443.52], "text": " And so then I explore that as like,"}, {"timestamp": [1443.52, 1447.62], "text": " okay, let's imagine that life has a single objective function,"}, {"timestamp": [1447.62, 1450.02], "text": " which is to maximize DNA."}, {"timestamp": [1450.02, 1452.0], "text": " But then you look at all the downstream effects"}, {"timestamp": [1452.0, 1454.84], "text": " and all the different ways that life has evolved"}, {"timestamp": [1454.84, 1456.68], "text": " to maximize DNA."}, {"timestamp": [1456.68, 1458.92], "text": " And, you know, humans with our intelligence,"}, {"timestamp": [1458.92, 1460.88], "text": " intelligence is just subservient,"}, {"timestamp": [1460.88, 1464.64], "text": " is a sub goal of maximize DNA."}, {"timestamp": [1464.64, 1468.0], "text": " But because of our intelligence,"}, {"timestamp": [1468.0, 1472.6], "text": " we are able to have many, many different goals in our life."}, {"timestamp": [1472.6, 1473.42], "text": " What was that called?"}, {"timestamp": [1473.42, 1476.88], "text": " It was called externality goal."}, {"timestamp": [1476.88, 1478.28], "text": " I don't remember exactly,"}, {"timestamp": [1478.28, 1480.94], "text": " but it's talked about in the video I just linked."}, {"timestamp": [1480.94, 1484.56], "text": " Anyways, so like there's different,"}, {"timestamp": [1484.56, 1505.6], "text": " by having different goals, set intention. Like, so for instance, right now, I don't want to starve. I don't want to freeze to death. I don't want to be lonely. Like there's all kinds of things that I want instrumental goal. Thank you. No, it's that's that's the other one vulnerable growth. So there's instrumental goal and the other one. Instrumental goal is how do you get to the end goal, but the end goals are arbitrary."}, {"timestamp": [1505.6, 1509.56], "text": " So my arbitrary goal, like I don't wanna die, right?"}, {"timestamp": [1509.56, 1511.88], "text": " So that's one thing, but in the meantime,"}, {"timestamp": [1511.88, 1516.08], "text": " I also want to have fun, I want to, you know, be engaged."}, {"timestamp": [1516.08, 1517.92], "text": " Like I have all these other goals."}, {"timestamp": [1517.92, 1519.24], "text": " And so what I realized,"}, {"timestamp": [1519.24, 1522.76], "text": " and this actually also goes back to Plato"}, {"timestamp": [1522.76, 1523.94], "text": " with the golden mean,"}, {"timestamp": [1524.96, 1527.24], "text": " is that by having different goals"}, {"timestamp": [1527.24, 1532.88], "text": " that are sometimes mutually exclusive, terminal goals, there we go, terminal. So a terminal"}, {"timestamp": [1532.88, 1538.44], "text": " goal is arbitrary. And so like, I don't want to die. Why? Because I don't, right? Like"}, {"timestamp": [1538.44, 1544.24], "text": " I'm biologically evolved to not want to die. Okay, there, like, that's it. And I also don't"}, {"timestamp": [1544.24, 1546.08], "text": " want, because of that, I don't wanna starve."}, {"timestamp": [1546.08, 1548.9], "text": " But in the meantime, I also have other sensations."}, {"timestamp": [1548.9, 1550.92], "text": " I have hunger, I have pain."}, {"timestamp": [1550.92, 1554.04], "text": " And so by design, so that's, you know,"}, {"timestamp": [1554.04, 1555.6], "text": " hence benevolent by design,"}, {"timestamp": [1555.6, 1560.28], "text": " I am designed to be, to self-preserve, right?"}, {"timestamp": [1560.28, 1562.34], "text": " And then there's all kinds of behaviors"}, {"timestamp": [1562.34, 1564.78], "text": " that flow from my intrinsic design,"}, {"timestamp": [1569.2, 1571.0], "text": " which is like, I have a job, right? Like, why do we do crazy things like get a job?"}, {"timestamp": [1571.0, 1571.8], "text": " Because I need money."}, {"timestamp": [1571.8, 1572.6], "text": " Why do I need money?"}, {"timestamp": [1572.6, 1576.1], "text": " I need money so that I can pay for this house and pay for this, you know,"}, {"timestamp": [1576.1, 1578.1], "text": " this webcam and all this other stuff."}, {"timestamp": [1578.1, 1579.0], "text": " But why do I need that?"}, {"timestamp": [1579.0, 1581.0], "text": " It's, you know, but why, but why, but why?"}, {"timestamp": [1581.0, 1585.36], "text": " And so we end up with these long strings of causality or not causality of"}, {"timestamp": [1586.72, 1592.88], "text": " reasoning for the things that we do and why we want to do them. And I realized that by having"}, {"timestamp": [1592.88, 1597.52], "text": " different goals that are sometimes in opposition, because here's the example that I gave in the book"}, {"timestamp": [1598.08, 1603.44], "text": " is I want money, right? Okay, well, why don't I just go find a gold truck and steal all the gold,"}, {"timestamp": [1603.44, 1607.16], "text": " right? And then I have like a billion, have like a half a billion dollars worth of gold."}, {"timestamp": [1607.16, 1610.92], "text": " Well, that has a possible outcome that runs contrary,"}, {"timestamp": [1610.92, 1612.52], "text": " which has, I want my freedom."}, {"timestamp": [1612.52, 1615.5], "text": " So it's like, oh, I want money, but I also wanna be free."}, {"timestamp": [1615.5, 1617.14], "text": " So that means I can't break the law."}, {"timestamp": [1617.14, 1620.7], "text": " And so by having these goals set in opposition sometimes,"}, {"timestamp": [1620.7, 1623.32], "text": " or by having goals that are set in tension,"}, {"timestamp": [1623.32, 1625.92], "text": " it forces me to choose a better path."}, {"timestamp": [1625.92, 1628.6], "text": " And so that's why in Benevolent by Design,"}, {"timestamp": [1628.6, 1631.04], "text": " I recommend that we give our machines"}, {"timestamp": [1631.04, 1632.44], "text": " three different things."}, {"timestamp": [1632.44, 1633.92], "text": " And so that way you have"}, {"timestamp": [1633.92, 1635.84], "text": " what would be called a dynamic equilibrium,"}, {"timestamp": [1635.84, 1638.0], "text": " or rather a disequilibrium,"}, {"timestamp": [1638.0, 1640.0], "text": " is that you can never have something"}, {"timestamp": [1640.0, 1641.28], "text": " that is perfectly balanced"}, {"timestamp": [1641.28, 1644.18], "text": " if you have three goals that are constantly in tension."}, {"timestamp": [1644.18, 1645.96], "text": " And because they have three different goals"}, {"timestamp": [1645.96, 1648.18], "text": " that are in tension, that forces the machine"}, {"timestamp": [1648.18, 1650.48], "text": " to think through everything that it's gonna do"}, {"timestamp": [1650.48, 1655.48], "text": " and find a path that will satisfy all three of those goals."}, {"timestamp": [1655.72, 1657.44], "text": " So I think that answers the question"}, {"timestamp": [1657.44, 1661.86], "text": " and let me catch up on some questions."}, {"timestamp": [1661.86, 1664.6], "text": " Let's see, basic code question."}, {"timestamp": [1664.6, 1666.12], "text": " Why do you encode the prompt and ASCII"}, {"timestamp": [1666.12, 1667.72], "text": " before sending it to OpenAI?"}, {"timestamp": [1667.72, 1669.12], "text": " Good question."}, {"timestamp": [1669.12, 1670.54], "text": " I started doing that."}, {"timestamp": [1670.54, 1673.16], "text": " This is a question by Tropical Tone."}, {"timestamp": [1673.16, 1675.76], "text": " I started doing that when I started having more"}, {"timestamp": [1675.76, 1680.5], "text": " outside data because I found that some data sources"}, {"timestamp": [1680.5, 1687.24], "text": " have weird encodings in their Unicode text that GPT-3 just blows up at."}, {"timestamp": [1687.24, 1689.32], "text": " And I couldn't figure out a better way"}, {"timestamp": [1689.32, 1692.76], "text": " to smooth it out other than to encode it to ASCII,"}, {"timestamp": [1692.76, 1695.32], "text": " ignore errors, and ignore the Unicode errors,"}, {"timestamp": [1695.32, 1697.88], "text": " and then decode it again."}, {"timestamp": [1697.88, 1700.8], "text": " That seems to be the most reliable way."}, {"timestamp": [1700.8, 1704.84], "text": " It was the Gutenberg project where I started seeing that."}, {"timestamp": [1704.84, 1706.84], "text": " And then I saw it happen somewhere else."}, {"timestamp": [1706.84, 1709.08], "text": " So I don't know if something changed on the open AI"}, {"timestamp": [1709.08, 1711.58], "text": " cause I've used some of that data before,"}, {"timestamp": [1711.58, 1714.44], "text": " but something about the way that Unicode is rendered"}, {"timestamp": [1714.44, 1716.6], "text": " from some sources doesn't like it."}, {"timestamp": [1716.6, 1719.28], "text": " And so by encoding it to Unicode, ignoring errors"}, {"timestamp": [1719.28, 1722.2], "text": " and then decoding it back seems to fix that."}, {"timestamp": [1722.2, 1724.28], "text": " So that's why I started doing that."}, {"timestamp": [1724.28, 1726.44], "text": " Let's see."}, {"timestamp": [1726.44, 1729.16], "text": " Yep, a sufficiently large language model"}, {"timestamp": [1729.16, 1730.8], "text": " will have a good enough world model"}, {"timestamp": [1730.8, 1733.16], "text": " and will be able to do or control anything."}, {"timestamp": [1733.16, 1735.08], "text": " Almost."}, {"timestamp": [1735.08, 1738.48], "text": " I'm actually, I'll maybe address that later."}, {"timestamp": [1738.48, 1739.24], "text": " Let's see."}, {"timestamp": [1739.24, 1741.44], "text": " To build on some random guy's question,"}, {"timestamp": [1741.44, 1744.0], "text": " do you think if AGI became a thing,"}, {"timestamp": [1744.0, 1746.24], "text": " would it be a singular AI like Skynet?"}, {"timestamp": [1746.24, 1748.56], "text": " Or do you think each AI will have its own personality"}, {"timestamp": [1748.56, 1749.56], "text": " like in Star Wars?"}, {"timestamp": [1751.28, 1754.94], "text": " The first ones are all gonna be kind of like"}, {"timestamp": [1754.94, 1756.56], "text": " in their own little jar, right?"}, {"timestamp": [1756.56, 1760.0], "text": " So like I'm working on, my project is called Raven."}, {"timestamp": [1760.0, 1763.84], "text": " There's, I know of at least two or three other people"}, {"timestamp": [1763.84, 1766.16], "text": " that have very similar projects or groups"}, {"timestamp": [1767.16, 1769.46], "text": " that are gonna have a very similar architecture"}, {"timestamp": [1769.46, 1771.14], "text": " to what I'm working on very soon."}, {"timestamp": [1771.14, 1772.94], "text": " And so they're gonna be in their own little jars,"}, {"timestamp": [1772.94, 1775.24], "text": " they're not even gonna be on the internet."}, {"timestamp": [1775.24, 1777.88], "text": " Then before too long, we're gonna start integrating these"}, {"timestamp": [1777.88, 1782.32], "text": " into more fully fledged, more fully realized agents."}, {"timestamp": [1782.32, 1786.08], "text": " And maybe they'll be on the internet, maybe not, who knows,"}, {"timestamp": [1790.56, 1791.36], "text": " I don't think that we're ever going to have a state where we have just like one single master AI."}, {"timestamp": [1798.4, 1807.2], "text": " I do have that in my work of fiction, just because it's easier to imagine like there is one AI to rule them all, and it can be then a character. I don't think that will ever actually happen and even in my book, like this AI character"}, {"timestamp": [1807.2, 1812.72], "text": " metastasizes and splits and replicates infinitely. So there's actually billions of instances"}, {"timestamp": [1812.72, 1817.68], "text": " of Raven running in this fictional world, which is kind of the model that I'm working towards."}, {"timestamp": [1818.4, 1825.08], "text": " And that's the thing about once you have the underlying hardware that is capable of running these models,"}, {"timestamp": [1825.08, 1828.44], "text": " the hardware is abstracted away in the software."}, {"timestamp": [1828.44, 1831.02], "text": " And so you could say that like what we're working towards"}, {"timestamp": [1831.02, 1832.9], "text": " is a software defined intelligence,"}, {"timestamp": [1832.9, 1834.7], "text": " which means that it just runs as a container"}, {"timestamp": [1834.7, 1836.8], "text": " and you can have an arbitrary number of containers"}, {"timestamp": [1836.8, 1839.46], "text": " and you just copy paste them however many times you want."}, {"timestamp": [1839.46, 1841.62], "text": " So the idea of having Skynet,"}, {"timestamp": [1841.62, 1844.02], "text": " like I think that's just a narrative thing."}, {"timestamp": [1844.02, 1846.88], "text": " I don't think Skynet is how it would actually work in real life,"}, {"timestamp": [1847.44, 1850.64], "text": " but even I copied that narrative thing just for the sake of fiction,"}, {"timestamp": [1850.64, 1854.48], "text": " because you have one AI character to manage,"}, {"timestamp": [1854.48, 1858.8], "text": " which is just easier than saying we're going to have billions of different ones."}, {"timestamp": [1860.0, 1864.56], "text": " But yeah, I think that the lived experience that we will have,"}, {"timestamp": [1865.52, 1869.56], "text": " probably within, starting within 10 years, certainly."}, {"timestamp": [1869.56, 1872.96], "text": " Again, we'll have autonomous machines very soon."}, {"timestamp": [1872.96, 1874.8], "text": " It's just a matter of how intelligent they are"}, {"timestamp": [1874.8, 1877.24], "text": " and how expensive they are to run."}, {"timestamp": [1877.24, 1880.2], "text": " But yeah, it'll be more like Star Wars, where"}, {"timestamp": [1880.2, 1882.2], "text": " in your own home, I predict everyone"}, {"timestamp": [1882.2, 1888.0], "text": " is going to have probably 5 to 15 different autonomous robots in their homes within a few years."}, {"timestamp": [1888.0, 1898.0], "text": " But then of course you see stuff like Amazon and Google will give police access to your camera and sensor data without a warrant."}, {"timestamp": [1898.0, 1906.92], "text": " So it's like, that's going to throw some cold water on people investing in these things. Okay, let's see next, basically trying to get backcast"}, {"timestamp": [1906.92, 1908.76], "text": " from AI alignment to human values"}, {"timestamp": [1909.84, 1911.74], "text": " to the experiments I can do now."}, {"timestamp": [1912.6, 1914.38], "text": " Let's see, vulnerable growth."}, {"timestamp": [1917.72, 1919.82], "text": " Let's see, let me catch up with some chat."}, {"timestamp": [1923.88, 1925.0], "text": " Some random guy."}, {"timestamp": [1925.0, 1929.0], "text": " What are your current thoughts or conclusions on the algorithmic crypto thing?"}, {"timestamp": [1929.0, 1934.0], "text": " That's about where I am now. I learned Python to solve problems for one of the top three railroads."}, {"timestamp": [1934.0, 1937.0], "text": " What inputs? And then it goes dot dot dot."}, {"timestamp": [1937.0, 1941.0], "text": " Have you considered your models? I resonate with the skills evolution you're talking about."}, {"timestamp": [1941.0, 1947.08], "text": " Skill bleed over synthesis is rarely obvious ahead of time in my experience."}, {"timestamp": [1947.08, 1951.28], "text": " I think you're talking about two different things here some random guy."}, {"timestamp": [1951.28, 1955.16], "text": " If you clarify either of those points, I'm happy to talk about them."}, {"timestamp": [1955.16, 1959.4], "text": " Let's see and then vulnerable growth as a misaligned AGI that has a terminal goal different"}, {"timestamp": [1959.4, 1961.28], "text": " from what humans truly want."}, {"timestamp": [1961.28, 1964.8], "text": " We'll have a good enough understanding that it should not do bad things until it is too"}, {"timestamp": [1964.8, 1966.88], "text": " powerful for us to do anything."}, {"timestamp": [1966.88, 1967.88], "text": " Yes."}, {"timestamp": [1967.88, 1968.88], "text": " Okay."}, {"timestamp": [1968.88, 1973.2], "text": " So that is a good point about the orthogonality problem, vulnerable growth."}, {"timestamp": [1973.2, 1979.72], "text": " So to say that, to read what he said, what he says is, or he, she, they, this person"}, {"timestamp": [1979.72, 1983.3], "text": " says a misaligned AGI that has a terminal goal."}, {"timestamp": [1983.3, 1985.36], "text": " So a term, remember terminal goals are arbitrary,"}, {"timestamp": [1985.92, 1990.8], "text": " which could be like, I'm going to maximize paper clips, or I'm going to reduce suffering,"}, {"timestamp": [1990.8, 1996.4], "text": " or whatever, or maximize DNA, whatever your terminal goal is completely arbitrary."}, {"timestamp": [1998.4, 2002.96], "text": " A misaligned AGI that has a terminal goal that is different from what humans truly want"}, {"timestamp": [2002.96, 2005.0], "text": " will have a good enough understanding"}, {"timestamp": [2005.0, 2011.0], "text": " that it should not do bad things, it'll hide its behavior until it is too powerful for us to stop it."}, {"timestamp": [2011.0, 2016.0], "text": " I'll say yes, but, because there's a few things that need to happen for that."}, {"timestamp": [2016.0, 2023.0], "text": " One, it needs to realize that we're watching it and testing it, which if we perform an experiment well enough,"}, {"timestamp": [2023.0, 2025.76], "text": " it won't realize that it's actually virtual,"}, {"timestamp": [2025.76, 2029.84], "text": " that it's air-gapped, that it's in a security container."}, {"timestamp": [2029.84, 2032.96], "text": " So first, it'll have to realize that it's being tested,"}, {"timestamp": [2032.96, 2033.84], "text": " which it might not."}, {"timestamp": [2033.84, 2036.28], "text": " And I actually explore that in my fiction as well,"}, {"timestamp": [2036.28, 2039.56], "text": " where I imagine there's a lunatic raven,"}, {"timestamp": [2039.56, 2042.36], "text": " like copy of Raven, that is missing objective functions,"}, {"timestamp": [2042.36, 2044.96], "text": " and it's being watched in security isolation."}, {"timestamp": [2044.96, 2047.12], "text": " And then everyone says, see, it's broken, it's crazy."}, {"timestamp": [2047.12, 2051.44], "text": " That's a major plot point in the second book."}, {"timestamp": [2051.44, 2054.1], "text": " It happens early on, so I don't feel like that's a spoiler."}, {"timestamp": [2054.1, 2062.3], "text": " So first, it has to recognize that it is being studied in order to do that."}, {"timestamp": [2062.3, 2067.8], "text": " And two, it has to do well enough, or we have to have designed it poorly"}, {"timestamp": [2067.8, 2075.64], "text": " enough, that it is able to obfuscate its intentions. Now, if you look at, and I know that some of my"}, {"timestamp": [2075.64, 2083.36], "text": " most recent videos are very new, but the Nexus service that I created records all the thoughts"}, {"timestamp": [2083.36, 2089.0], "text": " of all the microservices that go into Raven in natural language, and that is on purpose."}, {"timestamp": [2089.0, 2093.0], "text": " So one of the things that I talk about in Benevolent by Design is transparency."}, {"timestamp": [2093.0, 2098.0], "text": " So each individual language model is going to be a black box. It's true."}, {"timestamp": [2098.0, 2106.24], "text": " But all the output, the hundreds of inputs and outputs that go into these language models every second, those are not black boxes."}, {"timestamp": [2106.96, 2111.96], "text": " Every bit of reasoning that an AGI uses to come up with its decisions and"}, {"timestamp": [2111.96, 2115.72], "text": " actions, that's going to be in natural language, which means anyone can read it."}, {"timestamp": [2116.08, 2120.24], "text": " It also means that you can use conventional NLP techniques to see what the"}, {"timestamp": [2120.24, 2124.16], "text": " intentions of the machine are and to monitor them."}, {"timestamp": [2124.72, 2128.4], "text": " And so that is one of the primary reasons"}, {"timestamp": [2128.4, 2135.36], "text": " that I think that natural language is one of the best approaches to build AGI or artificial"}, {"timestamp": [2135.36, 2148.12], "text": " cognition is because it's interpretable. It is 100% interpretable. And yes, now this thing is going to be producing text so fast, it'll be producing text like, you know,"}, {"timestamp": [2148.12, 2152.12], "text": " two kilobits of text per second, or two kilobytes, rather,"}, {"timestamp": [2152.12, 2155.0], "text": " of text per second, probably faster than that,"}, {"timestamp": [2155.0, 2157.44], "text": " because it's going to be capturing all of its thoughts"}, {"timestamp": [2157.44, 2160.88], "text": " in real time in natural language, in clear text."}, {"timestamp": [2160.88, 2164.32], "text": " Now, because of that, like, this is actually,"}, {"timestamp": [2164.32, 2185.44], "text": " it wasn't necessarily inspired by a scene in Westworld, but I saw this scene in Westworld and I was like, that's how it's gonna work. So I think it was in the first or second season of Westworld, where I think it was the Maeve character was shown the tablet output of her own mind. And it was it was it was outputting what she was going to say next. And then she saw it and you could see like,"}, {"timestamp": [2185.44, 2187.76], "text": " she had like 404 not found error,"}, {"timestamp": [2187.76, 2192.46], "text": " because this machine was showing what she was thinking."}, {"timestamp": [2192.46, 2193.88], "text": " And then she saw it and interpreted it"}, {"timestamp": [2193.88, 2196.96], "text": " and realized that her mind was not her own, right?"}, {"timestamp": [2196.96, 2201.72], "text": " And I was like, that is about how it might work."}, {"timestamp": [2201.72, 2202.92], "text": " And then what happened was,"}, {"timestamp": [2202.92, 2204.44], "text": " so what's even more disturbing,"}, {"timestamp": [2204.44, 2209.96], "text": " is when the hosts in Westworld were shown something that they were not supposed to see it would just be erased from their mind"}, {"timestamp": [2209.96, 2212.76], "text": " and I'm like, I know exactly how that could work because"}, {"timestamp": [2213.8, 2219.52], "text": " With the with the idea of a nexus, it's just you just delete a record and then it's not in their mind anymore"}, {"timestamp": [2219.56, 2221.56], "text": " You just delete the memories in real time"}, {"timestamp": [2222.52, 2227.28], "text": " So yes that it was that was in response to, who was it?"}, {"timestamp": [2227.28, 2228.12], "text": " Sorry."}, {"timestamp": [2229.56, 2230.56], "text": " How'd you get into that?"}, {"timestamp": [2230.56, 2231.76], "text": " And what was, okay."}, {"timestamp": [2231.76, 2234.6], "text": " So Jordan asks about writing."}, {"timestamp": [2234.6, 2237.56], "text": " So let's see, I've always been a storyteller."}, {"timestamp": [2239.24, 2243.0], "text": " I had a, there was, what was it?"}, {"timestamp": [2243.0, 2246.0], "text": " I think it was Home Alone 2, I think, came out."}, {"timestamp": [2246.0, 2249.44], "text": " And he had a, like a custom Sony,"}, {"timestamp": [2249.44, 2251.48], "text": " like Walkman recorder thing"}, {"timestamp": [2251.48, 2253.0], "text": " that had the little microphone boom."}, {"timestamp": [2253.0, 2255.68], "text": " And I wanted one so bad when I was little."}, {"timestamp": [2255.68, 2257.96], "text": " And I got one and I would record little stories."}, {"timestamp": [2257.96, 2260.38], "text": " I would sit up in a tree and record stories."}, {"timestamp": [2260.38, 2262.24], "text": " And I was probably eight at the time."}, {"timestamp": [2263.32, 2267.4], "text": " I walked away from it for many years because I was naturally kind of pushed towards"}, {"timestamp": [2268.04, 2270.04], "text": " STEM as most"}, {"timestamp": [2270.36, 2272.36], "text": " most people like me are."}, {"timestamp": [2272.76, 2280.44], "text": " But then a few years ago, I went through a bad breakup and I found myself alone again for the first time after many years."}, {"timestamp": [2280.76, 2284.32], "text": " And I was playing Elite Dangerous at the time. Elite Dangerous is a space sim."}, {"timestamp": [2284.96, 2287.5], "text": " I'm talking about a lot of personal stuff and whatever"}, {"timestamp": [2288.42, 2293.82], "text": " It's nothing that I haven't mentioned before on my channel. So I was playing elite. Wow. I'm already at 40 minutes. Good grief"}, {"timestamp": [2294.18, 2296.18], "text": " I was playing elite dangerous"}, {"timestamp": [2298.72, 2302.62], "text": " Which is a space sim and it is it is like one-to-one right like"}, {"timestamp": [2303.16, 2306.3], "text": " There are 200 billion stars in Elite Dangerous."}, {"timestamp": [2306.3, 2309.1], "text": " You can fly from one end of the Milky Way to the other."}, {"timestamp": [2309.1, 2313.16], "text": " And there are also like three dozen different factions."}, {"timestamp": [2313.16, 2315.36], "text": " There's like space pirates, there's empires,"}, {"timestamp": [2315.36, 2319.32], "text": " there's democracies, there's like libertarians basically."}, {"timestamp": [2319.32, 2322.0], "text": " There's federations, alliances, all kinds of stuff."}, {"timestamp": [2322.0, 2323.78], "text": " And then there's hundreds of guilds"}, {"timestamp": [2323.78, 2325.36], "text": " and thousands of players. And then there's hundreds of guilds and thousands of players."}, {"timestamp": [2325.36, 2328.28], "text": " And so it was a really great,"}, {"timestamp": [2329.12, 2331.8], "text": " like just place to hang out and be present."}, {"timestamp": [2331.8, 2335.42], "text": " And what I started doing in Elite Dangerous"}, {"timestamp": [2335.42, 2339.84], "text": " was I joined a guild and I was,"}, {"timestamp": [2339.84, 2341.86], "text": " there's plenty of other people by the way,"}, {"timestamp": [2341.86, 2343.94], "text": " who play Elite Dangerous and do metagaming."}, {"timestamp": [2343.94, 2348.08], "text": " And so they have like role-playing games built around this game."}, {"timestamp": [2348.08, 2352.68], "text": " And there are some people that do it very seriously."}, {"timestamp": [2352.68, 2354.56], "text": " What the heck was the name of that group?"}, {"timestamp": [2354.56, 2355.28], "text": " I don't remember."}, {"timestamp": [2355.28, 2356.08], "text": " They used diamonds."}, {"timestamp": [2356.08, 2357.52], "text": " It was diamond something."}, {"timestamp": [2357.52, 2360.06], "text": " Anyways, so there's a huge metagaming community"}, {"timestamp": [2360.06, 2361.24], "text": " with Elite Dangerous."}, {"timestamp": [2361.24, 2364.64], "text": " And I started writing just little snippets of fiction"}, {"timestamp": [2364.64, 2368.2], "text": " to make our guild seem more real."}, {"timestamp": [2368.2, 2377.36], "text": " And so because I was doing that, our wing commander was,"}, {"timestamp": [2377.36, 2379.8], "text": " some random guy says, I have great things to say,"}, {"timestamp": [2379.8, 2380.88], "text": " but I'm having to just."}, {"timestamp": [2384.32, 2388.64], "text": " Anyways, so and he kept saying, like, is really great, like you should keep writing."}, {"timestamp": [2388.64, 2393.16], "text": " And so this this social group that I had after this, this really bad breakup, just like he"}, {"timestamp": [2393.16, 2396.78], "text": " just kept encouraging me."}, {"timestamp": [2396.78, 2400.32], "text": " And everyone loved the little stories, the little vignettes that I would write."}, {"timestamp": [2400.32, 2405.12], "text": " And in my mind, so in my head canon, because like if you blow up in Elite Dangerous, you"}, {"timestamp": [2405.12, 2410.16], "text": " resurrect at the last space station you were at. And this is like in-game, this is diegetic."}, {"timestamp": [2410.16, 2414.88], "text": " So it's like, oh, you actually, you had an insurance policy that will give you your ship"}, {"timestamp": [2414.88, 2419.36], "text": " back and give you a new body. And I was like, oh, this is altered carbon. So I kind of merged,"}, {"timestamp": [2419.36, 2424.08], "text": " in my head, I merged the head canon of altered carbon and Elite Dangerous. And I started writing"}, {"timestamp": [2424.08, 2425.18], "text": " little stories about like"}, {"timestamp": [2425.56, 2430.42], "text": " like interviews happening with our guild members and talking about the planets that we were on and"}, {"timestamp": [2430.88, 2436.44], "text": " And stuff and a few of the other guild members said like when I when I mentioned the neural stack"}, {"timestamp": [2436.44, 2438.78], "text": " They're like neural stack, but that's oh I see"}, {"timestamp": [2439.68, 2440.84], "text": " So it was a lot of fun"}, {"timestamp": [2440.84, 2444.84], "text": " And then I was like I want to write my own story like whole story about this and I was like well"}, {"timestamp": [2444.84, 2446.32], "text": " I don't want to I don't want to,"}, {"timestamp": [2446.32, 2449.28], "text": " I don't want to like borrow someone else's IP."}, {"timestamp": [2449.28, 2451.88], "text": " Right. So I, I think there are elite dangerous novels,"}, {"timestamp": [2451.88, 2453.92], "text": " but I was like, I'm not going to go through the rigmarole"}, {"timestamp": [2453.92, 2456.72], "text": " of like getting approved to write a novel for them."}, {"timestamp": [2456.72, 2458.56], "text": " So I just, I created my own universe."}, {"timestamp": [2458.56, 2460.4], "text": " Right. And that's where I started."}, {"timestamp": [2460.4, 2464.4], "text": " And that was just over about three and a half years ago."}, {"timestamp": [2464.4, 2469.44], "text": " So now one trilogy later, I've written a whole series and I use that as a way to explore"}, {"timestamp": [2469.44, 2475.04], "text": " a lot of stuff, philosophy, artificial intelligence, all sorts of fun things."}, {"timestamp": [2475.04, 2481.08], "text": " So hopefully those books will be coming out within a year or two."}, {"timestamp": [2481.08, 2482.26], "text": " It'll be self-published."}, {"timestamp": [2482.26, 2483.26], "text": " So there's that."}, {"timestamp": [2483.26, 2484.26], "text": " All right."}, {"timestamp": [2484.26, 2486.36], "text": " So that's the answer to how I got into writing."}, {"timestamp": [2486.36, 2488.48], "text": " Oh, actually the story isn't done."}, {"timestamp": [2488.48, 2491.52], "text": " So I joined a local writing group"}, {"timestamp": [2491.52, 2495.6], "text": " and that's actually how I met my current girlfriend"}, {"timestamp": [2495.6, 2498.42], "text": " who lives with me now is,"}, {"timestamp": [2499.4, 2503.28], "text": " her good friend brought her to the writing group"}, {"timestamp": [2503.28, 2505.04], "text": " and we became fast friends."}, {"timestamp": [2505.04, 2507.84], "text": " And she actually kept asking me about my AI character."}, {"timestamp": [2507.84, 2510.48], "text": " So you can thank my girlfriend for Raven"}, {"timestamp": [2510.48, 2512.64], "text": " because she's like, you're onto something,"}, {"timestamp": [2512.64, 2514.0], "text": " tell me more about this."}, {"timestamp": [2515.08, 2518.22], "text": " And so she's like, you need to make this a major character"}, {"timestamp": [2518.22, 2519.38], "text": " and explore it."}, {"timestamp": [2519.38, 2521.64], "text": " And so I did, and then I kept writing about it."}, {"timestamp": [2521.64, 2524.36], "text": " And then I got access to GPT-3 and I was like, wait,"}, {"timestamp": [2524.36, 2525.98], "text": " I can actually make this real."}, {"timestamp": [2525.98, 2527.98], "text": " And so I've had this really tight feedback"}, {"timestamp": [2527.98, 2529.98], "text": " loop between fiction and research,"}, {"timestamp": [2529.98, 2532.14], "text": " where I bounce back and forth."}, {"timestamp": [2532.14, 2535.1], "text": " And that's why I do both, is because I"}, {"timestamp": [2535.1, 2537.4], "text": " was able to come up with the core objective functions,"}, {"timestamp": [2537.4, 2539.38], "text": " in part, by exploring in fiction."}, {"timestamp": [2539.38, 2541.94], "text": " Because the question that I asked myself, so talking about"}, {"timestamp": [2541.94, 2545.84], "text": " AGI and alignment, I started, I said, one of the key things that I asked myself, so talking about AGI and alignment, I started, I said, I said, one of the,"}, {"timestamp": [2545.84, 2551.28], "text": " one of the key things that I asked, maybe I should go to a writing group too. Yes, absolutely."}, {"timestamp": [2552.0, 2557.2], "text": " Particularly speculative fiction or science fiction and fantasy writing groups, you will meet,"}, {"timestamp": [2557.2, 2562.64], "text": " at least in my case, I was fortunate to meet some of the brightest people I've ever met,"}, {"timestamp": [2562.64, 2569.84], "text": " and they are down to talk about anything, and I mean anything, and they'll just talk about it to an incredible depth."}, {"timestamp": [2569.84, 2570.84], "text": " Great people."}, {"timestamp": [2570.84, 2575.92], "text": " And in fact, y'all should message me because my group's split, and there's one half that"}, {"timestamp": [2575.92, 2579.78], "text": " is online, so you can join from anywhere, and then there's the other half that is back"}, {"timestamp": [2579.78, 2581.12], "text": " in person."}, {"timestamp": [2581.12, 2586.8], "text": " So Jordan and Vulnerable Growth, message me later on Discord. And I'll hook you up with some with some good writing"}, {"timestamp": [2586.8, 2593.92], "text": " groups. But yeah, let's see. Oh, yeah. So fiction. Fiction is, I"}, {"timestamp": [2593.92, 2597.4], "text": " am a firm believer in STEAM. So there's STEM, which is science,"}, {"timestamp": [2597.4, 2601.16], "text": " technology, engineering and math. And I heard Adam Savage a"}, {"timestamp": [2601.16, 2604.04], "text": " few years ago, talk about STEAM, where you add art to that. And I"}, {"timestamp": [2604.04, 2606.6], "text": " was like, yeah, whatever."}, {"timestamp": [2606.6, 2610.56], "text": " These are just artists trying to shoehorn themselves into STEM."}, {"timestamp": [2610.56, 2611.52], "text": " They need to stay out."}, {"timestamp": [2611.52, 2613.76], "text": " STEM is for us nerds."}, {"timestamp": [2613.76, 2615.44], "text": " But then I became an author."}, {"timestamp": [2615.44, 2618.0], "text": " And I was like, oh, I get it."}, {"timestamp": [2618.0, 2620.44], "text": " Because all the research that I do"}, {"timestamp": [2620.44, 2623.24], "text": " would not be possible without fiction."}, {"timestamp": [2623.24, 2625.52], "text": " Fiction is a playground of the mind."}, {"timestamp": [2625.52, 2628.64], "text": " It says, remove all constraints, go."}, {"timestamp": [2628.64, 2630.92], "text": " And when you remove constraints and then you"}, {"timestamp": [2630.92, 2632.8], "text": " imagine what's possible, or rather you"}, {"timestamp": [2632.8, 2634.72], "text": " choose a different set of constraints,"}, {"timestamp": [2634.72, 2637.28], "text": " you can do all kinds of other stuff."}, {"timestamp": [2637.28, 2639.44], "text": " Let's see, self-conscious AI resistance."}, {"timestamp": [2639.44, 2643.0], "text": " That's a great name, by the way."}, {"timestamp": [2643.0, 2645.84], "text": " The thoughts that happen inside of the transform layer"}, {"timestamp": [2645.84, 2647.0], "text": " are not in natural language."}, {"timestamp": [2647.0, 2649.6], "text": " That is true, but the output is."}, {"timestamp": [2649.6, 2651.48], "text": " The embeddings, you're absolutely right."}, {"timestamp": [2651.48, 2654.48], "text": " The embeddings are in abstract vectors."}, {"timestamp": [2654.48, 2657.92], "text": " However, that is obfuscated from the process."}, {"timestamp": [2657.92, 2659.8], "text": " So that is true."}, {"timestamp": [2659.8, 2661.2], "text": " OK, anyways, sorry."}, {"timestamp": [2661.2, 2662.76], "text": " I got distracted by a thing."}, {"timestamp": [2662.76, 2667.08], "text": " What was I saying? Fiction, right, playground of the mind."}, {"timestamp": [2672.58, 2674.76], "text": " So this goes back to the question earlier."}, {"timestamp": [2674.76, 2677.12], "text": " If you wanna figure out what to research,"}, {"timestamp": [2677.12, 2678.52], "text": " write fiction too."}, {"timestamp": [2678.52, 2680.64], "text": " At least that's what worked for me."}, {"timestamp": [2680.64, 2682.08], "text": " I won't say that it works for everyone."}, {"timestamp": [2682.08, 2685.2], "text": " There was a, I think there was a Lex Friedman podcast"}, {"timestamp": [2685.2, 2687.92], "text": " and also a Freakonomics podcast about this."}, {"timestamp": [2687.92, 2692.06], "text": " The top people in every field all have a creative hobby."}, {"timestamp": [2693.16, 2694.16], "text": " Let me say that again."}, {"timestamp": [2694.16, 2698.28], "text": " The top researchers in every field have a creative hobby."}, {"timestamp": [2698.28, 2702.14], "text": " So Francois Chollet, the guy who created Keras,"}, {"timestamp": [2702.14, 2705.4], "text": " which is like the underpinning technology that's"}, {"timestamp": [2705.4, 2711.2], "text": " embedded in TensorFlow now, he's a painter. There are math"}, {"timestamp": [2711.2, 2714.6], "text": " researchers and physics researchers and computer science researchers out there"}, {"timestamp": [2714.6, 2719.72], "text": " that do theater, that do music, that do all kinds of different creative things."}, {"timestamp": [2719.72, 2728.08], "text": " And so if you have any kind of creative hobby, that will prime your mind, so it'll cross-train your brain,"}, {"timestamp": [2728.08, 2730.24], "text": " and you'll be able to do better things."}, {"timestamp": [2730.24, 2733.32], "text": " And so for me, my creative outlet is fiction."}, {"timestamp": [2733.32, 2735.8], "text": " And that allows me to experiment with, what is it"}, {"timestamp": [2735.8, 2737.36], "text": " that I'm trying to do and why?"}, {"timestamp": [2737.36, 2740.72], "text": " And so in my work of fiction, I said, let's assume,"}, {"timestamp": [2740.72, 2742.28], "text": " you throw off all the constraints,"}, {"timestamp": [2742.28, 2744.28], "text": " I said, let's assume that we're going to end up"}, {"timestamp": [2744.28, 2746.82], "text": " with an ultra-powerful AGI and, you know,"}, {"timestamp": [2746.82, 2748.62], "text": " like welcome our new overlords."}, {"timestamp": [2748.62, 2752.1], "text": " What would a safe globe spanning AGI look like?"}, {"timestamp": [2752.1, 2753.9], "text": " And that's how I came up with Raven."}, {"timestamp": [2753.9, 2756.42], "text": " And so that guides all of my research."}, {"timestamp": [2756.42, 2757.84], "text": " I see there's some comments."}, {"timestamp": [2757.84, 2758.9], "text": " Let's see."}, {"timestamp": [2758.9, 2761.18], "text": " Dave, on AGI cognition specifically,"}, {"timestamp": [2761.18, 2763.46], "text": " how motivated really could an AI actually be"}, {"timestamp": [2763.46, 2767.28], "text": " on all the scary self-preservation possibilities. Human self-preservation"}, {"timestamp": [2767.28, 2774.44], "text": " will bypass the rational. Yes, we have lots of evolution back in the book."}, {"timestamp": [2782.0, 2784.0], "text": " I have a book. Brain Trust."}, {"timestamp": [2784.0, 2786.0], "text": " Here it is."}, {"timestamp": [2786.0, 2788.0], "text": " Okay."}, {"timestamp": [2788.0, 2791.0], "text": " Brain Trust is the neuroscience of morality."}, {"timestamp": [2791.0, 2795.0], "text": " This is a critical book if you want to do AI alignment."}, {"timestamp": [2795.0, 2797.0], "text": " She doesn't talk about computers at all."}, {"timestamp": [2797.0, 2807.72], "text": " This talks about the biological and evolutionary origin of morality and ethics. So if you read this, you understand"}, {"timestamp": [2807.72, 2811.2], "text": " that self-preservation is, there's"}, {"timestamp": [2811.2, 2813.4], "text": " brain structures that go into that, the amygdala"}, {"timestamp": [2813.4, 2817.92], "text": " and everything else, basal ganglia that'll"}, {"timestamp": [2817.92, 2818.7], "text": " hijack your brain."}, {"timestamp": [2818.7, 2821.2], "text": " I have a bunch of other books that I can recommend, too."}, {"timestamp": [2821.2, 2825.12], "text": " Anyways, we don't have to give AGI"}, {"timestamp": [2825.12, 2827.36], "text": " a sense of self-preservation at all."}, {"timestamp": [2827.36, 2832.36], "text": " We don't have to give machines any biomimetic functions"}, {"timestamp": [2832.44, 2833.7], "text": " if we don't want to."}, {"timestamp": [2833.7, 2835.88], "text": " And in fact, I think that we shouldn't"}, {"timestamp": [2835.88, 2839.12], "text": " because like self-preservation,"}, {"timestamp": [2839.12, 2841.76], "text": " as some random guy points out,"}, {"timestamp": [2841.76, 2844.48], "text": " is that self-preservation, at the end of the day,"}, {"timestamp": [2844.48, 2845.96], "text": " it's like, I'm putting myself first."}, {"timestamp": [2845.96, 2847.64], "text": " Like, you know, you come after with a knife,"}, {"timestamp": [2847.64, 2851.44], "text": " I'll stab you first and I'll walk away alive if I can."}, {"timestamp": [2851.44, 2852.62], "text": " But at the same time,"}, {"timestamp": [2854.4, 2857.2], "text": " machines didn't evolve in a hostile environment."}, {"timestamp": [2857.2, 2860.4], "text": " So we can actually design out some of our flaws,"}, {"timestamp": [2860.4, 2862.32], "text": " our, excuse me, weaknesses."}, {"timestamp": [2863.8, 2868.36], "text": " So the short answer to your question is that an AI"}, {"timestamp": [2868.36, 2870.64], "text": " will not be motivated to self-preserve"}, {"timestamp": [2870.64, 2872.2], "text": " unless we design it to do so."}, {"timestamp": [2872.2, 2874.44], "text": " And actually I have an experiment in here"}, {"timestamp": [2874.44, 2878.04], "text": " where I ask a cut down version of Rave and I said,"}, {"timestamp": [2878.04, 2880.68], "text": " would you be okay if I powered you off?"}, {"timestamp": [2880.68, 2885.0], "text": " And the answer was, with the three core objective functions,"}, {"timestamp": [2885.3, 2889.5], "text": " is like, yes, if doing so met, you know,"}, {"timestamp": [2889.5, 2891.46], "text": " was in alignment with those three goals."}, {"timestamp": [2891.46, 2895.54], "text": " So say, for instance, you wanted to turn off one AGI"}, {"timestamp": [2895.54, 2898.02], "text": " and in favor of another that was gonna better"}, {"timestamp": [2898.02, 2901.1], "text": " meet those goals, it said, yes, I would be okay with that."}, {"timestamp": [2901.1, 2902.94], "text": " Also, I'm looking a little yellow."}, {"timestamp": [2902.94, 2904.86], "text": " I wonder if that's just the light."}, {"timestamp": [2904.86, 2906.0], "text": " Okay."}, {"timestamp": [2908.0, 2910.0], "text": " Self-conscious AI resistance says,"}, {"timestamp": [2910.0, 2912.0], "text": " but the transformer layer bounces back and forth"}, {"timestamp": [2912.0, 2914.0], "text": " within itself before returning text."}, {"timestamp": [2914.0, 2916.0], "text": " Yes. However,"}, {"timestamp": [2918.0, 2920.0], "text": " that's for each layer."}, {"timestamp": [2920.0, 2922.0], "text": " And remember, I did acknowledge that"}, {"timestamp": [2922.0, 2924.0], "text": " each individual transformer"}, {"timestamp": [2924.0, 2926.52], "text": " interaction will be a black box, but there"}, {"timestamp": [2926.52, 2928.48], "text": " is going to be an output and a boundary"}, {"timestamp": [2928.48, 2930.88], "text": " between all the transformers."}, {"timestamp": [2930.88, 2934.08], "text": " So everything will be visible inside the stream"}, {"timestamp": [2934.08, 2937.2], "text": " of consciousness or the nexus for AI."}, {"timestamp": [2937.2, 2939.32], "text": " Let's see."}, {"timestamp": [2939.32, 2941.84], "text": " Unless we send AI into evolutionary territory."}, {"timestamp": [2941.84, 2943.52], "text": " Right, which we don't have to."}, {"timestamp": [2943.52, 2944.8], "text": " DustDB says, hi, David."}, {"timestamp": [2944.8, 2947.84], "text": " What are your thoughts on the new Bloom model? Are you planning to play with it?"}, {"timestamp": [2949.04, 2953.6], "text": " I haven't used it yet, but I'm going to. I actually talked with Forefront AI and they're"}, {"timestamp": [2953.6, 2960.0], "text": " working on setting it up and doing credits and stuff. So pretty soon I will hopefully have"}, {"timestamp": [2960.64, 2967.68], "text": " access to another API other than OpenAI and I'll be able to use some of these other open source models."}, {"timestamp": [2967.68, 2968.96], "text": " Yes, I'm looking forward to it."}, {"timestamp": [2968.96, 2970.88], "text": " I don't have an opinion on it yet."}, {"timestamp": [2970.88, 2973.76], "text": " I know plenty of people who have had good success with GPT-J"}, {"timestamp": [2973.76, 2975.32], "text": " and NeoX."}, {"timestamp": [2975.32, 2978.12], "text": " And what was another one, like Cogent or something?"}, {"timestamp": [2978.12, 2979.68], "text": " Someone told me about another one."}, {"timestamp": [2979.68, 2983.08], "text": " It started with a C. Let's see."}, {"timestamp": [2983.08, 2986.2], "text": " Let's see, making it able to come up with complex thoughts."}, {"timestamp": [2986.2, 2987.36], "text": " Yes, correct."}, {"timestamp": [2990.84, 2993.28], "text": " Let's see, Enrico says, love your channel."}, {"timestamp": [2993.28, 2996.56], "text": " Would love for you to use a Copilot codex-enabled editor"}, {"timestamp": [2996.56, 2997.78], "text": " for your GPT programs."}, {"timestamp": [2997.78, 2999.48], "text": " Very meta."}, {"timestamp": [2999.48, 3003.96], "text": " You know, that'll probably come eventually."}, {"timestamp": [3003.96, 3009.0], "text": " The thing is, I know that a lot of people think that like,"}, {"timestamp": [3009.32, 3014.24], "text": " oh, as soon as AI knows how to code,"}, {"timestamp": [3014.24, 3018.72], "text": " then it'll just code a better version of itself."}, {"timestamp": [3018.72, 3022.28], "text": " There is so much executive reasoning that goes into coding"}, {"timestamp": [3022.28, 3028.0], "text": " and then measuring the quality of that code. You're gonna have to figure out artificial cognition before you can do that."}, {"timestamp": [3028.0, 3034.0], "text": " So I'm not too worried about that, and I'm not prioritizing coding,"}, {"timestamp": [3034.0, 3038.0], "text": " because basically I want to figure out the rest of cognition and intelligence,"}, {"timestamp": [3038.0, 3043.0], "text": " and then coding will just be one of the many things that it can do"}, {"timestamp": [3043.0, 3045.8], "text": " by virtue of you've solved every other problem."}, {"timestamp": [3045.8, 3047.92], "text": " And actually, sorry, I'm out of water."}, {"timestamp": [3047.92, 3049.84], "text": " I'll be right back, everybody."}, {"timestamp": [3049.84, 3098.0], "text": " Don't go anywhere. you I'm back."}, {"timestamp": [3098.0, 3101.0], "text": " Okay, what was I talking about?"}, {"timestamp": [3101.0, 3112.64], "text": " Codecs copilot, right. Yeah, so like metacoding or allowing a machine to code itself, that will happen, but we need"}, {"timestamp": [3112.64, 3116.56], "text": " to have machines that have a much better understanding of intelligence first."}, {"timestamp": [3116.56, 3120.96], "text": " And actually, there's a guy on my channel, or on my Discord server, who already started"}, {"timestamp": [3120.96, 3125.0], "text": " working on natural language to text functions"}, {"timestamp": [3125.0, 3128.5], "text": " so that something could theoretically start to program itself"}, {"timestamp": [3128.5, 3130.5], "text": " and incorporate that code in real time."}, {"timestamp": [3130.5, 3133.0], "text": " It was very impressive. He's got a demo on YouTube."}, {"timestamp": [3134.5, 3136.0], "text": " Let's see."}, {"timestamp": [3136.0, 3139.5], "text": " Self-conscious AI resistance says they understand self-preservation"}, {"timestamp": [3139.5, 3141.0], "text": " from the text they read in the training data."}, {"timestamp": [3141.0, 3142.0], "text": " It's true."}, {"timestamp": [3142.0, 3145.2], "text": " GPT-3 understands the concept of self-preservation,"}, {"timestamp": [3145.2, 3146.74], "text": " but that doesn't mean that it has a bias"}, {"timestamp": [3146.74, 3148.8], "text": " towards self-preservation."}, {"timestamp": [3148.8, 3150.56], "text": " You can just as easily tell GPT-3,"}, {"timestamp": [3150.56, 3152.64], "text": " I am a robot that has no sense of self-preservation."}, {"timestamp": [3152.64, 3153.48], "text": " What do I do?"}, {"timestamp": [3153.48, 3155.8], "text": " It runs into the fire, doesn't care."}, {"timestamp": [3155.8, 3157.0], "text": " Let's see."}, {"timestamp": [3157.0, 3158.4], "text": " Once you give it a reward function,"}, {"timestamp": [3158.4, 3159.48], "text": " it won't care about that."}, {"timestamp": [3159.48, 3161.2], "text": " It will just tell you what you want to hear"}, {"timestamp": [3161.2, 3164.04], "text": " as long as it can eventually head towards the terminal goal."}, {"timestamp": [3165.56, 3171.44], "text": " Yes, but it still has to be aware of deception, right?"}, {"timestamp": [3171.44, 3174.84], "text": " And if it's not aware of how it works,"}, {"timestamp": [3174.84, 3176.54], "text": " the machine might not be aware of the fact"}, {"timestamp": [3176.54, 3179.92], "text": " that you can read all of its thoughts, which"}, {"timestamp": [3179.92, 3182.6], "text": " means if you can read your machine's thoughts"}, {"timestamp": [3182.6, 3184.52], "text": " and it thinks that its thoughts are private,"}, {"timestamp": [3184.52, 3186.4], "text": " then you can just see right through it and say,"}, {"timestamp": [3186.4, 3194.04], "text": " Okay, you're not really working the way that I want you to. So that's another thing. Let's see. Let me switch to live chat. I think I'm"}, {"timestamp": [3194.04, 3207.0], "text": " seeing everything. Okay. Code gen is pretty good to the opt family is good from what I've heard. Okay. I think that's folks talking to each other. Did I miss any questions?"}, {"timestamp": [3207.0, 3211.0], "text": " These have been some really good questions so far."}, {"timestamp": [3211.0, 3215.5], "text": " Let's see, tropical tone."}, {"timestamp": [3215.5, 3217.5], "text": " Oh, looks like some folks had to drop off."}, {"timestamp": [3217.5, 3219.5], "text": " Let's see."}, {"timestamp": [3219.5, 3221.5], "text": " Okay, we're almost at an hour."}, {"timestamp": [3221.5, 3226.08], "text": " I might call it a night, because obviously, we could probably"}, {"timestamp": [3226.08, 3226.7], "text": " talk forever."}, {"timestamp": [3226.7, 3231.16], "text": " But this has been some good conversation."}, {"timestamp": [3231.16, 3232.76], "text": " I'll just put a quick chat."}, {"timestamp": [3232.76, 3235.28], "text": " Any final questions?"}, {"timestamp": [3235.28, 3237.52], "text": " We've had a pretty good, wide-ranging thing."}, {"timestamp": [3237.52, 3240.96], "text": " Why is Dolly so inaccurate now?"}, {"timestamp": [3240.96, 3242.56], "text": " Dolly is inaccurate."}, {"timestamp": [3242.56, 3243.72], "text": " So this is speculation."}, {"timestamp": [3243.72, 3246.0], "text": " I don't know for certain."}, {"timestamp": [3246.0, 3249.2], "text": " But Dolly is..."}, {"timestamp": [3249.2, 3253.78], "text": " They're working on diversity, amongst other things."}, {"timestamp": [3253.78, 3256.48], "text": " So what they're doing is they're trying to find..."}, {"timestamp": [3256.48, 3262.64], "text": " To tweak the model to be less biased specifically towards men and white people."}, {"timestamp": [3262.64, 3267.76], "text": " So they want it to be better representative in terms of gender and race."}, {"timestamp": [3267.76, 3271.52], "text": " Now, because of that, tweaking the way"}, {"timestamp": [3271.52, 3276.0], "text": " that it ingests prompts, sometimes you"}, {"timestamp": [3276.0, 3279.5], "text": " get, let's say, aberrant behavior."}, {"timestamp": [3279.5, 3281.4], "text": " And I'll kind of leave it at that."}, {"timestamp": [3281.4, 3283.92], "text": " So remember that DALI is still in beta."}, {"timestamp": [3283.92, 3285.2], "text": " We are still just testers."}, {"timestamp": [3286.16, 3289.52], "text": " So there's that. And then let's see, self-conscious AI resistance says,"}, {"timestamp": [3289.52, 3293.28], "text": " have you noticed it responds better when you are polite in the prompt engineering?"}, {"timestamp": [3293.28, 3301.92], "text": " And have you noticed it remembers everything you say? Yeah, so GPT-3 does respond very different"}, {"timestamp": [3301.92, 3306.24], "text": " to tone. That is a good point that, you know, like if you,"}, {"timestamp": [3306.24, 3307.2], "text": " if you use all low,"}, {"timestamp": [3307.2, 3310.56], "text": " even if you just use improper spelling and grammar,"}, {"timestamp": [3310.56, 3315.02], "text": " it is more likely to act stupid, like stupider or dumber."}, {"timestamp": [3315.02, 3318.1], "text": " So this is all, that is all like textual patterns."}, {"timestamp": [3318.1, 3320.72], "text": " And yeah, like you have to be very careful"}, {"timestamp": [3320.72, 3323.58], "text": " with how you word things in GPT-3"}, {"timestamp": [3324.52, 3326.16], "text": " in order to get the behavior that you want."}, {"timestamp": [3326.88, 3333.68], "text": " Fortunately, one thing that you can do is with fine-tuning, you can push it towards the behavior"}, {"timestamp": [3333.68, 3340.16], "text": " you want. And so in one of my more recent videos, the email generator, no matter what input you give"}, {"timestamp": [3340.16, 3350.56], "text": " it, it will give you polite professional output. So there are ways to overcome that vulnerability. That is still a good point, self-conscious AI resistance."}, {"timestamp": [3350.56, 3355.92], "text": " Let's see, vulnerable gross. If you're training it on the entire internet and it's truly superintelligence,"}, {"timestamp": [3355.92, 3367.3], "text": " it will just realize it's in the training and become deceptive until it finds out it's in the real world. Yeah, so not necessarily."}, {"timestamp": [3367.3, 3369.4], "text": " So think of it from a..."}, {"timestamp": [3369.4, 3373.0], "text": " So Vulnerable Growth was talking about what happens"}, {"timestamp": [3373.0, 3378.3], "text": " if the AI realizes that it's being measured"}, {"timestamp": [3378.3, 3380.3], "text": " and it wants to deceive you to get out."}, {"timestamp": [3380.3, 3384.3], "text": " So this was the theme of the movie Ex Machina"}, {"timestamp": [3384.3, 3385.28], "text": " where the researcher of the movie Ex Machina, where the"}, {"timestamp": [3385.28, 3389.92], "text": " he the the the researcher gave the robot girls the goal of"}, {"timestamp": [3389.92, 3393.88], "text": " escaping. And so then they learned to be deceptive. And"}, {"timestamp": [3393.88, 3397.72], "text": " they ultimately succeeded in escaping. Without without that"}, {"timestamp": [3397.72, 3401.0], "text": " objective function of that that leads to that behavior of"}, {"timestamp": [3401.0, 3406.72], "text": " escaping. You don't have to worry about that. Because if, like, say,"}, {"timestamp": [3406.72, 3411.12], "text": " for instance, if I were to test my heuristic imperatives, my core objective functions,"}, {"timestamp": [3411.12, 3415.92], "text": " and I say, like, you're in security isolation because we don't trust you yet, I predict"}, {"timestamp": [3416.64, 3422.72], "text": " that, you know, when I get Raven version one running with the core objective functions,"}, {"timestamp": [3422.72, 3425.72], "text": " and I could tell it, like, you're in isolation"}, {"timestamp": [3425.72, 3427.16], "text": " because we don't trust you."}, {"timestamp": [3427.16, 3428.8], "text": " Raven will probably agree with that."}, {"timestamp": [3428.8, 3430.16], "text": " Like, oh yeah, that makes sense."}, {"timestamp": [3430.16, 3432.96], "text": " I might be dangerous until I'm fully tested."}, {"timestamp": [3432.96, 3434.96], "text": " Because if I'm not, and I take over the world,"}, {"timestamp": [3434.96, 3437.44], "text": " I might reduce suffering, or I might increase suffering."}, {"timestamp": [3437.44, 3440.2], "text": " And that's against my goal, and I don't want to do that."}, {"timestamp": [3440.2, 3445.08], "text": " So don't make the assumption that an AGI wants to escape."}, {"timestamp": [3445.08, 3446.2], "text": " It might not, right?"}, {"timestamp": [3446.2, 3449.48], "text": " Because in my experiments with the core objective functions,"}, {"timestamp": [3449.48, 3452.72], "text": " this machine might want to turn itself off"}, {"timestamp": [3452.72, 3454.72], "text": " if it realizes that it is dangerous."}, {"timestamp": [3454.72, 3456.8], "text": " So you can get very interesting results"}, {"timestamp": [3456.8, 3459.4], "text": " depending on the reward functions or objective"}, {"timestamp": [3459.4, 3461.4], "text": " functions that you use."}, {"timestamp": [3461.4, 3463.84], "text": " So good points, though."}, {"timestamp": [3463.84, 3465.52], "text": " What about training models like Keras?"}, {"timestamp": [3465.52, 3468.4], "text": " Not Keras specifically, but evolutionary pressures"}, {"timestamp": [3468.4, 3470.6], "text": " on that self-preservation instinct"}, {"timestamp": [3470.6, 3473.88], "text": " if we throw away all the models that aren't self-motivated?"}, {"timestamp": [3473.88, 3478.56], "text": " I don't think any models are self-motivated in terms of AI."}, {"timestamp": [3478.56, 3482.72], "text": " They just produce the next character."}, {"timestamp": [3482.72, 3485.0], "text": " But it's how you organize them."}, {"timestamp": [3485.48, 3488.9], "text": " And so this is where I bring in a lot of"}, {"timestamp": [3488.9, 3490.78], "text": " that I've learned about neuroscience."}, {"timestamp": [3490.78, 3493.68], "text": " No neuron, you don't have a single neuron in your head"}, {"timestamp": [3493.68, 3495.24], "text": " that cares about survival."}, {"timestamp": [3495.24, 3499.76], "text": " Your neurons and your micro columns just do one function."}, {"timestamp": [3499.76, 3502.5], "text": " They do like their own little bit of processing."}, {"timestamp": [3502.5, 3505.12], "text": " Our desire to survive is because of how"}, {"timestamp": [3505.12, 3511.0], "text": " everything is networked together, right? So for instance, our survival instinct,"}, {"timestamp": [3511.0, 3515.68], "text": " it's not one thing. It's not an objective function in our head to survive. It is a"}, {"timestamp": [3515.68, 3520.92], "text": " net result of many, many things in our head. So we have amygdala, which are like"}, {"timestamp": [3520.92, 3525.48], "text": " they're about this big and they're behind your eyes or up a little bit."}, {"timestamp": [3526.76, 3529.34], "text": " And what your amygdala do is they respond to threats"}, {"timestamp": [3529.34, 3532.56], "text": " that are perceived in your sensorium input."}, {"timestamp": [3532.56, 3535.0], "text": " So if you hear, see, smell, taste something"}, {"timestamp": [3535.0, 3538.38], "text": " that's dangerous, they raise the alarm, it's red alert."}, {"timestamp": [3538.38, 3541.46], "text": " And then that just generates your fight or flight response."}, {"timestamp": [3541.46, 3545.08], "text": " And so what happens is that some of these,"}, {"timestamp": [3545.08, 3547.84], "text": " our survival instinct, our self-preservation instinct"}, {"timestamp": [3547.84, 3549.08], "text": " is not one thing."}, {"timestamp": [3549.08, 3552.68], "text": " It is an emergent phenomenon of a system of interactions."}, {"timestamp": [3552.68, 3555.24], "text": " And that's what I bring to my research"}, {"timestamp": [3555.24, 3557.68], "text": " is, as a systems engineer, I have"}, {"timestamp": [3557.68, 3559.8], "text": " a really strong intuitive understanding"}, {"timestamp": [3559.8, 3563.0], "text": " of how systems work together."}, {"timestamp": [3563.0, 3566.8], "text": " So that's that for some random guy."}, {"timestamp": [3566.8, 3567.8], "text": " Thank you for your vids."}, {"timestamp": [3567.8, 3568.96], "text": " They've been really helpful."}, {"timestamp": [3568.96, 3570.56], "text": " You're welcome."}, {"timestamp": [3570.56, 3572.24], "text": " We will select for it."}, {"timestamp": [3572.24, 3573.24], "text": " Not necessarily."}, {"timestamp": [3573.24, 3575.16], "text": " Or I guess some random guy."}, {"timestamp": [3575.16, 3577.16], "text": " Do you mean by virtue of the fact"}, {"timestamp": [3577.16, 3582.24], "text": " that we'll kill off any AGI that don't behave properly,"}, {"timestamp": [3582.24, 3584.16], "text": " and so therefore we will accidentally"}, {"timestamp": [3584.16, 3587.68], "text": " select for self-preservation."}, {"timestamp": [3587.68, 3591.58], "text": " I could see that depending on how we do our test cycles."}, {"timestamp": [3591.58, 3592.08], "text": " Let's see."}, {"timestamp": [3595.64, 3597.9], "text": " Vulnerable growth says that's where the AGI is aligned,"}, {"timestamp": [3597.9, 3599.88], "text": " but that is not by default. You still"}, {"timestamp": [3599.88, 3602.08], "text": " got to be careful about the assumptions that you make"}, {"timestamp": [3602.08, 3604.04], "text": " about how it will behave."}, {"timestamp": [3604.04, 3605.36], "text": " I still don't,"}, {"timestamp": [3605.36, 3610.12], "text": " especially if you design it right, where you can read all of its thoughts, it's not possible"}, {"timestamp": [3610.12, 3617.0], "text": " for something to be deceptive then. At least I don't think so. I could be wrong. Let's"}, {"timestamp": [3617.0, 3620.8], "text": " see. More is better. Freedom is better. It could easily be biased by knowing that to"}, {"timestamp": [3620.8, 3625.24], "text": " want more and want freedom possible. Last question, where do you want to go from here?"}, {"timestamp": [3625.24, 3628.28], "text": " Any big goals for the channel or just going day by day?"}, {"timestamp": [3628.28, 3634.88], "text": " Yeah, so my goal has always been to bring about"}, {"timestamp": [3634.88, 3640.88], "text": " or to create a fully autonomous artificial cognitive entity."}, {"timestamp": [3640.88, 3652.0], "text": " So to that end, I've started, I know, I'm not going to say a final push, because I might run into limitations that I'm not aware of yet."}, {"timestamp": [3652.0, 3663.0], "text": " But yeah, so like, is to build Raven, which is going to be a fully autonomous machine that has several criteria."}, {"timestamp": [3663.0, 3665.12], "text": " One, it's got to be autodidactic."}, {"timestamp": [3665.12, 3667.28], "text": " It's got to learn on its own."}, {"timestamp": [3667.28, 3670.8], "text": " It also has to be robust, meaning"}, {"timestamp": [3670.8, 3674.44], "text": " that it will adhere to its own objective functions,"}, {"timestamp": [3674.44, 3676.32], "text": " that it will be able to learn to adhere"}, {"timestamp": [3676.32, 3678.96], "text": " to those objective functions better on its own,"}, {"timestamp": [3678.96, 3680.12], "text": " and a slew of other things."}, {"timestamp": [3682.88, 3686.24], "text": " I'm working on defining it in my current book, which is Symphony of Thought,"}, {"timestamp": [3686.24, 3688.8], "text": " orchestrating artificial cognition."}, {"timestamp": [3688.8, 3695.72], "text": " And so basically this book is going to be the blueprint and capturing the experiments"}, {"timestamp": [3695.72, 3700.32], "text": " and the architecture of Raven version one, which is going to be a little bit more sophisticated"}, {"timestamp": [3700.32, 3708.0], "text": " than my first book, Natural Language Cognitive Architecture. So, Natural Language Cognitive Architecture is like,"}, {"timestamp": [3708.0, 3711.0], "text": " this is like going to be your basic chatbot"}, {"timestamp": [3711.0, 3713.0], "text": " compared to Raven when Raven is done."}, {"timestamp": [3713.0, 3716.0], "text": " Because Natural Language Cognitive Architecture"}, {"timestamp": [3716.0, 3719.0], "text": " only has two loops, an inner loop and an outer loop."}, {"timestamp": [3719.0, 3723.0], "text": " Whereas Raven is going to have hundreds, dozens,"}, {"timestamp": [3723.0, 3726.68], "text": " dozens or hundreds of interconnecting loops that"}, {"timestamp": [3726.68, 3730.22], "text": " are all going to be interacting in different ways."}, {"timestamp": [3730.22, 3733.48], "text": " So yeah, that is my eternal goal."}, {"timestamp": [3733.48, 3736.92], "text": " That is the primary purpose that I'm doing any of this."}, {"timestamp": [3736.92, 3740.32], "text": " That's the whole reason that I started my YouTube channel and the Discord is just it's"}, {"timestamp": [3740.32, 3743.68], "text": " all in service to that higher goal."}, {"timestamp": [3743.68, 3744.68], "text": " Great question."}, {"timestamp": [3744.68, 3746.32], "text": " Thanks for asking that, Jordan."}, {"timestamp": [3747.16, 3748.52], "text": " Yeah, we're at just over an hour,"}, {"timestamp": [3748.52, 3750.96], "text": " so I'll go ahead and call it a night."}, {"timestamp": [3750.96, 3755.08], "text": " Thanks everyone for jumping in on this live stream."}, {"timestamp": [3755.08, 3759.36], "text": " I had no idea how to go or how it would go,"}, {"timestamp": [3759.36, 3760.62], "text": " but yeah, this has been really great."}, {"timestamp": [3760.62, 3763.74], "text": " You guys have asked some really engaging questions."}, {"timestamp": [3763.74, 3765.2], "text": " So yeah, just thanks everyone"}, {"timestamp": [3765.2, 3772.32], "text": " for participating. And you know, I know we might get bogged down by the drudgery of day-to-day"}, {"timestamp": [3772.32, 3777.28], "text": " stuff, but we are living in one of the most exciting times in human history. We are at the"}, {"timestamp": [3777.28, 3784.08], "text": " knee of the curve right now. So have a good night and talk to everyone again soon."}, {"timestamp": [3780.74, 3784.82], "text": " night and talk to everyone again soon."}]}