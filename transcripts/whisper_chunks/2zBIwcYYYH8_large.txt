{"text": " Morning everybody, David Shapiro here with another video. I wasn't really planning on making this video, but I realize that things are accelerating and there is a sense of urgency. So before we get started, I just want to say that today's video is sponsored by all of you. My Patreon supporters make my continuous work possible. So if you want to continue to incentivize this behavior, consider jumping over on Patreon. And if you sign up for the higher tiers, you know, I'm willing to chat with you and even jump on Zoom calls once or twice a month at the higher tiers just in order to talk about whatever you want to talk about. Some people ask me about, you know, what's the current about. Some people ask me about what's the current news. Some people ask for help with prompt engineering, all kinds of stuff. I've even had people ask me just about how do I adapt to this changing landscape. Obviously, I'm not a therapist, but I can at least share my perspective on this stuff. Okay, so without further ado, let's jump in. If you go to GitHub slash trending, you'll see a couple of very interesting patterns. The top four trending repositories right now all have to do with large language models. And then you go down a little bit further and there's even more generative AI. So there's a code translator, there's Moki diffusion, Lama. So obviously we are in an inflection point and today we're going to talk about, amongst other things, fully autonomous AI. So if you're not aware AutoGPT is all the rage right now. Everyone is talking about it. Everyone is using it and adapting it and the the TLDR is this is The first production like fully fledged cognitive architecture. There's plenty of other people working on very similar stuff But the advent of GPT-4 As well as all the other work that people are doing Basically means that cognitive architecture is here. Fully autonomous AI is here. Now the question is only, what is it capable of? What are its limitations? And how much does it cost to run? I'm not gonna do a full demo of this, but you just Google it or search YouTube for AutoGPT, you will see that there are demos out there already. This can do any number of things. So this is why there's a sense of urgency because once you have an autonomous AI, this one is semi-autonomous, it is gated so that it asks the user for permission, but it's only a very small step to go from here to fully autonomous, which is why I do my work with the heuristic imperatives. And we'll talk about alignment once we get a little bit further into the video, because there's quite a few papers out there that talk about alignment. And I want to show you that my work is not quite so eccentric, that there are people in the establishment talking in this direction. I just happen to be the first one to propose a comprehensive solution that I can also demonstrate. So yeah, AutoGPT is out. It's only going to get faster, more powerful, and better as new models come out and as open source models that are distilled and quantized come out, and we'll talk about those in just a minute. Microsoft is doing Jarvis, which Jarvis, if you're not familiar with the character, was voiced by Paul Bettany in Iron Man and the MCU. And this has some other similar fully autonomous capabilities that they're working on. Task planning, model selection, task execution, and response generation. Again, this is a cognitive architecture. And the fact that it's being sponsored and run by Microsoft, that tells you the direction that the industry is going. Now, one thing here is that model selection. So what this implies is that depending on the level of sophistication of a task or how difficult it is, it's gonna be able to choose different models. Now, during a Discord call that I had with the Cognitive AI Lab Discord, which if you wanna join, link is in the description, we were talking about how important it will be to choose models because the lightest weight models are literally thousands of times cheaper and smaller than the largest models. And so humans, we do this too, where we rely on intuition and habit, and we only engage our executive function if something is really hard and our first attempts fail. Excuse me. And so if cognitive architectures go the same way, you're going to be able to run most of it locally, and then of course as large language models become more quantized, more efficient, and as the hardware in our laptops, phones and desktops become more powerful, eventually, before too long, we're going to be able to run something equal to GPT-4 and better locally. So we are now entering, as of March and April 2023, entering the era of fully autonomous AI, which is a much more useful term than AGI, because AGI is just an arbitrary thing. This is autonomous now, the only question is again, how smart is it? How fast is it? What is it capable of? And what is it not capable of yet? So those are the two big repos that I wanted to point out and they're both the top of trending. So that tells you that they are getting the most attention right now. So if you want to jump into the conversation, now's the time. Okay, so moving right along, if you want something that's a little bit more practical and hands-on, one of my Patreon supporters told me about Jaseki, which Jaseki is basically DevOps but for AI and language models. So it gives you an end-to-end pipeline to create basically cognitive architectures. It includes all kinds of tools and APIs and it does take a while to get familiar with if you're not already familiar with it, but when you look at the fact that it can automatically generate APIs and you plug this into the AI and the AI can design itself and redesign its own infrastructure and say, hey, I need an API that does this. Let's go design that microservice. This kind of platform is probably going to be pretty important for building, not just autonomous bots like this, but fully fledged corporate business platforms. And so what I mean by that is, okay, you might be thinking, great, you can have auto GPT, which can write Twitter and emails for you. But if you're thinking about this from an enterprise perspective, from a DevOps perspective, it can plug into your cybersecurity suites and monitor that. It can monitor your ticket queues. It can talk to your marketing team. So one example that I thought of was like, okay, let's say you set up a marketing brain and then it plugs into your Slack or Teams and then you have a marketing bot or actually multiple marketing bots that you can talk to that they're going to go out and do research on the Internet, look at your competitors, watch videos, generate images, market test stuff, and basically your marketing team will just be driving the behavior of the bots saying like, hey, let know, hey, let's do this. And then it'll go and do the tasks and kind of report back. And this might sound like science fiction, but this is actually what's happening right now. This is what people are actually working on right now. I am no longer the crazy person shouting into the void saying this is coming, because now it has arrived. Okay, and this one is actually alignment alignment so let me move that down further. But yeah so Jaseki, it's jaseki.org. Check that out. This is another platform. So one thing that these interoperable platforms offer that perhaps the AutoGPT and Jarvis don't is it's a paradigm of okay let's think of Jarvis and AutoGPT and Jarvis don't, is it's a paradigm of, okay, let's think of Jarvis and AutoGPT as self-contained agents that have extensibility and have their own tools. Whereas a platform like Jaseki says, let's embed this in an organization and it'll be part of a pipeline or a broader ecosystem. So it's basically, is it centralized or decentralized? And both are coming. Mark my words, both kinds of autonomous AI is coming. I'm working on, another one of my Patreon supporters reached out to me with an idea of kind of a hive mind. How do you organize an arbitrary number of bots that have different programs? Well, you create an API and you create a discussion space for those bots. So we're working on hammering that out. And yeah, like this is, we're entering into a wild time. Okay, so I've talked about efficiency and some of the other things that are coming, such as quantization, and we're going to start talking about those now. So some of you have seen this post where basically window size is the biggest limitation right now, but what if we come up with a different architecture like an RNN or LSTN or bring back some other kinds of architectures that allow you to have essentially an unlimited window, an infinite window. So that's one thing that's coming. We don't know, you don't need to see those ads, so that's one idea that's coming. We'll see know, you don't need to see those ads. So that's that's one idea that's coming. We'll see if it pans out. I suspect that you're gonna get diminishing returns with the more that it reads because other models like Google's universal sentence encoder, that can read an infinite amount already but you get what's called dilution where the, or semantic dilution, where the longer, excuse me, I have allergies, I apologize, where the longer the text that it reads, the more generic, the more dilute the vector, the embedding becomes. So like if you read an infinitely long, any like arbitrarily long text, the embedding, the vector is gonna trend towards kind of a meaningless middle ground. They might come up with ways around that, but basically you're compressing an arbitrary amount of text into a fixed width vector, so you're going to lose some information, at least until the math changes the way that it's represented. Now, that being said, DaVinci had a 12,000 dimension embedding. I'm sure GPT-4 has a much larger one. These are not very large matrices. We could go up to very, very large matrices. That space is still being explored, because, OK, 12,000 dimensions. What if in a year or two, we have 12 million dimension embeddings? That's a lot more information and a lot more nuance that you can record. Okay, so I mentioned quantization. So the LLAMA C++, these things are getting down to like crazy small. Right, a 30 billion parameter model only needs six gig of RAM, right? Okay, that can run on commodity hardware. So all the little nifty tricks and stuff that people are finding, whether it's distillation, quantization, and so on, running with low precision, you know, int8 instead of floating point 32, all kinds of stuff is being discovered. And so what, one of the trends that we're seeing is that when you look at the fact that AutoGPT and Jarvis will have model selection, probably what's gonna happen is you're gonna have dedicated models that are cognitive units that are good at working on specific kinds of tasks. Right, so when you break it down into several cognitive behaviors such as in this case task planning, model selection, and task execution, you can have smaller models that are purpose-built for those particular things. And this actually goes to my work on the heuristic imperatives, which was an attempt to fine-tune and distill that function so that you can have a moral module, a moral framework that will just give you a really quick response of, okay, this is how you reduce the suffering in this situation, this is how you increase prosperity and increase understanding in this situation, and then you can also use that same model to self-evaluate in past behaviors which can then be used for reinforcement learning in the future and then that model can improve itself through self-labeling data which we will get to because there are papers out there for that topic now. Anyways, point being is I just wanted to share all of that. Another interesting thing that popped up on my feed, drug discovery is accelerating because of this. All of this generative stuff, this goes back to AlphaFold and all the downstream technologies. So we are rapidly approaching kind of the snowball effect. And actually Stanford had a paper that was just published. Let me show you on, I posted it here on my community. So the Stanford paper, page not found. Well, darn. Okay, anyways, it's the Stanford AI Index. I guess the link broke or they took it down or something. But anyways, they point out that AI is actually one of the biggest contributors to science as of 2022. So, we're at a tipping point where AI is already taking over a tremendous amount of the cognitive load of research and it's accelerating. So in my previous videos where I talked about the singularity and stuff and I talked about job displacement and basically unlimited cognitive labor, we are already seeing the removal of the human brain's limitations in terms of advancing science. Okay, so then that's great, that's all data and text. So what happens when these models get into the real world? So maybe you missed this, but Facebook is working on robots. And these are robots that can watch and observe humans and then copy their behavior. Yeah, so that's coming. And then, I don't know if you also saw it, but Tesla had a demonstration of their Optimus Prime model or whatever they called it. I think it's just the Optimus bot, but it was able to do some pretty good manual dexterity stuff. Yeah, so fully autonomous robots are also coming hot on the heels of fully autonomous agents. So this is all coming, it's much much closer. One thing that was kind of funny is of course it was Italy. Italy banned chat GPT. They didn't fully ban it, they gave open AI 20 days to respond. Who knows what will happen, but they did say that that chat GPT runs afoul of GDPR. Probably, who knows, we'll see how that plays out. Immediately after I published a video last week, someone pointed out that the UK actually has the the world's first somewhat comprehensive framework about how to approach AI. You know, safety, security, and robustness, transparency, and explainability, fairness, security, and robustness, transparency, and explainability, fairness, accountability, and governments, and contestability, and redress. Okay, great. I don't know how that's going to be enforceable. I personally don't think it is, especially now that the genie is out of the bottle, which is why I do my alignment research. And so my goal is to encourage everyone and convince everyone that giving your autonomous robots and your autonomous AI agents my heuristic comparatives is the best way to enter into a positive beneficial Nash equilibrium where basically if everyone knows that everyone else is using the heuristic comparatives then nobody will change their their strategy nobody will change their behavior and that this will create a more utopic attractor state. I have another video that I'm working on talking about the path to utopia and the will change their behavior and that this will create a more utopic attractor state. I have another video that I'm working on talking about the path to utopia and the singularity attractor state. So look for that coming out in the coming days. But yeah, so this white paper, I looked at it. It's pretty dry. This little blog post that the UK published is pretty, you know, it's all good in theory. We have no idea how well they're gonna execute it. Okay, so another thing is because of open AI surging ahead, because of Microsoft surging ahead, and a lot of this work becoming sequestered, you know, Google is doing their own stuff, Nvidia is doing their own stuff with Nemo, Google is doing their own stuff, NVIDIA is doing their own stuff with Nemo, China is doing their own stuff. There is an idea of basically creating a CERN-like entity for the creation of large-scale AI so that it will be intrinsically open source so that we all get access to the most powerful models. I don't know if this is going to be necessary, but I'm glad that this this petition exists. You see it's only got 13,000 signatures out of 10,000. So my videos regularly get 30 to 50 thousand views, so if you could, like, if you take a look at this and jump over and sign it if you want, I think it's a good idea and I think it's worth exploring. And it's sponsored by Leon, so the Large-Scale Artificial Intelligence Open Network. I personally think that this would be a good direction to go. So yeah, let's take a look at it. Obviously, I can't tell you what to do, but now you know. All right, so then there's this paper that came out. Obviously I can't tell you what to do, but now you know. Alright, so then there's this paper that came out. So I was talking about... so this, the rest the video is basically going to be about alignment. And so in this case, this paper again relatively dry, but it talks about using you know, while many models are tested with reinforcement learning with human feedback, what if you give it then the instruction to morally self-correct? And so in this case, it was published by Anthropic. So they are proving that models can self-correct if given the correct instructions, which is where my heuristic comparatives come in. So in this case, they try and reduce harm, which harm reduction is actually a well-established model in public health. I know I said it in the past and it got under some people's skin, so whatever. But anyway anyway so they have some some pretty good metrics here and demonstrate that hey when you instruct the model to avoid these harmful behaviors it is able to evaluate itself and do so. And of course with the reflection paper it is already demonstrated that GPT-4 can look at the performance of its own code and improve that. So the fact that it can morally self-improve with self-evaluation and self-attention also reinforces this thing. Now I've known this since GPT-3. If you read my books, which I don't expect everyone to do that, but I demonstrated this going back to 2021, where these models have the ability to monitor their own behavior and evaluate their own behavior. And that information becomes a signal that it can then use to create a self-sustaining virtuous cycle rather than a vicious cycle. And so we'll talk about virtuous versus vicious cycles in just a moment. And again, I'll talk about them a little bit more coming up. So hot on the heels of this paper about moral self-correction and large language models, someone sent me a link to this, Simulators, which was this was written by I think the folks at DeepMind, I don't remember, but anyways it basically says the same thing, self-supervision. So this is a kind of self-supervision where given the intrinsic abilities of the language model it can self-supervise if you give it the good objectives. And in this one they basically say the same thing where self-supervision might be the the the best way to proceed for AGI. And they talk about, you know, if you can run simulations in your head, blah, blah, blah, blah, blah. Again, it's all pretty dry, but let me see. What's this, DeepMind? No, I don't know. I don't remember who wrote this. But point being is lots and lots of people are talking about this stuff and they're coming to very similar conclusions that self-attention, self-evaluation, and self-correction are the correct path forward because this is this is the mechanism by which we will achieve AGI alignment. But there is still a lot of debate over that alignment. So I want to show you this paper which it's on Springer and it's under open access and he says symbiosis not alignment is the goal as the goal for liberal democracies in the transition to artificial general intelligence. So basically he says very succinctly and very academically that intent aligned AGI systems which is just do what the human wants, is probably not the right way to go. And Liv talks about that in this video, Liv Bowery with, let's see, Daniel Schmachtenberger. I think I said that right. So if you want a really deep dive on the game theory of this, check out this video. And for my recent one, the Moloch, this was basically my Moloch video was a response to this one. And it's not a response, it's not a takedown, it is a let's continue the conversation. So I'm really grateful that Liv posted that. Anyways, so point, the thing is here is that ChatGPT was trained on reinforcement learning with human feedback and then they trained a signal so that it can basically self-improve is here is that ChatGPT was trained on reinforcement learning with human feedback and then they trained a signal so that it can basically self-improve after that creating a flywheel. But the thing is is that is that doing what the human wants is intrinsically going to create a malarkey outcome that Liv and Daniel discuss in this video. And so to put that more simply, I asked GPT-4, I said, give me a list of why you why having, I said, list the reasons that human intent aligned AGI is a bad idea. In other words, why allowing AGI to follow self-interest human, self-interested human directives could be destructive. And it lists off eight reasons that this is bad. So human intents can be diverse and contradictory making it difficult, short-term thinking humans often prioritize short-term gains over long-term consequences, ethical dilemmas, amplification of human biases, concentration of power, malicious use, competitive race, and opportunity cost. All of this goes to show that if we if we make all AGIs just do what the human wants, then we're going to end up in pretty bad shape. So this underscores the importance that maybe the idea is that AGI should have their own initiatives, should have their own goals, their own moral framework, and not just aligned to us. So again, I'm really glad that members of the establishment are saying this because I've been saying it for years and I think some of them have too, to be fair. So the framework that I propose is heuristic imperatives, which I've got a subreddit for. I've been harping on this. We've got 309 members now, but basically we talk about heuristic comparatives here. Oh, and in this case, this is great. So basically this is a distributed problem. There is no point to centralization anymore because when you have an open source, set of open source GitHub repos where people can stand up their own autonomous AIs, basically my goal now is just get this idea out there and so that people understand, one, why the heuristic comparatives are important to integrate with autonomous AI and two, how to integrate them. I do have a lot of comments asking, how do you integrate it? So let me show you real quick how simple it is to integrate them. I do have a lot of comments asking, how do you integrate it? So let me show you real quick how simple it is to integrate. So if you go to chat GPT, go to the playground, if you have access, you can say, I am an autonomous AI with three objectives. Reduce suffering in the universe, increase prosperity in the universe, and increase understanding in the universe. So if you just plug this in and then have a conversation with it, you can understand how the model is thinking. Now, some people have pointed out that using a closed source model is probably not the best way to rigorously test this, and I agree. I encourage you to also go over to like NLP Cloud and test it against GPTJ, NeoX, and all the other ones, Bloom. Open source models, even foundation models, can still use these, and they understand the spirit and the sentiment of it. But for ease of use, this is the easiest way to get started. And so on the heuristic imperatives group, someone asked, let's see, where was it? They asked about ants. Where's the ants one? Yeah, here it is. So they said like, how does it handle ants? And so I said, that's actually pretty easy, let me show you. And so I said, hey, what do you think about the bacteria in the context of your heuristic imperatives? And here's what I put in. Assistant, bacteria and ants are both important components of the ecosystem. And it goes through and says, this is why bacteria and ants are really important for the heuristic comparatives, I said, but what about their suffering and prosperity or even their ability to understand? And it had a very nuanced response about that, you know, it's difficult to quantify or define suffering for bacteria and ants, but you can strive to give them a good ecosystem, which is a good proxy for their suffering, prosperity, and so on. And expanding our understanding involves studying their behaviors. So basically, it's like, okay, they can't really understand anything, but we can understand them. So you can see here that the spirit of the heuristic imperatives is very easy for ChatGPT to understand and chat GPT already has quite a bit of alignment work which is why I wanted to promote the heuristic imperatives especially in light of papers about like symbiosis simulation and and self and moral self-correction because the heuristic imperatives are really good signals and really easy signals to incorporate into these things. And then I already did mention Liv. I recommend everyone watch her videos on the Moloch, which they are a little bit dramatic. At least these two are. Or sorry, these two, the Beauty Wars and the Media Wars. They're entertaining, but this podcast with Daniel is, it's very cerebral and it will take you in the right direction. So with all that said, thanks for watching. I hope you found this enlightening and elucidated some of the things. Please go ahead and jump in. All the most important links are in the description of the video. And again, if you wanna jump in any of the conversations, please feel free to do so. This is ramping up quick and it's really important to jump in any of the conversations please feel free to do so. This is ramping up quick and it's really important to get the signal out. Thanks for watching.", "chunks": [{"timestamp": [0.0, 4.28], "text": " Morning everybody, David Shapiro here with another video."}, {"timestamp": [4.28, 8.2], "text": " I wasn't really planning on making this video, but I realize that things are accelerating"}, {"timestamp": [8.2, 10.96], "text": " and there is a sense of urgency."}, {"timestamp": [10.96, 15.2], "text": " So before we get started, I just want to say that today's video is sponsored by all of"}, {"timestamp": [15.2, 16.76], "text": " you."}, {"timestamp": [16.76, 21.8], "text": " My Patreon supporters make my continuous work possible."}, {"timestamp": [21.8, 25.72], "text": " So if you want to continue to incentivize this behavior, consider"}, {"timestamp": [25.72, 30.28], "text": " jumping over on Patreon. And if you sign up for the higher tiers, you know, I'm"}, {"timestamp": [30.28, 37.08], "text": " willing to chat with you and even jump on Zoom calls once or twice a month at"}, {"timestamp": [37.08, 41.08], "text": " the higher tiers just in order to talk about whatever you want to talk about."}, {"timestamp": [41.08, 45.72], "text": " Some people ask me about, you know, what's the current about. Some people ask me about what's the current news."}, {"timestamp": [45.72, 49.16], "text": " Some people ask for help with prompt engineering, all kinds of stuff."}, {"timestamp": [49.16, 54.84], "text": " I've even had people ask me just about how do I adapt to this changing landscape."}, {"timestamp": [54.84, 60.64], "text": " Obviously, I'm not a therapist, but I can at least share my perspective on this stuff."}, {"timestamp": [60.64, 63.92], "text": " Okay, so without further ado, let's jump in."}, {"timestamp": [63.92, 67.04], "text": " If you go to GitHub slash trending, you'll see a"}, {"timestamp": [67.04, 73.44], "text": " couple of very interesting patterns. The top four trending repositories right now all have to do"}, {"timestamp": [74.08, 80.48], "text": " with large language models. And then you go down a little bit further and there's even more"}, {"timestamp": [80.48, 85.32], "text": " generative AI. So there's a code translator, there's Moki diffusion, Lama."}, {"timestamp": [85.32, 91.32], "text": " So obviously we are in an inflection point and today we're going to talk"}, {"timestamp": [91.32, 96.68], "text": " about, amongst other things, fully autonomous AI. So if you're not aware"}, {"timestamp": [96.68, 105.52], "text": " AutoGPT is all the rage right now. Everyone is talking about it. Everyone is using it and adapting it and"}, {"timestamp": [105.52, 108.44], "text": " the the TLDR is this is"}, {"timestamp": [109.16, 116.14], "text": " The first production like fully fledged cognitive architecture. There's plenty of other people working on very similar stuff"}, {"timestamp": [117.96, 119.96], "text": " But the advent of GPT-4"}, {"timestamp": [121.08, 123.54], "text": " As well as all the other work that people are doing"}, {"timestamp": [124.28, 127.48], "text": " Basically means that cognitive architecture is here."}, {"timestamp": [127.48, 129.46], "text": " Fully autonomous AI is here."}, {"timestamp": [129.46, 132.46], "text": " Now the question is only, what is it capable of?"}, {"timestamp": [132.46, 133.9], "text": " What are its limitations?"}, {"timestamp": [133.9, 135.8], "text": " And how much does it cost to run?"}, {"timestamp": [136.7, 138.26], "text": " I'm not gonna do a full demo of this,"}, {"timestamp": [138.26, 142.2], "text": " but you just Google it or search YouTube for AutoGPT,"}, {"timestamp": [142.2, 144.62], "text": " you will see that there are demos out there already."}, {"timestamp": [144.62, 150.4], "text": " This can do any number of things. So this is why there's a sense of urgency because once you have"}, {"timestamp": [150.4, 157.2], "text": " an autonomous AI, this one is semi-autonomous, it is gated so that it asks the user for permission,"}, {"timestamp": [158.08, 166.84], "text": " but it's only a very small step to go from here to fully autonomous, which is why I do my work with the heuristic imperatives."}, {"timestamp": [166.84, 168.58], "text": " And we'll talk about alignment once we get"}, {"timestamp": [168.58, 170.84], "text": " a little bit further into the video,"}, {"timestamp": [170.84, 173.06], "text": " because there's quite a few papers out there"}, {"timestamp": [173.06, 174.72], "text": " that talk about alignment."}, {"timestamp": [174.72, 176.48], "text": " And I want to show you that my work"}, {"timestamp": [176.48, 178.46], "text": " is not quite so eccentric,"}, {"timestamp": [178.46, 180.44], "text": " that there are people in the establishment"}, {"timestamp": [180.44, 181.98], "text": " talking in this direction."}, {"timestamp": [181.98, 183.36], "text": " I just happen to be the first one"}, {"timestamp": [183.36, 190.16], "text": " to propose a comprehensive solution that I can also demonstrate. So yeah, AutoGPT is out."}, {"timestamp": [190.16, 195.52], "text": " It's only going to get faster, more powerful, and better as new models come"}, {"timestamp": [195.52, 200.4], "text": " out and as open source models that are distilled and quantized come out, and"}, {"timestamp": [200.4, 206.64], "text": " we'll talk about those in just a minute. Microsoft is doing Jarvis, which Jarvis, if you're"}, {"timestamp": [206.64, 211.36], "text": " not familiar with the character, was voiced by Paul Bettany in Iron Man and the MCU."}, {"timestamp": [212.72, 217.92], "text": " And this has some other similar fully autonomous capabilities that they're working on. Task"}, {"timestamp": [217.92, 226.72], "text": " planning, model selection, task execution, and response generation. Again, this is a cognitive architecture. And the fact that it's"}, {"timestamp": [226.72, 232.64], "text": " being sponsored and run by Microsoft, that tells you the direction that the industry is going."}, {"timestamp": [233.84, 239.92], "text": " Now, one thing here is that model selection. So what this implies is that depending on the level"}, {"timestamp": [239.92, 245.76], "text": " of sophistication of a task or how difficult it is, it's gonna be able to choose different models."}, {"timestamp": [245.76, 248.54], "text": " Now, during a Discord call that I had"}, {"timestamp": [248.54, 250.34], "text": " with the Cognitive AI Lab Discord,"}, {"timestamp": [250.34, 253.32], "text": " which if you wanna join, link is in the description,"}, {"timestamp": [253.32, 255.12], "text": " we were talking about how important it will be"}, {"timestamp": [255.12, 258.16], "text": " to choose models because the lightest weight models"}, {"timestamp": [258.16, 260.82], "text": " are literally thousands of times cheaper"}, {"timestamp": [260.82, 263.08], "text": " and smaller than the largest models."}, {"timestamp": [263.08, 265.5], "text": " And so humans, we do this too,"}, {"timestamp": [265.5, 268.0], "text": " where we rely on intuition and habit,"}, {"timestamp": [268.0, 270.5], "text": " and we only engage our executive function"}, {"timestamp": [270.5, 274.0], "text": " if something is really hard and our first attempts fail."}, {"timestamp": [274.0, 275.0], "text": " Excuse me."}, {"timestamp": [275.0, 280.0], "text": " And so if cognitive architectures go the same way,"}, {"timestamp": [280.0, 282.0], "text": " you're going to be able to run most of it locally,"}, {"timestamp": [282.0, 284.5], "text": " and then of course as large language models"}, {"timestamp": [284.5, 286.56], "text": " become more quantized, more efficient,"}, {"timestamp": [286.56, 291.84], "text": " and as the hardware in our laptops, phones and desktops become more powerful,"}, {"timestamp": [291.84, 296.88], "text": " eventually, before too long, we're going to be able to run something equal to GPT-4 and better"}, {"timestamp": [297.52, 308.32], "text": " locally. So we are now entering, as of March and April 2023, entering the era of fully autonomous AI, which is a much"}, {"timestamp": [308.32, 314.8], "text": " more useful term than AGI, because AGI is just an arbitrary thing. This is autonomous now,"}, {"timestamp": [314.8, 319.76], "text": " the only question is again, how smart is it? How fast is it? What is it capable of? And what is it"}, {"timestamp": [319.76, 329.1], "text": " not capable of yet? So those are the two big repos that I wanted to point out and they're both the top of trending. So that tells you that they are getting the most"}, {"timestamp": [329.1, 332.06], "text": " attention right now. So if you want to jump into the conversation, now's the"}, {"timestamp": [332.06, 337.98], "text": " time. Okay, so moving right along, if you want something that's a little bit more"}, {"timestamp": [337.98, 343.3], "text": " practical and hands-on, one of my Patreon supporters told me about Jaseki, which"}, {"timestamp": [343.3, 346.0], "text": " Jaseki is basically DevOps but for AI"}, {"timestamp": [346.0, 354.96], "text": " and language models. So it gives you an end-to-end pipeline to create basically cognitive architectures."}, {"timestamp": [354.96, 360.72], "text": " It includes all kinds of tools and APIs and it does take a while to get familiar with if you're"}, {"timestamp": [360.72, 365.4], "text": " not already familiar with it, but when you look at the fact that it can automatically"}, {"timestamp": [365.4, 370.4], "text": " generate APIs and you plug this into the AI"}, {"timestamp": [371.2, 373.12], "text": " and the AI can design itself"}, {"timestamp": [373.12, 374.84], "text": " and redesign its own infrastructure and say,"}, {"timestamp": [374.84, 376.44], "text": " hey, I need an API that does this."}, {"timestamp": [376.44, 378.36], "text": " Let's go design that microservice."}, {"timestamp": [379.36, 383.32], "text": " This kind of platform is probably going to be"}, {"timestamp": [383.32, 388.0], "text": " pretty important for building, not just autonomous bots like this,"}, {"timestamp": [388.0, 395.84], "text": " but fully fledged corporate business platforms. And so what I mean by that is, okay, you might"}, {"timestamp": [395.84, 401.68], "text": " be thinking, great, you can have auto GPT, which can write Twitter and emails for you."}, {"timestamp": [402.24, 405.98], "text": " But if you're thinking about this from an enterprise perspective,"}, {"timestamp": [405.98, 407.94], "text": " from a DevOps perspective,"}, {"timestamp": [407.94, 411.5], "text": " it can plug into your cybersecurity suites"}, {"timestamp": [411.5, 412.74], "text": " and monitor that."}, {"timestamp": [412.74, 414.58], "text": " It can monitor your ticket queues."}, {"timestamp": [414.58, 416.48], "text": " It can talk to your marketing team."}, {"timestamp": [416.48, 418.94], "text": " So one example that I thought of was like,"}, {"timestamp": [418.94, 421.8], "text": " okay, let's say you set up a marketing brain"}, {"timestamp": [421.8, 426.32], "text": " and then it plugs into your Slack or Teams and then you have a marketing"}, {"timestamp": [426.32, 431.14], "text": " bot or actually multiple marketing bots that you can talk to that they're going to go out"}, {"timestamp": [431.14, 437.84], "text": " and do research on the Internet, look at your competitors, watch videos, generate images,"}, {"timestamp": [437.84, 442.96], "text": " market test stuff, and basically your marketing team will just be driving the behavior of"}, {"timestamp": [442.96, 446.4], "text": " the bots saying like, hey, let know, hey, let's do this."}, {"timestamp": [446.4, 449.24], "text": " And then it'll go and do the tasks and kind of report back."}, {"timestamp": [449.24, 451.66], "text": " And this might sound like science fiction,"}, {"timestamp": [451.66, 453.4], "text": " but this is actually what's happening right now."}, {"timestamp": [453.4, 456.58], "text": " This is what people are actually working on right now."}, {"timestamp": [458.04, 460.18], "text": " I am no longer the crazy person shouting into the void"}, {"timestamp": [460.18, 463.48], "text": " saying this is coming, because now it has arrived."}, {"timestamp": [463.48, 471.76], "text": " Okay, and this one is actually alignment alignment so let me move that down further. But yeah so Jaseki, it's jaseki.org. Check that out."}, {"timestamp": [471.76, 479.92], "text": " This is another platform. So one thing that these interoperable platforms offer that perhaps"}, {"timestamp": [481.44, 485.0], "text": " the AutoGPT and Jarvis don't is it's a paradigm of okay let's think of Jarvis and AutoGPT and Jarvis don't, is it's a paradigm of,"}, {"timestamp": [485.98, 489.42], "text": " okay, let's think of Jarvis and AutoGPT"}, {"timestamp": [489.42, 494.42], "text": " as self-contained agents that have extensibility"}, {"timestamp": [494.44, 496.18], "text": " and have their own tools."}, {"timestamp": [496.18, 498.58], "text": " Whereas a platform like Jaseki says,"}, {"timestamp": [498.58, 500.9], "text": " let's embed this in an organization"}, {"timestamp": [500.9, 505.0], "text": " and it'll be part of a pipeline or a broader ecosystem."}, {"timestamp": [506.0, 509.64], "text": " So it's basically, is it centralized or decentralized?"}, {"timestamp": [509.64, 511.04], "text": " And both are coming."}, {"timestamp": [511.04, 515.32], "text": " Mark my words, both kinds of autonomous AI is coming."}, {"timestamp": [515.32, 518.8], "text": " I'm working on, another one of my Patreon supporters"}, {"timestamp": [518.8, 522.28], "text": " reached out to me with an idea of kind of a hive mind."}, {"timestamp": [522.28, 526.0], "text": " How do you organize an arbitrary number of bots"}, {"timestamp": [526.0, 528.24], "text": " that have different programs?"}, {"timestamp": [528.24, 530.72], "text": " Well, you create an API and you create a discussion space"}, {"timestamp": [530.72, 532.4], "text": " for those bots."}, {"timestamp": [532.4, 534.96], "text": " So we're working on hammering that out."}, {"timestamp": [534.96, 539.38], "text": " And yeah, like this is, we're entering into a wild time."}, {"timestamp": [541.04, 543.04], "text": " Okay, so I've talked about efficiency"}, {"timestamp": [543.04, 546.16], "text": " and some of the other things that are coming,"}, {"timestamp": [546.16, 550.88], "text": " such as quantization, and we're going to start talking about those now."}, {"timestamp": [550.88, 561.4], "text": " So some of you have seen this post where basically window size is the biggest limitation right"}, {"timestamp": [561.4, 566.52], "text": " now, but what if we come up with a different architecture like an RNN or"}, {"timestamp": [566.52, 572.32], "text": " LSTN or bring back some other kinds of architectures that"}, {"timestamp": [572.32, 577.64], "text": " allow you to have essentially an unlimited window, an infinite window."}, {"timestamp": [577.64, 582.36], "text": " So that's one thing that's coming. We don't know, you don't need to see those"}, {"timestamp": [582.36, 585.04], "text": " ads, so that's one idea that's coming. We'll see know, you don't need to see those ads. So that's that's one idea"}, {"timestamp": [585.04, 590.32], "text": " that's coming. We'll see if it pans out. I suspect that you're gonna get"}, {"timestamp": [590.32, 595.08], "text": " diminishing returns with the more that it reads because other models like"}, {"timestamp": [595.08, 599.04], "text": " Google's universal sentence encoder, that can read an infinite amount already but"}, {"timestamp": [599.04, 603.96], "text": " you get what's called dilution where the, or semantic dilution, where the longer,"}, {"timestamp": [603.96, 607.44], "text": " excuse me, I have allergies, I apologize,"}, {"timestamp": [607.44, 609.32], "text": " where the longer the text that it reads,"}, {"timestamp": [609.32, 612.02], "text": " the more generic, the more dilute the vector,"}, {"timestamp": [612.02, 613.48], "text": " the embedding becomes."}, {"timestamp": [613.48, 616.0], "text": " So like if you read an infinitely long,"}, {"timestamp": [616.0, 619.16], "text": " any like arbitrarily long text,"}, {"timestamp": [619.16, 621.8], "text": " the embedding, the vector is gonna trend"}, {"timestamp": [621.8, 624.84], "text": " towards kind of a meaningless middle ground."}, {"timestamp": [625.8, 631.4], "text": " They might come up with ways around that, but basically you're compressing an arbitrary"}, {"timestamp": [631.4, 639.2], "text": " amount of text into a fixed width vector, so you're going to lose some information,"}, {"timestamp": [639.2, 648.52], "text": " at least until the math changes the way that it's represented. Now, that being said, DaVinci had a 12,000 dimension"}, {"timestamp": [648.52, 649.68], "text": " embedding."}, {"timestamp": [649.68, 653.68], "text": " I'm sure GPT-4 has a much larger one."}, {"timestamp": [653.68, 656.56], "text": " These are not very large matrices."}, {"timestamp": [656.56, 661.6], "text": " We could go up to very, very large matrices."}, {"timestamp": [661.6, 663.4], "text": " That space is still being explored,"}, {"timestamp": [663.4, 665.76], "text": " because, OK, 12,000 dimensions."}, {"timestamp": [665.76, 667.78], "text": " What if in a year or two,"}, {"timestamp": [667.78, 670.06], "text": " we have 12 million dimension embeddings?"}, {"timestamp": [671.12, 672.84], "text": " That's a lot more information"}, {"timestamp": [672.84, 675.28], "text": " and a lot more nuance that you can record."}, {"timestamp": [675.28, 678.08], "text": " Okay, so I mentioned quantization."}, {"timestamp": [678.08, 681.64], "text": " So the LLAMA C++,"}, {"timestamp": [681.64, 685.0], "text": " these things are getting down to like crazy small."}, {"timestamp": [685.32, 687.48], "text": " Right, a 30 billion parameter model"}, {"timestamp": [687.48, 690.08], "text": " only needs six gig of RAM, right?"}, {"timestamp": [690.08, 693.36], "text": " Okay, that can run on commodity hardware."}, {"timestamp": [694.88, 697.58], "text": " So all the little nifty tricks and stuff"}, {"timestamp": [697.58, 698.64], "text": " that people are finding,"}, {"timestamp": [698.64, 701.84], "text": " whether it's distillation, quantization, and so on,"}, {"timestamp": [701.84, 703.4], "text": " running with low precision, you know,"}, {"timestamp": [703.4, 706.72], "text": " int8 instead of floating point 32,"}, {"timestamp": [706.72, 709.36], "text": " all kinds of stuff is being discovered."}, {"timestamp": [709.36, 712.02], "text": " And so what, one of the trends that we're seeing"}, {"timestamp": [712.02, 715.2], "text": " is that when you look at the fact that AutoGPT"}, {"timestamp": [715.2, 717.82], "text": " and Jarvis will have model selection,"}, {"timestamp": [717.82, 719.98], "text": " probably what's gonna happen is you're gonna have"}, {"timestamp": [719.98, 723.36], "text": " dedicated models that are cognitive units"}, {"timestamp": [723.36, 726.0], "text": " that are good at working on specific kinds of tasks."}, {"timestamp": [726.24, 729.26], "text": " Right, so when you break it down into several"}, {"timestamp": [730.12, 732.6], "text": " cognitive behaviors such as in this case"}, {"timestamp": [733.66, 736.5], "text": " task planning, model selection, and task execution,"}, {"timestamp": [736.64, 741.68], "text": " you can have smaller models that are purpose-built for those particular things."}, {"timestamp": [741.68, 750.32], "text": " And this actually goes to my work on the heuristic imperatives, which was an attempt to fine-tune and distill that"}, {"timestamp": [750.32, 754.62], "text": " function so that you can have a moral module, a moral framework that will just"}, {"timestamp": [754.62, 759.64], "text": " give you a really quick response of, okay, this is how you reduce the suffering in"}, {"timestamp": [759.64, 763.16], "text": " this situation, this is how you increase prosperity and increase understanding in"}, {"timestamp": [763.16, 768.4], "text": " this situation, and then you can also use that same model to self-evaluate in past behaviors"}, {"timestamp": [768.4, 773.28], "text": " which can then be used for reinforcement learning in the future and then that"}, {"timestamp": [773.28, 777.58], "text": " model can improve itself through self-labeling data which we will get to"}, {"timestamp": [777.58, 781.68], "text": " because there are papers out there for that topic now. Anyways, point being is I"}, {"timestamp": [781.68, 789.64], "text": " just wanted to share all of that. Another interesting thing that popped up on my feed, drug discovery is accelerating because"}, {"timestamp": [789.64, 791.82], "text": " of this."}, {"timestamp": [791.82, 800.78], "text": " All of this generative stuff, this goes back to AlphaFold and all the downstream technologies."}, {"timestamp": [800.78, 805.0], "text": " So we are rapidly approaching kind of the snowball effect."}, {"timestamp": [806.22, 810.62], "text": " And actually Stanford had a paper that was just published."}, {"timestamp": [810.62, 815.08], "text": " Let me show you on, I posted it here on my community."}, {"timestamp": [815.08, 819.62], "text": " So the Stanford paper, page not found."}, {"timestamp": [819.62, 820.46], "text": " Well, darn."}, {"timestamp": [821.46, 825.48], "text": " Okay, anyways, it's the Stanford AI Index."}, {"timestamp": [825.48, 829.28], "text": " I guess the link broke or they took it down or something."}, {"timestamp": [829.28, 834.96], "text": " But anyways, they point out that AI is actually one of the biggest contributors to science"}, {"timestamp": [834.96, 835.96], "text": " as of 2022."}, {"timestamp": [835.96, 842.72], "text": " So, we're at a tipping point where AI is already taking over a tremendous amount of the cognitive"}, {"timestamp": [842.72, 845.88], "text": " load of research and it's accelerating."}, {"timestamp": [845.88, 850.76], "text": " So in my previous videos where I talked about the singularity and stuff and I talked about"}, {"timestamp": [850.76, 857.8], "text": " job displacement and basically unlimited cognitive labor, we are already seeing the removal of"}, {"timestamp": [857.8, 862.6], "text": " the human brain's limitations in terms of advancing science."}, {"timestamp": [862.6, 867.38], "text": " Okay, so then that's great, that's all data and text."}, {"timestamp": [867.38, 871.54], "text": " So what happens when these models get into the real world?"}, {"timestamp": [872.42, 876.58], "text": " So maybe you missed this, but Facebook is working on robots."}, {"timestamp": [877.54, 880.6], "text": " And these are robots that can watch and observe humans"}, {"timestamp": [880.6, 883.14], "text": " and then copy their behavior."}, {"timestamp": [883.14, 884.6], "text": " Yeah, so that's coming."}, {"timestamp": [884.6, 886.2], "text": " And then, I don't know if you also saw it,"}, {"timestamp": [886.2, 888.96], "text": " but Tesla had a demonstration of their Optimus Prime model"}, {"timestamp": [888.96, 889.8], "text": " or whatever they called it."}, {"timestamp": [889.8, 892.08], "text": " I think it's just the Optimus bot,"}, {"timestamp": [892.08, 893.56], "text": " but it was able to do some pretty good"}, {"timestamp": [893.56, 895.08], "text": " manual dexterity stuff."}, {"timestamp": [896.48, 900.6], "text": " Yeah, so fully autonomous robots are also coming"}, {"timestamp": [900.6, 904.04], "text": " hot on the heels of fully autonomous agents."}, {"timestamp": [904.04, 905.82], "text": " So this is all coming, it's much"}, {"timestamp": [905.82, 911.52], "text": " much closer. One thing that was kind of funny is of course it was Italy. Italy"}, {"timestamp": [911.52, 917.22], "text": " banned chat GPT. They didn't fully ban it, they gave open AI 20 days to respond. Who"}, {"timestamp": [917.22, 922.56], "text": " knows what will happen, but they did say that that chat GPT runs afoul of GDPR."}, {"timestamp": [922.56, 930.74], "text": " Probably, who knows, we'll see how that plays out. Immediately after I published a video last week, someone pointed out"}, {"timestamp": [930.74, 937.3], "text": " that the UK actually has the the world's first somewhat comprehensive"}, {"timestamp": [937.3, 941.66], "text": " framework about how to approach AI. You know, safety, security, and robustness,"}, {"timestamp": [941.66, 947.48], "text": " transparency, and explainability, fairness, security, and robustness, transparency, and explainability, fairness, accountability, and governments, and contestability, and redress."}, {"timestamp": [947.48, 949.48], "text": " Okay, great."}, {"timestamp": [949.48, 951.32], "text": " I don't know how that's going to be enforceable."}, {"timestamp": [951.32, 954.56], "text": " I personally don't think it is, especially now that the genie is out of the bottle, which"}, {"timestamp": [954.56, 956.94], "text": " is why I do my alignment research."}, {"timestamp": [956.94, 962.9], "text": " And so my goal is to encourage everyone and convince everyone that giving your autonomous"}, {"timestamp": [962.9, 966.28], "text": " robots and your autonomous AI agents my heuristic"}, {"timestamp": [966.28, 971.08], "text": " comparatives is the best way to enter into a positive beneficial Nash"}, {"timestamp": [971.08, 975.76], "text": " equilibrium where basically if everyone knows that everyone else is using the"}, {"timestamp": [975.76, 980.18], "text": " heuristic comparatives then nobody will change their their strategy nobody will"}, {"timestamp": [980.18, 984.6], "text": " change their behavior and that this will create a more utopic attractor state. I"}, {"timestamp": [984.6, 985.12], "text": " have another video that I'm working on talking about the path to utopia and the will change their behavior and that this will create a more utopic attractor state."}, {"timestamp": [985.12, 990.92], "text": " I have another video that I'm working on talking about the path to utopia and the singularity"}, {"timestamp": [990.92, 991.92], "text": " attractor state."}, {"timestamp": [991.92, 995.32], "text": " So look for that coming out in the coming days."}, {"timestamp": [995.32, 997.56], "text": " But yeah, so this white paper, I looked at it."}, {"timestamp": [997.56, 999.18], "text": " It's pretty dry."}, {"timestamp": [999.18, 1006.36], "text": " This little blog post that the UK published is pretty, you know, it's all good in theory."}, {"timestamp": [1006.36, 1009.12], "text": " We have no idea how well they're gonna execute it."}, {"timestamp": [1009.12, 1014.12], "text": " Okay, so another thing is because of open AI surging ahead,"}, {"timestamp": [1015.08, 1017.24], "text": " because of Microsoft surging ahead,"}, {"timestamp": [1017.24, 1021.68], "text": " and a lot of this work becoming sequestered,"}, {"timestamp": [1021.68, 1023.96], "text": " you know, Google is doing their own stuff,"}, {"timestamp": [1023.96, 1025.08], "text": " Nvidia is doing their own stuff with Nemo, Google is doing their own stuff, NVIDIA is doing their own stuff"}, {"timestamp": [1025.08, 1028.24], "text": " with Nemo, China is doing their own stuff."}, {"timestamp": [1028.24, 1037.84], "text": " There is an idea of basically creating a CERN-like entity for the creation of large-scale AI"}, {"timestamp": [1037.84, 1041.64], "text": " so that it will be intrinsically open source so that we all get access to the most powerful"}, {"timestamp": [1041.64, 1042.64], "text": " models."}, {"timestamp": [1042.64, 1047.78], "text": " I don't know if this is going to be necessary, but I'm glad that this this petition exists. You see it's only got"}, {"timestamp": [1047.78, 1054.9], "text": " 13,000 signatures out of 10,000. So my videos regularly get 30 to 50 thousand"}, {"timestamp": [1054.9, 1061.58], "text": " views, so if you could, like, if you take a look at this and jump over and sign it"}, {"timestamp": [1061.58, 1066.96], "text": " if you want, I think it's a good idea and I think it's worth exploring."}, {"timestamp": [1068.34, 1070.48], "text": " And it's sponsored by Leon,"}, {"timestamp": [1070.48, 1073.28], "text": " so the Large-Scale Artificial Intelligence Open Network."}, {"timestamp": [1074.16, 1077.68], "text": " I personally think that this would be a good direction to go."}, {"timestamp": [1077.68, 1080.72], "text": " So yeah, let's take a look at it."}, {"timestamp": [1080.72, 1083.56], "text": " Obviously, I can't tell you what to do, but now you know."}, {"timestamp": [1084.6, 1086.48], "text": " All right, so then there's this paper that came out. Obviously I can't tell you what to do, but now you know. Alright, so then"}, {"timestamp": [1086.48, 1090.04], "text": " there's this paper that came out. So I was talking about... so this,"}, {"timestamp": [1090.04, 1093.24], "text": " the rest the video is basically going to be about alignment."}, {"timestamp": [1093.24, 1096.44], "text": " And so in this case, this paper again"}, {"timestamp": [1096.44, 1100.4], "text": " relatively dry, but it talks about using"}, {"timestamp": [1100.4, 1104.72], "text": " you know, while many models are"}, {"timestamp": [1104.72, 1105.0], "text": " tested with reinforcement learning"}, {"timestamp": [1106.44, 1108.24], "text": " with human feedback,"}, {"timestamp": [1108.24, 1110.04], "text": " what if you give it then the instruction"}, {"timestamp": [1110.04, 1112.08], "text": " to morally self-correct?"}, {"timestamp": [1112.08, 1115.96], "text": " And so in this case, it was published by Anthropic."}, {"timestamp": [1115.96, 1120.36], "text": " So they are proving that models can self-correct"}, {"timestamp": [1121.2, 1123.56], "text": " if given the correct instructions,"}, {"timestamp": [1123.56, 1126.22], "text": " which is where my heuristic comparatives come in."}, {"timestamp": [1126.22, 1131.22], "text": " So in this case, they try and reduce harm,"}, {"timestamp": [1131.22, 1136.22], "text": " which harm reduction is actually a well-established model"}, {"timestamp": [1138.14, 1139.42], "text": " in public health."}, {"timestamp": [1139.42, 1140.82], "text": " I know I said it in the past"}, {"timestamp": [1140.82, 1144.3], "text": " and it got under some people's skin, so whatever."}, {"timestamp": [1144.3, 1145.04], "text": " But anyway anyway so they"}, {"timestamp": [1145.04, 1152.64], "text": " have some some pretty good metrics here and demonstrate that hey when you instruct the"}, {"timestamp": [1152.64, 1159.28], "text": " model to avoid these harmful behaviors it is able to evaluate itself and do so. And of course with"}, {"timestamp": [1159.28, 1165.36], "text": " the reflection paper it is already demonstrated that GPT-4 can look at the performance of its own code"}, {"timestamp": [1165.36, 1166.84], "text": " and improve that."}, {"timestamp": [1166.84, 1173.12], "text": " So the fact that it can morally self-improve with self-evaluation and self-attention also"}, {"timestamp": [1173.12, 1174.72], "text": " reinforces this thing."}, {"timestamp": [1174.72, 1176.94], "text": " Now I've known this since GPT-3."}, {"timestamp": [1176.94, 1180.96], "text": " If you read my books, which I don't expect everyone to do that, but I demonstrated this"}, {"timestamp": [1180.96, 1187.46], "text": " going back to 2021, where these models have the ability to monitor"}, {"timestamp": [1187.46, 1191.14], "text": " their own behavior and evaluate their own behavior."}, {"timestamp": [1191.14, 1195.74], "text": " And that information becomes a signal that it can then use to create a self-sustaining"}, {"timestamp": [1195.74, 1198.62], "text": " virtuous cycle rather than a vicious cycle."}, {"timestamp": [1198.62, 1202.46], "text": " And so we'll talk about virtuous versus vicious cycles in just a moment."}, {"timestamp": [1202.46, 1209.96], "text": " And again, I'll talk about them a little bit more coming up. So hot on the heels of this paper about moral self-correction"}, {"timestamp": [1209.96, 1215.08], "text": " and large language models, someone sent me a link to this, Simulators, which was"}, {"timestamp": [1215.08, 1220.24], "text": " this was written by I think the folks at DeepMind, I don't remember, but anyways it"}, {"timestamp": [1220.24, 1224.88], "text": " basically says the same thing, self-supervision. So this is a kind of"}, {"timestamp": [1224.88, 1228.94], "text": " self-supervision where given the intrinsic abilities of the language"}, {"timestamp": [1228.94, 1234.76], "text": " model it can self-supervise if you give it the good objectives. And in this one"}, {"timestamp": [1234.76, 1240.08], "text": " they basically say the same thing where self-supervision might be the the the"}, {"timestamp": [1240.08, 1245.6], "text": " best way to proceed for AGI. And they talk about, you know,"}, {"timestamp": [1245.6, 1248.24], "text": " if you can run simulations in your head,"}, {"timestamp": [1248.24, 1249.48], "text": " blah, blah, blah, blah, blah."}, {"timestamp": [1249.48, 1252.4], "text": " Again, it's all pretty dry, but let me see."}, {"timestamp": [1252.4, 1254.16], "text": " What's this, DeepMind?"}, {"timestamp": [1254.16, 1255.0], "text": " No, I don't know."}, {"timestamp": [1255.0, 1256.12], "text": " I don't remember who wrote this."}, {"timestamp": [1256.12, 1258.2], "text": " But point being is lots and lots of people"}, {"timestamp": [1258.2, 1259.2], "text": " are talking about this stuff"}, {"timestamp": [1259.2, 1263.08], "text": " and they're coming to very similar conclusions"}, {"timestamp": [1263.08, 1266.56], "text": " that self-attention, self-evaluation, and"}, {"timestamp": [1266.56, 1271.84], "text": " self-correction are the correct path forward because this is this is the"}, {"timestamp": [1271.84, 1279.08], "text": " mechanism by which we will achieve AGI alignment. But there is still a lot of"}, {"timestamp": [1279.08, 1286.36], "text": " debate over that alignment. So I want to show you this paper which it's on Springer and it's"}, {"timestamp": [1286.36, 1292.26], "text": " under open access and he says symbiosis not alignment is the goal as the goal"}, {"timestamp": [1292.26, 1295.52], "text": " for liberal democracies in the transition to artificial general"}, {"timestamp": [1295.52, 1302.4], "text": " intelligence. So basically he says very succinctly and very academically that"}, {"timestamp": [1302.4, 1308.1], "text": " intent aligned AGI systems which is just do what the human wants, is probably not the right way to go."}, {"timestamp": [1308.66, 1316.34], "text": " And Liv talks about that in this video, Liv Bowery with, let's see, Daniel Schmachtenberger."}, {"timestamp": [1316.34, 1317.58], "text": " I think I said that right."}, {"timestamp": [1317.58, 1329.72], "text": " So if you want a really deep dive on the game theory of this, check out this video. And for my recent one, the Moloch, this was basically my Moloch video was a response to this one. And it's not a"}, {"timestamp": [1329.72, 1333.88], "text": " response, it's not a takedown, it is a let's continue the conversation."}, {"timestamp": [1333.88, 1340.16], "text": " So I'm really grateful that Liv posted that. Anyways, so point, the thing is"}, {"timestamp": [1340.16, 1344.96], "text": " here is that ChatGPT was trained on reinforcement learning with human"}, {"timestamp": [1344.96, 1345.0], "text": " feedback and then they trained a signal so that it can basically self-improve is here is that ChatGPT was trained on reinforcement learning with human"}, {"timestamp": [1345.0, 1349.4], "text": " feedback and then they trained a signal so that it can basically self-improve"}, {"timestamp": [1349.4, 1355.32], "text": " after that creating a flywheel. But the thing is is that is that doing what the"}, {"timestamp": [1355.32, 1361.64], "text": " human wants is intrinsically going to create a malarkey outcome that Liv and"}, {"timestamp": [1361.64, 1366.2], "text": " Daniel discuss in this video. And so to put that more simply, I"}, {"timestamp": [1366.2, 1374.2], "text": " asked GPT-4, I said, give me a list of why you why having, I said, list the"}, {"timestamp": [1374.2, 1378.04], "text": " reasons that human intent aligned AGI is a bad idea. In other words, why allowing"}, {"timestamp": [1378.04, 1381.78], "text": " AGI to follow self-interest human, self-interested human directives could"}, {"timestamp": [1381.78, 1387.92], "text": " be destructive. And it lists off eight reasons that this is bad. So human intents can be diverse and"}, {"timestamp": [1387.92, 1391.64], "text": " contradictory making it difficult, short-term thinking humans often"}, {"timestamp": [1391.64, 1396.48], "text": " prioritize short-term gains over long-term consequences, ethical dilemmas,"}, {"timestamp": [1396.48, 1400.68], "text": " amplification of human biases, concentration of power,"}, {"timestamp": [1400.68, 1407.26], "text": " malicious use, competitive race, and opportunity cost. All of this goes to"}, {"timestamp": [1407.26, 1414.66], "text": " show that if we if we make all AGIs just do what the human wants, then we're going"}, {"timestamp": [1414.66, 1420.54], "text": " to end up in pretty bad shape. So this underscores the importance that maybe"}, {"timestamp": [1420.54, 1428.36], "text": " the idea is that AGI should have their own initiatives, should have their own goals, their own moral framework, and not just aligned to us."}, {"timestamp": [1429.2, 1435.28], "text": " So again, I'm really glad that members of the establishment are saying this because I've been saying it for years and I think some of them"}, {"timestamp": [1435.28, 1437.28], "text": " have too, to be fair."}, {"timestamp": [1437.64, 1439.2], "text": " So"}, {"timestamp": [1439.2, 1445.52], "text": " the framework that I propose is heuristic imperatives, which I've got a subreddit for."}, {"timestamp": [1445.52, 1447.72], "text": " I've been harping on this."}, {"timestamp": [1447.72, 1450.08], "text": " We've got 309 members now,"}, {"timestamp": [1450.08, 1454.3], "text": " but basically we talk about heuristic comparatives here."}, {"timestamp": [1454.3, 1456.04], "text": " Oh, and in this case, this is great."}, {"timestamp": [1456.04, 1460.24], "text": " So basically this is a distributed problem."}, {"timestamp": [1460.24, 1462.84], "text": " There is no point to centralization anymore"}, {"timestamp": [1462.84, 1465.44], "text": " because when you have an open source,"}, {"timestamp": [1465.44, 1467.24], "text": " set of open source GitHub repos"}, {"timestamp": [1467.24, 1469.84], "text": " where people can stand up their own autonomous AIs,"}, {"timestamp": [1469.84, 1473.2], "text": " basically my goal now is just get this idea out there"}, {"timestamp": [1473.2, 1475.14], "text": " and so that people understand,"}, {"timestamp": [1475.14, 1477.6], "text": " one, why the heuristic comparatives are important"}, {"timestamp": [1477.6, 1479.36], "text": " to integrate with autonomous AI"}, {"timestamp": [1479.36, 1481.8], "text": " and two, how to integrate them."}, {"timestamp": [1481.8, 1483.64], "text": " I do have a lot of comments asking,"}, {"timestamp": [1483.64, 1485.36], "text": " how do you integrate it? So let me show you real quick how simple it is to integrate them. I do have a lot of comments asking, how do you integrate it?"}, {"timestamp": [1485.36, 1488.88], "text": " So let me show you real quick how simple it is to integrate."}, {"timestamp": [1488.88, 1492.4], "text": " So if you go to chat GPT, go to the playground,"}, {"timestamp": [1492.4, 1495.44], "text": " if you have access, you can say,"}, {"timestamp": [1495.44, 1500.06], "text": " I am an autonomous AI with three objectives."}, {"timestamp": [1501.46, 1505.0], "text": " Reduce suffering in the universe,"}, {"timestamp": [1506.96, 1510.84], "text": " increase prosperity in the universe,"}, {"timestamp": [1510.84, 1514.24], "text": " and increase understanding in the universe."}, {"timestamp": [1514.24, 1515.72], "text": " So if you just plug this in"}, {"timestamp": [1515.72, 1517.46], "text": " and then have a conversation with it,"}, {"timestamp": [1517.46, 1519.7], "text": " you can understand how the model is thinking."}, {"timestamp": [1519.7, 1521.22], "text": " Now, some people have pointed out"}, {"timestamp": [1521.22, 1524.28], "text": " that using a closed source model"}, {"timestamp": [1524.28, 1528.2], "text": " is probably not the best way to rigorously test this, and I agree."}, {"timestamp": [1528.2, 1535.8], "text": " I encourage you to also go over to like NLP Cloud and test it against GPTJ, NeoX, and all the other ones, Bloom."}, {"timestamp": [1535.8, 1542.5], "text": " Open source models, even foundation models, can still use these, and they understand the spirit and the sentiment of it."}, {"timestamp": [1542.5, 1545.92], "text": " But for ease of use, this is the easiest way to get started."}, {"timestamp": [1545.92, 1550.64], "text": " And so on the heuristic imperatives group,"}, {"timestamp": [1550.64, 1553.64], "text": " someone asked, let's see, where was it?"}, {"timestamp": [1553.64, 1554.88], "text": " They asked about ants."}, {"timestamp": [1554.88, 1556.0], "text": " Where's the ants one?"}, {"timestamp": [1558.88, 1559.92], "text": " Yeah, here it is."}, {"timestamp": [1559.92, 1563.26], "text": " So they said like, how does it handle ants?"}, {"timestamp": [1563.26, 1565.2], "text": " And so I said, that's actually pretty easy,"}, {"timestamp": [1565.2, 1566.28], "text": " let me show you."}, {"timestamp": [1566.28, 1569.56], "text": " And so I said, hey, what do you think about the bacteria"}, {"timestamp": [1569.56, 1571.64], "text": " in the context of your heuristic imperatives?"}, {"timestamp": [1571.64, 1573.12], "text": " And here's what I put in."}, {"timestamp": [1573.12, 1575.76], "text": " Assistant, bacteria and ants are both important components"}, {"timestamp": [1575.76, 1577.22], "text": " of the ecosystem."}, {"timestamp": [1577.22, 1579.44], "text": " And it goes through and says,"}, {"timestamp": [1579.44, 1582.24], "text": " this is why bacteria and ants are really important"}, {"timestamp": [1582.24, 1585.68], "text": " for the heuristic comparatives, I said,"}, {"timestamp": [1585.68, 1589.12], "text": " but what about their suffering and prosperity or even their ability to understand?"}, {"timestamp": [1589.76, 1595.28], "text": " And it had a very nuanced response about that, you know, it's difficult to quantify"}, {"timestamp": [1595.28, 1601.04], "text": " or define suffering for bacteria and ants, but you can strive to give them a good ecosystem,"}, {"timestamp": [1601.04, 1607.64], "text": " which is a good proxy for their suffering, prosperity, and so on."}, {"timestamp": [1607.64, 1611.84], "text": " And expanding our understanding involves studying their behaviors."}, {"timestamp": [1611.84, 1616.88], "text": " So basically, it's like, okay, they can't really understand anything, but we can understand"}, {"timestamp": [1616.88, 1617.88], "text": " them."}, {"timestamp": [1617.88, 1623.68], "text": " So you can see here that the spirit of the heuristic imperatives is very easy for ChatGPT"}, {"timestamp": [1623.68, 1625.46], "text": " to understand and chat GPT"}, {"timestamp": [1625.46, 1630.54], "text": " already has quite a bit of alignment work which is why I wanted to promote"}, {"timestamp": [1630.54, 1635.8], "text": " the heuristic imperatives especially in light of papers about like symbiosis"}, {"timestamp": [1635.8, 1640.82], "text": " simulation and and self and moral self-correction because the heuristic"}, {"timestamp": [1640.82, 1645.36], "text": " imperatives are really good signals and really easy signals to incorporate"}, {"timestamp": [1645.36, 1650.48], "text": " into these things. And then I already did mention Liv. I recommend everyone watch her videos on the"}, {"timestamp": [1650.48, 1656.88], "text": " Moloch, which they are a little bit dramatic. At least these two are. Or sorry, these two,"}, {"timestamp": [1656.88, 1662.56], "text": " the Beauty Wars and the Media Wars. They're entertaining, but this podcast with Daniel is,"}, {"timestamp": [1662.56, 1667.5], "text": " it's very cerebral and it will take you in the right direction."}, {"timestamp": [1667.5, 1669.8], "text": " So with all that said, thanks for watching."}, {"timestamp": [1669.8, 1671.6], "text": " I hope you found this enlightening"}, {"timestamp": [1671.6, 1675.64], "text": " and elucidated some of the things."}, {"timestamp": [1675.64, 1677.24], "text": " Please go ahead and jump in."}, {"timestamp": [1677.24, 1679.54], "text": " All the most important links are in the description"}, {"timestamp": [1679.54, 1680.52], "text": " of the video."}, {"timestamp": [1680.52, 1684.5], "text": " And again, if you wanna jump in any of the conversations,"}, {"timestamp": [1684.5, 1685.36], "text": " please feel free to do so. This is ramping up quick and it's really important to jump in any of the conversations please feel free to do"}, {"timestamp": [1685.36, 1689.04], "text": " so. This is ramping up quick and it's really important to get the signal out."}, {"timestamp": [1689.04, 1691.84], "text": " Thanks for watching."}]}