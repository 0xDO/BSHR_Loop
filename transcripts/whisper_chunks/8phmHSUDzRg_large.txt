{"text": " Morning, everybody. David Shapiro here with another video. Today's video is going to be really exciting. We're going to discuss the Moloch problem, otherwise known as undesirable Nash equilibria and attractor states, or to put it more simply, dystopia or extinction in the context of artificial general intelligence. All right, so first we probably need to define Moloch. This is a concept that has been popularized by the likes of Liv Bowrey. I'm probably saying her name wrong. She was on Lex Friedman. And also a lot of people reacted pretty positively to my last video, AGI Unleashed. And so following the trend and the conversation, of course there's lots of people out there talking about the alleged inevitability of these negative outcomes. So Moloch, to put it very simply, is a situation where the system itself, the rules, structures, incentives, and constraints of a system intrinsically and inevitably flow towards undesirable lose-lose states or negative Nash equilibria. It was inspired by a demon that demands sacrifices and it creates a vicious cycle of more sacrifices. So a few examples of the Moloch, and I put it in scare quotes because I don't particularly like the term even though it is useful. So social media is one example of the Moloch and if you want to know more about that, watch Liv Bowry's videos about media and social media. They're pretty short short they're about 15 20 minutes each. But basically social media is pretty universally harmful. It does very few good things and yet people continue to use it. They're addictive and it's just a monster that keeps wanting to eat more and more of your time and it is not particularly helpful. That being said we keep using it because there are a few benefits of social media. For instance, YouTube. YouTube is a form of social media, but the cost to benefit signal is pretty bad. Another example of the Moloch is the example of arms races, whether it's nuclear proliferation, bioweapons, other weapons of mass destruction, and so on and so forth. Basically nobody really wants to live in a world where there are thousands and thousands of nuclear weapons and bioweapons and other weapons of mass destruction, yet we live in that world because of the incentive structure and the technology basically makes it an inevitability. And when you live in the world where we are literally like a few button pushes away from the destruction of the entire human race, that is not a good situation to be in. And then finally, most commonly, the tragedy of the commons, which is basically you end up with environmental depletion and destruction due to the incentives to exploit the environment for a number of reasons. And we'll talk about Moloch in more objective terms in just a moment. But you might have noticed that none of my videos have ads and that is because my videos are all sponsored by you, my Patreon supporters. So if you like what I'm working on, you want to incentivize my behavior, then please support me financially on Patreon. Go ahead and jump over. If you go to a higher tier, I'm happy to chat with you on an individual basis, either via Patreon chat. I'll even hop on video calls. So yeah, that is the plug plug and moving right back into the show Okay. So when you listen to people talk about Malik, it sounds like some kind of eldritch horror like Cthulhu There are a few big names out there right now I don't particularly agree with them. So I'm not gonna call anyone out but There are people that think that you know, we're all going to die. It's inevitable. Just give it. Give up now. Throw in the towel. So, rather than give this phenomenon a big, spooky, scary name that makes it sound like Cthulhu, let's break down the characteristics of Moloch into more conventional terms. So specifically, we're going to talk about market theory and game theory and describe the Moloch in those terms. So first is perverse incentives. So a perverse incentive is a systemic or structural rule or paradigm that creates behaviors that run contrary to the intended goals or desired states. For instance, with social media, the perverse incentive is that you end up doom scrolling, which makes you, you wanted to use social media to get happier, but you end up doom scrolling because the system incentivizes that behavior, which results in more anxiety, depression, rage, and so on. So perverse incentives also exist in the wide world, dealing with like corn subsidies, oil subsidies, all sorts of stuff. If you want more examples, just Google it. Like there's thousands and thousands of examples of perverse incentives. It extends into education, healthcare, all kinds of stuff. Market externalities. So market externality is a situation where a market behavior does not price in, or the market price does not reflect the true and total cost or benefit of something. So in some cases, there are positive market externalities. For instance, the cost of vaccination or public health campaigns is often much lower than the overall benefit. You get knock-on positive effects. Now that being said, there are also negative market externalities such as pollution and environmental degradation. In other words, the cost of cutting down a tree and selling that tree is much lower than the total cost of the impact to the environment. But because the environment is so huge and it is a large dynamic system, it is difficult to price that in without regulations and other things. So perverse incentives and market externalities, these are market theory concepts that contribute to the concept of the MOLOC. That's not the whole picture. An undesirable Nash equilibrium is a situation where no stakeholder or participant is incentivized to alter their behavior. In other words, they are using their optimal strategy and yet though everyone is using their own optimal strategy, it will still result in a net loss or undesirable outcomes for all participants anyways. So basically dystopia. And then finally an undesirable attractor state, which is the ultimate steady state or stable state that a system will result in, given the existing structures and rules, even though if it's an undesirable attractor state, it's an outcome that nobody really wants, even if that outcome seems inevitable. So again, like I said, I don't particularly like the term Moloch because it's big and spooky and scary, but it is a useful shorthand to basically say the set of perverse incentives and market externalities and everything else that goes into the market theory, economic theory and game theory of this, of any system could be negative. So it's basically the monster. Okay, so I've talked a lot about incentives and constraints. And so what I did was I worked to identify all of the kind of groups or the categories of stakeholders and also to elucidate their incentives and constraints. And keep in mind that the slide deck is a very concise shorthand for the paper that I'm working on. So but anyways, corporations, their primary incentive is to maximize profit and their biggest constraint is the law regulations, so on and so forth. For the military, they want to maximize their firepower and their biggest constraint is geopolitics, aka their military competitors as well as political constraints. For governments, governments have a multipolar set of incentives, right? They might want to maximize tax revenue, but they also want to maximize certain demographic priorities, economic priorities, GDP, so on and so forth. So governments have multipolar incentives, and the constraint is actually part of the incentive structure, which is the citizenry. Citizens have certain limits, right? We can only work so much, we can only have so much output. And another major constraint for governments is the natural resources of the land that they control. And then for individuals, we all want to maximize self-interest. This is an accepted paradigm in economic theory today, but our constraints are multipolar. Our constraints are time in the day, physical energy, food, money, the reach of our individual connections in our networks, so on and so forth. So we individuals have the most open-ended incentive, but we also have the most constraints. So this is just one way to think about, okay, all of the stakeholders in the entire globe have these different incentives and constraints, and we're all playing on the same stage, which is planet Earth. So given how big and dynamic the world is, it's not really possible to achieve a true Nash equilibrium, because by the time something happens in one area and all the effects are fully known and it's fully embedded into the market, the situation will have changed. That being said, there are large forces that are pushing us towards certain equilibrium. So for instance, the justice system disincentivizes certain behaviors like theft and murder to get what you want, and so part of our equilibrium, our individual Nash equilibrium, is that we pay our taxes, we don't kill, we don't steal, etc. etc. because it does not benefit us to deviate from that strategy. Likewise, corporations fall into Nash equilibrium where, and large they don't abuse their employees within reason, they don't abuse the environment within reason, they don't engage in theft and corruption within reason. Again, the constraints are there, but corporations are constantly testing their boundaries. But by and large corporations will play within the rules that are given to them. So because when you look at that, ditto for governments and militaries, because we are all operating with our incentives, our intrinsic motivations, or our incentives as well as those constraints, we all kind of fall into an optimal strategy. Now that being said, the optimal strategy for all of the stakeholders globally is presently still moving us towards dystopia, towards the attractor state, the undesirable attractor state of dystopia. However, that being said, we all want to move towards utopia, right? And this has happened plenty of times in the past. The Roman Empire collapsed. Plenty of other empires have collapsed, even though nobody, well, many people didn't want it, but some people did. And one thing I do want to add as a caveat is a lot of this is a huge oversimplification. I've spent basically the last 36 hours almost straight, except for sleeping, learning about this stuff because I realized how important it was. So changing the ultimate attractor state, changing it from the current dystopic trajectory that we're on to a more utopic trajectory requires structural changes to the whole system. Basically don't hate the player, change the game. Okay so I've mentioned this a couple times, added this slide in just in case you're not familiar with the Nash equilibrium. The TLDR of the Nash equilibrium is that you assume that all players in a game are rational and they choose the best strategy given the rules of the game and the behavior of the other players. The idea is that a Nash equilibrium is a stable outcome in which no player will benefit from changing their strategy. Now, that being said, you can have a desirable Nash equilibrium where everyone is cooperative and everyone is benefiting, or you can have a negative Nash equilibrium where basically everyone loses. And then you can also have a zero-sum game where you have winners and losers. So the very, very oversimplified TLDR is a Nash equilibrium can result in a win-win, a lose-lose, or a win-lose. Right now it looks like the Nash equilibrium of the whole world is heading towards lose-lose. Some people believe that is intrinsically win-lose, that there's winners and losers. I personally believe that we can head towards a win-win situation and I think that when people are being honest, most people want a win-win situation. It's just there's a sense of fatalism or a belief that it's not possible. And so if you honestly believe that win-win is not possible, then maybe you default to win-lose where, well, I don't mind if everyone else loses as long as I win. But what I'm going to try and do is help you understand and help the world move towards a belief and adoption of a win-win mentality. Okay so there are a couple of existing mitigation strategies that people are trying to use to avoid the dystopic or extinction outcome. So you know whether when you're looking at it in terms of attractor states, dystopia is one where basically everyone is miserable, right, or outright extinction. That's another possible attractor state because if humans go extinct then the world returns to stability without us, right. So it? So it would be irresponsible to say that neither of those outcomes are possible or likely. I'm not going to comment on how likely they are, but what I will say is that they are both possible. And right now, as I mentioned in the last slide, people believe that utopia is just not possible, so why even go for it? So mutually assured destruction is an example of an equilibrium, right? So an equilibrium where, hey, we all have the ability to kill each other so nobody make a move. And what was the movie? The the one with Brad Pitt where they're in Nazi Germany, you know, called it a Mexican standoff. Actually I probably shouldn't have said that. That's probably an offensive term. Anyways, mutually assured destruction, it's a well-known doctrine where basically there's a nuclear buildup on both sides so nobody pulls the trigger. That's on the military and geopolitical stage. In terms of capitalism and market theory, the current paradigm that is popular is called stakeholder capitalism. So stakeholder capitalism is the idea that rather than just trying to... it replaces shareholder capitalism. So shareholder capitalism prioritizes only the shareholders and their desires, which forces corporations to maximize profit at the expense of everything else. With stakeholder capitalism, the idea is to basically treat the entire world as your stakeholders, which includes private citizens that are not your customer, the employees, all over the world, governments, as well as the environment. So this is called ESG. This is promoted by BlackRock, which is Environmental, Social, and Governance. So that's basically a litmus test that BlackRock uses for investment. And then a more general way of looking at this is called the triple bottom line theory or doctrine, which basically says that on top of economic incentives you should also include environmental and social incentives or considerations. But all of these are broadly types of stakeholder capitalism. So both of these doctrines or ideas attempt to create a more desirable Nash equilibrium. So in the case of mutually assured destruction the equilibrium is we will we will maintain a nuclear arsenal but we won't use it. That is the optimal strategy. In the case of stakeholder capitalism the idea is we will adopt a broad array of behaviors that mean that we don't abuse employees, suppliers, or the environment while still making profit. That is the goal. Now I will say that both of these have very very deep flaws which would take many many videos to unpack but you know I think you get the idea. These are the current attempts that are stable-ish right now and working-ish right now but might also still be pushing us towards a dystopian outcome even if they are currently stable enough. Now, technology as a destabilizer. Technological leaps have always destabilized the system, starting with the printing press which led to religious and economic and political upheaval, looking at you, Martin Luther and French Revolution, then the Industrial Revolution, which led to huge social upheaval with urbanization, factories, and the dislocation of many jobs, which the Industrial Revolution also contributed directly to World War I and II because those were the first industrial-scale wars. Nuclear weapons, internet, silicon, all of the above lead to destabilization. AGI or autonomous AI systems, no different. It's just another technological leap that will call it that will destabilize everything again. And it's pretty much a foregone conclusion that the advancement of AI is going to destabilize stuff. So this forces us to ask questions. What is the new attractor state? If, well in the past the attractor state was different because you know technological abilities to affect the world were limited. Right? When the world was powered by coal, there was only so much damage we could do to each other and the world. But as technology advanced, the amount of damage possible went up. So the new attractor state also changed, as well as all the incentives of participants in the world. And that includes employers, individuals, governments, militaries, so on. Technology changes the game, changes the fundamental nature of the game of life or reality or however you want to call it. And so the question is, okay, with the rise of AGI how does that change the attractor state? And there's, as far as I can tell, there's basically three states. There's utopia, dystopia, and extinction. There's probably a lot of gray area in between and there might be a fourth kind of state that we're heading towards, but in terms of useful shorthand, utopia, dystopia, and extinction. So the follow-up question is what can we do to alter that attractor state? Is there anything that we can do structurally or systematically to favor one of those outcomes over another? And then finally, what is the optimal strategy for each of those kinds of stakeholders that I mentioned, individuals, corporations, governments, and militaries, to create a new Nash equilibrium in light of AGI. So basically we need a Nash equilibrium framework for implementing AGI to to push us towards a desirable or positive attractor state. So all that is a really complex way of saying we need a plan. We need a plan of implementing AGI in such a way that we will trend towards utopia rather than dystopia or extinction. Okay, so with all that in mind, what are some of the success criteria for this framework? What are the goals of this framework? How do we know if this framework is going to be successful? One, it needs to be easy to implement and understand. The reason is because the ability for individuals at all levels, whether it's individual persons like myself or corporations or even small nations, to implement AGI is ramping up. I was on Discord last night and there are people that just after tinkering for a few weeks have created fully autonomous AI systems. And one of the things that we discussed was, okay, if me or you or whoever, some of these people are not even coders, they learn to code with chat GPT. If everyone is going to be capable of creating autonomous AI systems now, and it's only going to ramp up over the coming months and years, then whatever framework that we come up with is going to have to be universally understandable, easy to implement, and easy to understand. If it's esoteric, no one will use it because they won't understand it. Number two, all stakeholders have to be incentivized to use this framework, or in other words, this framework must represent the optimal strategy so that people won't deviate from it. There's basically, everyone has to benefit from using it and there has to be compounding returns incentivizing everyone to say, hey, you should be using this framework because this is the optimal strategy for everyone. Above and beyond that, this framework needs to be adaptable and responsive or dynamic because again, the world changes and so it's really difficult to create a framework that is a hard set of rules to follow, which will result in unintended consequences and instabilities and other market failures. So it has to be context-dependent and changeable over time. Number four, this framework has to be inclusive and representative in that it cannot exclude any stakeholder. It cannot exclude any citizens from any nation or religion. It cannot exclude any corporation or government or military because like it or not we all share the same planet and we are all stakeholders in this outcome. And one thing that I want to address is that there have been cases where nations agree on like rules of engagement and rules of war. Like, we don't use napalm anymore because it was decided that, like, okay, this is inhumane. Or maybe it was white phosphorus. Anyways, there's certain kinds of weapons. Mustard gas. Those are things that, even though nations might go to war with each other, they still agree not to do certain things because they understand that the soldiers are stakeholders as well as the citizens who might get caught in the crossfire. So there is some precedent of nations agreeing on how to conduct war even though destruction is one of the goals of war. Number five, this framework has to be scalable and sustainable. It has to include the entire globe as well. And that's not just the people on the globe, it has to include the entire globe as well and that's not just the people on the globe it has to include the environments and ecosystems around the globe which we all depend on anyways. So I personally see humans as part of the ecosystem not separate from it. And finally this framework has to be transparent and trustworthy because perception is reality right? If people perceive a framework to be destructive, like ESG is a perfect example, the perception of ESG is awful. Why? Because it's championed by BlackRock, which is one of the most, I think it is the wealthiest company on the planet, right? And so because ESG is championed by a multi-trillion dollar corporation, it is not trusted and that perception makes it bad. I don't know whether or not ESG is good or bad, but the perception certainly is bad. So transparency and trustworthiness are critical for the success of this framework because if people don't trust it, they're not gonna use it either. And finally, so this is where I pitch my work. So my proposed solution to all of this is what I call the heuristic imperatives, which is a set of rules or principles that can be incorporated into AGI systems that will push it into this direction. And so these imperatives are one, reduceuce suffering in the universe, 2. Increase prosperity in the universe, and 3. Increase understanding in the universe. One way to say this is that it is a multi-objective optimization problem, meaning that it's not just one objective function, it's actually three that the AGI has to work on implementing. So in the last video, people asked how do you implement these. It's actually really really easy. You can just plug them into chat GPT and talk about it. There's a few places that you can get involved in the conversation. One is on Reddit. I created a new subreddit called r slash heuristic comparatives. People are sharing their work there so if you want to see the discussion jump in on that. I also have a lot of my own work up on GitHub, including a few papers that I have written and am working on under github.com slash Dave Schapp slash Here's to Comparatives. And then finally, the most active community to discuss this stuff is the Cognitive AI Lab Discord server, which I started over a year ago, and links to all this is in the description of the video. So because of that, I don't want to spend too much time rehashing stuff but I just wanted to connect to the conversation because again transparency and trustworthiness are really critical to this solution. But let's talk more broadly about this solution of the heuristic comparatives and these success criteria. So we outlined six success criteria for a framework that will push us towards a positive Nash equilibrium or a desirable attractor state, aka utopia. So the heuristic comparatives, as I mentioned, are very easy to implement. You can put them in the chat GPT system window. You can just include them in the conversation. You can use them for evaluation, cognitive control, historical self-evaluation, planning, prioritization. Super easy to implement. And as I mentioned, lots of people are having the discussions. Some of the autonomous AI entities that people have created, the AIs that they created actually end up usually being really fascinated by the heuristic imperatives and they kind of gravitate towards them saying like, oh yeah, this is my purpose. So it's really interesting to watch that work unfold. Number two, the stakeholders are all incentivized to use the heuristic imperatives because just imagining a state where you have less suffering, more prosperity, and more understanding is beneficial. Now above and beyond that, all stakeholders are incentivized to use the heuristic comparatives because then you have a level set playing field where you know that everyone is abiding by the same rules. Because when you have a game, imagine the game Monopoly. If someone is playing by a different set of rules, right? Because when you have a game, imagine the game Monopoly. If someone is playing by a different set of rules, you're not going to play with them, right? Even though it's a competition, you still say we're going to abide by the same rules, you collect $200 when you pass Go. If, on the other hand, everyone is playing by the heuristic imperatives, then you will be incentivized to adhere to those rules, knowing that the net effect is going to be beneficial for everyone. Number three, the heuristic imperatives are adaptable because they intrinsically incentivize learning and adaptation with the third heuristic imperative of increased understanding. This is what I also call the curiosity function. So basically, you don't want an AGI to be dumb and just satisfied with what it knows about the universe. You also don't want it to be satisfied with human ignorance. So by increasing understanding that includes that one that intrinsically makes AGI's curious, which means that they are going to want to learn and challenge their own beliefs, but likewise they will also encourage, not force, but encourage humans to learn and adapt. So the heuristic imperatives as a system is intrinsically adaptable because learning and curiosity are baked in. Number four, the heuristic imperatives are inclusive. Now in all the experiments that I've done going back to GPT-3 and now GPT-4, the language models already understand the spirit or the intention of the heuristic imperatives in that they should be all-inclusive. And so that makes them very, very context dependent. So for instance, if you plug in the heuristic imperatives to chat GPT and ask it about religion, it will advocate for tolerance and creating space for people to explore religion on their own. And if you further unpack that, chat GPT and going back to GPT-3 will say that things like individual autonomy is actually really important for humanity to thrive. They're scalable. The here's the comparatives. It used to just be very simply reduce suffering, increase prosperity and increase understanding. But I established the scope of in the universe because that preemptively answers a lot of questions because it's not just a matter of, okay, let's just look at Earth or let's just look at one nation. Let's consider the entire universe. So that is the scope of the imperatives. So it's not just global, it is universal. And then finally, the heuristic imperatives encourage transparency because they incentivize open communication, trust, and autonomy. But above and beyond that, they are transparent in that if everyone abides by them, everyone knows that everyone is playing by the same rules. Now, that being said, in the previous video I did address the Byzantine generals problem, which is that you might have agents in the system that are either defective, faulty, or malicious. And this is also addressed by the heuristic imperatives because what you will do is you will detect when an agent is not playing by the rules and you will track that and we'll talk about that in just a moment. So the positive Nash equilibrium that the heuristic imperatives encourage have four basic criteria that I was able to think of. One is mutual benefits. It is mutually beneficial if all agents in the system or all participants in the system adhere to the heuristic imperatives, meaning that the rising tide lifts all boats. If we all work to reduce suffering, if we all work to increase prosperity, and we all work to increase understanding, then we all benefit and we get compounding returns. Trust and reputation. So having shared goals and transparency is a natural result of the heuristic imperatives as I just mentioned. Resilience and cooperation. So this is an interesting outcome which is that for an equilibrium to be reached it has to be stable. And so the heuristic imperatives create a resilient system in which there's gonna be mutual policing as well as some self-correcting behaviors which we'll unpack more in a slide or two. But it is resilient because it encourages collaboration and cooperation as well as self-regulation and policing. And then finally, ultimately, long-term stability, that is the entire point of a Nash equilibrium and a desirable attractor state is one that is stable. You don't want chaos or instability in the future. So one thing that is becoming apparent, especially as I watch the landscape change, if you look at auto GPT, all kinds of people are going to be building their own autonomous systems. And so what we're creating is a decentralized AGI ecosystem. And so when this happens, when everyone can create an AGI with their own goals, with their own imperatives, with their own design and their own flaws, you're going to end up with a really, really kind of Wild West, dystopian, chaotic world. So, one way to mitigate this decentralization drift is to adhere to the heuristic imperatives. And as I mentioned, there's cooperative benefits, right? If you and everyone else you know working on autonomous AI's agrees on nothing else except the heuristic imperatives you'll have that framework in common and a lot of work will flow from that so the cooperative benefits and this is this goes between above and beyond individuals this includes corporations governments as well as militaries number two is AGI policing and self-regulation. So if you have millions of AGI's that all agree on the heuristic imperatives, even if they don't agree on anything else, they will police each other to say, hey we're gonna look out for other AGI's, rogue AGI's, that do not abide by the heuristic imperatives and we will collaborate to shut them down. And then finally, in many experiments that I've done, the heuristic imperatives result in self-regulation within the AGI. For instance, one of the things that we're afraid of is once AGI's become so powerful that they can reprogram themselves or spawn copies or reprogram each other or otherwise get control of their source code, that they're going to change their fundamental programming. If you make the assumption that an AGI can change its fundamental programming, spin up alternative copies of itself, then you completely have lost control. However, in my experiments with the heuristic imperatives, AGIs will shy away from creating copies of themselves or even modifying their own core programming out of fear of violating the heuristic imperatives. And so between policing each other and self-regulation, the heuristic imperatives create a very powerful self-correcting environment. Reputation management, number three, is another thing where, as I mentioned, the AGIs will work to infer the objectives of all other AGIs, whether or not it's known. This goes back to the Byzantine generals problem where you don't know how another AGI is programmed. They might have used the heuristic imperatives, but they might have been improperly implemented. Or you might have rogue elements that are created without the heuristic imperatives or other objectives that are more destructive. And then finally, stakeholder pressure. Between the four categories of stakeholders that I already illustrated, which is individuals, corporations, governments, and militaries, AGIs are going to be another stakeholder. Now whether or not you believe that they're conscious or sentient or have rights, I don't really think that's relevant because they will be powerful entities in and of themselves before too long. And so between those five types of stakeholders, there will be intra and inter group pressure to conform and adhere to the heuristic imperatives. Okay, so let's describe assuming all this works out and assuming that I'm right and assuming that I'm not crazy and that the trends continue and people are gonna keep building the AGI's that they're working on, what characteristics can we use to describe this desirable attractor state or utopia? So one is universal health and well-being. With a few exceptions of people that are stuck in self-destructive patterns, all humans want health and wellness. That's pretty much a given. Number two, again with a few outliers, people want environmental restoration and sustainability. Number three, individual liberty and personal autonomy. This is an intrinsic psychological need for all humans. Number four, knowledge and understanding. Curiosity and learning are universally beneficial, which is why education is one of the primary goals of nations and and unions of nations such as the United Nations, European Union, and so on. And then finally, peaceful coexistence. Nobody wants war and chaos. Some people think it's cool, you know, watching Lex Friedman talk to various people. They're like, oh, yeah, there is something attractive about thinking about Lex Friedman talked to various people, they're like, oh yeah, there is something attractive about thinking about catastrophe and cataclysm. We keep making disaster movies, for instance, but in terms of how we actually want to live, we all want peaceful coexistence. And so this desirable attractor state, a shorthand, is utopia. Now, I know I've painted a very rosy picture, as well as presented some challenges. So there are still a few challenges remaining that we need to address. And so one of those is misalignment and drift. So even with the heuristic imperatives, there might still be drift or misalignment, intentionally or otherwise. It could be that there's flaws in the implementation, the code, or maybe someone breaks them or says, hey, I'm going to do an experiment by deleting one of the heuristic imperatives. That could destabilize the system. Second, there can be unintended consequences. So one thing that it seems like it will inevitably happen is that AGI systems are going to outstrip and outpace human intellect. If that becomes the case, and they might also adopt other languages, right? Right now, most of them communicate in English because English is the bulk of the training data. But, you know, for instance, what if the AGIs ultimately communicate with a language that we cannot comprehend or understand, like binary or vectors or something else and then we can't even monitor what they're doing. What my hope is that the AGI's as part of being trustworthy and transparent will choose to continue to communicate exclusively in English. But that we can't assume that that will be true. Number three, concentration of power. Now, I did talk about how I believe that the heuristic imperatives will create an incentive structure that results in sharing of power, transparency, so on and so forth. That being said, there is still a tremendous amount of desire to concentrate power. And especially on the geopolitical stage, there are nations out there with mutually exclusive goals. And as long as nations exist with mutually exclusive goals or incompatible visions of how the planet should be, there will be concentrations of power, and those concentrations of power will be pitted against each other. So that is not something that the heuristic imperatives intrinsically address, but that is a reality of what exists today which can destabilize the system. So in the long run, I think part of the ideal state, the Nash equilibrium, is that power is not concentrated anywhere, but we need to overcome several major barriers as a species before we can achieve that. Number four is social resistance. Public skepticism, mistrust, and ignorance is one of the greatest enemies right now, which is why I am doing this work, which is why I chose YouTube as my primary platform to disseminate my information. Number five, malicious use. Again, as long as there are malicious actors, there might be deliberate deployments of AGI that are harmful which could destabilize the system. And finally, I do need to address this as well. The heuristic imperatives are a necessary foundation of this utopic outcome, this beneficial, sorry, attractive state that we're looking for, but they do not represent a complete solution. There are a few other things that are needed in order to achieve this outcome. One, collaboration and open dialogue. So, researchers, individuals, corporations, and governments all need to work together at a global scale. Anything short of global collaboration and cooperation could very well result in a negative outcome. And this is one of the things that Liv and other people talk about when talking about Moloch, is that it is a, what do they call it? I think a collaboration failure or a signal failure. I can't remember exactly how they describe it. But essentially collaboration is the antidote and open dialogue is the antidote to the ignorance and other negative signals and noise that contribute to the Moloch problem. Number two is regulatory frameworks and oversight. Again, it's not just a matter of coming together, it is that there are institutional changes that need to happen, such as legislation, councils, and summits, and other kinds of meetings and investments that need to happen at an institutional level, not just communication and dialogue, but the frameworks, the oversights, those also need to be implemented. Number three, education and awareness. As I just mentioned, public awareness and understanding is presently insufficient to overcome the negative attractor states that we're heading towards. And number four, continuous monitoring and improvement. This is not a solution that we solve once. It is an ongoing thing, just like how you don't just pass internet regulations and then you're done, you go home forever. You continuously monitor the changing dynamic environment so that you can course correct as you go. That is going to be necessary forever with AGI. It's not going to go away. Just like the EPA, the Environmental Protection Agency, didn't just create a set of guidelines and we're done, they pack it up. No, the EPA continuously does stress tests and pressure tests and measurements all over the nation to make sure that the policies are effective. And then of course as they gain more information, those policies change. We will need the same kind of vigilance applied to AGI systems and the AGI ecosystem. Okay, so that was a lot. Thank you for watching. That's about all I have today. Not that this was not much, but thanks anyways. And yeah, I hope that this helped and I hope that it gives you a little bit more confidence in the direction that we're going. Thanks. confidence in the direction that we're going. Thanks.", "chunks": [{"timestamp": [0.0, 2.04], "text": " Morning, everybody."}, {"timestamp": [2.04, 4.12], "text": " David Shapiro here with another video."}, {"timestamp": [4.12, 7.72], "text": " Today's video is going to be really exciting."}, {"timestamp": [7.72, 13.96], "text": " We're going to discuss the Moloch problem, otherwise known as undesirable Nash equilibria"}, {"timestamp": [13.96, 22.06], "text": " and attractor states, or to put it more simply, dystopia or extinction in the context of artificial"}, {"timestamp": [22.06, 23.48], "text": " general intelligence."}, {"timestamp": [23.48, 29.4], "text": " All right, so first we probably need to define Moloch."}, {"timestamp": [29.52, 32.08], "text": " This is a concept that has been popularized"}, {"timestamp": [32.08, 33.84], "text": " by the likes of Liv Bowrey."}, {"timestamp": [33.84, 35.04], "text": " I'm probably saying her name wrong."}, {"timestamp": [35.04, 36.72], "text": " She was on Lex Friedman."}, {"timestamp": [37.64, 40.86], "text": " And also a lot of people reacted pretty positively"}, {"timestamp": [40.86, 44.0], "text": " to my last video, AGI Unleashed."}, {"timestamp": [44.9, 48.48], "text": " And so following the trend and the conversation,"}, {"timestamp": [48.48, 50.36], "text": " of course there's lots of people out there"}, {"timestamp": [50.36, 55.24], "text": " talking about the alleged inevitability"}, {"timestamp": [55.24, 58.1], "text": " of these negative outcomes."}, {"timestamp": [58.1, 61.64], "text": " So Moloch, to put it very simply,"}, {"timestamp": [62.44, 69.24], "text": " is a situation where the system itself, the rules, structures,"}, {"timestamp": [69.24, 74.12], "text": " incentives, and constraints of a system intrinsically and inevitably flow"}, {"timestamp": [74.12, 80.32], "text": " towards undesirable lose-lose states or negative Nash equilibria. It was inspired"}, {"timestamp": [80.32, 86.2], "text": " by a demon that demands sacrifices and it creates a vicious cycle of more sacrifices."}, {"timestamp": [86.2, 91.52], "text": " So a few examples of the Moloch, and I put it in scare quotes because I don't particularly"}, {"timestamp": [91.52, 94.44], "text": " like the term even though it is useful."}, {"timestamp": [94.44, 99.88], "text": " So social media is one example of the Moloch and if you want to know more about that, watch"}, {"timestamp": [99.88, 104.04], "text": " Liv Bowry's videos about media and social media."}, {"timestamp": [104.04, 105.94], "text": " They're pretty short short they're about 15"}, {"timestamp": [105.94, 106.86], "text": " 20 minutes each."}, {"timestamp": [107.66, 109.9], "text": " But basically social media is"}, {"timestamp": [110.1, 111.76], "text": " pretty universally harmful."}, {"timestamp": [112.34, 114.0], "text": " It does very few good things"}, {"timestamp": [114.02, 115.82], "text": " and yet people continue to use it."}, {"timestamp": [115.84, 116.66], "text": " They're addictive"}, {"timestamp": [117.38, 119.38], "text": " and it's just a monster that keeps wanting"}, {"timestamp": [119.38, 121.26], "text": " to eat more and more of your time"}, {"timestamp": [122.02, 123.94], "text": " and it is not particularly"}, {"timestamp": [123.94, 124.42], "text": " helpful."}, {"timestamp": [125.4, 130.6], "text": " That being said we keep using it because there are a few benefits of social media."}, {"timestamp": [130.6, 131.6], "text": " For instance, YouTube."}, {"timestamp": [131.6, 137.8], "text": " YouTube is a form of social media, but the cost to benefit signal is pretty bad."}, {"timestamp": [137.8, 143.0], "text": " Another example of the Moloch is the example of arms races, whether it's nuclear proliferation,"}, {"timestamp": [143.0, 145.56], "text": " bioweapons, other weapons of mass destruction,"}, {"timestamp": [145.56, 147.32], "text": " and so on and so forth."}, {"timestamp": [147.32, 152.22], "text": " Basically nobody really wants to live in a world where there are thousands and thousands"}, {"timestamp": [152.22, 157.4], "text": " of nuclear weapons and bioweapons and other weapons of mass destruction, yet we live in"}, {"timestamp": [157.4, 164.32], "text": " that world because of the incentive structure and the technology basically makes it an inevitability."}, {"timestamp": [164.32, 169.12], "text": " And when you live in the world where we are literally like a few button pushes away from"}, {"timestamp": [169.12, 173.72], "text": " the destruction of the entire human race, that is not a good situation to be in."}, {"timestamp": [173.72, 179.96], "text": " And then finally, most commonly, the tragedy of the commons, which is basically you end"}, {"timestamp": [179.96, 186.72], "text": " up with environmental depletion and destruction due to the incentives to exploit the environment"}, {"timestamp": [186.72, 187.96], "text": " for a number of reasons."}, {"timestamp": [187.96, 192.72], "text": " And we'll talk about Moloch in more objective terms in just a moment."}, {"timestamp": [192.72, 197.36], "text": " But you might have noticed that none of my videos have ads and that is because my videos"}, {"timestamp": [197.36, 201.4], "text": " are all sponsored by you, my Patreon supporters."}, {"timestamp": [201.4, 206.3], "text": " So if you like what I'm working on, you want to incentivize my behavior, then"}, {"timestamp": [206.3, 212.3], "text": " please support me financially on Patreon. Go ahead and jump over. If you go to a higher"}, {"timestamp": [212.3, 218.34], "text": " tier, I'm happy to chat with you on an individual basis, either via Patreon chat. I'll even"}, {"timestamp": [218.34, 225.3], "text": " hop on video calls. So yeah, that is the plug plug and moving right back into the show"}, {"timestamp": [226.2, 234.88], "text": " Okay. So when you listen to people talk about Malik, it sounds like some kind of eldritch horror like Cthulhu"}, {"timestamp": [235.76, 238.5], "text": " There are a few big names out there right now"}, {"timestamp": [239.2, 243.24], "text": " I don't particularly agree with them. So I'm not gonna call anyone out but"}, {"timestamp": [243.8, 247.0], "text": " There are people that think that you know, we're all going to die."}, {"timestamp": [247.0, 248.0], "text": " It's inevitable."}, {"timestamp": [248.0, 249.0], "text": " Just give it."}, {"timestamp": [249.0, 250.0], "text": " Give up now."}, {"timestamp": [250.0, 251.0], "text": " Throw in the towel."}, {"timestamp": [251.0, 257.4], "text": " So, rather than give this phenomenon a big, spooky, scary name that makes it sound like"}, {"timestamp": [257.4, 263.56], "text": " Cthulhu, let's break down the characteristics of Moloch into more conventional terms."}, {"timestamp": [263.56, 265.22], "text": " So specifically, we're going to talk"}, {"timestamp": [265.22, 271.86], "text": " about market theory and game theory and describe the Moloch in those terms."}, {"timestamp": [271.86, 280.14], "text": " So first is perverse incentives. So a perverse incentive is a systemic or"}, {"timestamp": [280.14, 285.96], "text": " structural rule or paradigm that creates behaviors that run contrary to the intended"}, {"timestamp": [285.96, 288.64], "text": " goals or desired states."}, {"timestamp": [288.64, 293.32], "text": " For instance, with social media, the perverse incentive is that you end up doom scrolling,"}, {"timestamp": [293.32, 297.36], "text": " which makes you, you wanted to use social media to get happier, but you end up doom"}, {"timestamp": [297.36, 303.76], "text": " scrolling because the system incentivizes that behavior, which results in more anxiety,"}, {"timestamp": [303.76, 306.3], "text": " depression, rage, and so on."}, {"timestamp": [306.3, 310.84], "text": " So perverse incentives also exist in the wide world,"}, {"timestamp": [310.84, 313.72], "text": " dealing with like corn subsidies, oil subsidies,"}, {"timestamp": [313.72, 315.4], "text": " all sorts of stuff."}, {"timestamp": [315.4, 317.32], "text": " If you want more examples, just Google it."}, {"timestamp": [317.32, 319.92], "text": " Like there's thousands and thousands of examples"}, {"timestamp": [319.92, 321.06], "text": " of perverse incentives."}, {"timestamp": [321.06, 325.48], "text": " It extends into education, healthcare, all kinds of stuff."}, {"timestamp": [325.48, 331.96], "text": " Market externalities. So market externality is a situation where a market behavior does"}, {"timestamp": [331.96, 338.58], "text": " not price in, or the market price does not reflect the true and total cost or benefit"}, {"timestamp": [338.58, 350.68], "text": " of something. So in some cases, there are positive market externalities. For instance, the cost of vaccination or public health campaigns is often much lower than"}, {"timestamp": [350.68, 352.24], "text": " the overall benefit."}, {"timestamp": [352.24, 354.24], "text": " You get knock-on positive effects."}, {"timestamp": [354.24, 358.58], "text": " Now that being said, there are also negative market externalities such as pollution and"}, {"timestamp": [358.58, 360.42], "text": " environmental degradation."}, {"timestamp": [360.42, 365.24], "text": " In other words, the cost of cutting down a tree and selling that tree is much lower than"}, {"timestamp": [365.24, 368.88], "text": " the total cost of the impact to the environment."}, {"timestamp": [368.88, 373.64], "text": " But because the environment is so huge and it is a large dynamic system, it is difficult"}, {"timestamp": [373.64, 378.92], "text": " to price that in without regulations and other things."}, {"timestamp": [378.92, 384.04], "text": " So perverse incentives and market externalities, these are market theory concepts that contribute"}, {"timestamp": [384.04, 385.6], "text": " to the concept of the"}, {"timestamp": [385.6, 392.64], "text": " MOLOC. That's not the whole picture. An undesirable Nash equilibrium is a situation where no"}, {"timestamp": [393.52, 397.44], "text": " stakeholder or participant is incentivized to alter their behavior. In other words,"}, {"timestamp": [397.44, 403.04], "text": " they are using their optimal strategy and yet though everyone is using their own optimal strategy,"}, {"timestamp": [403.04, 408.0], "text": " it will still result in a net loss or undesirable outcomes for all participants anyways."}, {"timestamp": [409.0, 411.0], "text": " So basically dystopia."}, {"timestamp": [411.0, 414.0], "text": " And then finally an undesirable attractor state,"}, {"timestamp": [414.0, 418.0], "text": " which is the ultimate steady state or stable state that a system will result in,"}, {"timestamp": [418.0, 421.0], "text": " given the existing structures and rules,"}, {"timestamp": [421.0, 426.64], "text": " even though if it's an undesirable attractor state, it's an outcome that nobody"}, {"timestamp": [426.64, 430.7], "text": " really wants, even if that outcome seems inevitable."}, {"timestamp": [430.7, 433.96], "text": " So again, like I said, I don't particularly like the term Moloch because it's big and"}, {"timestamp": [433.96, 439.8], "text": " spooky and scary, but it is a useful shorthand to basically say the set of perverse incentives"}, {"timestamp": [439.8, 445.76], "text": " and market externalities and everything else that goes into the market theory,"}, {"timestamp": [445.76, 448.4], "text": " economic theory and game theory of this,"}, {"timestamp": [448.4, 451.52], "text": " of any system could be negative."}, {"timestamp": [451.52, 453.38], "text": " So it's basically the monster."}, {"timestamp": [454.48, 459.48], "text": " Okay, so I've talked a lot about incentives and constraints."}, {"timestamp": [459.96, 463.24], "text": " And so what I did was I worked to identify"}, {"timestamp": [463.24, 466.56], "text": " all of the kind of groups or the categories of stakeholders"}, {"timestamp": [467.4, 470.64], "text": " and also to elucidate their incentives and constraints."}, {"timestamp": [470.64, 476.8], "text": " And keep in mind that the slide deck is a very concise shorthand for the paper that I'm working on."}, {"timestamp": [477.6, 482.92], "text": " So but anyways, corporations, their primary incentive is to maximize profit"}, {"timestamp": [483.16, 487.24], "text": " and their biggest constraint is the law regulations, so on and so forth."}, {"timestamp": [487.24, 493.68], "text": " For the military, they want to maximize their firepower and their biggest constraint is geopolitics,"}, {"timestamp": [493.68, 500.0], "text": " aka their military competitors as well as political constraints."}, {"timestamp": [500.0, 505.0], "text": " For governments, governments have a multipolar set of incentives, right?"}, {"timestamp": [505.0, 516.0], "text": " They might want to maximize tax revenue, but they also want to maximize certain demographic priorities, economic priorities, GDP, so on and so forth."}, {"timestamp": [516.0, 525.16], "text": " So governments have multipolar incentives, and the constraint is actually part of the incentive structure, which is the citizenry."}, {"timestamp": [525.16, 527.44], "text": " Citizens have certain limits, right?"}, {"timestamp": [527.44, 531.12], "text": " We can only work so much, we can only have so much output."}, {"timestamp": [531.12, 533.6], "text": " And another major constraint for governments"}, {"timestamp": [533.6, 537.36], "text": " is the natural resources of the land that they control."}, {"timestamp": [537.36, 538.46], "text": " And then for individuals,"}, {"timestamp": [538.46, 540.22], "text": " we all want to maximize self-interest."}, {"timestamp": [540.22, 548.24], "text": " This is an accepted paradigm in economic theory today, but our constraints are multipolar."}, {"timestamp": [548.24, 555.76], "text": " Our constraints are time in the day, physical energy, food, money, the reach of our individual"}, {"timestamp": [555.76, 561.52], "text": " connections in our networks, so on and so forth. So we individuals have the most open-ended"}, {"timestamp": [561.52, 565.0], "text": " incentive, but we also have the most constraints."}, {"timestamp": [565.0, 569.84], "text": " So this is just one way to think about, okay, all of the stakeholders in the entire globe"}, {"timestamp": [569.84, 574.28], "text": " have these different incentives and constraints, and we're all playing on the same stage, which"}, {"timestamp": [574.28, 576.78], "text": " is planet Earth."}, {"timestamp": [576.78, 582.8], "text": " So given how big and dynamic the world is, it's not really possible to achieve a true"}, {"timestamp": [582.8, 587.16], "text": " Nash equilibrium, because by the time something happens in one"}, {"timestamp": [587.16, 592.56], "text": " area and all the effects are fully known and it's fully embedded into the market, the situation"}, {"timestamp": [592.56, 594.3], "text": " will have changed."}, {"timestamp": [594.3, 601.96], "text": " That being said, there are large forces that are pushing us towards certain equilibrium."}, {"timestamp": [601.96, 606.24], "text": " So for instance, the justice system disincentivizes certain"}, {"timestamp": [606.24, 611.36], "text": " behaviors like theft and murder to get what you want, and so part of our equilibrium,"}, {"timestamp": [611.36, 616.32], "text": " our individual Nash equilibrium, is that we pay our taxes, we don't kill, we don't steal,"}, {"timestamp": [616.32, 623.76], "text": " etc. etc. because it does not benefit us to deviate from that strategy. Likewise, corporations fall"}, {"timestamp": [623.76, 626.8], "text": " into Nash equilibrium where, and large they don't"}, {"timestamp": [626.8, 632.4], "text": " abuse their employees within reason, they don't abuse the environment within reason, they don't"}, {"timestamp": [632.4, 639.12], "text": " engage in theft and corruption within reason. Again, the constraints are there, but corporations"}, {"timestamp": [639.12, 645.12], "text": " are constantly testing their boundaries. But by and large corporations will play within the"}, {"timestamp": [645.12, 649.56], "text": " rules that are given to them. So because when you look at that, ditto for"}, {"timestamp": [649.56, 655.08], "text": " governments and militaries, because we are all operating with our incentives,"}, {"timestamp": [655.08, 658.76], "text": " our intrinsic motivations, or our incentives as well as those constraints,"}, {"timestamp": [658.76, 663.92], "text": " we all kind of fall into an optimal strategy. Now that being said, the optimal"}, {"timestamp": [663.92, 665.04], "text": " strategy for all of the"}, {"timestamp": [665.04, 670.2], "text": " stakeholders globally is presently still moving us towards dystopia, towards the"}, {"timestamp": [670.2, 674.82], "text": " attractor state, the undesirable attractor state of dystopia. However, that"}, {"timestamp": [674.82, 678.66], "text": " being said, we all want to move towards utopia, right? And this has happened"}, {"timestamp": [678.66, 682.44], "text": " plenty of times in the past. The Roman Empire collapsed. Plenty of other empires"}, {"timestamp": [682.44, 686.56], "text": " have collapsed, even though nobody, well, many people didn't want"}, {"timestamp": [686.56, 689.28], "text": " it, but some people did."}, {"timestamp": [689.28, 695.0], "text": " And one thing I do want to add as a caveat is a lot of this is a huge oversimplification."}, {"timestamp": [695.0, 700.24], "text": " I've spent basically the last 36 hours almost straight, except for sleeping, learning about"}, {"timestamp": [700.24, 703.28], "text": " this stuff because I realized how important it was."}, {"timestamp": [703.28, 708.76], "text": " So changing the ultimate attractor state, changing it from the current dystopic trajectory that"}, {"timestamp": [708.76, 714.32], "text": " we're on to a more utopic trajectory requires structural changes to the whole"}, {"timestamp": [714.32, 720.48], "text": " system. Basically don't hate the player, change the game. Okay so I've mentioned"}, {"timestamp": [720.48, 723.4], "text": " this a couple times, added this slide in just in case you're not familiar with"}, {"timestamp": [723.4, 725.44], "text": " the Nash equilibrium."}, {"timestamp": [725.44, 731.48], "text": " The TLDR of the Nash equilibrium is that you assume that all players in a game are rational"}, {"timestamp": [731.48, 736.64], "text": " and they choose the best strategy given the rules of the game and the behavior of the"}, {"timestamp": [736.64, 738.52], "text": " other players."}, {"timestamp": [738.52, 745.0], "text": " The idea is that a Nash equilibrium is a stable outcome in which no player will benefit"}, {"timestamp": [745.0, 749.36], "text": " from changing their strategy. Now, that being said, you can have a desirable Nash"}, {"timestamp": [749.36, 753.2], "text": " equilibrium where everyone is cooperative and everyone is benefiting,"}, {"timestamp": [753.2, 757.16], "text": " or you can have a negative Nash equilibrium where basically everyone"}, {"timestamp": [757.16, 760.72], "text": " loses. And then you can also have a zero-sum game where you have winners and"}, {"timestamp": [760.72, 765.42], "text": " losers. So the very, very oversimplified TLDR is a Nash"}, {"timestamp": [765.42, 771.28], "text": " equilibrium can result in a win-win, a lose-lose, or a win-lose. Right now it"}, {"timestamp": [771.28, 775.72], "text": " looks like the Nash equilibrium of the whole world is heading towards lose-lose."}, {"timestamp": [775.72, 779.44], "text": " Some people believe that is intrinsically win-lose, that there's"}, {"timestamp": [779.44, 783.82], "text": " winners and losers. I personally believe that we can head towards a win-win"}, {"timestamp": [783.82, 789.76], "text": " situation and I think that when people are being honest, most people want a win-win situation."}, {"timestamp": [789.76, 794.76], "text": " It's just there's a sense of fatalism or a belief that it's not possible."}, {"timestamp": [794.76, 799.56], "text": " And so if you honestly believe that win-win is not possible, then maybe you default to"}, {"timestamp": [799.56, 803.64], "text": " win-lose where, well, I don't mind if everyone else loses as long as I win."}, {"timestamp": [803.64, 808.96], "text": " But what I'm going to try and do is help you understand and help the world move towards a belief"}, {"timestamp": [808.96, 815.84], "text": " and adoption of a win-win mentality. Okay so there are a couple of existing"}, {"timestamp": [815.84, 821.28], "text": " mitigation strategies that people are trying to use to avoid the dystopic or"}, {"timestamp": [821.28, 827.32], "text": " extinction outcome. So you know whether when you're looking at"}, {"timestamp": [827.32, 832.48], "text": " it in terms of attractor states, dystopia is one where basically"}, {"timestamp": [832.48, 837.92], "text": " everyone is miserable, right, or outright extinction. That's another possible"}, {"timestamp": [837.92, 842.2], "text": " attractor state because if humans go extinct then the world returns to"}, {"timestamp": [842.2, 847.0], "text": " stability without us, right. So it? So it would be irresponsible to say that"}, {"timestamp": [847.0, 850.0], "text": " neither of those outcomes are possible or likely."}, {"timestamp": [850.0, 852.0], "text": " I'm not going to comment on how likely they are,"}, {"timestamp": [852.0, 854.0], "text": " but what I will say is that they are both possible."}, {"timestamp": [854.0, 857.0], "text": " And right now, as I mentioned in the last slide,"}, {"timestamp": [857.0, 861.0], "text": " people believe that utopia is just not possible,"}, {"timestamp": [861.0, 863.0], "text": " so why even go for it?"}, {"timestamp": [863.0, 865.76], "text": " So mutually assured destruction is an"}, {"timestamp": [865.76, 872.68], "text": " example of an equilibrium, right? So an equilibrium where, hey, we all have the"}, {"timestamp": [872.68, 877.72], "text": " ability to kill each other so nobody make a move. And what was the movie? The"}, {"timestamp": [877.72, 880.72], "text": " the one with Brad Pitt where they're in Nazi Germany, you know, called it a"}, {"timestamp": [880.72, 885.04], "text": " Mexican standoff. Actually I probably shouldn't have said"}, {"timestamp": [885.04, 889.12], "text": " that. That's probably an offensive term. Anyways, mutually assured destruction,"}, {"timestamp": [889.12, 893.24], "text": " it's a well-known doctrine where basically there's a nuclear"}, {"timestamp": [893.24, 898.76], "text": " buildup on both sides so nobody pulls the trigger. That's on the military and"}, {"timestamp": [898.76, 903.96], "text": " geopolitical stage. In terms of capitalism and market theory, the current"}, {"timestamp": [903.96, 907.64], "text": " paradigm that is popular is called stakeholder capitalism."}, {"timestamp": [907.64, 915.0], "text": " So stakeholder capitalism is the idea that rather than just trying to... it replaces shareholder capitalism."}, {"timestamp": [915.0, 920.0], "text": " So shareholder capitalism prioritizes only the shareholders and their desires,"}, {"timestamp": [920.0, 925.28], "text": " which forces corporations to maximize profit at the expense of everything else."}, {"timestamp": [925.28, 931.52], "text": " With stakeholder capitalism, the idea is to basically treat the entire world as your"}, {"timestamp": [931.52, 938.56], "text": " stakeholders, which includes private citizens that are not your customer, the employees,"}, {"timestamp": [938.56, 943.84], "text": " all over the world, governments, as well as the environment. So this is called ESG. This is"}, {"timestamp": [943.84, 948.96], "text": " promoted by BlackRock, which is Environmental, Social, and Governance. So that's basically a litmus"}, {"timestamp": [948.96, 954.96], "text": " test that BlackRock uses for investment. And then a more general way of looking"}, {"timestamp": [954.96, 959.98], "text": " at this is called the triple bottom line theory or doctrine, which basically says"}, {"timestamp": [959.98, 965.56], "text": " that on top of economic incentives you should also include"}, {"timestamp": [965.56, 971.0], "text": " environmental and social incentives or considerations. But all of these are"}, {"timestamp": [971.0, 977.48], "text": " broadly types of stakeholder capitalism. So both of these doctrines or ideas"}, {"timestamp": [977.48, 983.12], "text": " attempt to create a more desirable Nash equilibrium. So in the case of"}, {"timestamp": [983.12, 985.04], "text": " mutually assured destruction the"}, {"timestamp": [985.04, 990.24], "text": " equilibrium is we will we will maintain a nuclear arsenal but we won't use it."}, {"timestamp": [990.24, 995.52], "text": " That is the optimal strategy. In the case of stakeholder capitalism the idea is we"}, {"timestamp": [995.52, 1000.36], "text": " will adopt a broad array of behaviors that mean that we don't abuse employees,"}, {"timestamp": [1000.36, 1005.32], "text": " suppliers, or the environment while still making profit. That is the goal. Now I"}, {"timestamp": [1005.32, 1009.92], "text": " will say that both of these have very very deep flaws which would take many"}, {"timestamp": [1009.92, 1015.12], "text": " many videos to unpack but you know I think you get the idea. These are the"}, {"timestamp": [1015.12, 1020.6], "text": " current attempts that are stable-ish right now and working-ish right now but"}, {"timestamp": [1020.6, 1025.24], "text": " might also still be pushing us towards a dystopian outcome even if they"}, {"timestamp": [1025.24, 1034.16], "text": " are currently stable enough. Now, technology as a destabilizer. Technological"}, {"timestamp": [1034.16, 1038.92], "text": " leaps have always destabilized the system, starting with the printing press"}, {"timestamp": [1038.92, 1045.0], "text": " which led to religious and economic and political upheaval,"}, {"timestamp": [1045.16, 1049.44], "text": " looking at you, Martin Luther and French Revolution,"}, {"timestamp": [1049.44, 1050.96], "text": " then the Industrial Revolution,"}, {"timestamp": [1050.96, 1053.68], "text": " which led to huge social upheaval"}, {"timestamp": [1053.68, 1056.0], "text": " with urbanization, factories,"}, {"timestamp": [1056.0, 1059.12], "text": " and the dislocation of many jobs,"}, {"timestamp": [1059.12, 1061.52], "text": " which the Industrial Revolution also contributed"}, {"timestamp": [1061.52, 1063.8], "text": " directly to World War I and II"}, {"timestamp": [1063.8, 1070.04], "text": " because those were the first industrial-scale wars. Nuclear weapons, internet, silicon, all of the"}, {"timestamp": [1070.04, 1076.6], "text": " above lead to destabilization. AGI or autonomous AI systems, no different. It's"}, {"timestamp": [1076.6, 1079.44], "text": " just another technological leap that will call it that will destabilize"}, {"timestamp": [1079.44, 1086.32], "text": " everything again. And it's pretty much a foregone conclusion that the advancement of AI is"}, {"timestamp": [1086.32, 1093.48], "text": " going to destabilize stuff. So this forces us to ask questions. What is the"}, {"timestamp": [1093.48, 1100.8], "text": " new attractor state? If, well in the past the attractor state was different"}, {"timestamp": [1100.8, 1107.2], "text": " because you know technological abilities to affect the world were limited."}, {"timestamp": [1107.2, 1112.8], "text": " Right? When the world was powered by coal, there was only so much damage we could do to each other and the world."}, {"timestamp": [1112.8, 1118.0], "text": " But as technology advanced, the amount of damage possible went up."}, {"timestamp": [1118.0, 1124.4], "text": " So the new attractor state also changed, as well as all the incentives of participants in the world."}, {"timestamp": [1124.4, 1129.2], "text": " And that includes employers, individuals, governments, militaries, so on. Technology"}, {"timestamp": [1129.2, 1135.16], "text": " changes the game, changes the fundamental nature of the game of life or reality or"}, {"timestamp": [1135.16, 1139.84], "text": " however you want to call it. And so the question is, okay, with the rise of AGI"}, {"timestamp": [1139.84, 1143.96], "text": " how does that change the attractor state? And there's, as far as I can tell,"}, {"timestamp": [1143.96, 1147.56], "text": " there's basically three states. There's utopia, dystopia, and extinction."}, {"timestamp": [1148.82, 1152.76], "text": " There's probably a lot of gray area in between and there might be a fourth kind of"}, {"timestamp": [1153.1, 1158.54], "text": " state that we're heading towards, but in terms of useful shorthand, utopia, dystopia, and extinction."}, {"timestamp": [1158.78, 1163.6], "text": " So the follow-up question is what can we do to alter that attractor state?"}, {"timestamp": [1163.6, 1169.6], "text": " Is there anything that we can do structurally or systematically to favor one of those outcomes"}, {"timestamp": [1169.6, 1175.52], "text": " over another? And then finally, what is the optimal strategy for each of those kinds of"}, {"timestamp": [1175.52, 1179.52], "text": " stakeholders that I mentioned, individuals, corporations, governments, and militaries,"}, {"timestamp": [1180.48, 1192.72], "text": " to create a new Nash equilibrium in light of AGI. So basically we need a Nash equilibrium framework for implementing AGI to"}, {"timestamp": [1192.72, 1196.96], "text": " to push us towards a desirable or positive attractor state."}, {"timestamp": [1196.96, 1201.2], "text": " So all that is a really complex way of saying we need a plan."}, {"timestamp": [1201.2, 1207.28], "text": " We need a plan of implementing AGI in such a way that we will trend towards"}, {"timestamp": [1207.28, 1216.08], "text": " utopia rather than dystopia or extinction. Okay, so with all that in mind, what are some of the"}, {"timestamp": [1216.08, 1221.2], "text": " success criteria for this framework? What are the goals of this framework? How do we know if"}, {"timestamp": [1221.2, 1229.96], "text": " this framework is going to be successful? One, it needs to be easy to implement and understand. The reason is because the"}, {"timestamp": [1229.96, 1234.16], "text": " ability for individuals at all levels, whether it's individual persons like"}, {"timestamp": [1234.16, 1240.04], "text": " myself or corporations or even small nations, to implement AGI is ramping up."}, {"timestamp": [1240.04, 1248.5], "text": " I was on Discord last night and there are people that just after tinkering for a few weeks have created fully autonomous AI systems."}, {"timestamp": [1248.5, 1259.0], "text": " And one of the things that we discussed was, okay, if me or you or whoever, some of these people are not even coders, they learn to code with chat GPT."}, {"timestamp": [1259.0, 1266.38], "text": " If everyone is going to be capable of creating autonomous AI systems now, and it's only going"}, {"timestamp": [1266.38, 1272.36], "text": " to ramp up over the coming months and years, then whatever framework that we come up with"}, {"timestamp": [1272.36, 1278.6], "text": " is going to have to be universally understandable, easy to implement, and easy to understand."}, {"timestamp": [1278.6, 1288.44], "text": " If it's esoteric, no one will use it because they won't understand it. Number two, all stakeholders have to be incentivized"}, {"timestamp": [1288.44, 1290.92], "text": " to use this framework, or in other words,"}, {"timestamp": [1290.92, 1294.08], "text": " this framework must represent the optimal strategy"}, {"timestamp": [1294.08, 1296.56], "text": " so that people won't deviate from it."}, {"timestamp": [1296.56, 1299.52], "text": " There's basically, everyone has to benefit from using it"}, {"timestamp": [1299.52, 1302.6], "text": " and there has to be compounding returns incentivizing"}, {"timestamp": [1302.6, 1305.4], "text": " everyone to say, hey, you should be using this framework"}, {"timestamp": [1305.4, 1309.32], "text": " because this is the optimal strategy for everyone."}, {"timestamp": [1309.32, 1313.28], "text": " Above and beyond that, this framework needs to be adaptable and responsive or dynamic"}, {"timestamp": [1313.28, 1319.36], "text": " because again, the world changes and so it's really difficult to create a framework that"}, {"timestamp": [1319.36, 1326.12], "text": " is a hard set of rules to follow, which will result in unintended consequences and"}, {"timestamp": [1326.12, 1330.08], "text": " instabilities and other market failures. So it has to be context-dependent and"}, {"timestamp": [1330.08, 1335.12], "text": " changeable over time. Number four, this framework has to be inclusive and"}, {"timestamp": [1335.12, 1340.48], "text": " representative in that it cannot exclude any stakeholder. It cannot exclude any"}, {"timestamp": [1340.48, 1345.52], "text": " citizens from any nation or religion. It cannot exclude any corporation or"}, {"timestamp": [1345.52, 1351.16], "text": " government or military because like it or not we all share the same planet and"}, {"timestamp": [1351.16, 1359.04], "text": " we are all stakeholders in this outcome. And one thing that I want to address is"}, {"timestamp": [1359.04, 1364.96], "text": " that there have been cases where nations agree on like rules of engagement and"}, {"timestamp": [1364.96, 1366.24], "text": " rules of war."}, {"timestamp": [1366.24, 1371.36], "text": " Like, we don't use napalm anymore because it was decided that, like, okay, this is inhumane."}, {"timestamp": [1371.36, 1372.96], "text": " Or maybe it was white phosphorus."}, {"timestamp": [1372.96, 1374.72], "text": " Anyways, there's certain kinds of weapons."}, {"timestamp": [1374.72, 1375.44], "text": " Mustard gas."}, {"timestamp": [1376.16, 1379.92], "text": " Those are things that, even though nations might go to war with each other,"}, {"timestamp": [1379.92, 1384.08], "text": " they still agree not to do certain things because they understand that the soldiers"}, {"timestamp": [1384.08, 1388.6], "text": " are stakeholders as well as the citizens who might get caught in the crossfire."}, {"timestamp": [1388.6, 1393.4], "text": " So there is some precedent of nations agreeing on how to conduct war even though destruction"}, {"timestamp": [1393.4, 1396.32], "text": " is one of the goals of war."}, {"timestamp": [1396.32, 1399.76], "text": " Number five, this framework has to be scalable and sustainable."}, {"timestamp": [1399.76, 1402.34], "text": " It has to include the entire globe as well."}, {"timestamp": [1402.34, 1409.38], "text": " And that's not just the people on the globe, it has to include the entire globe as well and that's not just the people on the globe it has to include the environments and ecosystems around the globe which we"}, {"timestamp": [1409.38, 1414.18], "text": " all depend on anyways. So I personally see humans as part of the ecosystem not"}, {"timestamp": [1414.18, 1419.06], "text": " separate from it. And finally this framework has to be transparent and"}, {"timestamp": [1419.06, 1424.6], "text": " trustworthy because perception is reality right? If people perceive a"}, {"timestamp": [1424.6, 1426.4], "text": " framework to be destructive,"}, {"timestamp": [1426.4, 1428.4], "text": " like ESG is a perfect example,"}, {"timestamp": [1428.4, 1430.68], "text": " the perception of ESG is awful."}, {"timestamp": [1430.68, 1432.6], "text": " Why? Because it's championed by BlackRock,"}, {"timestamp": [1432.6, 1433.72], "text": " which is one of the most,"}, {"timestamp": [1433.72, 1437.72], "text": " I think it is the wealthiest company on the planet, right?"}, {"timestamp": [1437.72, 1440.64], "text": " And so because ESG is championed"}, {"timestamp": [1440.64, 1444.64], "text": " by a multi-trillion dollar corporation,"}, {"timestamp": [1444.64, 1448.12], "text": " it is not trusted and that perception makes it bad."}, {"timestamp": [1449.36, 1451.24], "text": " I don't know whether or not ESG is good or bad,"}, {"timestamp": [1451.24, 1453.82], "text": " but the perception certainly is bad."}, {"timestamp": [1453.82, 1456.32], "text": " So transparency and trustworthiness are critical"}, {"timestamp": [1456.32, 1457.56], "text": " for the success of this framework"}, {"timestamp": [1457.56, 1458.48], "text": " because if people don't trust it,"}, {"timestamp": [1458.48, 1460.04], "text": " they're not gonna use it either."}, {"timestamp": [1461.6, 1467.1], "text": " And finally, so this is where I pitch my work."}, {"timestamp": [1467.1, 1473.5], "text": " So my proposed solution to all of this is what I call the heuristic imperatives, which"}, {"timestamp": [1473.5, 1478.88], "text": " is a set of rules or principles that can be incorporated into AGI systems that will push"}, {"timestamp": [1478.88, 1481.72], "text": " it into this direction."}, {"timestamp": [1481.72, 1486.32], "text": " And so these imperatives are one, reduceuce suffering in the universe, 2. Increase"}, {"timestamp": [1486.32, 1491.2], "text": " prosperity in the universe, and 3. Increase understanding in the universe. One way to say"}, {"timestamp": [1491.2, 1496.72], "text": " this is that it is a multi-objective optimization problem, meaning that it's not just one objective"}, {"timestamp": [1496.72, 1503.2], "text": " function, it's actually three that the AGI has to work on implementing. So in the last video,"}, {"timestamp": [1503.2, 1505.2], "text": " people asked how do you implement these. It's actually"}, {"timestamp": [1505.2, 1511.12], "text": " really really easy. You can just plug them into chat GPT and talk about it. There's a few places"}, {"timestamp": [1511.12, 1517.04], "text": " that you can get involved in the conversation. One is on Reddit. I created a new subreddit called"}, {"timestamp": [1517.6, 1522.08], "text": " r slash heuristic comparatives. People are sharing their work there so if you want to see the"}, {"timestamp": [1522.08, 1528.88], "text": " discussion jump in on that. I also have a lot of my own work up on GitHub, including a few papers that I have written"}, {"timestamp": [1528.88, 1534.24], "text": " and am working on under github.com slash Dave Schapp slash Here's to Comparatives."}, {"timestamp": [1534.24, 1538.2], "text": " And then finally, the most active community to discuss this stuff is the Cognitive AI"}, {"timestamp": [1538.2, 1542.44], "text": " Lab Discord server, which I started over a year ago, and links to all this is in the"}, {"timestamp": [1542.44, 1544.04], "text": " description of the video."}, {"timestamp": [1544.04, 1548.84], "text": " So because of that, I don't want to spend too much time rehashing stuff but I just"}, {"timestamp": [1548.84, 1552.36], "text": " wanted to connect to the conversation because again transparency and"}, {"timestamp": [1552.36, 1557.88], "text": " trustworthiness are really critical to this solution. But let's talk more"}, {"timestamp": [1557.88, 1563.08], "text": " broadly about this solution of the heuristic comparatives and these success"}, {"timestamp": [1563.08, 1568.38], "text": " criteria. So we outlined six success criteria for a framework"}, {"timestamp": [1568.38, 1571.64], "text": " that will push us towards a positive Nash equilibrium"}, {"timestamp": [1571.64, 1575.34], "text": " or a desirable attractor state, aka utopia."}, {"timestamp": [1575.34, 1577.1], "text": " So the heuristic comparatives, as I mentioned,"}, {"timestamp": [1577.1, 1578.5], "text": " are very easy to implement."}, {"timestamp": [1578.5, 1581.46], "text": " You can put them in the chat GPT system window."}, {"timestamp": [1581.46, 1584.28], "text": " You can just include them in the conversation."}, {"timestamp": [1584.28, 1585.92], "text": " You can use them for"}, {"timestamp": [1585.92, 1593.28], "text": " evaluation, cognitive control, historical self-evaluation, planning, prioritization."}, {"timestamp": [1593.28, 1597.52], "text": " Super easy to implement. And as I mentioned, lots of people are having the"}, {"timestamp": [1597.52, 1603.28], "text": " discussions. Some of the autonomous AI entities that people have created, the"}, {"timestamp": [1603.28, 1605.0], "text": " AIs that they created actually end up"}, {"timestamp": [1605.0, 1609.12], "text": " usually being really fascinated by the heuristic imperatives and they"}, {"timestamp": [1609.12, 1613.56], "text": " kind of gravitate towards them saying like, oh yeah, this is my purpose. So it's"}, {"timestamp": [1613.56, 1618.36], "text": " really interesting to watch that work unfold. Number two, the stakeholders are"}, {"timestamp": [1618.36, 1623.56], "text": " all incentivized to use the heuristic imperatives because just imagining a"}, {"timestamp": [1623.56, 1625.76], "text": " state where you have less suffering, more"}, {"timestamp": [1625.76, 1630.32], "text": " prosperity, and more understanding is beneficial. Now above and beyond that,"}, {"timestamp": [1630.32, 1633.16], "text": " all stakeholders are incentivized to use the heuristic"}, {"timestamp": [1633.16, 1638.36], "text": " comparatives because then you have a level set playing field where you know"}, {"timestamp": [1638.36, 1642.24], "text": " that everyone is abiding by the same rules. Because when you have a game,"}, {"timestamp": [1642.24, 1647.64], "text": " imagine the game Monopoly. If someone is playing by a different set of rules, right? Because when you have a game, imagine the game Monopoly. If someone is playing by a different set of rules, you're not going to play with them, right?"}, {"timestamp": [1647.64, 1652.04], "text": " Even though it's a competition, you still say we're going to abide by the"}, {"timestamp": [1652.04, 1658.0], "text": " same rules, you collect $200 when you pass Go. If, on the other hand, everyone is"}, {"timestamp": [1658.0, 1662.6], "text": " playing by the heuristic imperatives, then you will be incentivized to adhere"}, {"timestamp": [1662.6, 1667.0], "text": " to those rules, knowing that the net effect is going to be beneficial for everyone."}, {"timestamp": [1668.0, 1675.0], "text": " Number three, the heuristic imperatives are adaptable because they intrinsically incentivize learning and adaptation"}, {"timestamp": [1675.0, 1679.0], "text": " with the third heuristic imperative of increased understanding."}, {"timestamp": [1679.0, 1682.0], "text": " This is what I also call the curiosity function."}, {"timestamp": [1682.0, 1685.38], "text": " So basically, you don't want an AGI to be"}, {"timestamp": [1685.38, 1688.76], "text": " dumb and just satisfied with what it knows about the universe. You also don't"}, {"timestamp": [1688.76, 1692.9], "text": " want it to be satisfied with human ignorance. So by increasing understanding"}, {"timestamp": [1692.9, 1698.24], "text": " that includes that one that intrinsically makes AGI's curious, which"}, {"timestamp": [1698.24, 1701.6], "text": " means that they are going to want to learn and challenge their own beliefs,"}, {"timestamp": [1701.6, 1709.52], "text": " but likewise they will also encourage, not force, but encourage humans to learn and adapt. So the heuristic imperatives as a"}, {"timestamp": [1709.52, 1713.36], "text": " system is intrinsically adaptable because learning and curiosity are"}, {"timestamp": [1713.36, 1718.76], "text": " baked in. Number four, the heuristic imperatives are inclusive. Now in all the"}, {"timestamp": [1718.76, 1724.8], "text": " experiments that I've done going back to GPT-3 and now GPT-4, the language models"}, {"timestamp": [1724.8, 1727.38], "text": " already understand the spirit or the"}, {"timestamp": [1727.38, 1733.0], "text": " intention of the heuristic imperatives in that they should be all-inclusive."}, {"timestamp": [1733.0, 1736.0], "text": " And so that makes them very, very context dependent."}, {"timestamp": [1736.0, 1742.62], "text": " So for instance, if you plug in the heuristic imperatives to chat GPT and ask it about religion,"}, {"timestamp": [1742.62, 1745.12], "text": " it will advocate for tolerance and creating"}, {"timestamp": [1745.12, 1749.28], "text": " space for people to explore religion on their own. And if you further unpack that,"}, {"timestamp": [1749.28, 1756.24], "text": " chat GPT and going back to GPT-3 will say that things like individual autonomy is"}, {"timestamp": [1756.24, 1761.96], "text": " actually really important for humanity to thrive. They're scalable. The here's"}, {"timestamp": [1761.96, 1767.14], "text": " the comparatives. It used to just be very simply reduce suffering, increase"}, {"timestamp": [1767.14, 1768.88], "text": " prosperity and increase understanding."}, {"timestamp": [1768.88, 1773.14], "text": " But I established the scope of in the universe because that preemptively answers a lot of"}, {"timestamp": [1773.14, 1778.74], "text": " questions because it's not just a matter of, okay, let's just look at Earth or let's just"}, {"timestamp": [1778.74, 1780.02], "text": " look at one nation."}, {"timestamp": [1780.02, 1781.52], "text": " Let's consider the entire universe."}, {"timestamp": [1781.52, 1783.82], "text": " So that is the scope of the imperatives."}, {"timestamp": [1783.82, 1789.28], "text": " So it's not just global, it is universal. And then finally, the heuristic imperatives"}, {"timestamp": [1789.28, 1796.28], "text": " encourage transparency because they incentivize open communication, trust, and"}, {"timestamp": [1796.28, 1802.16], "text": " autonomy. But above and beyond that, they are transparent in that if everyone"}, {"timestamp": [1802.16, 1808.88], "text": " abides by them, everyone knows that everyone is playing by the same rules. Now, that being said, in the previous video I did address the Byzantine"}, {"timestamp": [1808.88, 1814.32], "text": " generals problem, which is that you might have agents in the system that are either defective,"}, {"timestamp": [1814.32, 1820.0], "text": " faulty, or malicious. And this is also addressed by the heuristic imperatives because what you"}, {"timestamp": [1820.0, 1830.96], "text": " will do is you will detect when an agent is not playing by the rules and you will track that and we'll talk about that in just a moment. So the positive Nash equilibrium that the heuristic"}, {"timestamp": [1830.96, 1837.76], "text": " imperatives encourage have four basic criteria that I was able to think of. One is mutual benefits."}, {"timestamp": [1837.76, 1845.04], "text": " It is mutually beneficial if all agents in the system or all participants in the system adhere to the heuristic"}, {"timestamp": [1845.04, 1851.12], "text": " imperatives, meaning that the rising tide lifts all boats. If we all work to reduce"}, {"timestamp": [1851.12, 1854.76], "text": " suffering, if we all work to increase prosperity, and we all work to increase"}, {"timestamp": [1854.76, 1860.16], "text": " understanding, then we all benefit and we get compounding returns. Trust and"}, {"timestamp": [1860.16, 1865.76], "text": " reputation. So having shared goals and transparency is a natural result of"}, {"timestamp": [1865.76, 1870.72], "text": " the heuristic imperatives as I just mentioned. Resilience and cooperation. So"}, {"timestamp": [1870.72, 1877.06], "text": " this is an interesting outcome which is that for an equilibrium to"}, {"timestamp": [1877.06, 1881.2], "text": " be reached it has to be stable. And so the heuristic imperatives create a"}, {"timestamp": [1881.2, 1886.28], "text": " resilient system in which there's gonna be mutual policing"}, {"timestamp": [1886.28, 1889.68], "text": " as well as some self-correcting behaviors"}, {"timestamp": [1889.68, 1892.84], "text": " which we'll unpack more in a slide or two."}, {"timestamp": [1892.84, 1897.78], "text": " But it is resilient because it encourages collaboration"}, {"timestamp": [1897.78, 1901.84], "text": " and cooperation as well as self-regulation and policing."}, {"timestamp": [1901.84, 1904.52], "text": " And then finally, ultimately, long-term stability,"}, {"timestamp": [1904.52, 1905.16], "text": " that is the entire"}, {"timestamp": [1905.16, 1911.0], "text": " point of a Nash equilibrium and a desirable attractor state is one"}, {"timestamp": [1911.0, 1920.0], "text": " that is stable. You don't want chaos or instability in the future. So one thing"}, {"timestamp": [1920.0, 1923.8], "text": " that is becoming apparent, especially as I watch the landscape change, if you look"}, {"timestamp": [1923.8, 1928.84], "text": " at auto GPT, all kinds of people are going to be building their own autonomous"}, {"timestamp": [1928.84, 1930.16], "text": " systems."}, {"timestamp": [1930.16, 1936.3], "text": " And so what we're creating is a decentralized AGI ecosystem."}, {"timestamp": [1936.3, 1941.24], "text": " And so when this happens, when everyone can create an AGI with their own goals, with their"}, {"timestamp": [1941.24, 1949.6], "text": " own imperatives, with their own design and their own flaws, you're going to end up with a really, really kind of Wild West, dystopian, chaotic world."}, {"timestamp": [1950.32, 1958.4], "text": " So, one way to mitigate this decentralization drift is to adhere to the heuristic imperatives."}, {"timestamp": [1959.36, 1969.28], "text": " And as I mentioned, there's cooperative benefits, right? If you and everyone else you know working on autonomous AI's agrees on nothing else except the heuristic"}, {"timestamp": [1969.28, 1972.96], "text": " imperatives you'll have that framework in common and a lot of work will flow"}, {"timestamp": [1972.96, 1977.8], "text": " from that so the cooperative benefits and this is this goes between above and"}, {"timestamp": [1977.8, 1980.88], "text": " beyond individuals this includes corporations governments as well as"}, {"timestamp": [1980.88, 1987.28], "text": " militaries number two is AGI policing and self-regulation. So if you"}, {"timestamp": [1987.28, 1991.82], "text": " have millions of AGI's that all agree on the heuristic imperatives, even if they"}, {"timestamp": [1991.82, 1996.32], "text": " don't agree on anything else, they will police each other to say, hey we're"}, {"timestamp": [1996.32, 2001.44], "text": " gonna look out for other AGI's, rogue AGI's, that do not abide by the"}, {"timestamp": [2001.44, 2009.84], "text": " heuristic imperatives and we will collaborate to shut them down. And then finally, in many experiments"}, {"timestamp": [2009.84, 2015.12], "text": " that I've done, the heuristic imperatives result in self-regulation within the AGI."}, {"timestamp": [2015.12, 2019.68], "text": " For instance, one of the things that we're afraid of is once AGI's become so"}, {"timestamp": [2019.68, 2023.84], "text": " powerful that they can reprogram themselves or spawn copies or reprogram"}, {"timestamp": [2023.84, 2029.2], "text": " each other or otherwise get control of their source code, that they're going to change their fundamental programming."}, {"timestamp": [2030.16, 2034.0], "text": " If you make the assumption that an AGI can change its fundamental programming,"}, {"timestamp": [2034.0, 2039.04], "text": " spin up alternative copies of itself, then you completely have lost control. However,"}, {"timestamp": [2039.04, 2045.12], "text": " in my experiments with the heuristic imperatives, AGIs will shy away from creating copies of themselves"}, {"timestamp": [2045.12, 2048.34], "text": " or even modifying their own core programming"}, {"timestamp": [2048.34, 2051.24], "text": " out of fear of violating the heuristic imperatives."}, {"timestamp": [2051.24, 2054.64], "text": " And so between policing each other and self-regulation,"}, {"timestamp": [2054.64, 2056.02], "text": " the heuristic imperatives create"}, {"timestamp": [2056.02, 2058.94], "text": " a very powerful self-correcting environment."}, {"timestamp": [2059.88, 2062.18], "text": " Reputation management, number three,"}, {"timestamp": [2062.18, 2066.32], "text": " is another thing where, as I mentioned, the AGIs will"}, {"timestamp": [2066.32, 2071.42], "text": " work to infer the objectives of all other AGIs, whether or not it's known."}, {"timestamp": [2071.42, 2078.04], "text": " This goes back to the Byzantine generals problem where you don't know how another AGI is programmed."}, {"timestamp": [2078.04, 2082.76], "text": " They might have used the heuristic imperatives, but they might have been improperly implemented."}, {"timestamp": [2082.76, 2087.6], "text": " Or you might have rogue elements that are created"}, {"timestamp": [2087.6, 2091.36], "text": " without the heuristic imperatives or other objectives that are more destructive."}, {"timestamp": [2091.36, 2096.88], "text": " And then finally, stakeholder pressure. Between the four categories of stakeholders that I already"}, {"timestamp": [2097.6, 2101.04], "text": " illustrated, which is individuals, corporations, governments, and militaries,"}, {"timestamp": [2101.04, 2105.12], "text": " AGIs are going to be another stakeholder. Now whether or not you believe"}, {"timestamp": [2105.12, 2109.28], "text": " that they're conscious or sentient or have rights, I don't really think that's relevant"}, {"timestamp": [2109.28, 2114.32], "text": " because they will be powerful entities in and of themselves before too long. And so between those"}, {"timestamp": [2114.32, 2122.56], "text": " five types of stakeholders, there will be intra and inter group pressure to conform and adhere to"}, {"timestamp": [2122.56, 2125.2], "text": " the heuristic imperatives."}, {"timestamp": [2127.2, 2127.72], "text": " Okay, so let's describe"}, {"timestamp": [2132.36, 2132.48], "text": " assuming all this works out and assuming that I'm right and assuming that I'm not crazy and that"}, {"timestamp": [2137.76, 2138.52], "text": " the trends continue and people are gonna keep building the AGI's that they're working on, what"}, {"timestamp": [2142.76, 2143.6], "text": " characteristics can we use to describe this desirable attractor state or"}, {"timestamp": [2147.28, 2151.6], "text": " utopia? So one is universal health and well-being. With a few exceptions of people that are stuck in self-destructive patterns, all"}, {"timestamp": [2151.6, 2156.4], "text": " humans want health and wellness. That's pretty much a given."}, {"timestamp": [2156.4, 2168.3], "text": " Number two, again with a few outliers, people want environmental restoration and sustainability. Number"}, {"timestamp": [2168.3, 2172.94], "text": " three, individual liberty and personal autonomy. This is an intrinsic"}, {"timestamp": [2172.94, 2177.86], "text": " psychological need for all humans. Number four, knowledge and understanding."}, {"timestamp": [2177.86, 2185.36], "text": " Curiosity and learning are universally beneficial, which is why education is one of the primary goals of"}, {"timestamp": [2186.92, 2191.0], "text": " nations and and unions of nations such as the United Nations,"}, {"timestamp": [2191.6, 2195.22], "text": " European Union, and so on. And then finally, peaceful coexistence."}, {"timestamp": [2196.2, 2201.68], "text": " Nobody wants war and chaos. Some people think it's cool, you know, watching Lex Friedman talk to various people. They're like,"}, {"timestamp": [2201.68, 2204.36], "text": " oh, yeah, there is something attractive about thinking about"}, {"timestamp": [2203.28, 2208.56], "text": " Lex Friedman talked to various people, they're like, oh yeah, there is something attractive about thinking about catastrophe and cataclysm. We keep making disaster movies, for instance,"}, {"timestamp": [2208.56, 2212.88], "text": " but in terms of how we actually want to live, we all want peaceful coexistence."}, {"timestamp": [2213.6, 2225.54], "text": " And so this desirable attractor state, a shorthand, is utopia. Now, I know I've painted a very rosy picture, as well as presented some challenges."}, {"timestamp": [2225.54, 2230.46], "text": " So there are still a few challenges remaining"}, {"timestamp": [2230.46, 2231.98], "text": " that we need to address."}, {"timestamp": [2231.98, 2235.0], "text": " And so one of those is misalignment and drift."}, {"timestamp": [2235.0, 2237.5], "text": " So even with the heuristic imperatives,"}, {"timestamp": [2237.5, 2240.74], "text": " there might still be drift or misalignment,"}, {"timestamp": [2240.74, 2242.08], "text": " intentionally or otherwise."}, {"timestamp": [2242.08, 2246.0], "text": " It could be that there's flaws in the implementation, the code,"}, {"timestamp": [2246.0, 2250.0], "text": " or maybe someone breaks them or says, hey, I'm going to do an experiment by deleting one of the"}, {"timestamp": [2250.0, 2255.68], "text": " heuristic imperatives. That could destabilize the system. Second, there can be unintended"}, {"timestamp": [2255.68, 2261.92], "text": " consequences. So one thing that it seems like it will inevitably happen is that AGI systems are"}, {"timestamp": [2261.92, 2265.92], "text": " going to outstrip and outpace human intellect."}, {"timestamp": [2270.4, 2270.96], "text": " If that becomes the case, and they might also adopt other languages, right? Right now,"}, {"timestamp": [2276.16, 2281.92], "text": " most of them communicate in English because English is the bulk of the training data. But, you know, for instance, what if the AGIs ultimately communicate with a language that we"}, {"timestamp": [2281.92, 2289.76], "text": " cannot comprehend or understand, like binary or vectors or something else and then we can't even monitor what they're"}, {"timestamp": [2289.76, 2295.08], "text": " doing. What my hope is that the AGI's as part of being trustworthy and"}, {"timestamp": [2295.08, 2301.12], "text": " transparent will choose to continue to communicate exclusively in English. But"}, {"timestamp": [2301.12, 2305.86], "text": " that we can't assume that that will be true."}, {"timestamp": [2305.86, 2307.94], "text": " Number three, concentration of power."}, {"timestamp": [2307.94, 2315.42], "text": " Now, I did talk about how I believe that the heuristic imperatives will create an incentive"}, {"timestamp": [2315.42, 2321.48], "text": " structure that results in sharing of power, transparency, so on and so forth."}, {"timestamp": [2321.48, 2326.24], "text": " That being said, there is still a tremendous amount of desire to concentrate power."}, {"timestamp": [2326.24, 2333.44], "text": " And especially on the geopolitical stage, there are nations out there with mutually exclusive goals."}, {"timestamp": [2333.44, 2338.48], "text": " And as long as nations exist with mutually exclusive goals or incompatible visions of"}, {"timestamp": [2338.48, 2343.68], "text": " how the planet should be, there will be concentrations of power, and those concentrations"}, {"timestamp": [2343.68, 2345.7], "text": " of power will be pitted against each other."}, {"timestamp": [2345.94, 2351.52], "text": " So that is not something that the heuristic imperatives intrinsically address, but that is a reality of what exists today"}, {"timestamp": [2352.38, 2356.46], "text": " which can destabilize the system. So in the long run,"}, {"timestamp": [2357.42, 2362.72], "text": " I think part of the ideal state, the Nash equilibrium, is that power is not concentrated anywhere,"}, {"timestamp": [2362.94, 2369.98], "text": " but we need to overcome several major barriers as a species before we can achieve that. Number four is social"}, {"timestamp": [2369.98, 2374.56], "text": " resistance. Public skepticism, mistrust, and ignorance is one of the greatest"}, {"timestamp": [2374.56, 2377.76], "text": " enemies right now, which is why I am doing this work, which is why I chose"}, {"timestamp": [2377.76, 2383.76], "text": " YouTube as my primary platform to disseminate my information. Number five,"}, {"timestamp": [2383.76, 2386.28], "text": " malicious use. Again, as long as"}, {"timestamp": [2386.28, 2391.9], "text": " there are malicious actors, there might be deliberate deployments of AGI that"}, {"timestamp": [2391.9, 2397.8], "text": " are harmful which could destabilize the system. And finally, I do need to address"}, {"timestamp": [2397.8, 2401.96], "text": " this as well. The heuristic imperatives are a necessary foundation of this"}, {"timestamp": [2401.96, 2407.04], "text": " utopic outcome, this beneficial, sorry,"}, {"timestamp": [2407.04, 2410.86], "text": " attractive state that we're looking for, but they do not represent a complete"}, {"timestamp": [2410.86, 2414.88], "text": " solution. There are a few other things that are needed in order to achieve this"}, {"timestamp": [2414.88, 2419.62], "text": " outcome. One, collaboration and open dialogue. So, researchers, individuals,"}, {"timestamp": [2419.62, 2427.72], "text": " corporations, and governments all need to work together at a global scale. Anything short of global collaboration and cooperation"}, {"timestamp": [2429.84, 2432.44], "text": " could very well result in a negative outcome."}, {"timestamp": [2432.44, 2434.28], "text": " And this is one of the things that Liv"}, {"timestamp": [2434.28, 2436.68], "text": " and other people talk about when talking about Moloch,"}, {"timestamp": [2436.68, 2438.6], "text": " is that it is a, what do they call it?"}, {"timestamp": [2438.6, 2441.56], "text": " I think a collaboration failure or a signal failure."}, {"timestamp": [2441.56, 2443.6], "text": " I can't remember exactly how they describe it."}, {"timestamp": [2443.6, 2447.5], "text": " But essentially collaboration is the antidote and open dialogue is the"}, {"timestamp": [2447.5, 2452.78], "text": " antidote to the ignorance and other negative signals and noise that"}, {"timestamp": [2452.78, 2457.06], "text": " contribute to the Moloch problem. Number two is regulatory frameworks and"}, {"timestamp": [2457.06, 2463.18], "text": " oversight. Again, it's not just a matter of coming together, it is that there are"}, {"timestamp": [2463.18, 2465.2], "text": " institutional changes that"}, {"timestamp": [2465.2, 2471.94], "text": " need to happen, such as legislation, councils, and summits, and other kinds of"}, {"timestamp": [2471.94, 2477.16], "text": " meetings and investments that need to happen at an institutional level, not"}, {"timestamp": [2477.16, 2483.12], "text": " just communication and dialogue, but the frameworks, the oversights, those also"}, {"timestamp": [2483.12, 2485.0], "text": " need to be implemented."}, {"timestamp": [2485.0, 2486.72], "text": " Number three, education and awareness."}, {"timestamp": [2486.72, 2492.24], "text": " As I just mentioned, public awareness and understanding is presently insufficient to"}, {"timestamp": [2492.24, 2496.56], "text": " overcome the negative attractor states that we're heading towards."}, {"timestamp": [2496.56, 2499.88], "text": " And number four, continuous monitoring and improvement."}, {"timestamp": [2499.88, 2502.16], "text": " This is not a solution that we solve once."}, {"timestamp": [2502.16, 2509.28], "text": " It is an ongoing thing, just like how you don't just pass internet regulations and then you're done, you go home forever. You continuously monitor the"}, {"timestamp": [2509.28, 2514.8], "text": " changing dynamic environment so that you can course correct as you go. That is going to be"}, {"timestamp": [2514.8, 2520.96], "text": " necessary forever with AGI. It's not going to go away. Just like the EPA, the Environmental"}, {"timestamp": [2520.96, 2525.12], "text": " Protection Agency, didn't just create a set of guidelines and"}, {"timestamp": [2525.12, 2527.32], "text": " we're done, they pack it up."}, {"timestamp": [2527.32, 2532.88], "text": " No, the EPA continuously does stress tests and pressure tests and measurements all over"}, {"timestamp": [2532.88, 2536.2], "text": " the nation to make sure that the policies are effective."}, {"timestamp": [2536.2, 2539.24], "text": " And then of course as they gain more information, those policies change."}, {"timestamp": [2539.24, 2547.92], "text": " We will need the same kind of vigilance applied to AGI systems and the AGI ecosystem. Okay, so that was a"}, {"timestamp": [2547.92, 2553.64], "text": " lot. Thank you for watching. That's about all I have today. Not that this was not"}, {"timestamp": [2553.64, 2558.76], "text": " much, but thanks anyways. And yeah, I hope that this helped and I hope that it"}, {"timestamp": [2558.76, 2564.48], "text": " gives you a little bit more confidence in the direction that we're going. Thanks."}, {"timestamp": [2558.62, 2563.12], "text": " confidence in the direction that we're going. Thanks."}]}