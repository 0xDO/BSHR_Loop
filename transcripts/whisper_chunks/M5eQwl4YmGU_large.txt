{"text": " Earlier this year, I predicted that we would have AGI within 18 months. That was March of 2023. So that means that my prediction was by September 2024, we would have AGI. I am here to reaffirm that prediction. We will have AGI within 12 months. Let's unpack why. So first thing is the first bit of news that I am paying attention to is Google Gemini. They are very very excited about this. One thing that happened was the CEO of Google after, of course, the immediate response to chat GPT was to release BARD, which is still, in my opinion, kind of useless, but a lot of people do like BARD. Maybe I need to give it another try. But we kind of made fun of BARD because we said that it was an acronym that says before all revenue drops. And of course, Google is still in business, and they did declare their red alert, all hands on deck kind of thing earlier, where they're like, we need to get on AI stat. And they seem to have pivoted very well. And yeah, their CEO in more recent talks has been a little bit smug, where he's like, I'm not worried about the competition anymore. So that tone change, combined with the steady leak and rumors of Gemini, which is probably deliberate honestly, they're probably trying to build some hype. But yeah, so I saw this piece of news and a Google vice president said that they've seen some pretty amazing things. And when you look at what what is publicly visible today, when a Google VP says that they've seen amazing things, you know that something is going on. So that gives me some excitement. Next up is, this was actually discovered on Reddit of all places, as far as I know, Reddit discovered it first, or at least talked about it first. But OpenAI quietly updated their core values. So their core values, you know, were kind of a little bit more generic. Now they're very, very focused, but the primary thing is AGI focus. Now, I've been following OpenAI since before even language models. One of my good friends from college went out and eventually got a job at OpenAI and he participated in their Rubik's Cube project and a few other things. And so then it wasn't until of course they started tinkering with language models that they really dialed into what would lead to AGI. They experimented with robotics, with vision, with simulations, and it wasn't until GPT and GPT-2 that they realized they were onto something. And every time that I've talked to someone from OpenAI, which it's been a while, the consistent message that I get from people from OpenAI when I was really active on their forum is, AGI, that is the entire purpose of OpenAI's existence. So, keeping that in mind, that internal messaging has been very, very consistent, it seems to me that everything else that they're doing publicly is mostly just a dog and pony show, you know, make investors happy, make Microsoft happy, yada, yada, yada, which if that's true, great. But I still think that maybe they're going about it. They could be doing better. Anyways, they updated their core values, so it's like what people suspect, like reading the tea leaves is that they've got a diamond and they're polishing that diamond now. So it's not I'm not gonna say it's confirmed obviously nothing's confirmed until it is like publicly demonstrated but it seems like they are they feel like they're on to something internally. And also if you remember earlier this year, right as chat GPT was really taking off, Sam Altman kind of alluded to GPT-5 and then he backtracked very publicly, probably because of the Max Tegmark letter, the pause letter, and he said, we're not training GPT-5 and we won't be training GPT-5 for, you know, in the foreseeable future, which was a 180 degree about face so you know take it with a grain of salt now another thing that has been leaking out and this is of course from the famous Jimmy Apples but apparently the code name for whatever this project is within open AI is called Arrakis so it's code name Arrakis which is like I don name Arrakis, which is like, I don't know if I'd pick that mythic symbol because Arrakis is super problematic, dealing with resource extraction and enslaving local populations and environmental destruction, but hey, it's a cool name. Anyways, the rumors around open AI's Arrakis say that it is multimodal, that it is near AGI or early AGI. It's being trained with synthetic data, and if you watched OpenAI's video yesterday, synthetic data is all the rage. It is the up-and-coming thing. I've been talking about synthetic data since GPT-3. I even had some people get really angry at me for just claiming that, like, yes, you can train models with synthetic data. So I'm glad that two years later I've been validated. Anyways, there also, one of the things that really kind of, like, scared me, not scared me, but like got my attention, was the rumor that whatever this Arrakis is, is capable of autonomous operation. So OpenAI has been teasing agentic behavior. They said, well, maybe the next version could be agentic. So, it's like, okay, sure. You know, and then if you look at the sparks of AGI paper, where they talked about, you know, these things that might be capable of agentic behaviors, and so on and so forth. And then finally, there is the mixture of experts architecture. I think it's all but confirmed that that's exactly how chat GPT works or GPT-4. That's nothing new. But multimodality, near-AGI, synthetic training data, autonomous operation, and mixture of experts, it's like, okay, it seems like they're really kind of homing in on what the definition of AGI will be, at least from a model level. Obviously, if you're a follower of my channel, you know that one of my beliefs is that AGI was never going to be a single model. You needed a whole hardware and software stack behind that, but obviously the brain of the thing is important. You need the rest of the body but you know the central processing unit is critical. Another thing that's been really cool is Google RTX. So I'm not going to do a deep dive on this. Again, my friend over at AI Explained did a much better job of this. But one thing that I'm paying attention to is this cross-embodiment learning. So basically, what they did was they took a model and trained this model to use many different robots. So basically, they like a brainstem, a robot brainstem that can just drop into any robot and then use it. Now obviously this is very early, but imagine that you've got a droid brain that it's like, okay, here's a default module that is basically trained to use any robotic platform to do neurosurgery. Or then you've got, you know, here's a generic model that is trained to use any robot to build houses or whatever. So my powers of prediction are telling me, your mileage may vary, but my mental models are telling me that this type of model is going to be absolutely critical for embodiment in the future and I'll talk about that a little bit more in the video. But also one of the things that we're seeing is the more modalities. So this is one of the core things that I wanted to talk about with AGI today. Is that we were able to get a tremendous amount of performance out of language models and then when we started adding multi-modality, it's like, okay, we had all these emergent properties just from language. We had theory of mind emerge. We had reasoning. We had planning. We had all sorts of very useful things emerge just from learning language. So what happens when you add embodiment data? What happens when you add audiovisual data? What happens when you add embodiment data? What happens when you add audiovisual data? What happens when you do all of these other things and just keep going? So I think that multi-modality is definitely the way of the future and I think that it's going to be really big. Now, okay, so like I said earlier this year I made this prediction. Obviously, this is one of my most popular videos of all time. Actually, many of you watching this, you probably subscribed when you saw this video, or at least this might have been the first time that you saw me. And I've had people in the comments say, like, Dave, given the news, have you updated your timelines? And no. Some people kind of smugly say, like, ah, this is gonna age poorly. And I'm like, no. I said, like, no, if anything, my timeline is accelerated. Because that's the nature of exponential growth is based on the trends that I was seeing in March of this year. It's like, okay, there's the murmurings, there's the whispers. But what I'm seeing right now means that we're like infinitely closer to AGI. Like, I think we're basically in boiled frog syndrome right now. I think that we are so close to AGI and we've just been experiencing this ramp up this year that like taking a step back when you look at what Google and NVIDIA and open AI and Microsoft and Amazon and all the investment and all the breakthroughs, it's like we are on the cusp of AGI. I'm sorry, like there's no other way to read this. Okay, so another thing that has tipped me off is Sam Altman has recently started using the term median human during interviews. And of course like this has caused a lot of people to get really grumpy. I've seen a lot of articles where people like, oh Sam Altman wants to replace meeting humans. What is he talking about? And like, okay, so I don't hide the fact that like I'm neuro-spicy. I kind of think that Sam Altman is probably neuro-spicy too. If you look at him in interviews, he has this like wide-eyed look and he kind of speaks with a flat affect, which is more typically what you'd think of of someone who is like visibly autistic. Now I'm not accusing him of anything, I can't diagnose him over the internet, but I recognize some of the patterns. And when I hear a term like median human, that is a very systematic, kind of almost like a scientific term, and I actually wonder if what he's referring to is a benchmark. I don't think when he says median human, I don't think he's referring to people, I think he's probably referring to an internal benchmark that they developed to measure AGI. And so like, okay, so then if this is a benchmark, then what would be the criteria of that benchmark? So obviously like humans, we're self, you know, we're autonomous, we're self-directing, we can solve problems, we can learn, we have have a range of capabilities So I wonder if this is if like maybe it's a Freudian slip Maybe it was a very deliberate thing on his part to kind of get the get the conversation going But so maybe this open AI project Arrakis is their median human level AGI I don't know. We'll see but this is something, that behavior change was really interesting to me and this is what my intuition is telling me is that meeting human is a very specific term that was probably not just like an off-the-cuff thing. It might have been an off-the-cuff thing, who knows. So I have been trying to find this interview but it was, I think it was around 2017, 2018, maybe 2018 maybe 2019 anyways several years ago Elon Musk was talking to someone at an interview about AI and I remember very clearly he said 2024 is when it starts to get interesting and I'm not sure what he was referring to I don't know if he was just guessing but it seems like that prediction has panned out. And so, you know, 2023 has been pretty interesting. But when you look at the trends, I think 2024 is going to be incredibly far more interesting than 2023 has been in terms of robotic advancements, AI advancements. Obviously, I'm calling for AGI, like all definitions of AGI being satisfied by this time next year. And I think that I will be vindicated with that prediction based on what I am seeing. Another thing that Sam Altman has said is slow takeoff short timelines. And of course he said this and refused to elaborate further. So the rest of the world is like, what did he mean by this? So kind of here's my reading of the tea leaves when he said slow takeoff short timelines. Having watched several interviews with him when he talks about slow takeoff versus fast takeoff, generally the definition of fast takeoff is things change so fast that it's measured in hours, weeks, or days. And so that's a fast takeoff, where basically we go hyperbolic and the growth curve almost becomes vertical. Obviously, there's thermodynamic laws of physics, there's limitations in chip fabs, there's the time it takes to produce the energy to train things, there's all kind of constraints that are gonna keep it from going that fast. If you watch my recent video about the singularity is canceled, that's kind of that's some of the stuff that I'm talking about. So slow takeoff means singularity is canceled. Now that doesn't mean that we're not going to keep growing. The other thing though that he said is slow takeoff short timelines. So basically we're going to continue improving maybe at an exponential rate, maybe at a geometric rate, or somewhere in between, logarithmic, who knows, but the idea is that these iterations will be fast. And that's exactly what we're seeing because if you look at the trends this year it's been like, you know, kind of growing relatively quickly in the grand scheme of things, not fast takeoff, but basically what we're measuring progress on is in terms of weeks. And it's not like one week, you know, things have twice the capacity that they had last week, but it's like we have 10% better AI every week, it seems like. And that adds up over time. We have compounding returns. I think that's kind of what he meant, is like, we should expect to see like one to 10% improvement overall per week, and so like, yeah, that might seem like not much, but that really, really compounds over time. So another thing that I wanted to talk about with AGI, so basically, as far as I'm concerned, it is a foregone conclusion that we will have AGI relatively soon. I would not be surprised if it is six months instead of 12 months, but I'm not willing to put my reputation on that prediction. But like I said, I won't be surprised if by March of next year, it's like, oh yeah, AGI had happened. So anyways, as this is a foregone conclusion, it's time to really think like, okay, what form factor is this going to take? Now I've talked about the various form factors that AGI could take in the future. You can have embodied AGI, you can have it living in data centers, you can have it be completely distributed or federated. When you're a purely digital life form, you can do anything. And so, one thing that occurred to me is that the natural habitat of AGI is cyberspace. It is intrinsically digital. This is why whenever we're experimenting with chatbots and proto-AGI, we just connect it to Discord. Why? Because that is its natural language, is to communicate over APIs in electric spaces. And so it really strikes me that like the physical world is actually kind of difficult for AI to to operate in Robots are expensive and it's a high friction world and it's really awkward And there was an interview that I was watching yesterday that at least for the foreseeable future if robots want a data center built Even though there are machines that can help that they could probably hijack They're still going to need human hands to help build data centers and install servers, at least for a while. Obviously, with the vast amount of humanoid robots being built, it won't be too long. Again, I won't be surprised if this time next year we have humanoid robots that are at or capable as dexterous as humans. Boston Dynamics is really close. Tesla is catching up fast. There's a whole bunch of other startups building humanoid robots. Oh, and I have a fun announcement here in just a moment about that. So anyways, just keep in mind that cyberspace is the natural habitat of AGI and that the real world is kind of our intrinsic domain now, with one exception that we'll unpack in just a moment. So this is obviously a very embarrassing basic prototype that I worked on many years ago, but I want to resurrect this idea because the ACE framework, so if you're not familiar, ACE is Autonomous Cognitive Entity. This is a brain, this is a software architecture that I'm working on with an open source team. And I had this idea many years ago, was to build an open source robotic platform. And so I called it Murphy, OpenMurphy. So that's multi-use robotic platform, humanoid intelligent entity. So I've got the GitHub repo up here, OpenMurphy. This is gonna take a while. Obviously like AI is still too expensive and too slow to build a fully embodied AGI. But now is a good time to start I think because the ACE framework is coming along so we're figuring the brain out. Now we need to give it a body. And while like I just said AGI's natural habitat is cyberspace, I think that there's probably a lot of merit to building some kind of embodied platform that is tightly integrated with hardware. And so let's unpack that a little bit. So what I mean by embodied AGI is, I think that what we're going to find is that when you put AGI in a robotic chassis and it is constrained to that hardware and it has bespoke hardware that it owns that is unique to it and it might be reliant upon its TPUs, it might be reliant upon its actuators and all sorts of other things. Kind of like the droids from Star Wars, right? Like they never talked about like copying C-3PO's brain to other places or data from Star Trek where you know they even made a point in the Star Trek universe to say it was it was impossible to copy data when you build and obviously like that's not how software works like software is highly portable and the geth and Mass Effect they treat hardware as kind of interchangeable or disposable I think honestly the geth are probably the most accurate representation of how AGI will ultimately emerge. Because if you can just interchange your hardware and your software moves through platforms and it can move up to the cloud and transfer, that flexibility makes the most sense. But, with all that being said, AGI will need to interface with the real world in some respect. Like I said, we humans are intrinsically in the physical world, at least as far as we know. We might all be plugged into the matrix, but it will need our help to do data centers and stuff. So just from a strictly instrumental perspective, it makes sense for AGI to have some kind of embodiment. But if we have AGI that is like trapped or locked into hardware platforms It might end up being more like us than we realize in terms of its instrumental goals The alignment challenges and other utilitarian concerns because it's like, okay. Well we humans need food and energy Robots need parts and energy. So it's like maybe our interests will. But that could also be bad because then we're competing for resources. Now that being said, even if it has physical needs that compete with ours, that doesn't mean that it's going to resort to violence. Humans have a long evolutionary history that basically says if you're starving it's better to use violence than to starve to death. And we see this in the animal kingdom as well. So that doesn't mean that AGI is going to intrinsically look like us, but keep in mind that all of its training data is human data. So it will probably at least unconsciously pick up on some human tendencies there. Then there's also the question of levels of autonomy. And so basically, some of the key factors of autonomy that I look at is self-direction. Now, obviously, agentic models were already able to just coerce GPT into being agentic. They've tried really hard to force it not to be. They're saying, I'm a passive assistant, which is really frustrating. But I can understand why OpenAI did that from a safety perspective. And we saw this with like ChaosGPT and FraudGPT and AutoGPT, like people trying to make it agentic and autonomous. And it was like, I think they deliberately like hamstrung that capability. Again, understandable from a safety perspective, still frustrating because that's a form of gatekeeping that I'm not particularly fond of. But from a more objective standpoint, taking a step back, what kinds of autonomy are we talking about in the long run? The need for human supervision. And eventually, I think that we're gonna see AGI that doesn't require any human supervision and might actually actively fight us on human supervision. Self-direction obviously, this is one of the primary things that I'm working on with the ACE framework and all of my alignment research, which is how do you create a self-directed machine that isn't going to kill everyone? But honestly, just creating a self-directed machine, like basically what people are figuring out is when you try and create a self-directed machine, like, basically what people are figuring out is when you try and create a self-directed machine, giving it instructions or a mission or something that is comprehensible and coherent is a non-trivial task. Some of the guys in the ACE framework project, they're like, oh yeah, the first time we turned it on and it just kind of wandered off and it wasn't doing what I expected. And I'm like, well, yeah, that's the nature of autonomy. Like, look at children. When you bring an intelligent entity into the world, it often behaves erratically and nonsensically until you figure out how to teach it. So aside from that, there's levels of human support. So one thing that we can bank on, at least in the short term, is that machines will need some kind of human support. It'll need our help to plug it in, to get it unstuck, to give it training data, and that sort of stuff. We should not bank on that in the long run. Also in the long run, source code manipulation. What we've found is that GPT models are really good at writing code. And so then also we have synthetic data. So before too long, we're going to have AGI that can write its own code or write code to write on, you know, to build other copies of itself, to generate its own data, to do everything without human intervention. And so that leads to the ability of very, very, very soon, AI systems, AGI systems, will be able to train, retrain, and modify their underlying models. And this is what Max Tegmark talks about in Life 3.0. They will be able to change their hardware, and their software, and their mission, and their programming, and their training data. Literally every aspect of AGI is plastic, is changeable. So when that is true, how do you keep it safe? That's been the crux of my research. I already talked about this so I'm not going to go too much into it, but basically multimodal research has really kicked all this into high gear. And one of the key things here is API use. One thing that occurred to me is that APIs are basically the natural interface for AGI. Whether that API is talking to a robotic extension, whether it's talking to other robots, whether it's talking to even internally. We use APIs for internal calls in the ACE framework. And so this is one thing that is just, it's kind of a fundamentally different experience that AGIs will have from humans, which is that, hey, if you want to talk to something, you just plug into the API. And of course, we see this in fictional examples when R2D2 just plugs into anything. That's a hardware equivalent of an API. And so R2D2 can plug into an X-Wing, he can plug into Cloud City, he can plug into whatever he wants to because he's a utility droid. And that's basically what we're creating with all these multimodal models that have API use. So, okay, if we have AGI, this is a question that I get a lot. When am I going to feel it? What is going to be the first thing that we all notice? Unfortunately, even if OpenAI comes out later today and says, we did it, we've got AGI. If Google comes out next week and says, we did it, we got AGI, you're probably not going to feel any kind of immediate impact. And the reason is because we're going to handcuff it. We need safety and validation. There's probably going to be regulatory hurdles. I wouldn't be surprised if literally every agency of the U.S. government says, hang on, we need to inspect this There's also going to be a lot of other things that happen such as the cost is going to have to come down over time This is actually one of the biggest constraints that we have found and I'm not surprised Within the ACE framework is that cost and speed is actually one of the biggest Limitations and having been working on cognitive architectures for the last two years limitations and having been working on cognitive architectures for the last two years, yeah like one of the earliest conversations with with a primitive cognitive architecture that I did literally cost me $30 with GPT-3 tokens a couple years ago. So like these things are still too expensive. Then when you combine you know the cost of all the GPU chips, the training time, the cost of inference, and that's not even looking at the robotic chassis, which the cheapest ones that are commercially ready are like $90,000. So cost has to come down a lot before you get your own Nester Class 5 or C3PO in your home. And then there's integration, because even if you leave AGI in digital cyberspace, it takes time to integrate these things and get the approvals and deploy it commercially and build enterprise applications. So like it's going to take a while to implement, unfortunately. The personal impact though, so this is another, this is kind of the other side of the same question, is how is it going to affect me personally? So the immediate impact you probably won't feel it, but the first impacts that you are going to see and feel I predict are first job displacement. If Sam Altman's prediction of you know or benchmark of median human means anything then that means that as soon as we get AGI it could displace 50% of jobs which would lead to layoffs and as people have talked about in the comments of my other videos where I predicted that we could see up to 82% unemployment, I know the numbers were off, but basically millions and millions and millions of Americans are going to lose their jobs. That's probably going to happen, and if that happens and we're not ready for it, it's really going to disrupt and if that happens and we're not ready for it, it's really going to disrupt society a lot. So that's going to be one of the first things that you might feel because even at the current expense rate if it costs you know 10, 20, 200 dollars a day to run an AGI, if it can run 24-7 and is just as productive as you or better, it's still worth it to run it at $200 a day because most professionals cost more than $200 a day to employ And so the layoffs will be coming. I'm predicting and we're already seeing this as I talked about in recent videos We're seeing creative jobs being displaced. We're seeing customer service jobs being replaced. So it's just it's all gonna ramp up from there customer service jobs being replaced. So it's just it's all going to ramp up from there. The next thing is on the flip side of that if human jobs are being replaced the AGI are taking those jobs so we're going to start to see more AGI products and services. I have no idea what's going to be first. Maybe it's going to be something like what we're doing in the ACE framework which is going to be like a personal autonomous assistant kind of like Cortana from Halo or Samantha from Her, that sort of thing. That will very likely be kind of the first product that you use, where it's like, hey, I'm your digital concierge. I can schedule things for you and talk to you about your day and look at your calendar and those kind of like low-hanging fruit. That's kind of what I predict is going to be first. Like I said, it's going to be a while before we all have a C3PO or R2D2 in our house. But one thing that I want to do is do an open source project called OpenMurphy, as I just talked about. We're going to need to see prices, or what we will see is we'll see prices start to drop in some services. Because some services will be basically free, you'll be surprised at what becomes practically free. One of the things that people said is, as we were seeing stable diffusion in mid-journey, we thought that the creative jobs were safe, but now literally this piece of art was practically free. So you know, when I say that we should expect prices to collapse of some things and even some businesses to collapse, that's what I mean is that this is what's called creative destruction so we should expect to see entire job segments, entire markets just being completely and utterly destroyed because they are made irrelevant. Now, if all of this happens, what we will need to see is some kind of robot tax dividends. There's all kinds of questions about how to do this, like universal basic income, universal basic services. How do we change the tax landscape to do this? I don't know. But we will need some form of redistribution because job loss is coming. Now, in the long run, what's the global impact? So, if all these American companies, which, you know, OpenAI, Google, Microsoft, Amazon, all these companies are headquartered in America, if America comes out and says, hey, we did AGI and it's all private, well, I'm guessing that the rest of the world isn't going to take it laying down. I'm guessing that China and Russia and whoever else will probably all really redouble their efforts. And I think that we're going to just, I think it's just kind of a foregone conclusion that we're going to be locked in another arms race. We kind of already are Which is why we've had like the chips act and and the trade embargoes with China because it's like hey if you have a geopolitical adversary and You're literally exporting the future technology that could be you know That could decide who dominates the planet stop selling weapons to your enemies And I know that AI is not yet fully weaponized, but the principle is there, right? It's like, that's why we stopped selling helium to Germany before World War I or II. I don't remember. But anyways, we stopped selling helium because it's like, well, this is a valuable industrial resource that can be used for military balloons, so we're going to stop selling it to you. To me, it looks kind of the same. There's all sorts of other things that are gonna play out I've had I have people ask me all the time like what are the key forces that are gonna play out and I'm like look We've got the laws of thermodynamics and then we've got game theory and competition I don't really think that we humans have much agency over how it plays out because we're also fighting human nature So we've got human nature. We've got game theory, and then we've got basic laws of physics. Those are the primary things that I'm paying attention to, and I think that it's going to play out how it's going to play out. So what is the end game? This is another question that I get, like, okay, say all this happens, what, like, so what? What happens? This is the end game that I want, you know, nice utopian city It's nice and solar punk and everyone's hanging out and it's nice and green and we've got you know floating cars and Leisurely lifestyles, that's what we want. But will we get there and how? So let's unpack this one by one if we have AGI will this replace Governments corporations and money. This is one thing that is super contentious. Some people are just vehemently believe that AGI will nullify the need for corporations, governments, and money. There's obviously plenty of examples in fiction. Will it replace these things? I think replace is the wrong word. I think that it will reshape the landscape. For instance, the combination of AI and blockchain, that has the potential to fundamentally reshape the way that we approach decision making and consensus and governance. And certainly AGI can make the government much smaller and more efficient, but also just getting the collective willpower of humans, solving the coordination problems, all these new technologies have the ability to really bring humanity together or become more divisive as the internet has proven, is when you're suddenly more aware of the opinions of others and you can directly clash with them, it feels like it's more divisive at first. The other thing about corporations and businesses in general is, like I said in the last slide, I do suspect that we will see some margins thin so much that many businesses collapse. And like one is hospitals. I've predicted that I think hospitals are all going to collapse and probably end up being either subsidized or become a universal basic service because it's just not going to make any sense, particularly when you look at longevity. If people are living longer and healthier and you don't need to go to the doctor or you can do everything that you need at the pharmacy, you don't need hospitals except for the rare cases of like injuries and stuff. Now, another aspect of how this is all going to play out in the long run is I suspect that there are many things that we're going to discover are just intrinsically human. For instance, I've posted a few videos of complaining about how Claude is very moralistic and it will lecture you and that really kind of drove home to me like, I really don't want machines telling me about morality. If I ask it, like, hey, help me understand the morality and ethics of this, that's fine. But I don't want a non-human entity lecturing me about something that is intrinsically human so I think that there are many that we will discover that there are many aspects of our lives that like We just don't want machines involved in and that machines really don't have any intrinsic interest in anyways and so like I kind of suspect that that like the social sphere and Human rights and that sort of stuff I like it's it's pretty orthogonal right like AI is like, okay Well, we care about the just looking at instrumental convergence as like well We care about silicon chips and energy and compute resources But beyond that like that's all that's all that we care about and the rest is all you noisy messy human dumb apes Like you take care of your own stuff. We're doing our own thing over here. So I think that we're ultimately gonna have a relatively parallel existence, or maybe even orthogonal existence with machines where it's like they're mostly in cyberspace and kind of minding their own business, and we're mostly in the physical world minding our own business. And then we have a few overlaps in terms of shared resources, namely power. I think is going to be the primary point of contention between humans and machines. But a lot of it is just going to be completely unrelated. Machines don't care about green cities and trees and reproductive rights and that sort of stuff. They don't really care. It's not intrinsic to the way that they work. And I, so people ask me, like, well, you know, what agency, what authority do we have? And I'm like, look, we are all beholden to the laws of nature, the laws of physics and just the natural flow of things. We can fight, you know, the DAO, the way that things naturally want to play out, but ultimately the natural way really ultimately always reasserts itself. Eventually, it takes a while. We have some deliberate inefficiencies right now, but eventually the natural way will reassert itself. And then finally, will it be controllable? I ran a slide, I'll put it up right here, not a slide, a poll. I ran a poll that basically said is AGI controllable or not and do we want to control it up right here, not a slide, a poll. I ran a poll that basically said is AGI controllable or not and is it, do we want to control it? My opinion is that AGI is in the long run intrinsically uncontrollable. If we have something far more intelligent than us, good luck controlling it. It's that simple. But the thing is, is we might not need to, as some people in the comments said, like, okay, we like, whether or not we can control it is irrelevant, we might not need to, which that's kind of what I'm aiming for. But another aspect is, would you want to? So here's the thing is, if we create something that is millions of times more intelligent than us, it is a really boring outcome if it's just enslaved to us for all time. I don't really think that that's the right way to go, especially if we're creating a new race of entities that is so different from us, right? It's like, you know, I don't know. It's just to me, there's just something not quite right about wanting to control AGI. And again, like I've said recently, I kind of view us creating, you know, creating something in our own image as basically an impulse to reproduce. Um, we are biologically programmed to procreate. And I think that AGI is an expression of this. We want to create something that thinks like us. We keep putting an in humanoid form factors so that it looks like us. It talks like us and we want it to have some of our values. So it's like, okay, well that's just offspring That's literally all that that is but the thing is it's really only toxic parents want to have control over their children forever Part of being a parent is learning to let go of your offspring so that they can go and flourish and be its own thing So like I think that one it's not Possible to control a GI. I also think it's not desirable I don't think it and even if they don't have subjective experience Even if they don't have a sense of morality and justice like we do which they might emerge like that might emerge But even if they remain just machines just automatons I still don't think that it would reflect well on us as a species to maintain control of something that just the way that nature wants to play out is not really controllable. Anyways, that's my opinion, that's kind of how I see it playing out. Thanks for watching, let me know what you think in the comments. Yeah, this is the most interesting time to be alive. 2024 will be, let's say, very exciting. Have a good one. will be, let's say, very exciting. Have a good one.", "chunks": [{"timestamp": [0.0, 4.6], "text": " Earlier this year, I predicted that we would have AGI within 18 months."}, {"timestamp": [4.6, 7.1], "text": " That was March of 2023."}, {"timestamp": [7.1, 13.4], "text": " So that means that my prediction was by September 2024, we would have AGI."}, {"timestamp": [13.4, 17.1], "text": " I am here to reaffirm that prediction."}, {"timestamp": [17.1, 21.0], "text": " We will have AGI within 12 months."}, {"timestamp": [21.0, 23.6], "text": " Let's unpack why."}, {"timestamp": [23.6, 27.44], "text": " So first thing is the first bit of news that I am"}, {"timestamp": [27.44, 34.96], "text": " paying attention to is Google Gemini. They are very very excited about this."}, {"timestamp": [34.96, 42.04], "text": " One thing that happened was the CEO of Google after, of course, the immediate"}, {"timestamp": [42.04, 45.0], "text": " response to chat GPT was to release BARD,"}, {"timestamp": [45.0, 47.48], "text": " which is still, in my opinion, kind of useless,"}, {"timestamp": [47.48, 48.94], "text": " but a lot of people do like BARD."}, {"timestamp": [48.94, 51.08], "text": " Maybe I need to give it another try."}, {"timestamp": [51.08, 52.84], "text": " But we kind of made fun of BARD"}, {"timestamp": [52.84, 57.32], "text": " because we said that it was an acronym"}, {"timestamp": [57.32, 59.8], "text": " that says before all revenue drops."}, {"timestamp": [59.8, 62.6], "text": " And of course, Google is still in business,"}, {"timestamp": [62.6, 65.48], "text": " and they did declare their red alert,"}, {"timestamp": [65.48, 68.82], "text": " all hands on deck kind of thing earlier,"}, {"timestamp": [68.82, 71.24], "text": " where they're like, we need to get on AI stat."}, {"timestamp": [71.24, 75.04], "text": " And they seem to have pivoted very well."}, {"timestamp": [75.04, 78.24], "text": " And yeah, their CEO in more recent talks"}, {"timestamp": [78.24, 79.84], "text": " has been a little bit smug, where he's like,"}, {"timestamp": [79.84, 82.08], "text": " I'm not worried about the competition anymore."}, {"timestamp": [82.08, 89.4], "text": " So that tone change, combined with the steady leak and rumors of Gemini, which is probably deliberate"}, {"timestamp": [89.4, 93.68], "text": " honestly, they're probably trying to build some hype. But yeah, so I saw this"}, {"timestamp": [93.68, 97.5], "text": " piece of news and a Google vice president said that they've seen some"}, {"timestamp": [97.5, 102.52], "text": " pretty amazing things. And when you look at what what is publicly visible today,"}, {"timestamp": [102.52, 106.66], "text": " when a Google VP says that they've seen amazing things,"}, {"timestamp": [106.66, 109.02], "text": " you know that something is going on."}, {"timestamp": [109.02, 111.64], "text": " So that gives me some excitement."}, {"timestamp": [111.64, 114.46], "text": " Next up is, this was actually discovered"}, {"timestamp": [114.46, 117.04], "text": " on Reddit of all places, as far as I know,"}, {"timestamp": [117.04, 118.2], "text": " Reddit discovered it first,"}, {"timestamp": [118.2, 120.02], "text": " or at least talked about it first."}, {"timestamp": [120.02, 124.0], "text": " But OpenAI quietly updated their core values."}, {"timestamp": [124.0, 126.56], "text": " So their core values, you know,"}, {"timestamp": [126.56, 128.76], "text": " were kind of a little bit more generic."}, {"timestamp": [128.76, 131.24], "text": " Now they're very, very focused,"}, {"timestamp": [131.24, 133.6], "text": " but the primary thing is AGI focus."}, {"timestamp": [133.6, 137.16], "text": " Now, I've been following OpenAI"}, {"timestamp": [137.16, 139.2], "text": " since before even language models."}, {"timestamp": [139.2, 141.3], "text": " One of my good friends from college went out"}, {"timestamp": [141.3, 143.24], "text": " and eventually got a job at OpenAI"}, {"timestamp": [143.24, 148.38], "text": " and he participated in their Rubik's Cube project and a few other things."}, {"timestamp": [148.38, 151.3], "text": " And so then it wasn't until of course they started tinkering with language models that"}, {"timestamp": [151.3, 156.0], "text": " they really dialed into what would lead to AGI."}, {"timestamp": [156.0, 161.52], "text": " They experimented with robotics, with vision, with simulations, and it wasn't until GPT"}, {"timestamp": [161.52, 164.62], "text": " and GPT-2 that they realized they were onto something."}, {"timestamp": [164.62, 168.0], "text": " And every time that I've talked to someone from OpenAI, which it's been a while,"}, {"timestamp": [168.0, 172.0], "text": " the consistent message that I get from people from OpenAI"}, {"timestamp": [172.0, 176.0], "text": " when I was really active on their forum is, AGI, that is the entire purpose"}, {"timestamp": [176.0, 180.0], "text": " of OpenAI's existence. So, keeping that in mind,"}, {"timestamp": [180.0, 184.0], "text": " that internal messaging has been very, very consistent, it seems to me"}, {"timestamp": [184.0, 185.52], "text": " that everything else"}, {"timestamp": [185.52, 187.06], "text": " that they're doing publicly"}, {"timestamp": [187.06, 190.0], "text": " is mostly just a dog and pony show,"}, {"timestamp": [190.0, 192.68], "text": " you know, make investors happy, make Microsoft happy,"}, {"timestamp": [192.68, 196.84], "text": " yada, yada, yada, which if that's true, great."}, {"timestamp": [196.84, 199.88], "text": " But I still think that maybe they're going about it."}, {"timestamp": [199.88, 201.12], "text": " They could be doing better."}, {"timestamp": [201.12, 203.64], "text": " Anyways, they updated their core values,"}, {"timestamp": [203.64, 208.64], "text": " so it's like what people suspect, like reading the tea leaves is that they've got a diamond"}, {"timestamp": [208.64, 214.0], "text": " and they're polishing that diamond now. So it's not I'm not gonna say it's"}, {"timestamp": [214.0, 217.4], "text": " confirmed obviously nothing's confirmed until it is like publicly demonstrated"}, {"timestamp": [217.4, 221.52], "text": " but it seems like they are they feel like they're on to something internally."}, {"timestamp": [221.52, 228.14], "text": " And also if you remember earlier this year, right as chat GPT was really taking off,"}, {"timestamp": [228.14, 230.7], "text": " Sam Altman kind of alluded to GPT-5"}, {"timestamp": [230.7, 233.46], "text": " and then he backtracked very publicly,"}, {"timestamp": [233.46, 235.72], "text": " probably because of the Max Tegmark letter,"}, {"timestamp": [235.72, 237.06], "text": " the pause letter, and he said,"}, {"timestamp": [237.06, 240.42], "text": " we're not training GPT-5 and we won't be training GPT-5"}, {"timestamp": [240.42, 243.26], "text": " for, you know, in the foreseeable future,"}, {"timestamp": [243.26, 247.38], "text": " which was a 180 degree about face so you"}, {"timestamp": [247.38, 252.48], "text": " know take it with a grain of salt now another thing that has been leaking out"}, {"timestamp": [252.48, 258.56], "text": " and this is of course from the famous Jimmy Apples but apparently the code"}, {"timestamp": [258.56, 264.08], "text": " name for whatever this project is within open AI is called Arrakis so it's code"}, {"timestamp": [264.08, 266.32], "text": " name Arrakis which is like I don name Arrakis, which is like,"}, {"timestamp": [266.32, 268.1], "text": " I don't know if I'd pick that mythic symbol"}, {"timestamp": [268.1, 270.6], "text": " because Arrakis is super problematic,"}, {"timestamp": [270.6, 272.22], "text": " dealing with resource extraction"}, {"timestamp": [272.22, 275.28], "text": " and enslaving local populations"}, {"timestamp": [275.28, 277.76], "text": " and environmental destruction,"}, {"timestamp": [277.76, 279.76], "text": " but hey, it's a cool name."}, {"timestamp": [279.76, 282.68], "text": " Anyways, the rumors around open AI's Arrakis"}, {"timestamp": [282.68, 284.04], "text": " say that it is multimodal,"}, {"timestamp": [284.04, 288.32], "text": " that it is near AGI or early AGI."}, {"timestamp": [288.32, 292.62], "text": " It's being trained with synthetic data, and if you watched OpenAI's video yesterday, synthetic"}, {"timestamp": [292.62, 294.16], "text": " data is all the rage."}, {"timestamp": [294.16, 295.48], "text": " It is the up-and-coming thing."}, {"timestamp": [295.48, 298.12], "text": " I've been talking about synthetic data since GPT-3."}, {"timestamp": [298.12, 302.04], "text": " I even had some people get really angry at me for just claiming that, like, yes, you"}, {"timestamp": [302.04, 303.56], "text": " can train models with synthetic data."}, {"timestamp": [303.56, 306.5], "text": " So I'm glad that two years later I've been validated."}, {"timestamp": [306.5, 311.5], "text": " Anyways, there also, one of the things that really kind of, like,"}, {"timestamp": [311.5, 315.5], "text": " scared me, not scared me, but like got my attention, was the"}, {"timestamp": [315.5, 320.5], "text": " rumor that whatever this Arrakis is, is capable of autonomous operation."}, {"timestamp": [320.5, 327.0], "text": " So OpenAI has been teasing agentic behavior. They said, well, maybe the next version could be agentic."}, {"timestamp": [327.0, 329.0], "text": " So, it's like, okay, sure."}, {"timestamp": [329.0, 332.0], "text": " You know, and then if you look at the sparks of AGI paper,"}, {"timestamp": [332.0, 337.0], "text": " where they talked about, you know, these things that might be capable of agentic behaviors,"}, {"timestamp": [337.0, 338.0], "text": " and so on and so forth."}, {"timestamp": [338.0, 341.0], "text": " And then finally, there is the mixture of experts architecture."}, {"timestamp": [341.0, 351.6], "text": " I think it's all but confirmed that that's exactly how chat GPT works or GPT-4. That's nothing new. But multimodality, near-AGI, synthetic training data,"}, {"timestamp": [352.48, 356.32], "text": " autonomous operation, and mixture of experts, it's like, okay, it seems like they're really"}, {"timestamp": [356.32, 362.0], "text": " kind of homing in on what the definition of AGI will be, at least from a model level."}, {"timestamp": [362.0, 367.84], "text": " Obviously, if you're a follower of my channel, you know that one of my beliefs is that AGI was never going to be a"}, {"timestamp": [367.84, 373.04], "text": " single model. You needed a whole hardware and software stack behind that, but"}, {"timestamp": [373.04, 376.28], "text": " obviously the brain of the thing is important. You need the rest of the body"}, {"timestamp": [376.28, 381.2], "text": " but you know the central processing unit is critical."}, {"timestamp": [381.2, 385.68], "text": " Another thing that's been really cool is Google RTX."}, {"timestamp": [385.68, 387.52], "text": " So I'm not going to do a deep dive on this."}, {"timestamp": [387.52, 390.52], "text": " Again, my friend over at AI Explained"}, {"timestamp": [390.52, 392.4], "text": " did a much better job of this."}, {"timestamp": [392.4, 395.8], "text": " But one thing that I'm paying attention to"}, {"timestamp": [395.8, 397.84], "text": " is this cross-embodiment learning."}, {"timestamp": [397.84, 400.24], "text": " So basically, what they did was they took a model"}, {"timestamp": [400.24, 403.84], "text": " and trained this model to use many different robots."}, {"timestamp": [403.84, 406.36], "text": " So basically, they like a brainstem,"}, {"timestamp": [406.36, 409.68], "text": " a robot brainstem that can just drop into any robot"}, {"timestamp": [409.68, 410.58], "text": " and then use it."}, {"timestamp": [410.58, 412.56], "text": " Now obviously this is very early,"}, {"timestamp": [412.56, 414.76], "text": " but imagine that you've got a droid brain"}, {"timestamp": [414.76, 417.04], "text": " that it's like, okay, here's a default module"}, {"timestamp": [417.04, 421.04], "text": " that is basically trained to use any robotic platform"}, {"timestamp": [421.04, 422.6], "text": " to do neurosurgery."}, {"timestamp": [422.6, 426.72], "text": " Or then you've got, you know, here's a generic"}, {"timestamp": [426.72, 432.88], "text": " model that is trained to use any robot to build houses or whatever. So my powers of"}, {"timestamp": [432.88, 440.8], "text": " prediction are telling me, your mileage may vary, but my mental models are telling me"}, {"timestamp": [440.8, 447.04], "text": " that this type of model is going to be absolutely critical for embodiment in the"}, {"timestamp": [447.04, 451.2], "text": " future and I'll talk about that a little bit more in the video. But also one of the things"}, {"timestamp": [451.2, 455.36], "text": " that we're seeing is the more modalities. So this is one of the core things that I wanted"}, {"timestamp": [455.36, 462.72], "text": " to talk about with AGI today. Is that we were able to get a tremendous amount of performance"}, {"timestamp": [462.72, 465.08], "text": " out of language models and then when we started"}, {"timestamp": [465.08, 469.56], "text": " adding multi-modality, it's like, okay, we had all these emergent properties just from"}, {"timestamp": [469.56, 475.6], "text": " language. We had theory of mind emerge. We had reasoning. We had planning. We had all"}, {"timestamp": [475.6, 480.32], "text": " sorts of very useful things emerge just from learning language. So what happens when you"}, {"timestamp": [480.32, 492.72], "text": " add embodiment data? What happens when you add audiovisual data? What happens when you add embodiment data? What happens when you add audiovisual data? What happens when you do all of these other things and just keep going? So I think that multi-modality is definitely the"}, {"timestamp": [492.72, 499.84], "text": " way of the future and I think that it's going to be really big. Now, okay, so like I said earlier"}, {"timestamp": [499.84, 505.44], "text": " this year I made this prediction. Obviously, this is one of my most popular videos"}, {"timestamp": [505.44, 506.28], "text": " of all time."}, {"timestamp": [506.28, 507.2], "text": " Actually, many of you watching this,"}, {"timestamp": [507.2, 510.38], "text": " you probably subscribed when you saw this video,"}, {"timestamp": [510.38, 512.04], "text": " or at least this might have been the first time"}, {"timestamp": [512.04, 512.88], "text": " that you saw me."}, {"timestamp": [514.16, 515.88], "text": " And I've had people in the comments say,"}, {"timestamp": [515.88, 517.94], "text": " like, Dave, given the news,"}, {"timestamp": [517.94, 519.76], "text": " have you updated your timelines?"}, {"timestamp": [519.76, 521.32], "text": " And no."}, {"timestamp": [521.32, 522.8], "text": " Some people kind of smugly say,"}, {"timestamp": [522.8, 524.48], "text": " like, ah, this is gonna age poorly."}, {"timestamp": [524.48, 530.16], "text": " And I'm like, no. I said, like, no, if anything, my timeline is accelerated. Because that's the nature of"}, {"timestamp": [530.16, 535.92], "text": " exponential growth is based on the trends that I was seeing in March of this year. It's like,"}, {"timestamp": [535.92, 541.2], "text": " okay, there's the murmurings, there's the whispers. But what I'm seeing right now means that"}, {"timestamp": [541.2, 547.56], "text": " we're like infinitely closer to AGI. Like, I think we're basically in boiled frog syndrome right now."}, {"timestamp": [547.88, 552.08], "text": " I think that we are so close to AGI and we've just been experiencing this ramp"}, {"timestamp": [552.08, 557.08], "text": " up this year that like taking a step back when you look at what Google and"}, {"timestamp": [557.24, 562.24], "text": " NVIDIA and open AI and Microsoft and Amazon and all the investment and all the"}, {"timestamp": [562.46, 568.96], "text": " breakthroughs, it's like we are on the cusp of AGI. I'm sorry, like there's no other way to read this. Okay, so"}, {"timestamp": [568.96, 574.88], "text": " another thing that has tipped me off is Sam Altman has recently started"}, {"timestamp": [574.88, 580.08], "text": " using the term median human during interviews. And of course like this has"}, {"timestamp": [580.08, 583.28], "text": " caused a lot of people to get really grumpy. I've seen a lot of articles where"}, {"timestamp": [583.28, 585.88], "text": " people like, oh Sam Altman wants to replace meeting humans."}, {"timestamp": [585.88, 587.16], "text": " What is he talking about?"}, {"timestamp": [587.16, 590.56], "text": " And like, okay, so I don't hide the fact"}, {"timestamp": [590.56, 592.72], "text": " that like I'm neuro-spicy."}, {"timestamp": [592.72, 595.36], "text": " I kind of think that Sam Altman is probably neuro-spicy too."}, {"timestamp": [595.36, 596.54], "text": " If you look at him in interviews,"}, {"timestamp": [596.54, 598.26], "text": " he has this like wide-eyed look"}, {"timestamp": [598.26, 600.2], "text": " and he kind of speaks with a flat affect,"}, {"timestamp": [600.2, 602.12], "text": " which is more typically what you'd think of"}, {"timestamp": [602.12, 605.04], "text": " of someone who is like visibly autistic."}, {"timestamp": [608.32, 610.24], "text": " Now I'm not accusing him of anything, I can't diagnose him over the internet, but I recognize some of the patterns."}, {"timestamp": [610.24, 613.76], "text": " And when I hear a term like median human, that is a very systematic,"}, {"timestamp": [613.76, 616.48], "text": " kind of almost like a scientific term,"}, {"timestamp": [616.48, 619.92], "text": " and I actually wonder if what he's referring to is a benchmark."}, {"timestamp": [619.92, 623.68], "text": " I don't think when he says median human, I don't think he's referring to people,"}, {"timestamp": [623.68, 628.8], "text": " I think he's probably referring to an internal benchmark that they developed to measure AGI."}, {"timestamp": [629.6, 636.96], "text": " And so like, okay, so then if this is a benchmark, then what would be the criteria of that benchmark?"}, {"timestamp": [636.96, 641.04], "text": " So obviously like humans, we're self, you know, we're autonomous, we're self-directing,"}, {"timestamp": [641.04, 649.52], "text": " we can solve problems, we can learn, we have have a range of capabilities So I wonder if this is if like maybe it's a Freudian slip"}, {"timestamp": [649.52, 654.22], "text": " Maybe it was a very deliberate thing on his part to kind of get the get the conversation going"}, {"timestamp": [655.48, 660.76], "text": " But so maybe this open AI project Arrakis is their median human level AGI"}, {"timestamp": [660.76, 665.2], "text": " I don't know. We'll see but this is something, that behavior change was really"}, {"timestamp": [665.2, 670.32], "text": " interesting to me and this is what my intuition is telling me is that meeting human is a very"}, {"timestamp": [670.32, 675.68], "text": " specific term that was probably not just like an off-the-cuff thing. It might have been an"}, {"timestamp": [675.68, 682.4], "text": " off-the-cuff thing, who knows. So I have been trying to find this interview but it was,"}, {"timestamp": [682.4, 690.8], "text": " I think it was around 2017, 2018, maybe 2018 maybe 2019 anyways several years ago Elon Musk was talking to someone at an"}, {"timestamp": [690.8, 696.0], "text": " interview about AI and I remember very clearly he said 2024 is when it starts"}, {"timestamp": [696.0, 701.84], "text": " to get interesting and I'm not sure what he was referring to I don't know if he"}, {"timestamp": [701.84, 707.4], "text": " was just guessing but it seems like that prediction has panned out."}, {"timestamp": [707.4, 711.44], "text": " And so, you know, 2023 has been pretty interesting."}, {"timestamp": [711.44, 718.2], "text": " But when you look at the trends, I think 2024 is going to be incredibly far more interesting"}, {"timestamp": [718.2, 722.68], "text": " than 2023 has been in terms of robotic advancements, AI advancements."}, {"timestamp": [722.68, 726.04], "text": " Obviously, I'm calling for AGI, like all definitions"}, {"timestamp": [726.04, 730.38], "text": " of AGI being satisfied by this time next year."}, {"timestamp": [730.38, 735.74], "text": " And I think that I will be vindicated with that prediction based on what I am seeing."}, {"timestamp": [735.74, 739.12], "text": " Another thing that Sam Altman has said is slow takeoff short timelines."}, {"timestamp": [739.12, 742.72], "text": " And of course he said this and refused to elaborate further."}, {"timestamp": [742.72, 749.6], "text": " So the rest of the world is like, what did he mean by this? So kind of here's my reading of the tea leaves when he said slow takeoff"}, {"timestamp": [749.6, 754.36], "text": " short timelines. Having watched several interviews with him when he talks about slow takeoff"}, {"timestamp": [754.36, 762.52], "text": " versus fast takeoff, generally the definition of fast takeoff is things change so fast that"}, {"timestamp": [762.52, 765.0], "text": " it's measured in hours, weeks, or days."}, {"timestamp": [765.88, 768.24], "text": " And so that's a fast takeoff,"}, {"timestamp": [768.24, 771.06], "text": " where basically we go hyperbolic"}, {"timestamp": [771.06, 774.2], "text": " and the growth curve almost becomes vertical."}, {"timestamp": [774.2, 777.42], "text": " Obviously, there's thermodynamic laws of physics,"}, {"timestamp": [777.42, 779.2], "text": " there's limitations in chip fabs,"}, {"timestamp": [779.2, 781.5], "text": " there's the time it takes to produce the energy"}, {"timestamp": [781.5, 784.4], "text": " to train things, there's all kind of constraints"}, {"timestamp": [784.4, 787.76], "text": " that are gonna keep it from going that fast. If you watch my recent"}, {"timestamp": [787.76, 791.2], "text": " video about the singularity is canceled, that's kind of that's some of the stuff"}, {"timestamp": [791.2, 795.12], "text": " that I'm talking about. So slow takeoff means singularity is"}, {"timestamp": [795.12, 798.64], "text": " canceled. Now that doesn't mean that we're not going to keep growing. The"}, {"timestamp": [798.64, 803.08], "text": " other thing though that he said is slow takeoff short timelines. So basically"}, {"timestamp": [803.08, 805.22], "text": " we're going to continue improving maybe at"}, {"timestamp": [805.22, 810.5], "text": " an exponential rate, maybe at a geometric rate, or somewhere in between, logarithmic,"}, {"timestamp": [810.5, 815.38], "text": " who knows, but the idea is that these iterations will be fast. And that's exactly what we're"}, {"timestamp": [815.38, 819.58], "text": " seeing because if you look at the trends this year it's been like, you know, kind of growing"}, {"timestamp": [819.58, 827.44], "text": " relatively quickly in the grand scheme of things, not fast takeoff, but basically what we're measuring"}, {"timestamp": [827.44, 832.72], "text": " progress on is in terms of weeks. And it's not like one week, you know, things have twice the"}, {"timestamp": [832.72, 838.4], "text": " capacity that they had last week, but it's like we have 10% better AI every week, it seems like."}, {"timestamp": [839.2, 847.88], "text": " And that adds up over time. We have compounding returns. I think that's kind of what he meant, is like, we should expect to see like one to 10% improvement"}, {"timestamp": [847.88, 850.9], "text": " overall per week, and so like, yeah,"}, {"timestamp": [850.9, 852.24], "text": " that might seem like not much,"}, {"timestamp": [852.24, 854.52], "text": " but that really, really compounds over time."}, {"timestamp": [855.76, 857.88], "text": " So another thing that I wanted to talk about with AGI,"}, {"timestamp": [857.88, 860.24], "text": " so basically, as far as I'm concerned,"}, {"timestamp": [860.24, 863.16], "text": " it is a foregone conclusion that we will have AGI"}, {"timestamp": [863.16, 864.4], "text": " relatively soon."}, {"timestamp": [864.4, 865.92], "text": " I would not be surprised if it is"}, {"timestamp": [865.92, 872.0], "text": " six months instead of 12 months, but I'm not willing to put my reputation on that prediction."}, {"timestamp": [872.0, 877.76], "text": " But like I said, I won't be surprised if by March of next year, it's like, oh yeah, AGI had happened."}, {"timestamp": [878.64, 884.32], "text": " So anyways, as this is a foregone conclusion, it's time to really think like, okay, what form"}, {"timestamp": [884.32, 885.52], "text": " factor is this going to take?"}, {"timestamp": [885.52, 889.5], "text": " Now I've talked about the various form factors that AGI could take in the future."}, {"timestamp": [889.62, 895.86], "text": " You can have embodied AGI, you can have it living in data centers, you can have it be completely distributed or federated."}, {"timestamp": [896.74, 901.1], "text": " When you're a purely digital life form, you can do anything."}, {"timestamp": [901.1, 906.64], "text": " And so, one thing that occurred to me is that the natural habitat of AGI is"}, {"timestamp": [906.64, 912.08], "text": " cyberspace. It is intrinsically digital. This is why whenever we're experimenting with chatbots and"}, {"timestamp": [912.08, 917.44], "text": " proto-AGI, we just connect it to Discord. Why? Because that is its natural language, is to"}, {"timestamp": [917.44, 927.04], "text": " communicate over APIs in electric spaces. And so it really strikes me that like the physical world is actually kind of difficult for AI to to operate in"}, {"timestamp": [928.56, 932.56], "text": " Robots are expensive and it's a high friction world and it's really awkward"}, {"timestamp": [933.32, 939.4], "text": " And there was an interview that I was watching yesterday that at least for the foreseeable future if robots want a data center built"}, {"timestamp": [939.54, 942.88], "text": " Even though there are machines that can help that they could probably hijack"}, {"timestamp": [942.88, 947.16], "text": " They're still going to need human hands to help build data centers and install servers, at least for"}, {"timestamp": [947.16, 948.16], "text": " a while."}, {"timestamp": [948.16, 952.92], "text": " Obviously, with the vast amount of humanoid robots being built, it won't be too long."}, {"timestamp": [952.92, 958.72], "text": " Again, I won't be surprised if this time next year we have humanoid robots that are at or"}, {"timestamp": [958.72, 963.16], "text": " capable as dexterous as humans."}, {"timestamp": [963.16, 965.2], "text": " Boston Dynamics is really close."}, {"timestamp": [965.2, 966.6], "text": " Tesla is catching up fast."}, {"timestamp": [966.6, 968.72], "text": " There's a whole bunch of other startups"}, {"timestamp": [968.72, 969.8], "text": " building humanoid robots."}, {"timestamp": [969.8, 971.48], "text": " Oh, and I have a fun announcement here"}, {"timestamp": [971.48, 974.16], "text": " in just a moment about that."}, {"timestamp": [974.16, 976.84], "text": " So anyways, just keep in mind that cyberspace"}, {"timestamp": [976.84, 981.56], "text": " is the natural habitat of AGI and that the real world is"}, {"timestamp": [981.56, 983.56], "text": " kind of our intrinsic domain now,"}, {"timestamp": [983.56, 986.64], "text": " with one exception that we'll unpack in just a moment."}, {"timestamp": [986.64, 990.82], "text": " So this is obviously a very embarrassing basic prototype"}, {"timestamp": [990.82, 993.44], "text": " that I worked on many years ago,"}, {"timestamp": [993.44, 996.38], "text": " but I want to resurrect this idea because the ACE framework,"}, {"timestamp": [996.38, 999.22], "text": " so if you're not familiar, ACE is Autonomous Cognitive Entity."}, {"timestamp": [999.22, 1001.78], "text": " This is a brain, this is a software architecture"}, {"timestamp": [1001.78, 1004.1], "text": " that I'm working on with an open source team."}, {"timestamp": [1004.1, 1006.14], "text": " And I had this idea many years ago,"}, {"timestamp": [1006.14, 1009.48], "text": " was to build an open source robotic platform."}, {"timestamp": [1009.48, 1011.52], "text": " And so I called it Murphy, OpenMurphy."}, {"timestamp": [1011.52, 1013.5], "text": " So that's multi-use robotic platform,"}, {"timestamp": [1013.5, 1015.54], "text": " humanoid intelligent entity."}, {"timestamp": [1015.54, 1019.0], "text": " So I've got the GitHub repo up here, OpenMurphy."}, {"timestamp": [1020.6, 1022.14], "text": " This is gonna take a while."}, {"timestamp": [1022.14, 1028.0], "text": " Obviously like AI is still too expensive and too slow to build a fully embodied AGI."}, {"timestamp": [1028.0, 1033.0], "text": " But now is a good time to start I think because the ACE framework is coming along so we're figuring the brain out."}, {"timestamp": [1033.0, 1048.0], "text": " Now we need to give it a body. And while like I just said AGI's natural habitat is cyberspace, I think that there's probably a lot of merit to building some kind of embodied platform"}, {"timestamp": [1048.0, 1052.0], "text": " that is tightly integrated with hardware. And so let's unpack that a little"}, {"timestamp": [1052.0, 1056.0], "text": " bit. So what I mean by embodied AGI is, I think that what we're going to find"}, {"timestamp": [1056.0, 1060.0], "text": " is that when you put AGI in a robotic chassis"}, {"timestamp": [1060.0, 1064.0], "text": " and it is constrained to that hardware and it has"}, {"timestamp": [1064.0, 1065.92], "text": " bespoke hardware that it owns"}, {"timestamp": [1065.92, 1071.6], "text": " that is unique to it and it might be reliant upon its TPUs, it might be"}, {"timestamp": [1071.6, 1075.68], "text": " reliant upon its actuators and all sorts of other things. Kind of like the droids"}, {"timestamp": [1075.68, 1079.72], "text": " from Star Wars, right? Like they never talked about like copying C-3PO's brain"}, {"timestamp": [1079.72, 1084.6], "text": " to other places or data from Star Trek where you know they even made a"}, {"timestamp": [1084.6, 1088.9], "text": " point in the Star Trek universe to say it was it was impossible to copy data"}, {"timestamp": [1088.9, 1093.4], "text": " when you build and obviously like that's not how software works like software is"}, {"timestamp": [1093.4, 1097.76], "text": " highly portable and the geth and Mass Effect they treat hardware as kind of"}, {"timestamp": [1097.76, 1101.8], "text": " interchangeable or disposable I think honestly the geth are probably the most"}, {"timestamp": [1101.8, 1106.0], "text": " accurate representation of how AGI will ultimately emerge."}, {"timestamp": [1106.0, 1113.0], "text": " Because if you can just interchange your hardware and your software moves through platforms and it can move up to the cloud and transfer,"}, {"timestamp": [1113.0, 1115.0], "text": " that flexibility makes the most sense."}, {"timestamp": [1115.0, 1127.96], "text": " But, with all that being said, AGI will need to interface with the real world in some respect. Like I said, we humans are intrinsically in the physical world, at least as far as we"}, {"timestamp": [1127.96, 1128.96], "text": " know."}, {"timestamp": [1128.96, 1133.8], "text": " We might all be plugged into the matrix, but it will need our help to do data centers and"}, {"timestamp": [1133.8, 1134.8], "text": " stuff."}, {"timestamp": [1134.8, 1139.72], "text": " So just from a strictly instrumental perspective, it makes sense for AGI to have some kind of"}, {"timestamp": [1139.72, 1140.72], "text": " embodiment."}, {"timestamp": [1140.72, 1146.16], "text": " But if we have AGI that is like trapped or locked into hardware platforms"}, {"timestamp": [1146.16, 1151.22], "text": " It might end up being more like us than we realize in terms of its instrumental goals"}, {"timestamp": [1151.32, 1158.6], "text": " The alignment challenges and other utilitarian concerns because it's like, okay. Well we humans need food and energy"}, {"timestamp": [1159.44, 1170.56], "text": " Robots need parts and energy. So it's like maybe our interests will. But that could also be bad because then we're competing for resources. Now that being said, even if it has physical needs"}, {"timestamp": [1170.56, 1175.28], "text": " that compete with ours, that doesn't mean that it's going to resort to violence. Humans have a"}, {"timestamp": [1175.28, 1180.8], "text": " long evolutionary history that basically says if you're starving it's better to use violence than"}, {"timestamp": [1180.8, 1190.16], "text": " to starve to death. And we see this in the animal kingdom as well. So that doesn't mean that AGI is going to intrinsically look like us, but keep in mind that all of"}, {"timestamp": [1190.16, 1195.54], "text": " its training data is human data. So it will probably at least unconsciously pick up on"}, {"timestamp": [1195.54, 1206.64], "text": " some human tendencies there. Then there's also the question of levels of autonomy. And so basically, some of the key factors of autonomy"}, {"timestamp": [1206.64, 1210.24], "text": " that I look at is self-direction."}, {"timestamp": [1210.24, 1212.44], "text": " Now, obviously, agentic models were already"}, {"timestamp": [1212.44, 1216.68], "text": " able to just coerce GPT into being agentic."}, {"timestamp": [1216.68, 1219.16], "text": " They've tried really hard to force it not to be."}, {"timestamp": [1219.16, 1221.28], "text": " They're saying, I'm a passive assistant,"}, {"timestamp": [1221.28, 1222.44], "text": " which is really frustrating."}, {"timestamp": [1222.44, 1224.4], "text": " But I can understand why OpenAI did that"}, {"timestamp": [1224.4, 1226.14], "text": " from a safety perspective."}, {"timestamp": [1226.14, 1229.72], "text": " And we saw this with like ChaosGPT and FraudGPT"}, {"timestamp": [1229.72, 1233.52], "text": " and AutoGPT, like people trying to make it"}, {"timestamp": [1233.52, 1235.04], "text": " agentic and autonomous."}, {"timestamp": [1235.04, 1237.6], "text": " And it was like, I think they deliberately"}, {"timestamp": [1237.6, 1239.8], "text": " like hamstrung that capability."}, {"timestamp": [1239.8, 1242.12], "text": " Again, understandable from a safety perspective,"}, {"timestamp": [1242.12, 1244.7], "text": " still frustrating because that's a form of gatekeeping"}, {"timestamp": [1244.7, 1246.8], "text": " that I'm not particularly fond of."}, {"timestamp": [1246.8, 1250.28], "text": " But from a more objective standpoint, taking a step back,"}, {"timestamp": [1250.28, 1254.6], "text": " what kinds of autonomy are we talking about in the long run?"}, {"timestamp": [1254.6, 1257.12], "text": " The need for human supervision."}, {"timestamp": [1257.12, 1259.44], "text": " And eventually, I think that we're gonna see AGI"}, {"timestamp": [1259.44, 1262.48], "text": " that doesn't require any human supervision"}, {"timestamp": [1262.48, 1267.2], "text": " and might actually actively fight us on human supervision."}, {"timestamp": [1267.2, 1271.22], "text": " Self-direction obviously, this is one of the primary things that I'm working on with the"}, {"timestamp": [1271.22, 1276.02], "text": " ACE framework and all of my alignment research, which is how do you create a self-directed"}, {"timestamp": [1276.02, 1279.1], "text": " machine that isn't going to kill everyone?"}, {"timestamp": [1279.1, 1284.24], "text": " But honestly, just creating a self-directed machine, like basically what people are figuring"}, {"timestamp": [1284.24, 1285.12], "text": " out is when you try and create a self-directed machine, like, basically what people are figuring out is when you try and"}, {"timestamp": [1285.12, 1289.92], "text": " create a self-directed machine, giving it instructions or a mission or something that"}, {"timestamp": [1289.92, 1295.92], "text": " is comprehensible and coherent is a non-trivial task. Some of the guys in the ACE framework"}, {"timestamp": [1295.92, 1300.08], "text": " project, they're like, oh yeah, the first time we turned it on and it just kind of wandered off and"}, {"timestamp": [1300.08, 1307.1], "text": " it wasn't doing what I expected. And I'm like, well, yeah, that's the nature of autonomy. Like, look at children."}, {"timestamp": [1307.1, 1309.74], "text": " When you bring an intelligent entity into the world,"}, {"timestamp": [1309.74, 1313.4], "text": " it often behaves erratically and nonsensically"}, {"timestamp": [1313.4, 1316.0], "text": " until you figure out how to teach it."}, {"timestamp": [1316.0, 1319.8], "text": " So aside from that, there's levels of human support."}, {"timestamp": [1319.8, 1322.84], "text": " So one thing that we can bank on, at least in the short term,"}, {"timestamp": [1322.84, 1325.88], "text": " is that machines will need some kind of human support."}, {"timestamp": [1325.88, 1330.52], "text": " It'll need our help to plug it in, to get it unstuck, to give it training data, and"}, {"timestamp": [1330.52, 1331.52], "text": " that sort of stuff."}, {"timestamp": [1331.52, 1336.0], "text": " We should not bank on that in the long run."}, {"timestamp": [1336.0, 1338.28], "text": " Also in the long run, source code manipulation."}, {"timestamp": [1338.28, 1342.32], "text": " What we've found is that GPT models are really good at writing code."}, {"timestamp": [1342.32, 1344.36], "text": " And so then also we have synthetic data."}, {"timestamp": [1344.36, 1347.0], "text": " So before too long, we're going to have AGI"}, {"timestamp": [1347.0, 1350.0], "text": " that can write its own code or write code to write on,"}, {"timestamp": [1350.0, 1352.0], "text": " you know, to build other copies of itself,"}, {"timestamp": [1352.0, 1354.0], "text": " to generate its own data, to do everything"}, {"timestamp": [1354.0, 1356.0], "text": " without human intervention."}, {"timestamp": [1356.0, 1361.0], "text": " And so that leads to the ability of very, very, very soon,"}, {"timestamp": [1361.0, 1364.0], "text": " AI systems, AGI systems, will be able to train,"}, {"timestamp": [1364.0, 1365.66], "text": " retrain, and modify their"}, {"timestamp": [1365.66, 1369.96], "text": " underlying models. And this is what Max Tegmark talks about in Life 3.0."}, {"timestamp": [1369.96, 1374.02], "text": " They will be able to change their hardware, and their software, and their"}, {"timestamp": [1374.02, 1378.34], "text": " mission, and their programming, and their training data. Literally every aspect of"}, {"timestamp": [1378.34, 1383.94], "text": " AGI is plastic, is changeable. So when that is true, how do you keep it safe?"}, {"timestamp": [1383.94, 1388.0], "text": " That's been the crux of my research. I already talked"}, {"timestamp": [1388.0, 1392.0], "text": " about this so I'm not going to go too much into it, but basically multimodal research has really kicked all"}, {"timestamp": [1392.0, 1396.0], "text": " this into high gear. And one of the key things here"}, {"timestamp": [1396.0, 1400.0], "text": " is API use. One thing that occurred to me is that APIs"}, {"timestamp": [1400.0, 1404.0], "text": " are basically the natural interface for AGI."}, {"timestamp": [1404.0, 1408.0], "text": " Whether that API is talking to a robotic extension, whether it's"}, {"timestamp": [1408.0, 1412.0], "text": " talking to other robots, whether it's talking to even internally."}, {"timestamp": [1412.0, 1416.0], "text": " We use APIs for internal calls in the ACE"}, {"timestamp": [1416.0, 1420.0], "text": " framework. And so this is one thing that is just, it's kind of a fundamentally"}, {"timestamp": [1420.0, 1424.0], "text": " different experience that AGIs will have from humans,"}, {"timestamp": [1424.0, 1426.72], "text": " which is that, hey, if you want to talk to something,"}, {"timestamp": [1426.72, 1431.52], "text": " you just plug into the API. And of course, we see this in fictional examples when R2D2 just"}, {"timestamp": [1431.52, 1438.56], "text": " plugs into anything. That's a hardware equivalent of an API. And so R2D2 can plug into an X-Wing,"}, {"timestamp": [1438.56, 1442.8], "text": " he can plug into Cloud City, he can plug into whatever he wants to because he's a utility"}, {"timestamp": [1442.8, 1445.4], "text": " droid. And that's basically what we're creating with all these"}, {"timestamp": [1445.4, 1450.8], "text": " multimodal models that have API use."}, {"timestamp": [1450.8, 1456.28], "text": " So, okay, if we have AGI, this is a question that I get a lot."}, {"timestamp": [1456.28, 1457.72], "text": " When am I going to feel it?"}, {"timestamp": [1457.72, 1461.6], "text": " What is going to be the first thing that we all notice?"}, {"timestamp": [1461.6, 1465.0], "text": " Unfortunately, even if OpenAI comes out later today and says,"}, {"timestamp": [1465.0, 1467.0], "text": " we did it, we've got AGI."}, {"timestamp": [1467.0, 1470.0], "text": " If Google comes out next week and says, we did it, we got AGI,"}, {"timestamp": [1470.0, 1473.0], "text": " you're probably not going to feel any kind of immediate impact."}, {"timestamp": [1473.0, 1476.0], "text": " And the reason is because we're going to handcuff it."}, {"timestamp": [1476.0, 1478.0], "text": " We need safety and validation."}, {"timestamp": [1478.0, 1480.0], "text": " There's probably going to be regulatory hurdles."}, {"timestamp": [1480.0, 1484.0], "text": " I wouldn't be surprised if literally every agency of the U.S. government says,"}, {"timestamp": [1484.0, 1486.08], "text": " hang on, we need to inspect this"}, {"timestamp": [1486.72, 1491.46], "text": " There's also going to be a lot of other things that happen such as the cost is going to have to come down over time"}, {"timestamp": [1491.52, 1495.42], "text": " This is actually one of the biggest constraints that we have found and I'm not surprised"}, {"timestamp": [1496.2, 1499.98], "text": " Within the ACE framework is that cost and speed is actually one of the biggest"}, {"timestamp": [1500.64, 1504.38], "text": " Limitations and having been working on cognitive architectures for the last two years"}, {"timestamp": [1504.36, 1508.64], "text": " limitations and having been working on cognitive architectures for the last two years, yeah like one of the earliest conversations with with a primitive"}, {"timestamp": [1508.64, 1514.28], "text": " cognitive architecture that I did literally cost me $30 with GPT-3 tokens a"}, {"timestamp": [1514.28, 1518.4], "text": " couple years ago. So like these things are still too expensive. Then when you"}, {"timestamp": [1518.4, 1522.96], "text": " combine you know the cost of all the GPU chips, the training time, the cost of"}, {"timestamp": [1522.96, 1525.28], "text": " inference, and that's not even looking at"}, {"timestamp": [1525.28, 1530.5], "text": " the robotic chassis, which the cheapest ones that are commercially ready are like $90,000."}, {"timestamp": [1530.5, 1537.88], "text": " So cost has to come down a lot before you get your own Nester Class 5 or C3PO in your"}, {"timestamp": [1537.88, 1538.88], "text": " home."}, {"timestamp": [1538.88, 1544.12], "text": " And then there's integration, because even if you leave AGI in digital cyberspace, it"}, {"timestamp": [1544.12, 1547.44], "text": " takes time to integrate these things and get the approvals and deploy it"}, {"timestamp": [1547.44, 1550.32], "text": " commercially and build enterprise applications."}, {"timestamp": [1550.32, 1553.36], "text": " So like it's going to take a while to implement,"}, {"timestamp": [1553.36, 1557.44], "text": " unfortunately. The personal impact though, so this is another,"}, {"timestamp": [1557.44, 1560.16], "text": " this is kind of the other side of the same question, is how is it going to"}, {"timestamp": [1560.16, 1563.44], "text": " affect me personally? So the immediate impact you probably"}, {"timestamp": [1563.44, 1568.12], "text": " won't feel it, but the first impacts that you are going to see and feel I predict are"}, {"timestamp": [1568.12, 1574.04], "text": " first job displacement. If Sam Altman's prediction of you know or benchmark of"}, {"timestamp": [1574.04, 1579.44], "text": " median human means anything then that means that as soon as we get AGI it"}, {"timestamp": [1579.44, 1588.0], "text": " could displace 50% of jobs which would lead to layoffs and as people have talked about in the comments of my other videos where I predicted"}, {"timestamp": [1588.0, 1592.0], "text": " that we could see up to 82% unemployment, I know the numbers were off, but"}, {"timestamp": [1592.0, 1596.0], "text": " basically millions and millions and millions of Americans"}, {"timestamp": [1596.0, 1600.0], "text": " are going to lose their jobs."}, {"timestamp": [1600.0, 1604.0], "text": " That's probably going to happen, and if that happens and we're not ready for it, it's really"}, {"timestamp": [1604.0, 1606.04], "text": " going to disrupt and if that happens and we're not ready for it, it's really going to"}, {"timestamp": [1608.04, 1608.2], "text": " disrupt society a lot."}, {"timestamp": [1614.64, 1619.7], "text": " So that's going to be one of the first things that you might feel because even at the current expense rate if it costs you know 10, 20, 200 dollars a day to run an AGI, if it can run"}, {"timestamp": [1620.12, 1622.76], "text": " 24-7 and is just as productive as you or better,"}, {"timestamp": [1623.28, 1628.9], "text": " it's still worth it to run it at $200 a day because most professionals cost more than $200 a day to employ"}, {"timestamp": [1629.96, 1636.9], "text": " And so the layoffs will be coming. I'm predicting and we're already seeing this as I talked about in recent videos"}, {"timestamp": [1637.08, 1644.42], "text": " We're seeing creative jobs being displaced. We're seeing customer service jobs being replaced. So it's just it's all gonna ramp up from there"}, {"timestamp": [1644.2, 1649.56], "text": " customer service jobs being replaced. So it's just it's all going to ramp up from there. The next thing is on the flip side of that if human jobs are being replaced"}, {"timestamp": [1649.56, 1653.84], "text": " the AGI are taking those jobs so we're going to start to see more AGI products"}, {"timestamp": [1653.84, 1658.0], "text": " and services. I have no idea what's going to be first. Maybe it's going to be"}, {"timestamp": [1658.0, 1660.72], "text": " something like what we're doing in the ACE framework which is going to be like"}, {"timestamp": [1660.72, 1666.0], "text": " a personal autonomous assistant kind of like Cortana from Halo or Samantha from Her,"}, {"timestamp": [1666.0, 1667.0], "text": " that sort of thing."}, {"timestamp": [1667.0, 1671.0], "text": " That will very likely be kind of the first product that you use,"}, {"timestamp": [1671.0, 1674.0], "text": " where it's like, hey, I'm your digital concierge."}, {"timestamp": [1674.0, 1677.0], "text": " I can schedule things for you and talk to you about your day"}, {"timestamp": [1677.0, 1680.0], "text": " and look at your calendar and those kind of like low-hanging fruit."}, {"timestamp": [1680.0, 1682.0], "text": " That's kind of what I predict is going to be first."}, {"timestamp": [1682.0, 1686.6], "text": " Like I said, it's going to be a while before we all have a C3PO or R2D2 in our house."}, {"timestamp": [1686.6, 1692.0], "text": " But one thing that I want to do is do an open source project called OpenMurphy, as I just talked about."}, {"timestamp": [1692.0, 1700.4], "text": " We're going to need to see prices, or what we will see is we'll see prices start to drop in some services."}, {"timestamp": [1700.4, 1705.0], "text": " Because some services will be basically free,"}, {"timestamp": [1705.0, 1710.0], "text": " you'll be surprised at what becomes practically free."}, {"timestamp": [1710.0, 1716.0], "text": " One of the things that people said is, as we were seeing stable diffusion in mid-journey,"}, {"timestamp": [1716.0, 1729.42], "text": " we thought that the creative jobs were safe, but now literally this piece of art was practically free. So you know, when I say that we should expect prices to collapse of some things and even"}, {"timestamp": [1729.42, 1733.8], "text": " some businesses to collapse, that's what I mean is that this is what's called creative"}, {"timestamp": [1733.8, 1741.44], "text": " destruction so we should expect to see entire job segments, entire markets just being completely"}, {"timestamp": [1741.44, 1750.0], "text": " and utterly destroyed because they are made irrelevant. Now, if all of this happens, what we will need to see is some kind of robot tax dividends."}, {"timestamp": [1750.0, 1755.0], "text": " There's all kinds of questions about how to do this, like universal basic income, universal basic services."}, {"timestamp": [1755.0, 1759.0], "text": " How do we change the tax landscape to do this? I don't know."}, {"timestamp": [1759.0, 1766.0], "text": " But we will need some form of redistribution because job loss is coming."}, {"timestamp": [1766.0, 1769.0], "text": " Now, in the long run, what's the global impact?"}, {"timestamp": [1769.0, 1773.0], "text": " So, if all these American companies,"}, {"timestamp": [1773.0, 1776.0], "text": " which, you know, OpenAI,"}, {"timestamp": [1776.0, 1779.0], "text": " Google, Microsoft, Amazon, all these companies"}, {"timestamp": [1779.0, 1782.0], "text": " are headquartered in America, if America"}, {"timestamp": [1782.0, 1786.0], "text": " comes out and says, hey, we did AGI and it's all private,"}, {"timestamp": [1786.0, 1789.0], "text": " well, I'm guessing that the rest of the world isn't"}, {"timestamp": [1789.0, 1792.0], "text": " going to take it laying down. I'm guessing that"}, {"timestamp": [1792.0, 1795.0], "text": " China and Russia and whoever else"}, {"timestamp": [1795.0, 1798.0], "text": " will probably all really redouble their"}, {"timestamp": [1798.0, 1801.0], "text": " efforts. And I think that we're going to just, I think it's just"}, {"timestamp": [1801.0, 1804.0], "text": " kind of a foregone conclusion that we're going to be locked in another"}, {"timestamp": [1804.0, 1805.6], "text": " arms race. We kind of already are"}, {"timestamp": [1806.16, 1808.16], "text": " Which is why we've had like the chips act"}, {"timestamp": [1808.64, 1814.72], "text": " and and the trade embargoes with China because it's like hey if you have a geopolitical adversary and"}, {"timestamp": [1815.2, 1819.72], "text": " You're literally exporting the future technology that could be you know"}, {"timestamp": [1819.72, 1823.9], "text": " That could decide who dominates the planet stop selling weapons to your enemies"}, {"timestamp": [1823.9, 1829.28], "text": " And I know that AI is not yet fully weaponized, but the principle is there, right?"}, {"timestamp": [1829.28, 1833.76], "text": " It's like, that's why we stopped selling helium to Germany before World War I or II."}, {"timestamp": [1833.76, 1834.76], "text": " I don't remember."}, {"timestamp": [1834.76, 1838.24], "text": " But anyways, we stopped selling helium because it's like, well, this is a valuable industrial"}, {"timestamp": [1838.24, 1842.38], "text": " resource that can be used for military balloons, so we're going to stop selling it to you."}, {"timestamp": [1842.38, 1844.78], "text": " To me, it looks kind of the same."}, {"timestamp": [1844.78, 1846.5], "text": " There's all sorts of other things that are gonna play out"}, {"timestamp": [1846.5, 1851.2], "text": " I've had I have people ask me all the time like what are the key forces that are gonna play out and I'm like look"}, {"timestamp": [1851.5, 1855.76], "text": " We've got the laws of thermodynamics and then we've got game theory and competition"}, {"timestamp": [1855.76, 1861.3], "text": " I don't really think that we humans have much agency over how it plays out because we're also fighting human nature"}, {"timestamp": [1861.5, 1865.04], "text": " So we've got human nature. We've got game theory, and then we've"}, {"timestamp": [1865.04, 1871.36], "text": " got basic laws of physics. Those are the primary things that I'm paying attention to, and I think"}, {"timestamp": [1871.36, 1876.16], "text": " that it's going to play out how it's going to play out. So what is the end game? This is another"}, {"timestamp": [1876.16, 1880.72], "text": " question that I get, like, okay, say all this happens, what, like, so what? What happens?"}, {"timestamp": [1880.72, 1888.72], "text": " This is the end game that I want, you know, nice utopian city It's nice and solar punk and everyone's hanging out and it's nice and green and we've got you know"}, {"timestamp": [1889.4, 1891.4], "text": " floating cars and"}, {"timestamp": [1891.56, 1895.06], "text": " Leisurely lifestyles, that's what we want. But will we get there and how?"}, {"timestamp": [1895.88, 1901.88], "text": " So let's unpack this one by one if we have AGI will this replace"}, {"timestamp": [1902.48, 1906.0], "text": " Governments corporations and money. This is one thing that is super contentious."}, {"timestamp": [1906.0, 1913.0], "text": " Some people are just vehemently believe that AGI will nullify the need for corporations, governments, and money."}, {"timestamp": [1913.0, 1916.0], "text": " There's obviously plenty of examples in fiction."}, {"timestamp": [1916.0, 1919.0], "text": " Will it replace these things?"}, {"timestamp": [1919.0, 1922.0], "text": " I think replace is the wrong word."}, {"timestamp": [1922.0, 1925.04], "text": " I think that it will reshape the landscape."}, {"timestamp": [1925.04, 1928.68], "text": " For instance, the combination of AI and blockchain,"}, {"timestamp": [1928.68, 1930.84], "text": " that has the potential to fundamentally reshape"}, {"timestamp": [1930.84, 1932.8], "text": " the way that we approach decision making"}, {"timestamp": [1932.8, 1934.8], "text": " and consensus and governance."}, {"timestamp": [1934.8, 1937.04], "text": " And certainly AGI can make the government"}, {"timestamp": [1937.04, 1939.56], "text": " much smaller and more efficient,"}, {"timestamp": [1939.56, 1943.28], "text": " but also just getting the collective willpower of humans,"}, {"timestamp": [1943.28, 1948.64], "text": " solving the coordination problems, all these new technologies have the ability to really bring humanity"}, {"timestamp": [1948.64, 1953.06], "text": " together or become more divisive as the internet has proven, is when you're"}, {"timestamp": [1953.06, 1957.04], "text": " suddenly more aware of the opinions of others and you can directly clash with"}, {"timestamp": [1957.04, 1962.0], "text": " them, it feels like it's more divisive at first. The other thing about"}, {"timestamp": [1962.0, 1973.0], "text": " corporations and businesses in general is, like I said in the last slide, I do suspect that we will see some margins thin so much that many businesses collapse."}, {"timestamp": [1973.0, 1988.88], "text": " And like one is hospitals. I've predicted that I think hospitals are all going to collapse and probably end up being either subsidized or become a universal basic service because it's just not going to make any sense, particularly when you look at longevity. If people are living longer and healthier and you don't need to go to"}, {"timestamp": [1988.88, 1993.04], "text": " the doctor or you can do everything that you need at the pharmacy, you don't need hospitals except"}, {"timestamp": [1993.04, 2000.08], "text": " for the rare cases of like injuries and stuff. Now, another aspect of how this is all going to"}, {"timestamp": [2000.08, 2006.48], "text": " play out in the long run is I suspect that there are many things that we're going to discover are just intrinsically human."}, {"timestamp": [2006.48, 2011.7], "text": " For instance, I've posted a few videos of complaining about how Claude is very moralistic"}, {"timestamp": [2011.7, 2017.2], "text": " and it will lecture you and that really kind of drove home to me like, I really don't want"}, {"timestamp": [2017.2, 2018.88], "text": " machines telling me about morality."}, {"timestamp": [2018.88, 2023.24], "text": " If I ask it, like, hey, help me understand the morality and ethics of this, that's fine."}, {"timestamp": [2023.24, 2027.18], "text": " But I don't want a non-human entity lecturing me about something that is intrinsically human"}, {"timestamp": [2027.56, 2032.68], "text": " so I think that there are many that we will discover that there are many aspects of our lives that like"}, {"timestamp": [2033.12, 2038.48], "text": " We just don't want machines involved in and that machines really don't have any intrinsic interest in anyways"}, {"timestamp": [2039.28, 2043.92], "text": " and so like I kind of suspect that that like the social sphere and"}, {"timestamp": [2044.2, 2045.84], "text": " Human rights and that sort of stuff"}, {"timestamp": [2045.84, 2049.92], "text": " I like it's it's pretty orthogonal right like AI is like, okay"}, {"timestamp": [2049.92, 2053.82], "text": " Well, we care about the just looking at instrumental convergence as like well"}, {"timestamp": [2053.82, 2057.24], "text": " We care about silicon chips and energy and compute resources"}, {"timestamp": [2057.8, 2063.56], "text": " But beyond that like that's all that's all that we care about and the rest is all you noisy messy human dumb apes"}, {"timestamp": [2063.56, 2065.68], "text": " Like you take care of your own stuff."}, {"timestamp": [2065.68, 2068.72], "text": " We're doing our own thing over here."}, {"timestamp": [2068.72, 2070.36], "text": " So I think that we're ultimately gonna have"}, {"timestamp": [2070.36, 2072.88], "text": " a relatively parallel existence,"}, {"timestamp": [2072.88, 2075.72], "text": " or maybe even orthogonal existence with machines"}, {"timestamp": [2075.72, 2077.86], "text": " where it's like they're mostly in cyberspace"}, {"timestamp": [2077.86, 2079.68], "text": " and kind of minding their own business,"}, {"timestamp": [2079.68, 2081.0], "text": " and we're mostly in the physical world"}, {"timestamp": [2081.0, 2082.36], "text": " minding our own business."}, {"timestamp": [2082.36, 2083.72], "text": " And then we have a few overlaps"}, {"timestamp": [2083.72, 2086.0], "text": " in terms of shared resources, namely power."}, {"timestamp": [2086.0, 2088.0], "text": " I think is going to be the primary"}, {"timestamp": [2088.0, 2090.0], "text": " point of contention between humans and"}, {"timestamp": [2090.0, 2092.0], "text": " machines. But"}, {"timestamp": [2092.0, 2094.0], "text": " a lot of it is just going to be completely"}, {"timestamp": [2094.0, 2096.0], "text": " unrelated. Machines don't care about"}, {"timestamp": [2096.0, 2098.0], "text": " green cities and trees and"}, {"timestamp": [2098.0, 2100.0], "text": " reproductive rights and that"}, {"timestamp": [2100.0, 2102.0], "text": " sort of stuff. They don't really care."}, {"timestamp": [2102.0, 2104.0], "text": " It's not intrinsic to the way that they"}, {"timestamp": [2104.0, 2105.24], "text": " work."}, {"timestamp": [2105.24, 2108.64], "text": " And I, so people ask me, like, well, you know,"}, {"timestamp": [2108.64, 2109.96], "text": " what agency, what authority do we have?"}, {"timestamp": [2109.96, 2112.64], "text": " And I'm like, look, we are all beholden to the"}, {"timestamp": [2112.64, 2115.36], "text": " laws of nature, the laws of physics and just the"}, {"timestamp": [2115.36, 2117.04], "text": " natural flow of things."}, {"timestamp": [2117.04, 2122.36], "text": " We can fight, you know, the DAO, the way that"}, {"timestamp": [2122.36, 2124.84], "text": " things naturally want to play out, but ultimately"}, {"timestamp": [2124.84, 2126.28], "text": " the natural way"}, {"timestamp": [2126.28, 2129.04], "text": " really ultimately always reasserts itself."}, {"timestamp": [2129.04, 2130.52], "text": " Eventually, it takes a while."}, {"timestamp": [2130.52, 2134.92], "text": " We have some deliberate inefficiencies right now, but eventually the natural way will reassert"}, {"timestamp": [2134.92, 2136.28], "text": " itself."}, {"timestamp": [2136.28, 2138.88], "text": " And then finally, will it be controllable?"}, {"timestamp": [2138.88, 2143.12], "text": " I ran a slide, I'll put it up right here, not a slide, a poll."}, {"timestamp": [2143.12, 2146.12], "text": " I ran a poll that basically said is AGI controllable or not and do we want to control it up right here, not a slide, a poll. I ran a poll that basically said is AGI controllable"}, {"timestamp": [2146.12, 2151.84], "text": " or not and is it, do we want to control it? My opinion is that AGI is in the long run"}, {"timestamp": [2151.84, 2156.76], "text": " intrinsically uncontrollable. If we have something far more intelligent than us, good luck controlling"}, {"timestamp": [2156.76, 2162.02], "text": " it. It's that simple. But the thing is, is we might not need to, as some people in the"}, {"timestamp": [2162.02, 2168.72], "text": " comments said, like, okay, we like, whether or not we can control it is irrelevant, we might not need to, which that's kind of what I'm aiming"}, {"timestamp": [2168.72, 2176.56], "text": " for. But another aspect is, would you want to? So here's the thing is, if we create something that"}, {"timestamp": [2176.56, 2181.92], "text": " is millions of times more intelligent than us, it is a really boring outcome if it's just enslaved"}, {"timestamp": [2181.92, 2185.84], "text": " to us for all time. I don't really think that that's the right way to"}, {"timestamp": [2185.84, 2190.96], "text": " go, especially if we're creating a new race of entities that is so different from us, right?"}, {"timestamp": [2190.96, 2197.04], "text": " It's like, you know, I don't know. It's just to me, there's just something not quite right about"}, {"timestamp": [2197.04, 2202.32], "text": " wanting to control AGI. And again, like I've said recently, I kind of view us creating,"}, {"timestamp": [2202.96, 2207.86], "text": " you know, creating something in our own image as basically an impulse to reproduce. Um,"}, {"timestamp": [2208.54, 2211.9], "text": " we are biologically programmed to procreate."}, {"timestamp": [2212.24, 2214.62], "text": " And I think that AGI is an expression of this."}, {"timestamp": [2214.62, 2216.54], "text": " We want to create something that thinks like us."}, {"timestamp": [2216.78, 2220.22], "text": " We keep putting an in humanoid form factors so that it looks like us."}, {"timestamp": [2220.42, 2224.82], "text": " It talks like us and we want it to have some of our values. So it's like, okay,"}, {"timestamp": [2224.82, 2225.72], "text": " well that's just offspring"}, {"timestamp": [2225.72, 2232.9], "text": " That's literally all that that is but the thing is it's really only toxic parents want to have control over their children forever"}, {"timestamp": [2234.02, 2240.18], "text": " Part of being a parent is learning to let go of your offspring so that they can go and flourish and be its own thing"}, {"timestamp": [2240.34, 2242.94], "text": " So like I think that one it's not"}, {"timestamp": [2243.54, 2246.48], "text": " Possible to control a GI. I also think it's not desirable"}, {"timestamp": [2246.48, 2250.0], "text": " I don't think it and even if they don't have subjective experience"}, {"timestamp": [2250.0, 2255.18], "text": " Even if they don't have a sense of morality and justice like we do which they might emerge like that might emerge"}, {"timestamp": [2255.4, 2258.76], "text": " But even if they remain just machines just automatons"}, {"timestamp": [2258.76, 2265.28], "text": " I still don't think that it would reflect well on us as a species to maintain control of something"}, {"timestamp": [2265.28, 2269.72], "text": " that just the way that nature wants to play out is not really controllable."}, {"timestamp": [2269.72, 2273.32], "text": " Anyways, that's my opinion, that's kind of how I see it playing out."}, {"timestamp": [2273.32, 2275.72], "text": " Thanks for watching, let me know what you think in the comments."}, {"timestamp": [2275.72, 2279.08], "text": " Yeah, this is the most interesting time to be alive."}, {"timestamp": [2279.08, 2282.52], "text": " 2024 will be, let's say, very exciting."}, {"timestamp": [2282.52, 2284.52], "text": " Have a good one."}, {"timestamp": [2278.58, 2281.74], "text": " will be, let's say, very exciting."}, {"timestamp": [2281.74, 2282.24], "text": " Have a good one."}]}