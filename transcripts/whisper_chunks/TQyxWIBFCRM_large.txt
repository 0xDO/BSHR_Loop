{"text": " Hi, and welcome to another episode of the Spatial Web AI podcast. Today we have a very special guest. We have David Shapiro, AI researcher and thought leader and just overall really smart guy. David, it's such a thrill for me to have you here on the show. I'd love it if you just maybe introduce yourself to our audience. Yeah, thanks Denise for having me. My name is David Shapiro. I have been in IT infrastructure cloud automation since 2007, independent AI researcher since 2009. And then of course, you know, we're in the era of GPT-2, GPT-3, and now GPT-4. And, you know, so pretty much the rest is history, but yeah, I've been an independent researcher and technologist for quite a while. And then just about 16 to 18 months ago, I got on YouTube to spread the word basically, and that's how we got to where we are today. Awesome. Well, I'm so excited to have you here because I have a lot of questions. And I know you're, you're the one to ask. So I've watched a lot of your videos. I've heard a lot of your talks on AI and what you see for the future of AI. And so a lot of people talk about the fears of, this may happen or that kind of thing. And I don't see a lot of people out there really putting out the message for all the positives and all the great things that AI can do for us and for humanity and our civilization. And so what do you see as the biggest advantages for AI? Yeah. So, you know, you're right. There's it's, it's incredibly polarizing, right. And there's a few videos out there, commentators and big names. Certainly there's really zealous advocates on one side, and then there's the doomsayers on the other. And it's really rare that we see something that is this polarizing, right? Where it's just like people are saying like this is going to save everyone, it's going to cure everything or it's literally going to kill everyone. So, like, I try and take a more realistic approach. And I don't try and downplay the dangers. Whether it's just innocent misuse or mistakes, like there's always the potential for harm. Then of course there's like wanton misuse of people deliberately using new technologies because all technologies are dual use. Whether you build a nuclear bomb or a nuclear reactor, whether you build better medicines or bioweapons, right? All technology is dual use. And so with any technology, this is AI, whatever else, you always need to be very careful and very deliberate about how you deploy it. Now, in terms of like where I'm at on the spectrum, you know, as a career problem solver, because even before the rise of AI, that was what I made my career based on was being able to solve problems that other people couldn't. And so I have not found any evidence that any of the problems confronting us are unsolvable. And, you know, every time someone says, well, what about this? And it's like, okay, well, you've identified the problem. That's half the battle, right? You know, like people will complain, you know, about like, hey, Mesa optimization, which for anyone who's not familiar, Mesa optimization is a phenomenon inside of deep learning models, where it optimizes for not what you think it is. It's kind of like has an internal layer. That's not necessarily deceptive. You might perceive it as deceptive where it's like, okay I'm giving you the answer that you want, but only because I know that that's what you think you want. And I'm working around it. And I'm like, but if you've articulated that problem and you're working on that problem that means it's a solvable problem. And this has been the nature of science since forever where you solve one problem at a time and you move on to the next one. Now, that being said, with artificial intelligence, we are confronted with the possibility of creating something much more intelligent than ourselves. And that raises all kinds of other questions, existential questions, like what is the point of being alive? I think Elon Musk had a tweet a few weeks ago, it's called X now, but he said like, if, what did he say? It was something like, if AI is smarter than all of us, what's the point of being alive or something like that, right? It would just summed it up in one tweet, which is really touches into the existential dread that some people have. And so in many cases, I think that a lot of people might have a, just have a problem articulating that that's their fear. And so, you know, I'm here to validate that's a legitimate fear. But at the same time, you know, society has gone through huge changes in the past. You know, 200 years ago, 90% of people were farmers. I don't know about you, but I don't want to be a farmer. So I'm glad that we went through that change, right? You know, an entire way of life was destroyed. And I'm sure for the people that lost their jobs, lost their farms, you know, had to leave their farm and go to the city, it was awful, right. And I'm not saying that their suffering was worthwhile, or that it was righteous or noble, I'm saying like, they suffered, and it really was awful for them. But in hindsight, in retrospect, I'm glad that society went through that change. And so that's why like, you know, it's, I'm, I'm neither a Doomer nor like a hardcore optimist. Like that's, that's my overview of, of, uh, of kind of where we're at and where we're going. Yeah. Um, so, and I agree with you and I, I see this future that's going to be improved for humanity, but I know there's going to be like a little bit of a painful transition in that period too. So I don't want to diminish that by any means, you know. So what are some of the biggest hurdles that you see that we need to overcome? Like as we're, you know, transitioning into this space of AGI and then even ASI, super intelligence? If you'd asked me a few months ago, I would have given you one set of answers. We need to figure out autonomy. We need to figure out yada, yada, a litany of technical and scientific problems. But of course, everything's moving lightning fast. So I don't really see any technical barriers anymore, especially the rate at which AI models are improving. Really the biggest barriers are that of organization and communication. And so what I mean by that is, first and foremost, there's just so much news happening all the time. There's hundreds and hundreds of startups. There's 5,000 AI papers per month being published right now, and that's probably going to be 6,000 next month. And so you have this rate of change that is, it's a decentralized, like tidal wave, you can't stop it. And so then how do you manage that? And it's like, well, you got to learn to surf, right? You got to stay on top of the wave cause you can't stop it, you can't push it back. So that's one of the main problems. And then of course, there's always going to be like lagging indicators and people, you know, failing to adapt, you know, there's always people out there that are kind of in denial about like what's happening and the magnitude of it. And I'm like, look guys, like your way of life is gonna change whether you like it or not. And some people they're just like, well, I'm skeptical. And of course people trust their own opinion more than they trust the news or the facts or whatever. That's a whole other can of worms. But those coordination failures, those communication failures, I think represent probably the biggest risk because by the time some people are aware, it's gonna be too late. And so like I was just listening to a podcast earlier, one of the bigger names, and I'm listening to all the entrepreneurs, the people with billion plus dollar companies, and even people in the industrial space are really getting on board with generative AI, spatial web, decentralization. And so these are the people that are connected in, that are the real movers and shakers in the industry. And in the podcast I was listening to earlier, they were talking about like, oh, yeah, you know, like, we're going to get like, automate as much of the factory as we can, as much of the logistics chain as you can. And what they weren't saying though, is what happens to the people who lose their jobs. And from a business perspective, it's not really their job. Like they are, like every business has an obligation to their shareholders or customers or whatever, to provide goods and services for cheaper. That's it. That's the only objective function they have. But what happens is that in the past, so here's, sum it all up. In the past, we had this notion that technology creates more jobs. It's not true. That was never true. What technology does is that it lowers the cost of goods and services so that money can be allocated elsewhere, which creates new sectors, new demand. So that example that I gave a minute ago, where 200 years ago people were farmers, suddenly there was knowledge work, there was service industry. And so then people could, their food was cheaper, their homes were cheaper, so on and so forth. They could then afford to buy consumer goods and electronics and other stuff. So that created new demands, not necessarily new jobs. And so this is the primary shift that I think that people have not have yet to wrap their heads around. And I call it post-labor economics, which is we're not going to be working anymore. So then how does the economy work? If robots and machines and everything take all of our jobs, what how does the economy function? It's going to function. We got to figure it out. And then it's an, that is an unsolved problem. Yeah. And it's interesting because, you know, when I think of like what the spatial web is, is, you know, opening up, you know, with this evolving internet, wherever we, we move from a library of websites, web pages into spatial domains, you know, where every single thing in any space becomes a domain. Permissions are at every touchpoint, context is baked into every touchpoint and it's informing the AI, right? So, you know, when I see this type of a system, you know, and then no jobs, and we have to find new meaning, but also businesses, corporations, they have to find new ways to insert themselves into the lives of the consumers, because, you know, advertising, all the stuff that we've seen before is not going to work. They're going to have to have this native experience. You bring in like, you know, cryptocurrencies, you know, value exchange versus fiat and, you know, decentralization, all this stuff. You know, I kind of envision a future. Obviously it's going to take a while to get to that point, but where our daily lives just start rewarding us. We start like the system itself is giving us value back for interacting with the system. And we could have this future where you wake up, you make your bed and then you're rewarded. You're not even aware these things are really happening, but they're happening. And it could be the sheet manufacturer, the bed manufacturer, that's part of their marketing dollars are now in rewards, you know, like I definitely see, like, there's a lot of options for this future of innovative ways where, you know, I don't know. You know, I just thought of a good example while you were kind of talking through that. And, I just thought of a good example while you were kind of talking through that. And, you know, there's a construction site near my house, as pretty much anyone in any growing city, you're going to be surrounded by construction. And every now and then there's screws or nails that are in the road. And every time I see one, I pick it up and make sure that it's not in the road anymore so that somebody doesn't like pop their tire or whatever. And so I created value, or rather I protected value that I'm never gonna be rewarded for. And so one of the things that I'm really excited about in terms of Spatial Web, Web 3.0, what we used to call internet of things and fog computing when I was back, back when I was at Cisco, fog computing is fortunately a term that died a very quick death. But the idea that it was all around you all the time. And that's kind of the spirit of what we're building towards and the idea that you could track everything that like some sensor could say like, oh, hey, there's a broken fence over there, someone go fix it, or there's something in the road. What was it? There was a case of, this was many years ago on the highway near me where a box of metal parts fell off the back of a truck and it shut the entire highway down for hours, right? Because if the state troopers had to stop traffic, clear the road, you go over with a magnet sweeper and it's like, how many millions of dollars of productivity were lost because someone made a dumb mistake, right? And then, and then I was like pretty frustrated. I was like, you know, for me it was an inconvenience because I was stuck in traffic for an extra 45 minutes. But what if like there was a lifesaving physician who didn't make it to a surgery for a newborn because of that? Right. And then, so then it's like a matter of accountability. And of course, like you don't necessarily want to use a new technology just to punish people. But what if some of these technologies could have said, like, if there was an overpass camera and it phoned him and, you know, the driver of that truck and said, hey, you've got an unsecured box, pull over, fix that. Because it's, you know, it's about prevention. And so then, you know, there's all these ideas of if you had the information, how could you use it? Right. And it's a matter of gleaning all that information from all the different layers of reality that are all around us. I was on a call earlier with a prospective client who's building like a data platform to facilitate this stuff. And I don't think that they had realized, like, I was like, you guys are building a spatial web platform, and they're like, what do you mean? And I was like, I just sent him a link to spatial web. I was like, this is what they had realized, like, I was like, you guys are building a spatial web platform and they're like, what do you mean? And I was like, I just sent them a link to spatial web. I was like, this is what you're doing, right? Is how do you coordinate? Because what did they call it? They called it like a context driven AI learning platform. I was like, context, right? You know, so it's like, you know, but the thing is, is there's so much information to keep track of in the world, right? This was something that my cousin many years ago, he's an electrical engineer, started putting sensors on dumpsters, right? So that was like a really early version of spatial awareness, where it could record like every time someone threw a bag into the dumpster and predict when the dumpster was full, so that the dispatch would only send trucks out to pick up dumpsters once they were full. So that's, you know, but that was a really specific use case. Now, imagine if you can have like more general purpose things, any camera, any microphone, kind of detect the presence of whatever needs to be done and make that a public good. That is really what I'm most excited about. And of course there are a tremendous amount of technical hurdles to achieve that future. I remember that was actually one of the original promises of Bluetooth when it was first published many years ago, was, oh, hey, there's this open protocol that can allow all your devices to talk to each other. And of course, like we had just this utter failure of imagination back in the day, where it was like, you know, your dishwasher will talk to your fridge and we're like, why? Right. But that was because that was in the early 2000s and that was as far ahead as we could think. But anyways, yeah. So there's a tremendous amount of opportunity, you know, but then it opens up all kinds of cans of worms, like privacy, number one, first and foremost, I think is probably the biggest thing. And that's what I talked with my perspective client with earlier was like, okay, so you got the context, but then how do you keep track of who knows what and who is privileged to what information? So there's a number of problems still there. Yeah, so to me, that's one of the advantages, the really cool things about the Spatial Web Protocol because the HSTP, the Hyperspace Transaction Protocol, because in the spatial realm, permissions, everything is permissionable at every touch point. So then you can program in credentials and all kinds of information about ownership and who can access. And it will create this future where you can be in control of your data, your data realm, and you can give permissions that have expirations to them even, like all that kind of stuff. So I feel like these issues that we see a lot of them are because we're in this web two environment, and we're still in this worldwide web where we have limitations that, you know, I really feel like a lot of benefits going to come from the, this, this evolution. That makes sense. Yeah. And, you know, and it's like kind of what you described as imagining like geofencing my house, right? So, but then in a spatial web, web 3.0 environment, then it's like, okay, well, Dave, you are registered as the owner of this piece of property. So then any data that originates or takes place in there, you ultimately have some rights to in terms of privacy or profiting from or whatever. I think that there's a tremendous amount of opportunity there. One idea that a lot of people have proposed lately is the idea of data as a public good or personal data governance as a way of kind of participating in the economy. And of course, data is the new oil and there's a million ways that data can be used and I'm not even familiar with all of them. But it strikes me that having that sort of point of origination governance and tracking of data is really important to building that new economy. Again, like even if you can address it and even if you can, you know, assign privileges and permissions, it's still like, I guess the thing that I haven't figured out yet is, okay, well, how do you balance privacy with that reward for participating? And maybe that's the trade-off. Right, and honestly, that's kind of what I see is that right now we give away all our data. We don't really have a choice, but we will have a choice. And there will be incentives that appeal to us enough to trade data, trade some of that privacy, but it'll become a choice, a personal choice versus complete lack of empowerment. That makes sense. Yeah. What do you think? What that will look like? Because I often look at to, you know, like, when social media first started, and it was like, you know, the stranger danger, don't put your picture on the internet. And now, you know, like, so I think, too, as we evolve with technology, we actually become freer with our ideas around it. Yeah. You know, I'm reminded of there's a whole raft of studies that talk about how behavior and even cognition changes when you know that you're being watched. And you know, I am a very private person, and yet I have, you know, 66,000 subscribers on YouTube. And so like, you know, I remember watching the numbers and then my wife was like, people have spent more time looking at you than I will look at you in our entire life together. And it's just like, you know, this is the world that we live in. And so some of it is a level of comfort, like just becoming familiar with having a public versus private life. And then in a previous podcast that I was on, we talked about this idea of like the value or danger of having online all the time versus like, what did he call it? But basically having like islands that are like black holes of information. And I said that like, actually you probably would want that. You'd probably want some places where you know that there are no electronics, that there are gonna be like complete like reservoirs or whatever, you know, like going out into nature basically or even in urban or suburban areas. And I'm not, you know, I'm not saying one way or another, I'm not saying like, oh, spatial web is gonna have to account for this or this is gonna be a problem. It's just an observation to think about. Because, you know, in many places in the world, you're still, you're going to, you're offline, right? There's, you know, where I grew up. Yeah, right. You go dark. And I think that that's a good thing, though. I think that I think that we'll probably want to want to conserve some, some of that, whether it's like, I actually fully predict that some buildings as part of their privacy or whatever, it's gonna be like, you know how there's like, you know, no firearms allowed, it'll be like no spatial web allowed or whatever in some buildings for privacy reasons. So I don't know, just the reason that this is all top of mind is because one of my best friends is like, he's like a privacy nut. Like his two favorite topics are like guns and privacy. And oh, by the way, the Clintons are all in control of everything. So like I agree with him on the privacy thing, not the other two. But the idea is that like privacy is actually really important. And it's a balance. There's a bunch of other podcasts. There's actually like married couples from the CIA have podcasts online. Now. This is a crazy time to be alive talking about privacy versus convenience. So, you know, this is this is the world that we're heading towards. So the big lead rooms pretty much pretty much. Wow. Do you really think that's going to happen though? I mean, do you really think, because, you know, I don't know, like I, I, I see the desire for it and I see the need for, you know, having spaces you can go where you're not connected. Like you can disconnect. Right. As long as it's opt in and opt out, as long as you can toggle a switch, right? And it's like, okay, you know, this is a non-entity. And like Google has been working on this for many years, like with Street View, right? Like when they realized that, you know, having their vans drive around with cameras, that was a big privacy thing. And so then they came up with AI to like blur out faces, right. And so this is not necessarily a new problem, in terms of respect, you know, like it's, you have to opt in if you want to participate. And I think that having that as kind of a standard could could very well, you know, suit some people, suit many people. And I don't, you know, I don't know where which side of the fence I would fall on right now. Cause like imagining a future where it's like, okay, well, you know, like Dave, you know, every time you go out into public, you know, and you go to the grocery store, the grocery store is watching you to figure out like what products you're looking for. And that data is going to be used to better serve your area. And you're going to get compensated just by going to the grocery store and picking out your groceries. Like, okay, you'll compensate me for something that the grocery store knows already anyways. But on the other hand, it's just like, I'm aware of how information can be misused. In the past, everything from library records to land records have been used for genocide. And so it's like, that is really kind of terrifying. And just because it's convenient and you have incentives to participate in that system, that doesn't necessarily mean that it's automatically a good thing forever. And so one thing that is increasingly popular in blockchain technologies is what is it? ZKP, zero knowledge protocols or whatever, or sorry, zero knowledge proofs. And so the idea that you can have some information that is permanently private, that can still be useful. I'm not convinced that that technology is mature yet, but there's certainly a lot of energy being put into that kind of research. But if that gets solved, then I think that there'll be a lot more comfort in participating opt-in versus opt-out in terms of, do you want to be on the grid, yes or no? Right, right. And yeah, and to my understanding with the Spatial Web Protocol, it enables an ecosystem of zero knowledge proofs, self-sovereign identity and zero knowledge proofs. So, that'd be interesting to see that. So, now the idea of connecting or disconnecting by choice, what happens when we get to brain computer interface? Well, I've been a fan of science fiction for a long time and there's an anime called Ghost in the Shell and hacking people that are on the grid is actually something that is like central to that. And now I'm playing Cyberpunk 2077, which also like anyone on the grid can get hacked at any time. And like, that's pretty terrifying. I probably, let's just say, I'm not gonna be an early adopter of BCI, of brain computer analysis. You know, and if the technology gets proven out and it gets proven safe and then you have a switch or something where you can just say, cut off all radio signals, basically you need some fail safes. MARIE RENO SOUDALIS Yeah. built in. Probably more than one, actually. You probably need several fail-safes, automatic fail-safes, voluntary disconnects, that sort of thing. In my history as an IT infrastructure admin, you know, there's a concept called defense in depth, the cybersecurity principle. And so there's basically you look at security as layers of an onion, right? It goes, there's physical security, there's network security, there's network security, there's all kinds of layers. And then there's also the concept called air gapping. And so air gapping is when there is no physical connection between one system and another. Like it's literally not possible unless you like take a thumb drive from one system and plug it into another. Right now, all of our brains are air gapped from each other. Like the only way that I can talk to you is through like words and voice and hand signals and whatever. But you have like several layers of. You know, agency and security before any information even gets into your brain. Um, and so I would need to see, uh, a lot, a lot of defense and depth protocols around BCI before I would be comfortable with it. Yeah, you know, it's crazy because that makes me think of, you know, the stuff I've seen lately with, they're doing with the machine learning with the MRIs and being able to like have you watch a video and then literally just by reading your thoughts, they're able to replicate the video. And it's so close to what you actually were observing just by the pattern recognition in your, in your brainwaves of your thoughts. That's kind of interesting because when you're talking about like the separation of, you know, your thoughts are protected within your shell. Yeah. Yeah. Might not, might not be for long. You know, there was, that was a, that was actually one of the plot points in Westworld. I don't know if you ever watched the HBO TV show Westworld. I watched a few of them. Yeah. Yeah. So one of the things, and at the time I thought this was so contrived and I was like, I was like, this is so dumb. But what they said, it was, it was a relatively small plot point, but that all the hats that they had the guests wearing, were scanning their brains for the last few decades. And that's what that was how they got the good models of human brains. And I'm like, that's not possible. It's not that easy. And then here we are a couple years later, and it literally is that easy to like record brain, almost that easy to record brainwaves. And get, you know, the thing the thing about that technology, brain imaging and being able to reconstruct images, thoughts, and words. I think there was one where they were able to reconstruct an image just from recall, asking the subject to think about something, not actively see it, but think about it and then infer what they were recalling. So, you know, that, that level of thing of, of, uh, of insight inference, you know, into someone's brain kind of reminds me of actually the, the dawn of psychology. So back in the days of Sigmund Freud and Carl Jung, it was actually offensive to the Victorians, to the idea that you could infer what was going on in someone's mind from their behavior, from their words, from unconscious ticks, because to them, it was stiff upper lip, Victorian society, the idea that your thoughts were intrinsically and forever private and that someone could infer what was going on in your mind, the contents of your mind or your experience from something that got out was like really terrifying to them. And so like now we're taking that to the next level with technology. I'm sure we'll get used to it just as, you know, the Victorians got over themselves eventually with psychology and everything else. You know, now we have professional lie detectors and behaviorists and that sort of stuff. And it's just, it's commonplace. Yeah. so interesting. So you brought up something interesting earlier when you were talking about Elon Musk and his texts, you know, where do we find meaning? So AI advances, everything is automated. We're in this world now where literally like we are living AI adjacent, you know. Whether it's before the brain interface, we were just adjacent through our devices and all of that. I mean, obviously AI, when you talk about all this new data and you talk about all these data points and everything else, it's just increasing and increasing and becoming very overwhelming. So we're going to need AI to parse it all for us, you know, or like that's, that's going to become very necessary. So we're in that state, jobs have kind of gone away. How are people finding meaning? Yeah. So I think, I think that society is ready for this because the, some of the anxiety is there and putting words to it. I think I mentioned this earlier, or I alluded to it earlier, is that people are afraid of being irrelevant. Right? It's, you know, before, like, if just jobs go away, if just jobs go away, then people's biggest fear is, how do I take care of myself? How do I take care of the people that I care about? But above and beyond that, in a world of superintelligence where machines are a million times smarter than all humans combined, and to Elon's tweet, what is the point of even existing if you're basically as dumb as an ant compared to the machines, the evident truth, I'm not going to say self-evident, but to me what seems to be evident is that humans will become irrelevant. But here's the trick, is we were always irrelevant. From a cosmic perspective, we only convinced ourselves that we were important and that we were relevant and that we were super special. And that preoccupation with our own specialness and uniqueness, which is embedded, not just in religious narratives, it's embedded in all kinds of narratives around the world, really has caused a lot of problems. I just started reading Sapiens by Noah Yuval Harari. I think I said his name right. I have terrible name sometimes. Anyways, and he talks about how the speed of our evolution is actually really problematic because a hundred thousand years ago or two hundred thousand years ago, we were in the middle of the food chain. And so we, you know, we ate things that are smaller than us, but then around a hundred thousand years ago, we got really good with weapons and we started working our way up the food chain, but we're still really insecure. We're like anxious chimps that happened to be at the top of the food chain. And so now we're like, what are we doing here? And so we're just scared all the time, which is why we have anxiety and depression. And so he didn't say it quite like that, but you know, that's kind of the inference that I'm drawing. And so, you know, we, we had this rapid cognitive explosion of not just tool use, but society and, and ultimately to science and everything else. And so we have the ability to ask these gigantic existential questions. Why are we here? What does it all mean? And then basically fabricate answers, right? We can come up with narratives and stories and myths and religions and whatever else. And, you know, so I'm not gonna say that we're not cognitively equipped to answer that. I'm just, what I'm saying is that we have a tremendous amount of intellectual and cultural baggage that we've been asking about meaning for such a long time. And what we're coming back to, actually, is this kind of this leap back 200,000 years to where we're just going to be happy to have food and company and friends and a trip down to the lake. Like, that's how we're supposed to live, because that's how we lived for literally millions of years until we accidentally tamed fire and tools. And it was just, you know, Douglas Adams said like, we should just go back to the trees and go back to the oceans, you know, and living simply, I think, is the answer that a lot of people are gonna find. Now, that being said, living simply isn't for everyone, right? Not everyone wants to live, you know, a cottage core existence or slow living or whatever. But there's, so one concept that I've been working on is, is the importance of mission. And so having a mission in your life is that's, that's where I'm at. So like, I have a very clear, clearly articulated mission that is not something that AI can, can dislocate yet. And if it does great great, mission accomplished. But there are any number of missions that you can have that AI will never be able to take from you. So like one example that's very personal is a girl that I used to date, her mission was to become a mom. That was what she wanted most in life. And so she became a mom. And like, that was her jam. There's another person, another girlfriend that I used to date, actually, I've dated a lot of people, where her mission was to write a beautiful coming of age story. It's a great mission, but it was her mission, right? Even if AI can write it for her, that's not the point. And so, you know, back to basics, living sure. But I think in terms of meaning, I think that a lot of people are going to find that they're going to be empowered by AI to pursue those missions, whatever they happen to be, whether it's personal achievement, right? Yeah. Yep. So like, you know, some people like, I want to climb Everest or I want to run an Iron man or whatever, like those are all completely valid missions that can give people a tremendous amount of meaning in their life, even without any kind of cosmic meaning. So that's kind of where I'm at in terms of understanding how society might change in light of AI and everything. Yeah. And I feel the same, you know, purpose and making a difference. And, you know, I think we've been so geared to like scratching and clawing for survival for so long that, you know, it's hard for us to think of other ways. But I think once that's removed, you know, and we're out of this kind of like stress environment, making a difference is going to look different. And I, you know, I, I know this sounds really utopian, you know, when I talk like this, but, but to me, I see when we're not in the scratching and clawing for survival kind of, you know, scenario, I feel like people will look a little more outward to their communities and how they can help each other and make a difference with each other. And I feel like it's gonna bring these qualities that kind of, you know, increase empathy and different things like that, because we're not gonna be so much in this competitive mindset, more of like a- Yeah, it's switching to an abundance mindset, right? Yeah, yeah. So it's going from scarcity to abundance. And, you know, this is actually the first time I learned about this was actually in the non-monogamy community. Uh, so the polyamory community is one of the things that they teach you when you, when you're learning about non-monogamy is going from a scarcity mindset of love to an abundant mindset of love and realizing that there is actually more affection to go around. And that, and that around and that the only barrier is thinking that you can only love one person at a time or that you can only enjoy one person's company at a time. And it's a completely artificial type of scarcity. And I think that that represents a good model of going from a scarcity mindset to an abundant mindset. And people are like, oh, well, there's no resources that are hyper abundant. It's like, well, air is, right? You don't have to pay for air or sunlight. And so we have plenty or ground like dirt, literally the term dirt cheap, right? Because there's so much of it, you don't have to pay for it, right? And so we do have models of abundant goods and services, nearly hyper abundant goods and services. But yeah, certainly pivoting to that mentality. And you know, I've been, I've been working on practicing what I preach. I was a heck of a workaholic. You know, like since, since retiring from my corporate job, I've actually worked more even though I didn't need to, because I was just so fixated on my mission. And over the last couple of months, I've been working on slowing down and actually, like I said, practicing what I preach and moving more towards that, like, well, what, like, why are we here? Right, like, what is, like, what is, what is my goal now? And so I've had just a few breakthroughs, you know, one breakthrough a few weeks ago was just that, that realization that like, we never needed meaning in the first place. That meaning was always unnecessary. And it was just like, I would like literally lay awake with that, just like on repeat in my brain, because it's, it felt so profound that I was just like, it was stuck in my head until like I started living it. And then in the, just in the last few days, um, the, the, the, the other biggest thing, cause I was like on the edge of burnout. I'm like, why am I still on in, in burnout? Because like, I have total control over my schedule. And then I realized that it was choice. It was my choice to be where I am. And so I was like, Oh, like I had been stuck in, in a stuck in a mindset of not having agency for so long, you know, working for companies or working for other people or being told what to do, or just being in situations where I didn't feel like I had control, that I was still in this like, this state of learned helplessness. And then I realized like, wait, that's not true. Like I'm actually in control of everything in my life right now. And that I realized like, wait, that's not true. Like I'm actually in control of everything in my life right now. And that was another like profound realization. And ever since then it's, I've been like just in so much better shape. And so I think that a lot of people, Oh, go ahead. I was just going to say that that reminds me of that meme that I saw that said a Rolex isn't a flex. If it tells you when your lunch break is over. That's a good one. That's a good one. Yeah. Yep. You know, if, if, if, if all it is, is just a trinket that you've collected on the rat race, is it really that worth it? You know, and, um, there's a lot of YouTubers and a lot of, uh, influencers and, and, and social commentators out there that are doing these experiments. Um, one of my favorite is, of my favorite is this YouTuber called Sorrel and she started this free human project. And so the free human project is like, she goes around and interviews people, like usually other creators, not always. And she talks to them like, are you a free human? What does that mean? Like, how did you get out of the rat race? And a lot of these people actually work harder than like, you know, anyone at a desk job, but they work harder because it's fulfilling to them and it speaks to their autonomy. And I think that, I think that what, I think that a lot of people are actually afraid of that autonomy, because it's like, well, what am I going to do with all that free time? And it does take a while. Like it, it's taken me about six months to figure Cause it's like, well, what am I going to do with all that free time? And it does take a while. Like it's taken me about six months to figure out like, okay, well now what? Like, how do I live? And that kind of thing. Don't you think too, that a lot of that has to do with like intrinsic motivation for people too? Like they'd rather be told they have to do this, this and this in this order, kind of a thing, because when they're left to their own devices, they don't have the motivation for it. And maybe it comes down to what you were just referring to that, you know, it has to be something you're passionate about and that you love, you know, the best work, work that you love is the best sort of play, you know. You know, cause I've thought about that, that idea that it's like, okay, well people just need structure. They need to be told what to do. And I certainly think that some people thrive with more structure, right? There's everyone knows someone or has friends of friends that were like just complete nut cases during high school. And then they went and joined the military and they got their life together, right? Some people just really need structure and discipline and something outside of themselves, right? And then those of us that are crazy artists, like we completely chafe under structure, right? Structure is like toxic for us. So, you know, I guess the first part of the answer is that it varies from person to person. But the second part of that answer is that I think that a lot of it is just learned. I think that it's habit and I think that it's inertia because the idea of having a nine to five, right? It provides a little bit of comfort in that surety and just that daily routine, the schedule, the regularity, it's predictable, it feels comfortable and safe. But at the same time, like, you know, I, there's a, I've got a few friends in the area that are still technically in the nine to five, but like, it's more like 1030 ish to 230 ish. And then a couple of meetings here and there, you know, and one of my best friends, like we were working on his sailboat um you know in the middle of the work day and it was just he had done everything he needed to do and so then it's just kind of letting go of those expectations and those those pressures but I think that I think that probably people if maybe not if but when more people kind of uh break free or step back like say for instance we get to a four-day work week or a 20 hour work week, which there's lots of experiments going on. And I think that's coming soon. Um, at least for some companies, I think people are going to realize that there's actually like, there's a lot, it's actually a lot easier to figure out what to do with your time. So I actually have this trick that I do where if I'm stuck in work mode, I'll just turn off all electronics and lay on the couch until I get bored. And it usually only takes five minutes, right? But we're so used to constant simulation. So I just like turn everything off, you know, five minutes and either I'll be like, oh, this feels really good and take a nap. Or I'll like get inspiration, like, oh, I should go to the lake or I should call my friend or whatever. And just slowing down just long enough to start to feel bored is a really good way to just give yourself structure because sometimes it's as simple as like, you know, heck, just before we got on this call, I was like, I'm going to put away my laundry. Right. I could wait till after, but like, I'm going to do that right now. And it just, it feels good, you know, tidy house, tidy mind. So I think that people will learn a lot of tricks, you know, as they kind of take, take ownership of their life and their time again. Yeah. You know, it, it, it's interesting that you say that too, that you have, have to be mindful about slowing down and giving your brain that open space. And I think that that's harder for people these days because we're, we always have our devices and it's always throwing random data at us, you know, like random distraction. And to me, that can feel really overwhelming. And I was actually just having a conversation with some people recently where, you know, it's like, I can't even open my phone without just being, it's almost like I'm running a marathon and there's just some things that keep jumping up in the ground and just diverting my path, you know, and it's really hard to stay on. Like, you know, I'll open up my phone just to see the weather and a notification, you know, pops up and takes me down a whole rabbit hole. And I'm like, 20 minutes later, I just wanted the weather, you know? So, you know, and I think these algorithms are getting way more aggressive in trying to capture our attention and stuff to where I've been feeling it lately. I have been feeling like overwhelmed with just random information being thrown at me that I didn't ask for, didn't want, but somehow, you know, it's like this constant thing you have to be, it's almost a battle kind of a thing. Oh, it very much is. And the reason is because phones and algorithms and apps can be updated faster than our evolutionary algorithms can. And so, you know, game, like gamification for your attention, attention engineering, it's all called attention economy. Right. And so this is, this is one of the reasons earlier in the conversation that I mentioned that privacy and opting out actually have to be like front and center is because you can game someone's biology and neurology more than any amount of discipline can allow you to check out. So like you need the physical friction, and I don't mean friction like rubbing your hands together, I mean like friction in terms of layers of making your phone usable. Like I honestly make my phone as useless as possible. I have like almost no apps on it. It's in silent, do not disturb. And then I have like the digital wellness, like turned up to Mac. So like it will turn off Chrome after 30 minutes of use every day. And so like by doing literally everything I can to put a barrier from my phone being useful, your brain will eventually turn down and say, like, instead of reaching for your phone, I still instinctively reach for it. But it's like, it kind of turns down the desire, that reward mechanism for it. And then, like I said earlier, I leave it off as much as I can. Because like, you know, if someone needs to get ahold of me, like they can get ahold of me in some other way or like, really, what's the worst thing that's going to happen? You know, because like, I've got my wife and my dog here and that's who I care about most. And so then, you know, I check my phone every now and then. But yeah, like, and so this goes back to Spatial Web, because the information about you can be used against you. And so this is the, you know, going all the way back to the beginning of the conversation. Any technology is dual use. And so that's where like, okay, if you have, you know, 500 sensors throughout your home, because most of us have at least 20 or 30 today. Like if you actually go and count them all up, like we've got like 20 or 30 microphones, cameras, various other kinds of sensors that are all network connected today, that's gonna be 10X in a few years. If that information is not correctly protected and handled, then it can be used against you. And most people just will not have the digital literacy to know better, right? And so that's why it has to be like opt out by default, or you have to have a good way of opting in or out at any time in order to protect. And this is, it's not just about ethics, just it's about sanity and about how we want to live, right? Yeah, yeah. So. No, you know, it's funny that you say that about your phone because I've always been like that, you know, ever since I got a smartphone, it's like I make all the notifications turned off. Like I don't want like that, you know, ever since I got a smartphone, it's like I make all the notifications turned off. Like I don't want interruptions, you know, like I don't, you know, even my email, it's like, I'll check it, you know, like I'm a responsible adult. I will check it every 10, 15 minutes during a work period or whatever, where I might be getting important information served up to me, but I'm not going to have it be an intrusion and an interruption in my life and I actually had I had an ex boyfriend several years back and he would be like, you know, why are you? Why are you notifications? You know he was making it a thing like I was trying to hide something. I'm all dude. I'm just protecting my mental health. Yep. There's that defense in depth, right? If you don't see it, it's less, less present. You know, one thing that I really hope, and there's actually a few startups, and I think Apple, I think they just announced like they're going to like overhaul Siri or whatever. And but, you know, having, having spatial web and, and kind of networked AI, I'm actually really looking forward to, I don't think we're gonna be able to fully get away from it, but like getting rid of like screens for the most part, where it's like, if you have any computing need, it's gonna be voice, like in Star Trek, you say, computer, like, you know, where's the closest sushi? And you don't even have to look at your phone and it just, it knows like, hey, you know, when you went to this sushi place, you liked it and you didn't like this other one, let's recommend, you know, just all these kinds of automatic things that, that can, that can reduce the friction to living your life rather than the screens trying to capture, you know, cause eyes on screen is how social media companies make money right now, right? Like the YouTube algorithm, the Facebook algorithm, the Facebook algorithm, the Twitter algorithm, granted it's now meta and it's now X. Anyways, that's a sign of getting older. We remembered the original, back in my day on MySpace. Yeah. I hold a Twitter space every Tuesday and I was calling it Twitter Tuesdays and well now it's just Tuesdays. Yeah. So we're going to live through these permutations. And having those boundaries, because again, what is it that the companies want? They want profit. Okay, great. Right? I don't bemoan them that because how do they get profit? They provide goods and services that we need for cheaper. Okay, great. But how can we renegotiate that social contract where they get what they want without harming us and we get what we need without having to sacrifice too much privacy or time or mental health or whatever? And this is what I think we're moving towards. And this is so part of my mission, the next part of my mission, is to start the conversation around negotiating this new social contract, this relationship between us, businesses, government and whatever other entities, pillars of society are out there. Probably media will be part of that conversation as well. Banks. Go ahead. Do you have any predictions for like how you see that unfolding, like in the near versus the far future? Yeah, so the biggest problem, and I alluded to this at the beginning, but the biggest problem, the first problem is, we the people are about to lose a tremendous amount of power and that is gonna be through labor. 100 years ago, collective bargaining allowed the creation of unions and man, like the rail and coal tycoons, they hated their employees. If you go back and read what they said, they literally wanted to use the army and the National Guard to force people back into the mines. They wanted to use violence to force labor. That is how much disdain the barons had for the working man until Teddy Roosevelt, the ludicrous firebrand of a man, came and said, no, we're going to do things differently. And obviously, corporations are not nearly that bad today, but if left unchecked, we could backslide in that direction. And when human labor is replaced by machine labor, we lose a huge bargaining chip. And so that's the biggest thing, the biggest concern in my mind. We still have the vote, right? We still have voting power, but if we lose labor, then we also lose a lot of financial power. Because right now we can vote with our dollars, right? You don't like what a company does, more often than not, you can allocate your money to a different company, right? We also have control of our time, more or less. You can say like, well, I don't like how Twitter's behaving, so I'm going to go use a different social media platform. But the power dynamics of society, and I mean like all of human civilization, is about to change drastically. And I don't know what the solution is for that. I just started working on this like yesterday. So it'll take a little while to catch up, but I'm just like, this new social contract is the biggest unsolved problem in my mind. Interesting. Yeah. Okay. So let's talk for a second about maybe AI governance then. Let's talk for a second about maybe AI governance then. And because it seems like that's gonna have a big play in how we are, you know, our level of autonomy in this new realm, you know, our level of influence in this new realm, you know, where do you see that heading? Well, so there's a lot of good news on the governance and ethics side. The United Nations, the EU, even US Congress, Great Britain, pretty much all the major powers in the West are on board with facilitating innovation, but doing it safely. They're mostly looking at it through the lens of consumer protection, which is a good start. The EU AI Act includes in its language, a few like, banned use cases, like creating a social credit system like they have in China. So like, that's great. Like that's a really good start in terms of setting the tone, setting the policy in order to ensure that investment goes in the correct direction, correct direction, as best we can figure up front in order to, because here's the thing is like, we see, here's what gives me a lot of confidence. Sorry, I'm kind of scattered. The way that GDPR has shaped the landscape in Europe, and I've talked to finance people and venture capitalists, and if it's not GDPR compliant, they won't touch it. And GDPR is so strong, and because we have a global technology economy, even American companies that technically don't have to abide by GDPR, a lot of them are required to buy their investors because they're like, well, if you're not GDPR compliant, you can't expand into Europe. So we're not going to invest in you right now until you become compliant. It's like, okay, wow. I don't want to say strong arm, that's not the right thing, but if you can use regulation to enforce a level of ethics and good behavior in corporate governance, then that gives me some faith that we can do the same thing with AI. And whether it starts in America or Europe or Great Britain or wherever, or UN, because the UN chief is, is amenable to the idea of creating an AI watchdog and internationally, I watchdog, they're all taking it seriously. So this is actually one domain where I'm actually like pretty confident that we'll figure it out because it's like the promise is there, right? The carrot is, is huge. You know, we're talking like a hundred trillion dollars in global GDP in the next, you know, 10 years, that's an insane amount, but you have to do it safely because if it backfires, it could be really bad for everyone. And so everyone wants the good outcome and downplay the bad outcome, go left instead of right, basically. Yeah. So what are your thoughts? Because I know you've put out a lot of content regarding the human alignment with AI. Um, so what, what are your thoughts in how, how that's best achieved? I know you've had some, um, you've had some things to say about using AI to align AI to humans. Yeah. You know, so here's, so first I have to just have the caveat like I'm not a machine learning researcher. I am, however, an engineer and I have done a lot of work with synthesizing data sets and fine tuning models. I personally haven't found any evidence that aligning a single model is particularly difficult. You can pretty much get an AI machine to do whatever you want and think however you want. And then if it doesn't behave correctly, you can create a policy to steer it back to, you know, whatever it is that you want it to do. But in the grander scheme of things, there's a lot of other pressures at play. And so, you know, you can think about it in terms of like evolutionary pressures, right? Some people have started talking about it, like how AI will evolve. And while that that is a term that's typically applied to organic life forms, it's actually pretty, pretty pertinent to apply it to the evolution of AI, because what what is evolution, but variation and selection, right? And so there's a million models out there, and we're choosing the ones that better suit our needs. And so we're selecting models that are faster, more efficient, more intelligent. And so we're already selecting for AI models that kind of intrinsically steer them in a certain direction. Because if you use an AI model, like chat GPT or Bing when it first came out is a great example, just all the disturbing things that Bing would say because it was not correctly aligned. And so people just, you know, they would play with it, but they didn't use it seriously. And then by steering that, looking at it from a product perspective, just steering those AI products in a way that makes them more user friendly and more useful. There's almost this kind of natural alignment happening in my mind. So I'm not even as worried about that. But what I am most worried about is nations, militaries, corporations, people using it, and the competitive dynamics in the broader market landscape. Because here's the thing is, as I mentioned earlier, companies are incentivized to provide goods and services as cheaply as possible. Now that might mean undercutting their competition through one way or another. And sometimes that means cutting corners. It might mean cutting ethical corners or quality corners or whatever else. And if you get locked in this race condition where everyone's like stuck in a race to the bottom, you might end up saying like, okay, well we can use this other model that's cheaper and faster, but it's not as intelligent, or maybe it's not as ethical. And so then if you get locked in this like race to the bottom death spiral, you might end up with misaligned AI, even if you get locked in this race to the bottom death spiral, you might end up with misaligned AI, even if you know how to align it in the lab. So I'm not as concerned about what we can do in the lab as I am the economic paradigm. And that's why I'm talking about renegotiating the social contract so that at the macro scale, all the incentives align towards creating a better future, a utopian future, as some of us like to say. Yeah. So did you see the report that Versus AI, Dentons, and the Spatial Web Foundation put out? I think they published it like a week and a half, two weeks ago. It was called the Future of AI Governance. I took a look at it, but I didn't get a chance to read it in depth. I took a look at it, but I didn't get a chance to read it in depth. So one of the things that's interesting to me and what they proposed is, you know, by using the spatial web protocol, you can bake human laws. You know, you can make them programmable and understandable by the AI and the AI can therefore comply, abide by them and, you know, act accordingly. And so when you're talking about, you know, AI governance and being able to actually program certain things into the AI that way, you know, that's, that's an interesting thought to me, because it's, it's very different than when you're dealing with like these machine learning models where they're black boxes. So yes, you can tweak the parameters and all of that, but the processing itself is obscured. So you can just try to steer it, but with their technology, it looks like you can actually program it. And then the active inference AI is actually explainable at AI. It can self-reflect, it can actually self-report on how it's coming to its decisions and all of that. So, you know, I'm just kind of curious what your thoughts are on that. Yeah, you know, you've touched on something that I often advocate for and that is that AI today is no longer a math problem. Many people that are still researching alignment from a mathematical perspective and optimization perspective, I think they might be kind of falling a little bit behind the state of AI today, because to your point about laws, some people say, oh, well, language is too squishy and it's not good at articulating goals. I'm like, tell that to the entire legal system. Tell that to people writing international treaties and contract negotiators and like every lawyer from Harvard, right? Language is a very, very powerful tool to articulate what it is that you want, what it is that you don't want, the boundaries that you want. And so as we invent more AIs, whether it's spatial web protocols, active inference, or just language models, all of the above, that can understand language, and not just the letter of the language, but the spirit and intent of the language. And again, I have not seen any evidence to think that like, you know, super intelligence is going to kind of officially over-optimize because of something that was misworded once. Right. Like the, these AI models that we're coming up with and all these paradigms are very good at understanding the nuance and spirit and intention of the words that we're using, whether it's laws, whether it's principles, that sort of thing. So like, that's one of the reasons that I'm like super not worried about it, because like the machine will do what we design it to do and what we tell it to do. And eventually it's going to be better at it than we are. Heck, I already use the models to like, I like, Hey, I'm trying to say this. Can you word this better? And it'll ask me a couple of questions and it'll spit out a better explanation. I'm like, yeah, let's use that. I know I do the same thing and I'm just like, oh, this is so much easier than just trying to use my own brain power to come up with. I mean, it's like having another brain to bounce ideas off of at all times and that's's super helpful. Oh, yeah. So one thing I wanted to ask you, and I know we're running, we're probably getting close on time here, but did you see the there was an article? So researchers from Carnegie Mellon, and this was just like maybe two days ago, they've found that it doesn't matter which one of these machine models, whether it's the, you know, chat GPT or Palm or, you know, LLAMA, Lama, whatever, however you want to say that. But so they're finding that they can all be hacked very easily. And, and by, by, you know, uh, gaming the whole suffix string side of it, by, by asking the, the bot itself to reply with sure here's, you know, then it just will allow you to bypass a lot of the guardrails because it automatically then goes into this, I'm helpful and I'll answer anything you're, kind of a thing. And the craziness is that it's been like every time, I guess with the meta one that's open source where all of the weights are public too, right? They're able to game that 100% of the, nearly 100% of the time. And then even with ones where it's more proprietary, you know, you can't really see the training side of the model, they're still finding that it's, you know, like 50, 60%. But if it's like an ensemble of attacks, it goes up to like 86% of, you know, where they're able to jailbreak them, you know, and that's a real problem. So what are your thoughts there? Yeah, so none of that is remotely surprising to me. You know, I started using GPT models back in GPT-2, GPT-3, before any alignment, when they were just like hot off the press, like, you know, they would spit out almost just pure gibberish. And so, you know, every time I see one of those papers, like, oh, new jailbreak, I'm like, yeah, whatever. This is just how it works. You know, it's kind of like, it would be kind of like, you know, just mashing the keyboard of a calculator, and then like, it didn't give me the answer that I wanted. It's like, well, yeah, because like, you're not using it correctly. And so from a software perspective, so this is, this is my infrastructure engineer hat coming out. If you use any computer component incorrectly, it's going to, you're going to get stack overflows., you're gonna get segmentation faults, you're gonna get indexing errors. Every piece of technology, whether it's a database or a server or a smartphone, if you don't use it right, it's gonna break, it's that simple. And so the problem here, and I've been beating this drum for a while, is that a language model is only one component of the stack. And a lot of people, particularly the ML engineers and researchers, they think it's the whole stack. And so from my, back in my corporate days, I remember having a conversation with a software architect and he said, oh, you infrastructure guys aren't going to need to be around anymore because we're going to automate everything. I said, okay, well, what do you mean by everything? He's like, well, this, this, and this. I was like, okay, what about databases? What about backups? What about security? He's like, well, this, this, and this. I was like, okay, what about databases? What about backups? What about security? What about, you know, this, this, and this? And he's like, oh, well, none of that matters. And I'm like, no, actually, all of the entire tech stack matters. And so when someone says, oh, well, this machine learning model broke because we gamed it, it's like, well, yeah, because like you opened the carburetor, the engine and poured water, and of course, the engine stalled. But when you have a car, you have several layers of mechanisms that ensure that the air and fuel that gets injected into the engine is the correct temperature, and shape, and format. And likewise, a language model on its own is just like having a motor sitting there. So yes, of course, the motor is going to be vulnerable to being broken, to being exploited, to being gamified. And, you know, I think that as we create smarter and smarter machines, they're going to realize this and they're going to put, you know, layers around themselves, even so much as to understand that they're gonna need to have some layers of processing before it even accepts something. And so this is the work that I've done with cognitive architectures, where you have some input, but you can scrutinize it, you keep it at arm's length, you figure out what do I do with this input, what context. So this is why that client I mentioned earlier reached out, because maintaining context is super important for ensuring that language models or any AI model behaves the way that you need it to. And so just like, you know, any mechanical component, any software component, if you, you know, it's designed to do one thing and then you put it in a bad environment, of course it's not gonna behave correctly. So this is all just part of the process of, you know, yes, okay, the machine learning folks, they have a shiny new toy and they're playing with it and they're figuring out ways to break it. Okay, great. From a software development tech stack perspective, it's just like, okay, that's all fine. So I'm not, I'm super not worried about any of that. That's just part of the process of figuring out like, you know, oh, hey, like we have a new device, like what's it good for? What isn't it good for? How do you break it? It's all part of the process. Like, you know, the military does the same thing anytime that you give them a new piece of hardware, right? Like, what's it good for? How do you break it? Like, how is it vulnerable? So this is all just the testing phase. Yeah, very cool. Yeah, so maybe one last question. What do you think is the biggest myth that, you know, people are buying that in regard to AI right now? You know, I don't know that it's a myth, but it's certainly an open question and that is whether or not it's conscious or sentient. And this is where there's some really fascinating conversations happening. And so I ran a poll on my YouTube channel and 57% of people said that it's probably not sentient yet but will be, that machines will be sentient at some point in the future. That's a big chunk of people. Now, granted, there's a lot of selection bias because it's my audience, but it's way higher than I thought. I thought it would be like maybe 20%. And my own mind has been shifting on whether or not machines can be sentient or conscious or whatever term you want to use for it. And basically, so Max Tegmark wrote in his book, Life 3.0, there's a whole chapter dedicated to substrate independence, which is like, okay, so here you have vacuum tubes and silicon and biology, like all matter computes in the same way, basically. It's just a matter of how it's organized. And so if you make the assumption that, you know, the universe is a bottom up kind of emergence model of intelligence and consciousness and sentience, then there is no reason that a machine could not also be conscious or sentient, but it's just a matter of organizing the information in the right pattern. And there's actually some evidence to support this, very recent evidence in neuroscience, which looks at the way that like magnetic, or maybe it's electrical waves, anyways, the way that signals propagate throughout and across the brain, and that by looking at those very carefully, you can predict not just like whether or not someone is conscious, but aspects of their conscious experience, kind of like what we're talking about earlier with the fMRI, as you can tell what is consciously in someone's mind. And so then, okay, then that indicates that consciousness is likely an energetic thing. It's more about energy than it is matter. And so it's like, that's just a fundamentally different way of thinking about consciousness and sentience. But then it's like, okay, well, how big does that get? Because, uh, like electricity travels at the speed of light. So does that mean that the entire internet could be conscious one day? Is it already conscious? Like who knows, right? Just the number of possibilities are crazy. But what I will say, the biggest myth around that is even if it is sentient, that doesn't mean that it experiences itself or existence like we do. That doesn't mean that it's going to have the same emotions that we do, like fear of death or loneliness or anything like that. So that's what I always urge caution is like, okay, yes, the machine might be conscious, but don't make the assumption that it wants love. Don't make the assumption that it's going to Don't make the assumption that it's gonna fear death or that it's hungry. You're right, just because it tells you that it's hungry. Right. So that's kind of the biggest, like I said, it's not really a myth, but kind of the misconception, I guess. Yeah, that's interesting. That's very thought provoking. Yeah. And I feel like that could lead to a whole nother conversation. Oh yeah, oh yeah. No, it keeps going. Which I would love to have actually to a whole nother conversation. Oh yeah. Oh yeah. I would love to have actually, well, let's do it. Um, well, thank you so much, Dave. This has been such a pleasure having you on our show today. And, you know, I just so appreciate you being here with us. And, um, I'd love to have you back sometime, uh, in the future as this evolves, because things, like you said, things are happening so quickly. And I just know that this conversation will probably just become more and more fun over time. Absolutely. Looking forward to it. Just let me know. So thank you again. And thanks everyone for tuning in. Oh, Dave, how can people reach out to you? How can they find you and your content? And maybe let people know that. So the best way to reach out is LinkedIn. Just include a note as to why you're connecting. Right now, I'm connecting mostly with, there's kind of three categories of people that I connect with. Businesses, if they want to consult, I do speaking and training, amongst a few other things. I am also connecting with academics. So some research labs that just want to either discuss or collaborate on a paper. And then finally, policy folks, you know, either in government or think tanks or whatever, thinking more about like the economic side. So LinkedIn or Patreon, I do one off consulting gigs through my Patreon. So you can sign up there. That's the two best ways to get ahold of me. Okay, awesome. And I'll put those links in the show notes too. And gosh, thank you so much, Dave. It's been a pleasure. Thanks, everyone for tuning in. And we'll see you next time. tuning in and we'll see you next time.", "chunks": [{"timestamp": [0.0, 17.8], "text": " Hi, and welcome to another episode of the Spatial Web AI podcast."}, {"timestamp": [17.8, 20.52], "text": " Today we have a very special guest."}, {"timestamp": [20.52, 28.68], "text": " We have David Shapiro, AI researcher and thought leader and just overall really smart guy."}, {"timestamp": [30.0, 31.92], "text": " David, it's such a thrill for me"}, {"timestamp": [31.92, 33.8], "text": " to have you here on the show."}, {"timestamp": [33.8, 36.96], "text": " I'd love it if you just maybe introduce yourself"}, {"timestamp": [36.96, 38.84], "text": " to our audience."}, {"timestamp": [38.84, 41.04], "text": " Yeah, thanks Denise for having me."}, {"timestamp": [41.04, 42.6], "text": " My name is David Shapiro."}, {"timestamp": [42.6, 48.0], "text": " I have been in IT infrastructure cloud automation since 2007,"}, {"timestamp": [48.56, 54.8], "text": " independent AI researcher since 2009. And then of course, you know, we're in the era of GPT-2,"}, {"timestamp": [54.8, 60.16], "text": " GPT-3, and now GPT-4. And, you know, so pretty much the rest is history, but yeah, I've been"}, {"timestamp": [60.16, 69.26], "text": " an independent researcher and technologist for quite a while. And then just about 16 to 18 months ago, I got on YouTube to spread the word"}, {"timestamp": [69.26, 71.66], "text": " basically, and that's how we got to where we are today."}, {"timestamp": [72.84, 73.48], "text": " Awesome."}, {"timestamp": [73.72, 78.48], "text": " Well, I'm so excited to have you here because I have a lot of questions."}, {"timestamp": [79.76, 82.28], "text": " And I know you're, you're the one to ask."}, {"timestamp": [82.32, 86.64], "text": " So I've watched a lot of your videos. I've heard a lot of your"}, {"timestamp": [86.64, 95.92], "text": " talks on AI and what you see for the future of AI. And so a lot of people talk about the fears of,"}, {"timestamp": [96.88, 101.52], "text": " this may happen or that kind of thing. And I don't see a lot of people out there really putting out"}, {"timestamp": [101.52, 110.8], "text": " the message for all the positives and all the great things that AI can do for us and for humanity and our civilization. And so what do you see as the biggest"}, {"timestamp": [110.8, 118.88], "text": " advantages for AI? Yeah. So, you know, you're right. There's it's, it's incredibly polarizing,"}, {"timestamp": [118.88, 124.4], "text": " right. And there's a few videos out there, commentators and big names. Certainly there's"}, {"timestamp": [127.04, 132.48], "text": " really zealous advocates on one side, and then there's the doomsayers on the other. And it's really rare that we see something that is"}, {"timestamp": [132.48, 136.48], "text": " this polarizing, right? Where it's just like people are saying like this is going to save"}, {"timestamp": [136.48, 140.48], "text": " everyone, it's going to cure everything or it's literally going to kill everyone."}, {"timestamp": [140.48, 145.0], "text": " So, like, I try and take a more realistic approach."}, {"timestamp": [145.0, 149.0], "text": " And I don't try and downplay the dangers."}, {"timestamp": [149.0, 154.84], "text": " Whether it's just innocent misuse or mistakes, like there's always the potential for harm."}, {"timestamp": [154.84, 159.2], "text": " Then of course there's like wanton misuse of people deliberately using new technologies"}, {"timestamp": [159.2, 162.7], "text": " because all technologies are dual use."}, {"timestamp": [162.7, 165.3], "text": " Whether you build a nuclear bomb or a nuclear reactor, whether"}, {"timestamp": [165.3, 170.78], "text": " you build better medicines or bioweapons, right? All technology is dual use. And so"}, {"timestamp": [170.78, 175.98], "text": " with any technology, this is AI, whatever else, you always need to be very careful and"}, {"timestamp": [175.98, 182.46], "text": " very deliberate about how you deploy it. Now, in terms of like where I'm at on the spectrum,"}, {"timestamp": [182.46, 185.52], "text": " you know, as a career problem solver, because even before the"}, {"timestamp": [185.52, 190.08], "text": " rise of AI, that was what I made my career based on was being able to solve problems that other"}, {"timestamp": [190.08, 196.48], "text": " people couldn't. And so I have not found any evidence that any of the problems confronting"}, {"timestamp": [196.48, 201.76], "text": " us are unsolvable. And, you know, every time someone says, well, what about this? And it's"}, {"timestamp": [201.76, 205.12], "text": " like, okay, well, you've identified the problem. That's half the battle, right?"}, {"timestamp": [205.12, 206.96], "text": " You know, like people will complain, you know,"}, {"timestamp": [206.96, 208.92], "text": " about like, hey, Mesa optimization,"}, {"timestamp": [208.92, 211.0], "text": " which for anyone who's not familiar,"}, {"timestamp": [211.0, 213.64], "text": " Mesa optimization is a phenomenon"}, {"timestamp": [213.64, 215.44], "text": " inside of deep learning models,"}, {"timestamp": [215.44, 219.68], "text": " where it optimizes for not what you think it is."}, {"timestamp": [219.68, 221.84], "text": " It's kind of like has an internal layer."}, {"timestamp": [221.84, 223.94], "text": " That's not necessarily deceptive."}, {"timestamp": [223.94, 226.52], "text": " You might perceive it as deceptive where it's like, okay"}, {"timestamp": [226.52, 228.36], "text": " I'm giving you the answer that you want, but only"}, {"timestamp": [228.36, 230.82], "text": " because I know that that's what you think you want."}, {"timestamp": [230.82, 232.84], "text": " And I'm working around it."}, {"timestamp": [232.84, 235.76], "text": " And I'm like, but if you've articulated that problem"}, {"timestamp": [235.76, 237.04], "text": " and you're working on that problem"}, {"timestamp": [237.04, 238.96], "text": " that means it's a solvable problem."}, {"timestamp": [238.96, 241.88], "text": " And this has been the nature of science since forever"}, {"timestamp": [241.88, 243.86], "text": " where you solve one problem at a time"}, {"timestamp": [243.86, 246.16], "text": " and you move on to the next one."}, {"timestamp": [246.16, 250.68], "text": " Now, that being said, with artificial intelligence,"}, {"timestamp": [250.68, 252.88], "text": " we are confronted with the possibility"}, {"timestamp": [252.88, 255.6], "text": " of creating something much more intelligent than ourselves."}, {"timestamp": [255.6, 258.52], "text": " And that raises all kinds of other questions,"}, {"timestamp": [258.52, 259.56], "text": " existential questions,"}, {"timestamp": [259.56, 260.84], "text": " like what is the point of being alive?"}, {"timestamp": [260.84, 262.68], "text": " I think Elon Musk had a tweet a few weeks ago,"}, {"timestamp": [262.68, 268.72], "text": " it's called X now, but he said like, if, what did he say? It was something like, if AI is smarter than all of us,"}, {"timestamp": [268.72, 272.48], "text": " what's the point of being alive or something like that, right? It would just summed it up in one"}, {"timestamp": [272.48, 276.8], "text": " tweet, which is really touches into the existential dread that some people have."}, {"timestamp": [276.8, 281.52], "text": " And so in many cases, I think that a lot of people might have a, just have a problem articulating"}, {"timestamp": [281.52, 286.18], "text": " that that's their fear. And so, you know, I'm here to validate that's a legitimate fear."}, {"timestamp": [286.18, 292.56], "text": " But at the same time, you know, society has gone through huge changes in the past."}, {"timestamp": [292.56, 295.88], "text": " You know, 200 years ago, 90% of people were farmers."}, {"timestamp": [295.88, 297.48], "text": " I don't know about you, but I don't want to be a farmer."}, {"timestamp": [297.48, 299.8], "text": " So I'm glad that we went through that change, right?"}, {"timestamp": [299.8, 303.16], "text": " You know, an entire way of life was destroyed."}, {"timestamp": [303.16, 306.98], "text": " And I'm sure for the people that lost their jobs, lost their farms, you know, had"}, {"timestamp": [306.98, 310.02], "text": " to leave their farm and go to the city, it was awful, right."}, {"timestamp": [310.02, 312.62], "text": " And I'm not saying that their suffering was worthwhile, or"}, {"timestamp": [312.62, 314.88], "text": " that it was righteous or noble, I'm saying like, they suffered,"}, {"timestamp": [314.88, 318.24], "text": " and it really was awful for them. But in hindsight, in"}, {"timestamp": [318.24, 321.18], "text": " retrospect, I'm glad that society went through that"}, {"timestamp": [321.18, 325.88], "text": " change. And so that's why like, you know, it's, I'm, I'm neither a"}, {"timestamp": [325.88, 328.0], "text": " Doomer nor like a hardcore optimist."}, {"timestamp": [328.0, 331.96], "text": " Like that's, that's my overview of, of, uh, of kind of where"}, {"timestamp": [331.96, 332.92], "text": " we're at and where we're going."}, {"timestamp": [333.8, 334.48], "text": " Yeah."}, {"timestamp": [334.6, 341.24], "text": " Um, so, and I agree with you and I, I see this future that's going"}, {"timestamp": [341.24, 346.26], "text": " to be improved for humanity, but I know there's going to be like a little"}, {"timestamp": [346.26, 350.76], "text": " bit of a painful transition in that period too. So I don't want to diminish that by any"}, {"timestamp": [350.76, 359.48], "text": " means, you know. So what are some of the biggest hurdles that you see that we need to overcome?"}, {"timestamp": [359.48, 368.0], "text": " Like as we're, you know, transitioning into this space of AGI and then even ASI, super intelligence?"}, {"timestamp": [368.0, 372.44], "text": " If you'd asked me a few months ago, I would have given you one set of answers."}, {"timestamp": [372.44, 374.04], "text": " We need to figure out autonomy."}, {"timestamp": [374.04, 379.8], "text": " We need to figure out yada, yada, a litany of technical and scientific problems."}, {"timestamp": [379.8, 381.68], "text": " But of course, everything's moving lightning fast."}, {"timestamp": [381.68, 385.36], "text": " So I don't really see any technical barriers anymore,"}, {"timestamp": [385.36, 391.2], "text": " especially the rate at which AI models are improving. Really the biggest barriers are"}, {"timestamp": [392.32, 400.0], "text": " that of organization and communication. And so what I mean by that is, first and foremost,"}, {"timestamp": [400.0, 404.24], "text": " there's just so much news happening all the time. There's hundreds and hundreds of startups."}, {"timestamp": [404.24, 409.44], "text": " There's 5,000 AI papers per month being published right now, and that's probably going to be 6,000"}, {"timestamp": [409.44, 418.48], "text": " next month. And so you have this rate of change that is, it's a decentralized, like tidal wave,"}, {"timestamp": [418.48, 422.56], "text": " you can't stop it. And so then how do you manage that? And it's like, well, you got to learn to"}, {"timestamp": [422.56, 426.34], "text": " surf, right? You got to stay on top of the wave"}, {"timestamp": [427.5, 430.68], "text": " cause you can't stop it, you can't push it back."}, {"timestamp": [430.68, 433.02], "text": " So that's one of the main problems."}, {"timestamp": [433.02, 434.9], "text": " And then of course, there's always going to be"}, {"timestamp": [434.9, 438.66], "text": " like lagging indicators and people, you know,"}, {"timestamp": [438.66, 439.94], "text": " failing to adapt, you know,"}, {"timestamp": [439.94, 444.16], "text": " there's always people out there that are kind of in denial"}, {"timestamp": [444.16, 446.78], "text": " about like what's happening and the magnitude of it."}, {"timestamp": [446.78, 449.1], "text": " And I'm like, look guys, like your way of life"}, {"timestamp": [449.1, 451.48], "text": " is gonna change whether you like it or not."}, {"timestamp": [451.48, 454.0], "text": " And some people they're just like, well, I'm skeptical."}, {"timestamp": [454.0, 456.48], "text": " And of course people trust their own opinion"}, {"timestamp": [456.48, 458.98], "text": " more than they trust the news or the facts or whatever."}, {"timestamp": [458.98, 460.68], "text": " That's a whole other can of worms."}, {"timestamp": [461.72, 463.16], "text": " But those coordination failures,"}, {"timestamp": [463.16, 464.42], "text": " those communication failures,"}, {"timestamp": [464.42, 466.96], "text": " I think represent probably the biggest risk"}, {"timestamp": [466.96, 469.38], "text": " because by the time some people are aware,"}, {"timestamp": [469.38, 471.02], "text": " it's gonna be too late."}, {"timestamp": [471.02, 474.08], "text": " And so like I was just listening to a podcast earlier,"}, {"timestamp": [474.08, 476.14], "text": " one of the bigger names,"}, {"timestamp": [476.14, 478.1], "text": " and I'm listening to all the entrepreneurs,"}, {"timestamp": [478.1, 480.66], "text": " the people with billion plus dollar companies,"}, {"timestamp": [480.66, 482.94], "text": " and even people in the industrial space"}, {"timestamp": [482.94, 486.0], "text": " are really getting on board with generative AI,"}, {"timestamp": [486.0, 488.0], "text": " spatial web, decentralization."}, {"timestamp": [488.0, 494.0], "text": " And so these are the people that are connected in, that are the real movers and shakers in the industry."}, {"timestamp": [494.0, 499.0], "text": " And in the podcast I was listening to earlier, they were talking about like, oh, yeah, you know,"}, {"timestamp": [499.0, 506.76], "text": " like, we're going to get like, automate as much of the factory as we can, as much of the logistics chain as you can."}, {"timestamp": [506.76, 508.92], "text": " And what they weren't saying though,"}, {"timestamp": [508.92, 512.48], "text": " is what happens to the people who lose their jobs."}, {"timestamp": [512.48, 515.56], "text": " And from a business perspective, it's not really their job."}, {"timestamp": [515.56, 519.04], "text": " Like they are, like every business has an obligation"}, {"timestamp": [519.04, 521.64], "text": " to their shareholders or customers or whatever,"}, {"timestamp": [521.64, 524.2], "text": " to provide goods and services for cheaper."}, {"timestamp": [524.2, 525.18], "text": " That's it."}, {"timestamp": [525.18, 527.12], "text": " That's the only objective function they have."}, {"timestamp": [527.12, 530.48], "text": " But what happens is that in the past,"}, {"timestamp": [530.48, 532.52], "text": " so here's, sum it all up."}, {"timestamp": [532.52, 534.76], "text": " In the past, we had this notion"}, {"timestamp": [534.76, 536.56], "text": " that technology creates more jobs."}, {"timestamp": [536.56, 537.38], "text": " It's not true."}, {"timestamp": [537.38, 538.22], "text": " That was never true."}, {"timestamp": [538.22, 540.7], "text": " What technology does is that it lowers the cost"}, {"timestamp": [540.7, 543.32], "text": " of goods and services so that money can be allocated"}, {"timestamp": [543.32, 545.28], "text": " elsewhere, which creates new sectors,"}, {"timestamp": [545.28, 550.24], "text": " new demand. So that example that I gave a minute ago, where 200 years ago people were farmers,"}, {"timestamp": [550.24, 556.0], "text": " suddenly there was knowledge work, there was service industry. And so then people could,"}, {"timestamp": [556.72, 560.96], "text": " their food was cheaper, their homes were cheaper, so on and so forth. They could then afford to buy"}, {"timestamp": [560.96, 567.2], "text": " consumer goods and electronics and other stuff. So that created new demands, not necessarily new jobs."}, {"timestamp": [567.56, 571.92], "text": " And so this is the primary shift that I think that people have not"}, {"timestamp": [571.92, 573.28], "text": " have yet to wrap their heads around."}, {"timestamp": [573.28, 577.6], "text": " And I call it post-labor economics, which is we're not going to be working anymore."}, {"timestamp": [577.92, 579.88], "text": " So then how does the economy work?"}, {"timestamp": [580.12, 582.96], "text": " If robots and machines and everything take all of our jobs,"}, {"timestamp": [583.12, 585.28], "text": " what how does the economy function?"}, {"timestamp": [585.28, 589.28], "text": " It's going to function. We got to figure it out. And then it's an, that is an unsolved problem."}, {"timestamp": [589.28, 600.8], "text": " Yeah. And it's interesting because, you know, when I think of like what the spatial web is,"}, {"timestamp": [600.8, 615.0], "text": " is, you know, opening up, you know, with this evolving internet, wherever we, we move from a library of websites, web pages into spatial domains, you know, where every single thing in any space becomes a domain."}, {"timestamp": [615.0, 627.44], "text": " Permissions are at every touchpoint, context is baked into every touchpoint and it's informing the AI, right? So, you know, when I see this type of a system, you know,"}, {"timestamp": [627.44, 634.72], "text": " and then no jobs, and we have to find new meaning, but also businesses, corporations,"}, {"timestamp": [634.72, 640.56], "text": " they have to find new ways to insert themselves into the lives of the consumers, because,"}, {"timestamp": [640.56, 645.04], "text": " you know, advertising, all the stuff that we've seen before is not going to work."}, {"timestamp": [645.08, 647.4], "text": " They're going to have to have this native experience."}, {"timestamp": [647.84, 651.12], "text": " You bring in like, you know, cryptocurrencies, you know, value"}, {"timestamp": [651.12, 655.64], "text": " exchange versus fiat and, you know, decentralization, all this stuff."}, {"timestamp": [656.08, 658.58], "text": " You know, I kind of envision a future."}, {"timestamp": [658.6, 662.24], "text": " Obviously it's going to take a while to get to that point, but where"}, {"timestamp": [662.96, 667.2], "text": " our daily lives just start rewarding us. We start like the system itself"}, {"timestamp": [667.2, 674.32], "text": " is giving us value back for interacting with the system. And we could have this future where you"}, {"timestamp": [674.32, 679.04], "text": " wake up, you make your bed and then you're rewarded. You're not even aware these things"}, {"timestamp": [679.04, 683.12], "text": " are really happening, but they're happening. And it could be the sheet manufacturer,"}, {"timestamp": [683.12, 689.92], "text": " the bed manufacturer, that's part of their marketing dollars are now in rewards, you know, like I definitely see,"}, {"timestamp": [689.92, 699.36], "text": " like, there's a lot of options for this future of innovative ways where, you know, I don't know."}, {"timestamp": [699.36, 704.72], "text": " You know, I just thought of a good example while you were kind of talking through that. And,"}, {"timestamp": [704.56, 709.48], "text": " I just thought of a good example while you were kind of talking through that. And, you know, there's a construction site near my house, as pretty much anyone in any"}, {"timestamp": [709.48, 712.2], "text": " growing city, you're going to be surrounded by construction."}, {"timestamp": [712.2, 714.74], "text": " And every now and then there's screws or nails that are in the road."}, {"timestamp": [714.74, 718.8], "text": " And every time I see one, I pick it up and make sure that it's not in the road anymore"}, {"timestamp": [718.8, 722.52], "text": " so that somebody doesn't like pop their tire or whatever."}, {"timestamp": [722.52, 725.56], "text": " And so I created value, or rather I protected value"}, {"timestamp": [725.56, 727.52], "text": " that I'm never gonna be rewarded for."}, {"timestamp": [727.52, 730.52], "text": " And so one of the things that I'm really excited about"}, {"timestamp": [730.52, 733.88], "text": " in terms of Spatial Web, Web 3.0,"}, {"timestamp": [733.88, 735.6], "text": " what we used to call internet of things"}, {"timestamp": [735.6, 738.6], "text": " and fog computing when I was back, back when I was at Cisco,"}, {"timestamp": [738.6, 740.76], "text": " fog computing is fortunately a term"}, {"timestamp": [740.76, 742.2], "text": " that died a very quick death."}, {"timestamp": [742.2, 746.28], "text": " But the idea that it was all around you all the time."}, {"timestamp": [746.28, 750.44], "text": " And that's kind of the spirit of what we're building towards"}, {"timestamp": [750.44, 752.72], "text": " and the idea that you could track everything"}, {"timestamp": [752.72, 754.44], "text": " that like some sensor could say like,"}, {"timestamp": [754.44, 757.52], "text": " oh, hey, there's a broken fence over there,"}, {"timestamp": [757.52, 761.32], "text": " someone go fix it, or there's something in the road."}, {"timestamp": [761.32, 762.16], "text": " What was it?"}, {"timestamp": [762.16, 767.28], "text": " There was a case of, this was many years ago on the highway"}, {"timestamp": [767.28, 773.44], "text": " near me where a box of metal parts fell off the back of a truck and it shut the entire highway down"}, {"timestamp": [773.44, 779.52], "text": " for hours, right? Because if the state troopers had to stop traffic, clear the road, you go over"}, {"timestamp": [779.52, 787.22], "text": " with a magnet sweeper and it's like, how many millions of dollars of productivity were lost because someone made a dumb mistake, right?"}, {"timestamp": [787.22, 789.52], "text": " And then, and then I was like pretty frustrated."}, {"timestamp": [789.52, 793.24], "text": " I was like, you know, for me it was an inconvenience because I was stuck in traffic for an extra"}, {"timestamp": [793.24, 794.44], "text": " 45 minutes."}, {"timestamp": [794.44, 799.24], "text": " But what if like there was a lifesaving physician who didn't make it to a surgery for a newborn"}, {"timestamp": [799.24, 800.24], "text": " because of that?"}, {"timestamp": [800.24, 801.24], "text": " Right."}, {"timestamp": [801.24, 802.76], "text": " And then, so then it's like a matter of accountability."}, {"timestamp": [802.76, 806.04], "text": " And of course, like you don't necessarily want to use a new technology just to punish people."}, {"timestamp": [806.04, 810.16], "text": " But what if some of these technologies could have said, like, if there was an overpass camera"}, {"timestamp": [810.16, 813.12], "text": " and it phoned him and, you know, the driver of that truck and said, hey,"}, {"timestamp": [813.28, 815.6], "text": " you've got an unsecured box, pull over, fix that."}, {"timestamp": [815.6, 817.46], "text": " Because it's, you know, it's about prevention."}, {"timestamp": [817.46, 823.72], "text": " And so then, you know, there's all these ideas of if you had the information, how could you use it?"}, {"timestamp": [823.76, 827.76], "text": " Right. And it's a matter of gleaning all that information"}, {"timestamp": [827.76, 830.28], "text": " from all the different layers of reality"}, {"timestamp": [830.28, 831.72], "text": " that are all around us."}, {"timestamp": [831.72, 834.28], "text": " I was on a call earlier with a prospective client"}, {"timestamp": [834.28, 837.56], "text": " who's building like a data platform"}, {"timestamp": [837.56, 839.72], "text": " to facilitate this stuff."}, {"timestamp": [839.72, 842.0], "text": " And I don't think that they had realized,"}, {"timestamp": [842.0, 843.26], "text": " like, I was like, you guys are building"}, {"timestamp": [843.26, 845.62], "text": " a spatial web platform, and they're like, what do you mean? And I was like, I just sent him a link to spatial web. I was like, this is what they had realized, like, I was like, you guys are building a spatial web platform and they're like, what do you mean?"}, {"timestamp": [845.62, 847.52], "text": " And I was like, I just sent them a link to spatial web."}, {"timestamp": [847.52, 849.38], "text": " I was like, this is what you're doing, right?"}, {"timestamp": [849.38, 851.42], "text": " Is how do you coordinate?"}, {"timestamp": [851.42, 852.26], "text": " Because what did they call it?"}, {"timestamp": [852.26, 857.26], "text": " They called it like a context driven AI learning platform."}, {"timestamp": [858.02, 860.06], "text": " I was like, context, right?"}, {"timestamp": [860.06, 861.78], "text": " You know, so it's like, you know,"}, {"timestamp": [861.78, 863.98], "text": " but the thing is, is there's so much information"}, {"timestamp": [863.98, 865.44], "text": " to keep track of in the world, right?"}, {"timestamp": [865.44, 872.2], "text": " This was something that my cousin many years ago, he's an electrical engineer, started putting sensors on dumpsters, right?"}, {"timestamp": [872.2, 883.48], "text": " So that was like a really early version of spatial awareness, where it could record like every time someone threw a bag into the dumpster and predict when the dumpster was full,"}, {"timestamp": [883.68, 886.98], "text": " so that the dispatch would only send trucks out"}, {"timestamp": [886.98, 888.96], "text": " to pick up dumpsters once they were full."}, {"timestamp": [888.96, 889.8], "text": " So that's, you know,"}, {"timestamp": [889.8, 891.48], "text": " but that was a really specific use case."}, {"timestamp": [891.48, 896.44], "text": " Now, imagine if you can have like more general purpose"}, {"timestamp": [896.44, 898.84], "text": " things, any camera, any microphone,"}, {"timestamp": [898.84, 902.58], "text": " kind of detect the presence of whatever needs to be done"}, {"timestamp": [902.58, 904.8], "text": " and make that a public good."}, {"timestamp": [904.8, 907.44], "text": " That is really what I'm most excited about."}, {"timestamp": [907.44, 909.08], "text": " And of course there are a tremendous amount"}, {"timestamp": [909.08, 912.4], "text": " of technical hurdles to achieve that future."}, {"timestamp": [912.4, 914.36], "text": " I remember that was actually one of the original promises"}, {"timestamp": [914.36, 917.28], "text": " of Bluetooth when it was first published many years ago,"}, {"timestamp": [917.28, 919.44], "text": " was, oh, hey, there's this open protocol"}, {"timestamp": [919.44, 921.88], "text": " that can allow all your devices to talk to each other."}, {"timestamp": [921.88, 924.16], "text": " And of course, like we had just this utter failure"}, {"timestamp": [924.16, 928.24], "text": " of imagination back in the day, where it was like, you know, your dishwasher will talk to your fridge"}, {"timestamp": [928.24, 934.56], "text": " and we're like, why? Right. But that was because that was in the early 2000s and that was as far"}, {"timestamp": [934.56, 939.12], "text": " ahead as we could think. But anyways, yeah. So there's a tremendous amount of opportunity,"}, {"timestamp": [940.16, 944.32], "text": " you know, but then it opens up all kinds of cans of worms, like privacy, number one,"}, {"timestamp": [944.32, 945.2], "text": " first and foremost,"}, {"timestamp": [945.2, 946.6], "text": " I think is probably the biggest thing."}, {"timestamp": [946.6, 948.68], "text": " And that's what I talked with my perspective client"}, {"timestamp": [948.68, 951.94], "text": " with earlier was like, okay, so you got the context,"}, {"timestamp": [951.94, 953.86], "text": " but then how do you keep track of who knows what"}, {"timestamp": [953.86, 956.32], "text": " and who is privileged to what information?"}, {"timestamp": [956.32, 959.36], "text": " So there's a number of problems still there."}, {"timestamp": [959.36, 964.36], "text": " Yeah, so to me, that's one of the advantages,"}, {"timestamp": [964.72, 968.32], "text": " the really cool things about the Spatial Web Protocol"}, {"timestamp": [968.32, 978.96], "text": " because the HSTP, the Hyperspace Transaction Protocol, because in the spatial realm, permissions,"}, {"timestamp": [978.96, 981.2], "text": " everything is permissionable at every touch point."}, {"timestamp": [981.2, 988.8], "text": " So then you can program in credentials and all kinds of information about"}, {"timestamp": [988.8, 996.4], "text": " ownership and who can access. And it will create this future where you can be in control of your"}, {"timestamp": [996.4, 1003.52], "text": " data, your data realm, and you can give permissions that have expirations to them even, like all that"}, {"timestamp": [1003.52, 1005.44], "text": " kind of stuff. So I feel"}, {"timestamp": [1005.44, 1011.28], "text": " like these issues that we see a lot of them are because we're in this web two environment,"}, {"timestamp": [1011.28, 1017.04], "text": " and we're still in this worldwide web where we have limitations that, you know, I really feel like"}, {"timestamp": [1018.32, 1021.76], "text": " a lot of benefits going to come from the, this, this evolution."}, {"timestamp": [1021.76, 1027.34], "text": " That makes sense. Yeah. And, you know, and it's like kind of what you described"}, {"timestamp": [1027.34, 1030.42], "text": " as imagining like geofencing my house, right?"}, {"timestamp": [1030.42, 1035.42], "text": " So, but then in a spatial web, web 3.0 environment,"}, {"timestamp": [1035.56, 1039.18], "text": " then it's like, okay, well, Dave,"}, {"timestamp": [1039.18, 1042.46], "text": " you are registered as the owner of this piece of property."}, {"timestamp": [1042.46, 1045.42], "text": " So then any data that originates or takes place in there,"}, {"timestamp": [1045.42, 1048.54], "text": " you ultimately have some rights to in terms of privacy"}, {"timestamp": [1048.54, 1050.82], "text": " or profiting from or whatever."}, {"timestamp": [1050.82, 1052.42], "text": " I think that there's a tremendous amount"}, {"timestamp": [1052.42, 1053.82], "text": " of opportunity there."}, {"timestamp": [1053.82, 1057.62], "text": " One idea that a lot of people have proposed lately"}, {"timestamp": [1057.62, 1061.38], "text": " is the idea of data as a public good"}, {"timestamp": [1061.38, 1064.02], "text": " or personal data governance as a way"}, {"timestamp": [1064.02, 1068.2], "text": " of kind of participating in the economy."}, {"timestamp": [1068.2, 1070.56], "text": " And of course, data is the new oil"}, {"timestamp": [1070.56, 1072.8], "text": " and there's a million ways that data can be used"}, {"timestamp": [1072.8, 1074.74], "text": " and I'm not even familiar with all of them."}, {"timestamp": [1074.74, 1079.28], "text": " But it strikes me that having that sort of"}, {"timestamp": [1079.28, 1082.74], "text": " point of origination governance and tracking of data"}, {"timestamp": [1082.74, 1087.26], "text": " is really important to building that new economy."}, {"timestamp": [1087.26, 1089.78], "text": " Again, like even if you can address it"}, {"timestamp": [1089.78, 1092.8], "text": " and even if you can, you know,"}, {"timestamp": [1092.8, 1096.06], "text": " assign privileges and permissions, it's still like,"}, {"timestamp": [1096.06, 1100.02], "text": " I guess the thing that I haven't figured out yet is,"}, {"timestamp": [1100.02, 1102.62], "text": " okay, well, how do you balance privacy"}, {"timestamp": [1102.62, 1105.32], "text": " with that reward for participating?"}, {"timestamp": [1105.32, 1108.28], "text": " And maybe that's the trade-off."}, {"timestamp": [1108.28, 1111.84], "text": " Right, and honestly, that's kind of what I see"}, {"timestamp": [1111.84, 1114.68], "text": " is that right now we give away all our data."}, {"timestamp": [1114.68, 1117.88], "text": " We don't really have a choice, but we will have a choice."}, {"timestamp": [1117.88, 1121.72], "text": " And there will be incentives that appeal to us"}, {"timestamp": [1121.72, 1129.84], "text": " enough to trade data, trade some of that privacy, but it'll become a choice,"}, {"timestamp": [1131.36, 1137.68], "text": " a personal choice versus complete lack of empowerment. That makes sense. Yeah."}, {"timestamp": [1138.96, 1147.2], "text": " What do you think? What that will look like? Because I often look at to, you know, like, when social media first started, and it was like, you"}, {"timestamp": [1147.2, 1150.2], "text": " know, the stranger danger, don't put your picture on the"}, {"timestamp": [1150.2, 1153.6], "text": " internet. And now, you know, like, so I think, too, as we"}, {"timestamp": [1153.6, 1160.84], "text": " evolve with technology, we actually become freer with our"}, {"timestamp": [1160.84, 1161.92], "text": " ideas around it."}, {"timestamp": [1161.96, 1170.16], "text": " Yeah. You know, I'm reminded of there's a whole raft of studies that talk about how behavior"}, {"timestamp": [1170.16, 1173.72], "text": " and even cognition changes when you know that you're being watched."}, {"timestamp": [1173.72, 1178.92], "text": " And you know, I am a very private person, and yet I have, you know, 66,000 subscribers"}, {"timestamp": [1178.92, 1180.12], "text": " on YouTube."}, {"timestamp": [1180.12, 1186.06], "text": " And so like, you know, I remember watching the numbers and then my wife was like,"}, {"timestamp": [1186.06, 1187.88], "text": " people have spent more time looking at you"}, {"timestamp": [1187.88, 1190.98], "text": " than I will look at you in our entire life together."}, {"timestamp": [1190.98, 1192.76], "text": " And it's just like, you know,"}, {"timestamp": [1192.76, 1194.54], "text": " this is the world that we live in."}, {"timestamp": [1194.54, 1198.0], "text": " And so some of it is a level of comfort,"}, {"timestamp": [1198.0, 1199.24], "text": " like just becoming familiar"}, {"timestamp": [1199.24, 1202.92], "text": " with having a public versus private life."}, {"timestamp": [1202.92, 1207.18], "text": " And then in a previous podcast that I was on,"}, {"timestamp": [1207.18, 1211.92], "text": " we talked about this idea of like the value or danger"}, {"timestamp": [1211.92, 1215.4], "text": " of having online all the time versus like,"}, {"timestamp": [1215.4, 1217.1], "text": " what did he call it?"}, {"timestamp": [1217.1, 1218.96], "text": " But basically having like islands"}, {"timestamp": [1218.96, 1221.76], "text": " that are like black holes of information."}, {"timestamp": [1221.76, 1224.1], "text": " And I said that like, actually you probably would want that."}, {"timestamp": [1224.1, 1226.96], "text": " You'd probably want some places where you know"}, {"timestamp": [1226.96, 1228.38], "text": " that there are no electronics,"}, {"timestamp": [1228.38, 1232.26], "text": " that there are gonna be like complete like reservoirs"}, {"timestamp": [1232.26, 1235.7], "text": " or whatever, you know, like going out into nature basically"}, {"timestamp": [1235.7, 1239.26], "text": " or even in urban or suburban areas."}, {"timestamp": [1239.26, 1241.52], "text": " And I'm not, you know, I'm not saying one way or another,"}, {"timestamp": [1241.52, 1242.44], "text": " I'm not saying like,"}, {"timestamp": [1242.44, 1244.26], "text": " oh, spatial web is gonna have to account for this"}, {"timestamp": [1244.26, 1247.76], "text": " or this is gonna be a problem. It's just an observation to think about."}, {"timestamp": [1247.76, 1251.76], "text": " Because, you know, in many places in the world, you're still, you're going to, you're offline,"}, {"timestamp": [1251.76, 1256.88], "text": " right? There's, you know, where I grew up. Yeah, right. You go dark. And I think that that's a"}, {"timestamp": [1256.88, 1262.48], "text": " good thing, though. I think that I think that we'll probably want to want to conserve some,"}, {"timestamp": [1262.48, 1266.44], "text": " some of that, whether it's like, I actually fully predict"}, {"timestamp": [1266.44, 1269.12], "text": " that some buildings as part of their privacy or whatever,"}, {"timestamp": [1269.12, 1270.72], "text": " it's gonna be like, you know how there's like,"}, {"timestamp": [1270.72, 1272.28], "text": " you know, no firearms allowed,"}, {"timestamp": [1272.28, 1275.22], "text": " it'll be like no spatial web allowed or whatever"}, {"timestamp": [1275.22, 1277.22], "text": " in some buildings for privacy reasons."}, {"timestamp": [1278.16, 1281.6], "text": " So I don't know, just the reason that this is all top"}, {"timestamp": [1281.6, 1283.84], "text": " of mind is because one of my best friends is like,"}, {"timestamp": [1283.84, 1285.0], "text": " he's like a privacy nut."}, {"timestamp": [1285.0, 1289.4], "text": " Like his two favorite topics are like guns and privacy."}, {"timestamp": [1289.4, 1292.0], "text": " And oh, by the way, the Clintons are all in control of everything."}, {"timestamp": [1292.0, 1296.0], "text": " So like I agree with him on the privacy thing, not the other two."}, {"timestamp": [1296.0, 1300.4], "text": " But the idea is that like privacy is actually really important."}, {"timestamp": [1300.4, 1302.2], "text": " And it's a balance."}, {"timestamp": [1302.2, 1304.2], "text": " There's a bunch of other podcasts."}, {"timestamp": [1304.2, 1307.5], "text": " There's actually like married couples from the CIA have podcasts"}, {"timestamp": [1307.5, 1307.8], "text": " online."}, {"timestamp": [1307.8, 1308.0], "text": " Now."}, {"timestamp": [1308.0, 1311.3], "text": " This is a crazy time to be alive talking about privacy versus"}, {"timestamp": [1311.3, 1311.9], "text": " convenience."}, {"timestamp": [1311.9, 1315.2], "text": " So, you know, this is this is the world that we're heading"}, {"timestamp": [1315.2, 1315.6], "text": " towards."}, {"timestamp": [1315.6, 1320.7], "text": " So the big lead rooms pretty much pretty much."}, {"timestamp": [1320.7, 1321.6], "text": " Wow."}, {"timestamp": [1321.6, 1323.3], "text": " Do you really think that's going to happen though?"}, {"timestamp": [1323.3, 1325.6], "text": " I mean, do you really think,"}, {"timestamp": [1333.76, 1339.36], "text": " because, you know, I don't know, like I, I, I see the desire for it and I see the need for, you know, having spaces you can go where you're not connected."}, {"timestamp": [1341.12, 1346.44], "text": " Like you can disconnect. Right. As long as it's opt in and opt out,"}, {"timestamp": [1346.44, 1349.24], "text": " as long as you can toggle a switch, right?"}, {"timestamp": [1349.24, 1351.64], "text": " And it's like, okay, you know, this is a non-entity."}, {"timestamp": [1351.64, 1354.16], "text": " And like Google has been working on this for many years,"}, {"timestamp": [1354.16, 1355.66], "text": " like with Street View, right?"}, {"timestamp": [1355.66, 1358.4], "text": " Like when they realized that, you know,"}, {"timestamp": [1358.4, 1360.86], "text": " having their vans drive around with cameras,"}, {"timestamp": [1360.86, 1363.24], "text": " that was a big privacy thing."}, {"timestamp": [1363.24, 1367.76], "text": " And so then they came up with AI to like blur out faces, right. And so this is not"}, {"timestamp": [1367.76, 1371.76], "text": " necessarily a new problem, in terms of respect, you know, like"}, {"timestamp": [1371.76, 1375.56], "text": " it's, you have to opt in if you want to participate. And I think"}, {"timestamp": [1375.56, 1379.48], "text": " that having that as kind of a standard could could very well,"}, {"timestamp": [1379.52, 1383.04], "text": " you know, suit some people, suit many people. And I don't, you"}, {"timestamp": [1383.04, 1387.0], "text": " know, I don't know where which side of the fence I would fall on right now."}, {"timestamp": [1387.32, 1391.0], "text": " Cause like imagining a future where it's like, okay, well, you know, like Dave, you"}, {"timestamp": [1391.0, 1394.9], "text": " know, every time you go out into public, you know, and you go to the grocery store,"}, {"timestamp": [1394.9, 1398.52], "text": " the grocery store is watching you to figure out like what products you're looking for."}, {"timestamp": [1398.52, 1401.88], "text": " And that data is going to be used to better serve your area."}, {"timestamp": [1402.12, 1405.6], "text": " And you're going to get compensated just by going to the grocery store and"}, {"timestamp": [1405.6, 1410.72], "text": " picking out your groceries. Like, okay, you'll compensate me for something that the grocery"}, {"timestamp": [1410.72, 1420.32], "text": " store knows already anyways. But on the other hand, it's just like, I'm aware of how information can"}, {"timestamp": [1420.32, 1426.16], "text": " be misused. In the past, everything from library records to land records have been used"}, {"timestamp": [1426.16, 1433.2], "text": " for genocide. And so it's like, that is really kind of terrifying. And just because it's convenient"}, {"timestamp": [1433.2, 1438.8], "text": " and you have incentives to participate in that system, that doesn't necessarily mean that it's"}, {"timestamp": [1438.8, 1448.76], "text": " automatically a good thing forever. And so one thing that is increasingly popular in blockchain"}, {"timestamp": [1448.76, 1455.0], "text": " technologies is what is it? ZKP, zero knowledge protocols or whatever, or sorry, zero knowledge"}, {"timestamp": [1455.0, 1462.24], "text": " proofs. And so the idea that you can have some information that is permanently private,"}, {"timestamp": [1462.24, 1466.76], "text": " that can still be useful. I'm not convinced that that technology is mature yet,"}, {"timestamp": [1466.76, 1468.12], "text": " but there's certainly a lot of energy"}, {"timestamp": [1468.12, 1471.12], "text": " being put into that kind of research."}, {"timestamp": [1471.12, 1472.92], "text": " But if that gets solved,"}, {"timestamp": [1472.92, 1475.48], "text": " then I think that there'll be a lot more comfort"}, {"timestamp": [1475.48, 1479.96], "text": " in participating opt-in versus opt-out"}, {"timestamp": [1479.96, 1483.76], "text": " in terms of, do you want to be on the grid, yes or no?"}, {"timestamp": [1483.76, 1484.6], "text": " Right, right."}, {"timestamp": [1484.6, 1486.66], "text": " And yeah, and to my understanding"}, {"timestamp": [1486.66, 1488.22], "text": " with the Spatial Web Protocol,"}, {"timestamp": [1488.22, 1492.74], "text": " it enables an ecosystem of zero knowledge proofs,"}, {"timestamp": [1492.74, 1495.22], "text": " self-sovereign identity and zero knowledge proofs."}, {"timestamp": [1495.22, 1498.6], "text": " So, that'd be interesting to see that."}, {"timestamp": [1500.74, 1505.0], "text": " So, now the idea of connecting or disconnecting by choice,"}, {"timestamp": [1507.24, 1510.32], "text": " what happens when we get to brain computer interface?"}, {"timestamp": [1512.12, 1515.9], "text": " Well, I've been a fan of science fiction for a long time"}, {"timestamp": [1515.9, 1519.84], "text": " and there's an anime called Ghost in the Shell"}, {"timestamp": [1519.84, 1522.2], "text": " and hacking people that are on the grid"}, {"timestamp": [1522.2, 1526.66], "text": " is actually something that is like central to that."}, {"timestamp": [1526.66, 1529.16], "text": " And now I'm playing Cyberpunk 2077,"}, {"timestamp": [1529.16, 1531.42], "text": " which also like anyone on the grid"}, {"timestamp": [1531.42, 1533.22], "text": " can get hacked at any time."}, {"timestamp": [1533.22, 1535.3], "text": " And like, that's pretty terrifying."}, {"timestamp": [1536.22, 1539.1], "text": " I probably, let's just say,"}, {"timestamp": [1539.1, 1541.26], "text": " I'm not gonna be an early adopter of BCI,"}, {"timestamp": [1541.26, 1542.5], "text": " of brain computer analysis."}, {"timestamp": [1542.5, 1548.28], "text": " You know, and if the technology gets proven out and it gets"}, {"timestamp": [1548.28, 1553.6], "text": " proven safe and then you have a switch or something where you can just say, cut off"}, {"timestamp": [1553.6, 1558.64], "text": " all radio signals, basically you need some fail safes."}, {"timestamp": [1558.64, 1560.36], "text": " MARIE RENO SOUDALIS Yeah."}, {"timestamp": [1560.36, 1565.32], "text": " built in. Probably more than one, actually. You probably need several fail-safes,"}, {"timestamp": [1565.32, 1567.88], "text": " automatic fail-safes, voluntary disconnects,"}, {"timestamp": [1567.88, 1568.84], "text": " that sort of thing."}, {"timestamp": [1571.04, 1573.6], "text": " In my history as an IT infrastructure admin,"}, {"timestamp": [1573.6, 1576.76], "text": " you know, there's a concept called defense in depth,"}, {"timestamp": [1576.76, 1578.96], "text": " the cybersecurity principle."}, {"timestamp": [1578.96, 1581.14], "text": " And so there's basically you look at security"}, {"timestamp": [1581.14, 1583.2], "text": " as layers of an onion, right?"}, {"timestamp": [1583.2, 1584.8], "text": " It goes, there's physical security,"}, {"timestamp": [1584.8, 1585.24], "text": " there's network security, there's network"}, {"timestamp": [1585.24, 1591.44], "text": " security, there's all kinds of layers. And then there's also the concept called air gapping."}, {"timestamp": [1591.44, 1595.76], "text": " And so air gapping is when there is no physical connection between one system and another."}, {"timestamp": [1595.76, 1600.4], "text": " Like it's literally not possible unless you like take a thumb drive from one system and"}, {"timestamp": [1600.4, 1605.16], "text": " plug it into another. Right now, all of our brains are air gapped from each other."}, {"timestamp": [1608.76, 1610.78], "text": " Like the only way that I can talk to you is through like words and voice and hand signals and whatever."}, {"timestamp": [1611.08, 1613.04], "text": " But you have like several layers of."}, {"timestamp": [1613.48, 1617.64], "text": " You know, agency and security before any information even gets into your brain."}, {"timestamp": [1618.0, 1623.12], "text": " Um, and so I would need to see, uh, a lot, a lot of defense and depth"}, {"timestamp": [1623.24, 1625.0], "text": " protocols around BCI"}, {"timestamp": [1625.06, 1627.22], "text": " before I would be comfortable with it."}, {"timestamp": [1627.22, 1630.82], "text": " Yeah, you know, it's crazy because that makes me think of,"}, {"timestamp": [1630.82, 1632.86], "text": " you know, the stuff I've seen lately with,"}, {"timestamp": [1632.86, 1637.02], "text": " they're doing with the machine learning with the MRIs"}, {"timestamp": [1637.02, 1639.9], "text": " and being able to like have you watch a video"}, {"timestamp": [1639.9, 1642.94], "text": " and then literally just by reading your thoughts,"}, {"timestamp": [1642.94, 1644.74], "text": " they're able to replicate the video."}, {"timestamp": [1644.74, 1649.48], "text": " And it's so close to what you actually were observing just by the"}, {"timestamp": [1649.48, 1653.44], "text": " pattern recognition in your, in your brainwaves of your thoughts."}, {"timestamp": [1654.28, 1660.42], "text": " That's kind of interesting because when you're talking about like the separation"}, {"timestamp": [1660.42, 1663.38], "text": " of, you know, your thoughts are protected within your shell."}, {"timestamp": [1664.94, 1665.76], "text": " Yeah."}, {"timestamp": [1665.76, 1666.58], "text": " Yeah."}, {"timestamp": [1666.58, 1667.66], "text": " Might not, might not be for long."}, {"timestamp": [1667.66, 1668.7], "text": " You know, there was, that was a,"}, {"timestamp": [1668.7, 1671.02], "text": " that was actually one of the plot points in Westworld."}, {"timestamp": [1671.02, 1671.86], "text": " I don't know if you ever watched"}, {"timestamp": [1671.86, 1673.58], "text": " the HBO TV show Westworld."}, {"timestamp": [1673.58, 1675.02], "text": " I watched a few of them."}, {"timestamp": [1675.02, 1675.86], "text": " Yeah."}, {"timestamp": [1675.86, 1676.68], "text": " Yeah."}, {"timestamp": [1676.68, 1677.52], "text": " So one of the things,"}, {"timestamp": [1677.52, 1679.66], "text": " and at the time I thought this was so contrived"}, {"timestamp": [1679.66, 1682.08], "text": " and I was like, I was like, this is so dumb."}, {"timestamp": [1682.08, 1683.82], "text": " But what they said, it was,"}, {"timestamp": [1683.82, 1686.72], "text": " it was a relatively small plot point, but that all the hats"}, {"timestamp": [1686.72, 1688.8], "text": " that they had the guests wearing, were scanning their"}, {"timestamp": [1688.8, 1691.56], "text": " brains for the last few decades. And that's what that was how"}, {"timestamp": [1691.56, 1693.76], "text": " they got the good models of human brains. And I'm like,"}, {"timestamp": [1693.76, 1696.4], "text": " that's not possible. It's not that easy. And then here we are"}, {"timestamp": [1696.4, 1699.28], "text": " a couple years later, and it literally is that easy to like"}, {"timestamp": [1699.28, 1703.64], "text": " record brain, almost that easy to record brainwaves. And get,"}, {"timestamp": [1703.64, 1707.8], "text": " you know, the thing the thing about that technology,"}, {"timestamp": [1707.92, 1710.9], "text": " brain imaging and being able to reconstruct images,"}, {"timestamp": [1710.9, 1712.76], "text": " thoughts, and words."}, {"timestamp": [1712.76, 1715.0], "text": " I think there was one where they were able to reconstruct"}, {"timestamp": [1715.0, 1716.8], "text": " an image just from recall,"}, {"timestamp": [1716.8, 1719.56], "text": " asking the subject to think about something,"}, {"timestamp": [1719.56, 1721.92], "text": " not actively see it, but think about it"}, {"timestamp": [1721.92, 1725.16], "text": " and then infer what they were recalling."}, {"timestamp": [1731.16, 1735.08], "text": " So, you know, that, that level of thing of, of, uh, of insight inference, you know, into someone's brain kind of reminds me of actually the, the dawn of"}, {"timestamp": [1735.08, 1735.8], "text": " psychology."}, {"timestamp": [1735.8, 1740.44], "text": " So back in the days of Sigmund Freud and Carl Jung, it was actually offensive to"}, {"timestamp": [1740.44, 1747.0], "text": " the Victorians, to the idea that you could infer what was going on in someone's mind from their behavior,"}, {"timestamp": [1747.0, 1749.64], "text": " from their words, from unconscious ticks,"}, {"timestamp": [1749.64, 1752.92], "text": " because to them, it was stiff upper lip,"}, {"timestamp": [1752.92, 1755.56], "text": " Victorian society, the idea that your thoughts"}, {"timestamp": [1755.56, 1757.84], "text": " were intrinsically and forever private"}, {"timestamp": [1757.84, 1761.14], "text": " and that someone could infer what was going on in your mind,"}, {"timestamp": [1761.14, 1763.44], "text": " the contents of your mind or your experience"}, {"timestamp": [1763.44, 1764.76], "text": " from something that got out"}, {"timestamp": [1764.76, 1765.04], "text": " was like"}, {"timestamp": [1765.04, 1769.76], "text": " really terrifying to them. And so like now we're taking that to the next level with technology."}, {"timestamp": [1770.4, 1774.72], "text": " I'm sure we'll get used to it just as, you know, the Victorians got over themselves eventually"}, {"timestamp": [1774.72, 1779.44], "text": " with psychology and everything else. You know, now we have professional lie detectors and"}, {"timestamp": [1779.44, 1782.4], "text": " behaviorists and that sort of stuff. And it's just, it's commonplace."}, {"timestamp": [1783.04, 1785.0], "text": " Yeah. so interesting."}, {"timestamp": [1785.0, 1787.6], "text": " So you brought up something interesting earlier"}, {"timestamp": [1787.6, 1790.96], "text": " when you were talking about Elon Musk and his texts,"}, {"timestamp": [1790.96, 1793.16], "text": " you know, where do we find meaning?"}, {"timestamp": [1793.16, 1798.16], "text": " So AI advances, everything is automated."}, {"timestamp": [1798.4, 1800.68], "text": " We're in this world now where literally"}, {"timestamp": [1800.68, 1804.04], "text": " like we are living AI adjacent, you know."}, {"timestamp": [1804.04, 1807.84], "text": " Whether it's before the brain interface,"}, {"timestamp": [1807.84, 1814.0], "text": " we were just adjacent through our devices and all of that. I mean, obviously AI, when you talk about"}, {"timestamp": [1814.0, 1818.16], "text": " all this new data and you talk about all these data points and everything else, it's just increasing"}, {"timestamp": [1818.16, 1829.0], "text": " and increasing and becoming very overwhelming. So we're going to need AI to parse it all for us, you know, or like that's, that's going to become very necessary."}, {"timestamp": [1829.0, 1833.0], "text": " So we're in that state, jobs have kind of gone away."}, {"timestamp": [1833.0, 1835.0], "text": " How are people finding meaning?"}, {"timestamp": [1835.0, 1840.0], "text": " Yeah. So I think, I think that society is ready for this because"}, {"timestamp": [1840.0, 1844.0], "text": " the, some of the anxiety is there and putting words to it."}, {"timestamp": [1844.0, 1845.44], "text": " I think I mentioned this earlier,"}, {"timestamp": [1845.44, 1851.92], "text": " or I alluded to it earlier, is that people are afraid of being irrelevant. Right? It's, you know,"}, {"timestamp": [1852.48, 1858.0], "text": " before, like, if just jobs go away, if just jobs go away, then people's biggest fear is,"}, {"timestamp": [1858.0, 1861.52], "text": " how do I take care of myself? How do I take care of the people that I care about?"}, {"timestamp": [1862.08, 1866.24], "text": " But above and beyond that, in a world of superintelligence"}, {"timestamp": [1866.24, 1872.72], "text": " where machines are a million times smarter than all humans combined, and to Elon's tweet,"}, {"timestamp": [1872.72, 1878.64], "text": " what is the point of even existing if you're basically as dumb as an ant compared to the"}, {"timestamp": [1878.64, 1887.36], "text": " machines, the evident truth, I'm not going to say self-evident, but to me what seems to be evident"}, {"timestamp": [1887.36, 1891.52], "text": " is that humans will become irrelevant."}, {"timestamp": [1891.52, 1894.68], "text": " But here's the trick, is we were always irrelevant."}, {"timestamp": [1894.68, 1899.48], "text": " From a cosmic perspective, we only convinced ourselves that we were important and that"}, {"timestamp": [1899.48, 1907.66], "text": " we were relevant and that we were super special. And that preoccupation with our own specialness"}, {"timestamp": [1909.42, 1911.06], "text": " and uniqueness, which is embedded,"}, {"timestamp": [1911.06, 1912.52], "text": " not just in religious narratives,"}, {"timestamp": [1912.52, 1915.98], "text": " it's embedded in all kinds of narratives around the world,"}, {"timestamp": [1915.98, 1918.0], "text": " really has caused a lot of problems."}, {"timestamp": [1918.0, 1922.8], "text": " I just started reading Sapiens by Noah Yuval Harari."}, {"timestamp": [1924.18, 1925.1], "text": " I think I said his name right."}, {"timestamp": [1925.1, 1926.7], "text": " I have terrible name sometimes."}, {"timestamp": [1926.7, 1931.9], "text": " Anyways, and he talks about how the speed of our evolution is"}, {"timestamp": [1931.9, 1936.2], "text": " actually really problematic because a hundred thousand years"}, {"timestamp": [1936.2, 1940.6], "text": " ago or two hundred thousand years ago, we were in the middle"}, {"timestamp": [1940.6, 1941.4], "text": " of the food chain."}, {"timestamp": [1941.4, 1944.9], "text": " And so we, you know, we ate things that are smaller than us,"}, {"timestamp": [1944.9, 1949.44], "text": " but then around a hundred thousand years ago, we got really good with weapons and we started"}, {"timestamp": [1949.44, 1953.44], "text": " working our way up the food chain, but we're still really insecure. We're like anxious chimps"}, {"timestamp": [1954.32, 1957.92], "text": " that happened to be at the top of the food chain. And so now we're like, what are we doing here?"}, {"timestamp": [1957.92, 1962.24], "text": " And so we're just scared all the time, which is why we have anxiety and depression. And so"}, {"timestamp": [1963.2, 1970.64], "text": " he didn't say it quite like that, but you know, that's kind of the inference that I'm drawing. And so, you know, we, we had this rapid cognitive"}, {"timestamp": [1970.64, 1976.96], "text": " explosion of not just tool use, but society and, and ultimately to science and everything else."}, {"timestamp": [1976.96, 1980.96], "text": " And so we have the ability to ask these gigantic existential questions. Why are we here? What does"}, {"timestamp": [1980.96, 1986.76], "text": " it all mean? And then basically fabricate answers, right? We can come up with narratives and stories and myths"}, {"timestamp": [1986.76, 1989.28], "text": " and religions and whatever else."}, {"timestamp": [1989.28, 1992.18], "text": " And, you know, so I'm not gonna say"}, {"timestamp": [1992.18, 1994.34], "text": " that we're not cognitively equipped to answer that."}, {"timestamp": [1994.34, 1996.24], "text": " I'm just, what I'm saying is that we have"}, {"timestamp": [1996.24, 2000.04], "text": " a tremendous amount of intellectual and cultural baggage"}, {"timestamp": [2000.04, 2002.76], "text": " that we've been asking about meaning for such a long time."}, {"timestamp": [2002.76, 2005.16], "text": " And what we're coming back to, actually,"}, {"timestamp": [2005.16, 2008.52], "text": " is this kind of this leap back 200,000 years"}, {"timestamp": [2008.52, 2011.12], "text": " to where we're just going to be happy to have food"}, {"timestamp": [2011.12, 2014.68], "text": " and company and friends and a trip down to the lake."}, {"timestamp": [2014.68, 2016.56], "text": " Like, that's how we're supposed to live,"}, {"timestamp": [2016.56, 2019.76], "text": " because that's how we lived for literally millions of years"}, {"timestamp": [2019.76, 2023.2], "text": " until we accidentally tamed fire and tools."}, {"timestamp": [2023.2, 2025.88], "text": " And it was just, you know, Douglas Adams said like,"}, {"timestamp": [2025.88, 2027.36], "text": " we should just go back to the trees"}, {"timestamp": [2027.36, 2030.28], "text": " and go back to the oceans, you know,"}, {"timestamp": [2030.28, 2032.48], "text": " and living simply, I think,"}, {"timestamp": [2032.48, 2035.36], "text": " is the answer that a lot of people are gonna find."}, {"timestamp": [2035.36, 2037.48], "text": " Now, that being said,"}, {"timestamp": [2037.48, 2039.12], "text": " living simply isn't for everyone, right?"}, {"timestamp": [2039.12, 2040.84], "text": " Not everyone wants to live, you know,"}, {"timestamp": [2040.84, 2043.44], "text": " a cottage core existence or slow living or whatever."}, {"timestamp": [2044.88, 2049.32], "text": " But there's, so one concept that I've been working on is, is the importance"}, {"timestamp": [2049.32, 2050.76], "text": " of mission."}, {"timestamp": [2050.76, 2055.8], "text": " And so having a mission in your life is that's, that's where I'm at."}, {"timestamp": [2055.8, 2062.08], "text": " So like, I have a very clear, clearly articulated mission that is not something that AI can,"}, {"timestamp": [2062.08, 2063.08], "text": " can dislocate yet."}, {"timestamp": [2063.08, 2065.8], "text": " And if it does great great, mission accomplished."}, {"timestamp": [2065.8, 2069.64], "text": " But there are any number of missions that you can have"}, {"timestamp": [2069.64, 2071.48], "text": " that AI will never be able to take from you."}, {"timestamp": [2071.48, 2073.52], "text": " So like one example that's very personal"}, {"timestamp": [2073.52, 2075.88], "text": " is a girl that I used to date,"}, {"timestamp": [2075.88, 2077.88], "text": " her mission was to become a mom."}, {"timestamp": [2077.88, 2079.44], "text": " That was what she wanted most in life."}, {"timestamp": [2079.44, 2081.4], "text": " And so she became a mom."}, {"timestamp": [2081.4, 2084.04], "text": " And like, that was her jam."}, {"timestamp": [2084.04, 2087.64], "text": " There's another person, another girlfriend that I used to date, actually, I've dated"}, {"timestamp": [2087.64, 2094.56], "text": " a lot of people, where her mission was to write a beautiful coming of age story."}, {"timestamp": [2094.56, 2098.72], "text": " It's a great mission, but it was her mission, right?"}, {"timestamp": [2098.72, 2101.84], "text": " Even if AI can write it for her, that's not the point."}, {"timestamp": [2101.84, 2106.46], "text": " And so, you know, back to basics, living sure."}, {"timestamp": [2106.7, 2110.26], "text": " But I think in terms of meaning, I think that a lot of people are going to find"}, {"timestamp": [2110.58, 2115.82], "text": " that they're going to be empowered by AI to pursue those missions, whatever they"}, {"timestamp": [2115.82, 2121.3], "text": " happen to be, whether it's personal achievement, right? Yeah. Yep. So like,"}, {"timestamp": [2121.5, 2124.64], "text": " you know, some people like, I want to climb Everest or I want to run an Iron"}, {"timestamp": [2124.64, 2128.98], "text": " man or whatever, like those are all completely valid missions that can give people a tremendous"}, {"timestamp": [2128.98, 2134.36], "text": " amount of meaning in their life, even without any kind of cosmic meaning. So that's kind"}, {"timestamp": [2134.36, 2140.12], "text": " of where I'm at in terms of understanding how society might change in light of AI and"}, {"timestamp": [2140.12, 2141.12], "text": " everything."}, {"timestamp": [2141.12, 2145.96], "text": " Yeah. And I feel the same, you know, purpose and making a difference."}, {"timestamp": [2145.96, 2150.12], "text": " And, you know, I think we've been so geared to like scratching and clawing"}, {"timestamp": [2150.12, 2155.68], "text": " for survival for so long that, you know, it's hard for us to think of other ways."}, {"timestamp": [2155.68, 2160.88], "text": " But I think once that's removed, you know, and we're out of this kind of like stress"}, {"timestamp": [2160.88, 2165.44], "text": " environment, making a difference is going to look different."}, {"timestamp": [2165.48, 2168.96], "text": " And I, you know, I, I know this sounds really utopian, you"}, {"timestamp": [2168.96, 2173.4], "text": " know, when I talk like this, but, but to me, I see when we're not in the"}, {"timestamp": [2173.4, 2179.0], "text": " scratching and clawing for survival kind of, you know, scenario, I feel"}, {"timestamp": [2179.0, 2182.4], "text": " like people will look a little more outward to their communities and how"}, {"timestamp": [2182.4, 2185.4], "text": " they can help each other and make a difference with each other."}, {"timestamp": [2185.4, 2188.4], "text": " And I feel like it's gonna bring these qualities"}, {"timestamp": [2188.4, 2190.68], "text": " that kind of, you know, increase empathy"}, {"timestamp": [2190.68, 2191.86], "text": " and different things like that,"}, {"timestamp": [2191.86, 2193.24], "text": " because we're not gonna be so much"}, {"timestamp": [2193.24, 2197.6], "text": " in this competitive mindset, more of like a-"}, {"timestamp": [2197.6, 2200.4], "text": " Yeah, it's switching to an abundance mindset, right?"}, {"timestamp": [2200.4, 2201.44], "text": " Yeah, yeah."}, {"timestamp": [2201.44, 2204.42], "text": " So it's going from scarcity to abundance."}, {"timestamp": [2204.42, 2205.76], "text": " And, you know, this is actually"}, {"timestamp": [2205.76, 2210.4], "text": " the first time I learned about this was actually in the non-monogamy community. Uh, so the polyamory"}, {"timestamp": [2210.4, 2215.52], "text": " community is one of the things that they teach you when you, when you're learning about non-monogamy"}, {"timestamp": [2215.52, 2220.08], "text": " is going from a scarcity mindset of love to an abundant mindset of love and realizing that there"}, {"timestamp": [2220.08, 2228.42], "text": " is actually more affection to go around. And that, and that around and that the only barrier is thinking"}, {"timestamp": [2228.42, 2230.84], "text": " that you can only love one person at a time"}, {"timestamp": [2230.84, 2234.52], "text": " or that you can only enjoy one person's company at a time."}, {"timestamp": [2234.52, 2238.74], "text": " And it's a completely artificial type of scarcity."}, {"timestamp": [2238.74, 2241.78], "text": " And I think that that represents a good model"}, {"timestamp": [2241.78, 2244.92], "text": " of going from a scarcity mindset to an abundant mindset."}, {"timestamp": [2244.92, 2245.76], "text": " And people are like,"}, {"timestamp": [2245.76, 2247.76], "text": " oh, well, there's no resources that are hyper abundant."}, {"timestamp": [2247.76, 2249.36], "text": " It's like, well, air is, right?"}, {"timestamp": [2249.36, 2251.6], "text": " You don't have to pay for air or sunlight."}, {"timestamp": [2251.6, 2255.36], "text": " And so we have plenty or ground like dirt,"}, {"timestamp": [2255.36, 2258.12], "text": " literally the term dirt cheap, right?"}, {"timestamp": [2258.12, 2259.04], "text": " Because there's so much of it,"}, {"timestamp": [2259.04, 2261.0], "text": " you don't have to pay for it, right?"}, {"timestamp": [2261.0, 2266.2], "text": " And so we do have models of abundant goods and services, nearly hyper abundant goods"}, {"timestamp": [2266.2, 2267.2], "text": " and services."}, {"timestamp": [2267.2, 2270.4], "text": " But yeah, certainly pivoting to that mentality."}, {"timestamp": [2270.4, 2273.12], "text": " And you know, I've been, I've been working on practicing what I preach."}, {"timestamp": [2273.12, 2277.8], "text": " I was a heck of a workaholic."}, {"timestamp": [2277.8, 2282.04], "text": " You know, like since, since retiring from my corporate job, I've actually worked more"}, {"timestamp": [2282.04, 2285.8], "text": " even though I didn't need to, because I was just so fixated on my mission."}, {"timestamp": [2285.8, 2287.08], "text": " And over the last couple of months,"}, {"timestamp": [2287.08, 2289.52], "text": " I've been working on slowing down and actually,"}, {"timestamp": [2289.52, 2291.68], "text": " like I said, practicing what I preach"}, {"timestamp": [2291.68, 2293.8], "text": " and moving more towards that, like,"}, {"timestamp": [2293.8, 2295.56], "text": " well, what, like, why are we here?"}, {"timestamp": [2295.56, 2299.6], "text": " Right, like, what is, like, what is, what is my goal now?"}, {"timestamp": [2299.6, 2302.92], "text": " And so I've had just a few breakthroughs, you know,"}, {"timestamp": [2302.92, 2306.74], "text": " one breakthrough a few weeks ago was"}, {"timestamp": [2306.74, 2310.56], "text": " just that, that realization that like, we never needed meaning in the first place."}, {"timestamp": [2310.56, 2312.2], "text": " That meaning was always unnecessary."}, {"timestamp": [2312.2, 2316.34], "text": " And it was just like, I would like literally lay awake with that, just like on repeat in"}, {"timestamp": [2316.34, 2321.22], "text": " my brain, because it's, it felt so profound that I was just like, it was stuck in my head"}, {"timestamp": [2321.22, 2323.24], "text": " until like I started living it."}, {"timestamp": [2323.24, 2325.36], "text": " And then in the, just in the last few days,"}, {"timestamp": [2325.92, 2330.24], "text": " um, the, the, the, the other biggest thing, cause I was like on the edge of burnout. I'm like, why"}, {"timestamp": [2330.24, 2335.68], "text": " am I still on in, in burnout? Because like, I have total control over my schedule. And then I realized"}, {"timestamp": [2335.68, 2344.08], "text": " that it was choice. It was my choice to be where I am. And so I was like, Oh, like I had been stuck"}, {"timestamp": [2344.08, 2345.0], "text": " in, in a stuck in a mindset"}, {"timestamp": [2345.52, 2348.16], "text": " of not having agency for so long, you know,"}, {"timestamp": [2348.16, 2351.22], "text": " working for companies or working for other people"}, {"timestamp": [2351.22, 2354.4], "text": " or being told what to do, or just being in situations"}, {"timestamp": [2354.4, 2356.14], "text": " where I didn't feel like I had control,"}, {"timestamp": [2356.14, 2357.92], "text": " that I was still in this like,"}, {"timestamp": [2357.92, 2360.84], "text": " this state of learned helplessness."}, {"timestamp": [2360.84, 2364.0], "text": " And then I realized like, wait, that's not true."}, {"timestamp": [2364.0, 2365.28], "text": " Like I'm actually in control of everything in my life right now. And that I realized like, wait, that's not true. Like I'm actually in control"}, {"timestamp": [2365.28, 2369.38], "text": " of everything in my life right now. And that was another like profound realization. And"}, {"timestamp": [2369.38, 2373.14], "text": " ever since then it's, I've been like just in so much better shape. And so I think that"}, {"timestamp": [2373.14, 2374.84], "text": " a lot of people, Oh, go ahead."}, {"timestamp": [2374.84, 2379.84], "text": " I was just going to say that that reminds me of that meme that I saw that said a Rolex"}, {"timestamp": [2379.84, 2386.72], "text": " isn't a flex. If it tells you when your lunch break is over. That's a good one. That's a good one. Yeah."}, {"timestamp": [2387.6, 2393.76], "text": " Yep. You know, if, if, if, if all it is, is just a trinket that you've collected on the rat race,"}, {"timestamp": [2393.76, 2398.8], "text": " is it really that worth it? You know, and, um, there's a lot of YouTubers and a lot of, uh,"}, {"timestamp": [2398.8, 2403.44], "text": " influencers and, and, and social commentators out there that are doing these experiments."}, {"timestamp": [2403.44, 2407.24], "text": " Um, one of my favorite is, of my favorite is this YouTuber called Sorrel"}, {"timestamp": [2407.24, 2409.7], "text": " and she started this free human project."}, {"timestamp": [2409.7, 2411.5], "text": " And so the free human project is like,"}, {"timestamp": [2411.5, 2413.6], "text": " she goes around and interviews people,"}, {"timestamp": [2413.6, 2415.92], "text": " like usually other creators, not always."}, {"timestamp": [2417.04, 2419.12], "text": " And she talks to them like,"}, {"timestamp": [2419.12, 2420.0], "text": " are you a free human?"}, {"timestamp": [2420.0, 2421.08], "text": " What does that mean?"}, {"timestamp": [2421.08, 2422.72], "text": " Like, how did you get out of the rat race?"}, {"timestamp": [2422.72, 2426.24], "text": " And a lot of these people actually work harder"}, {"timestamp": [2426.24, 2429.72], "text": " than like, you know, anyone at a desk job,"}, {"timestamp": [2429.72, 2433.08], "text": " but they work harder because it's fulfilling to them"}, {"timestamp": [2433.08, 2435.6], "text": " and it speaks to their autonomy."}, {"timestamp": [2435.6, 2439.4], "text": " And I think that, I think that what,"}, {"timestamp": [2439.4, 2441.08], "text": " I think that a lot of people are actually afraid"}, {"timestamp": [2441.08, 2442.88], "text": " of that autonomy, because it's like, well,"}, {"timestamp": [2442.88, 2444.56], "text": " what am I going to do with all that free time?"}, {"timestamp": [2444.56, 2446.0], "text": " And it does take a while. Like it, it's taken me about six months to figure Cause it's like, well, what am I going to do with all that free time? And it does take a while."}, {"timestamp": [2446.0, 2448.28], "text": " Like it's taken me about six months to figure out like,"}, {"timestamp": [2448.28, 2449.6], "text": " okay, well now what?"}, {"timestamp": [2450.64, 2451.56], "text": " Like, how do I live?"}, {"timestamp": [2451.56, 2453.36], "text": " And that kind of thing."}, {"timestamp": [2453.36, 2456.16], "text": " Don't you think too, that a lot of that has to do"}, {"timestamp": [2456.16, 2460.04], "text": " with like intrinsic motivation for people too?"}, {"timestamp": [2460.04, 2463.32], "text": " Like they'd rather be told they have to do this, this"}, {"timestamp": [2463.32, 2465.04], "text": " and this in this order, kind of a thing,"}, {"timestamp": [2465.04, 2471.12], "text": " because when they're left to their own devices, they don't have the motivation for it. And maybe"}, {"timestamp": [2471.12, 2475.2], "text": " it comes down to what you were just referring to that, you know, it has to be something you're"}, {"timestamp": [2475.2, 2480.4], "text": " passionate about and that you love, you know, the best work, work that you love is the best"}, {"timestamp": [2480.4, 2485.64], "text": " sort of play, you know. You know, cause I've thought about that,"}, {"timestamp": [2485.64, 2487.04], "text": " that idea that it's like, okay,"}, {"timestamp": [2487.04, 2488.84], "text": " well people just need structure."}, {"timestamp": [2488.84, 2489.86], "text": " They need to be told what to do."}, {"timestamp": [2489.86, 2493.04], "text": " And I certainly think that some people thrive"}, {"timestamp": [2493.04, 2494.08], "text": " with more structure, right?"}, {"timestamp": [2494.08, 2497.36], "text": " There's everyone knows someone or has friends of friends"}, {"timestamp": [2497.36, 2500.24], "text": " that were like just complete nut cases during high school."}, {"timestamp": [2500.24, 2501.4], "text": " And then they went and joined the military"}, {"timestamp": [2501.4, 2502.84], "text": " and they got their life together, right?"}, {"timestamp": [2502.84, 2505.6], "text": " Some people just really need structure and discipline"}, {"timestamp": [2505.6, 2508.52], "text": " and something outside of themselves, right?"}, {"timestamp": [2508.52, 2511.08], "text": " And then those of us that are crazy artists,"}, {"timestamp": [2511.08, 2515.2], "text": " like we completely chafe under structure, right?"}, {"timestamp": [2515.2, 2517.08], "text": " Structure is like toxic for us."}, {"timestamp": [2517.08, 2519.52], "text": " So, you know, I guess the first part of the answer"}, {"timestamp": [2519.52, 2521.44], "text": " is that it varies from person to person."}, {"timestamp": [2521.44, 2523.08], "text": " But the second part of that answer"}, {"timestamp": [2523.08, 2532.96], "text": " is that I think that a lot of it is just learned. I think that it's habit and I think that it's inertia because the idea of having a nine to"}, {"timestamp": [2532.96, 2540.48], "text": " five, right? It provides a little bit of comfort in that surety and just that daily routine,"}, {"timestamp": [2540.48, 2546.16], "text": " the schedule, the regularity, it's predictable, it feels comfortable and safe."}, {"timestamp": [2550.4, 2551.12], "text": " But at the same time, like, you know, I, there's a, I've got a few friends in the area"}, {"timestamp": [2557.92, 2562.64], "text": " that are still technically in the nine to five, but like, it's more like 1030 ish to 230 ish. And then a couple of meetings here and there, you know, and one of my best friends, like we were"}, {"timestamp": [2562.64, 2565.12], "text": " working on his sailboat um you know in the"}, {"timestamp": [2565.12, 2569.44], "text": " middle of the work day and it was just he had done everything he needed to do and so then it's just"}, {"timestamp": [2569.44, 2574.0], "text": " kind of letting go of those expectations and those those pressures but I think that I think"}, {"timestamp": [2574.0, 2582.0], "text": " that probably people if maybe not if but when more people kind of uh break free or step back like say"}, {"timestamp": [2582.0, 2588.0], "text": " for instance we get to a four-day work week or a 20 hour work week, which there's lots of experiments going on. And I think that's coming"}, {"timestamp": [2588.0, 2592.88], "text": " soon. Um, at least for some companies, I think people are going to realize that there's actually"}, {"timestamp": [2592.88, 2598.16], "text": " like, there's a lot, it's actually a lot easier to figure out what to do with your time. So I"}, {"timestamp": [2598.16, 2603.6], "text": " actually have this trick that I do where if I'm stuck in work mode, I'll just turn off all"}, {"timestamp": [2603.6, 2605.44], "text": " electronics and lay on the couch until I"}, {"timestamp": [2605.44, 2610.24], "text": " get bored. And it usually only takes five minutes, right? But we're so used to constant simulation."}, {"timestamp": [2610.24, 2614.64], "text": " So I just like turn everything off, you know, five minutes and either I'll be like, oh, this feels"}, {"timestamp": [2614.64, 2619.6], "text": " really good and take a nap. Or I'll like get inspiration, like, oh, I should go to the lake"}, {"timestamp": [2619.6, 2624.4], "text": " or I should call my friend or whatever. And just slowing down just long enough to start to feel"}, {"timestamp": [2624.4, 2628.64], "text": " bored is a really good way to just give yourself structure because sometimes it's"}, {"timestamp": [2628.64, 2632.44], "text": " as simple as like, you know, heck, just before we got on this call, I was like, I'm going"}, {"timestamp": [2632.44, 2635.92], "text": " to put away my laundry. Right. I could wait till after, but like, I'm going to do that"}, {"timestamp": [2635.92, 2639.6], "text": " right now. And it just, it feels good, you know, tidy house, tidy mind. So I think that"}, {"timestamp": [2639.6, 2644.16], "text": " people will learn a lot of tricks, you know, as they kind of take, take ownership of their"}, {"timestamp": [2644.16, 2645.54], "text": " life and their time again."}, {"timestamp": [2645.54, 2646.54], "text": " Yeah."}, {"timestamp": [2646.54, 2652.2], "text": " You know, it, it, it's interesting that you say that too, that you have, have to be mindful"}, {"timestamp": [2652.2, 2656.34], "text": " about slowing down and giving your brain that open space."}, {"timestamp": [2656.34, 2660.02], "text": " And I think that that's harder for people these days because we're, we always have our"}, {"timestamp": [2660.02, 2666.32], "text": " devices and it's always throwing random data at us, you know, like random distraction. And to me,"}, {"timestamp": [2666.32, 2671.12], "text": " that can feel really overwhelming. And I was actually just having a conversation with some"}, {"timestamp": [2671.12, 2676.32], "text": " people recently where, you know, it's like, I can't even open my phone without just being,"}, {"timestamp": [2676.32, 2680.48], "text": " it's almost like I'm running a marathon and there's just some things that keep jumping up in"}, {"timestamp": [2680.48, 2685.6], "text": " the ground and just diverting my path, you know, and it's really hard to stay on."}, {"timestamp": [2685.6, 2687.28], "text": " Like, you know, I'll open up my phone"}, {"timestamp": [2687.28, 2689.68], "text": " just to see the weather and a notification,"}, {"timestamp": [2689.68, 2692.6], "text": " you know, pops up and takes me down a whole rabbit hole."}, {"timestamp": [2692.6, 2695.36], "text": " And I'm like, 20 minutes later, I just wanted the weather,"}, {"timestamp": [2695.36, 2696.2], "text": " you know?"}, {"timestamp": [2696.2, 2699.64], "text": " So, you know, and I think these algorithms"}, {"timestamp": [2699.64, 2701.68], "text": " are getting way more aggressive"}, {"timestamp": [2701.68, 2704.04], "text": " in trying to capture our attention"}, {"timestamp": [2704.04, 2707.56], "text": " and stuff to where I've been feeling it lately."}, {"timestamp": [2707.56, 2709.64], "text": " I have been feeling like overwhelmed"}, {"timestamp": [2709.64, 2712.8], "text": " with just random information being thrown at me"}, {"timestamp": [2712.8, 2716.72], "text": " that I didn't ask for, didn't want, but somehow, you know,"}, {"timestamp": [2717.6, 2719.84], "text": " it's like this constant thing you have to be,"}, {"timestamp": [2721.16, 2723.36], "text": " it's almost a battle kind of a thing."}, {"timestamp": [2723.36, 2725.48], "text": " Oh, it very much is."}, {"timestamp": [2725.48, 2731.0], "text": " And the reason is because phones and algorithms and apps can be updated faster than our evolutionary"}, {"timestamp": [2731.0, 2732.62], "text": " algorithms can."}, {"timestamp": [2732.62, 2736.88], "text": " And so, you know, game, like gamification for your attention, attention engineering,"}, {"timestamp": [2736.88, 2737.96], "text": " it's all called attention economy."}, {"timestamp": [2737.96, 2738.96], "text": " Right."}, {"timestamp": [2738.96, 2742.24], "text": " And so this is, this is one of the reasons earlier in the conversation that I mentioned"}, {"timestamp": [2742.24, 2751.12], "text": " that privacy and opting out actually have to be like front and center is because you can game someone's biology and neurology more than any amount of"}, {"timestamp": [2751.12, 2758.4], "text": " discipline can allow you to check out. So like you need the physical friction, and I don't mean"}, {"timestamp": [2758.4, 2764.16], "text": " friction like rubbing your hands together, I mean like friction in terms of layers of making your"}, {"timestamp": [2764.16, 2768.6], "text": " phone usable. Like I honestly make my phone as useless as possible."}, {"timestamp": [2768.6, 2770.76], "text": " I have like almost no apps on it."}, {"timestamp": [2770.76, 2773.64], "text": " It's in silent, do not disturb."}, {"timestamp": [2773.64, 2776.4], "text": " And then I have like the digital wellness, like turned up to Mac."}, {"timestamp": [2776.4, 2780.3], "text": " So like it will turn off Chrome after 30 minutes of use every day."}, {"timestamp": [2780.3, 2786.32], "text": " And so like by doing literally everything I can to put a barrier from my phone being useful,"}, {"timestamp": [2786.32, 2789.76], "text": " your brain will eventually turn down and say, like, instead of reaching for your phone,"}, {"timestamp": [2789.76, 2794.72], "text": " I still instinctively reach for it. But it's like, it kind of turns down the desire, that"}, {"timestamp": [2794.72, 2799.6], "text": " reward mechanism for it. And then, like I said earlier, I leave it off as much as I can."}, {"timestamp": [2800.4, 2809.8], "text": " Because like, you know, if someone needs to get ahold of me, like they can get ahold of me in some other way or like, really, what's the worst thing that's going to happen?"}, {"timestamp": [2809.8, 2814.64], "text": " You know, because like, I've got my wife and my dog here and that's who I care about most."}, {"timestamp": [2814.64, 2817.88], "text": " And so then, you know, I check my phone every now and then."}, {"timestamp": [2817.88, 2824.08], "text": " But yeah, like, and so this goes back to Spatial Web, because the information about you can"}, {"timestamp": [2824.08, 2825.28], "text": " be used against you."}, {"timestamp": [2825.28, 2829.76], "text": " And so this is the, you know, going all the way back to the beginning of the conversation."}, {"timestamp": [2829.76, 2837.28], "text": " Any technology is dual use. And so that's where like, okay, if you have, you know, 500 sensors"}, {"timestamp": [2837.28, 2841.28], "text": " throughout your home, because most of us have at least 20 or 30 today. Like if you actually go and"}, {"timestamp": [2841.28, 2845.38], "text": " count them all up, like we've got like 20 or 30 microphones, cameras,"}, {"timestamp": [2845.38, 2847.14], "text": " various other kinds of sensors"}, {"timestamp": [2847.14, 2848.7], "text": " that are all network connected today,"}, {"timestamp": [2848.7, 2851.56], "text": " that's gonna be 10X in a few years."}, {"timestamp": [2851.56, 2855.88], "text": " If that information is not correctly protected and handled,"}, {"timestamp": [2855.88, 2857.76], "text": " then it can be used against you."}, {"timestamp": [2857.76, 2861.14], "text": " And most people just will not have the digital literacy"}, {"timestamp": [2861.14, 2862.46], "text": " to know better, right?"}, {"timestamp": [2862.46, 2865.04], "text": " And so that's why it has to be like opt out by default,"}, {"timestamp": [2865.04, 2868.14], "text": " or you have to have a good way of opting in or out"}, {"timestamp": [2868.14, 2870.72], "text": " at any time in order to protect."}, {"timestamp": [2870.72, 2872.64], "text": " And this is, it's not just about ethics,"}, {"timestamp": [2872.64, 2876.2], "text": " just it's about sanity and about how we want to live, right?"}, {"timestamp": [2876.2, 2877.04], "text": " Yeah, yeah."}, {"timestamp": [2877.04, 2877.86], "text": " So."}, {"timestamp": [2877.86, 2880.26], "text": " No, you know, it's funny that you say that about your phone"}, {"timestamp": [2880.26, 2882.68], "text": " because I've always been like that, you know,"}, {"timestamp": [2882.68, 2884.24], "text": " ever since I got a smartphone,"}, {"timestamp": [2884.24, 2887.0], "text": " it's like I make all the notifications turned off. Like I don't want like that, you know, ever since I got a smartphone, it's like I make all the notifications turned off."}, {"timestamp": [2887.0, 2895.0], "text": " Like I don't want interruptions, you know, like I don't, you know, even my email, it's like, I'll check it, you know, like I'm a responsible adult."}, {"timestamp": [2895.0, 2903.0], "text": " I will check it every 10, 15 minutes during a work period or whatever, where I might be getting important information served up to me,"}, {"timestamp": [2903.0, 2906.64], "text": " but I'm not going to have it be an intrusion and an"}, {"timestamp": [2906.64, 2910.32], "text": " interruption in my life and I actually had I had an ex"}, {"timestamp": [2910.32, 2914.8], "text": " boyfriend several years back and he would be like, you know,"}, {"timestamp": [2914.8, 2917.68], "text": " why are you? Why are you notifications? You know he was"}, {"timestamp": [2917.68, 2919.6], "text": " making it a thing like I was trying to hide something. I'm"}, {"timestamp": [2919.6, 2924.4], "text": " all dude. I'm just protecting my mental health. Yep. There's"}, {"timestamp": [2924.4, 2925.9], "text": " that defense in depth, right?"}, {"timestamp": [2925.9, 2929.88], "text": " If you don't see it, it's less, less present."}, {"timestamp": [2929.88, 2933.04], "text": " You know, one thing that I really hope, and there's actually a few startups, and I think"}, {"timestamp": [2933.04, 2938.48], "text": " Apple, I think they just announced like they're going to like overhaul Siri or whatever."}, {"timestamp": [2938.48, 2945.0], "text": " And but, you know, having, having spatial web and, and kind of networked AI,"}, {"timestamp": [2946.06, 2947.44], "text": " I'm actually really looking forward to,"}, {"timestamp": [2947.44, 2949.78], "text": " I don't think we're gonna be able to fully get away from it,"}, {"timestamp": [2949.78, 2953.34], "text": " but like getting rid of like screens for the most part,"}, {"timestamp": [2953.34, 2956.26], "text": " where it's like, if you have any computing need,"}, {"timestamp": [2956.26, 2957.94], "text": " it's gonna be voice, like in Star Trek,"}, {"timestamp": [2957.94, 2959.62], "text": " you say, computer, like, you know,"}, {"timestamp": [2959.62, 2960.86], "text": " where's the closest sushi?"}, {"timestamp": [2960.86, 2962.3], "text": " And you don't even have to look at your phone"}, {"timestamp": [2962.3, 2968.32], "text": " and it just, it knows like, hey, you know, when you went to this sushi place, you liked it and you didn't like this other one, let's"}, {"timestamp": [2968.32, 2973.28], "text": " recommend, you know, just all these kinds of automatic things that, that can, that can reduce"}, {"timestamp": [2973.28, 2979.44], "text": " the friction to living your life rather than the screens trying to capture, you know, cause eyes"}, {"timestamp": [2979.44, 2984.4], "text": " on screen is how social media companies make money right now, right? Like the YouTube algorithm,"}, {"timestamp": [2984.4, 2985.36], "text": " the Facebook algorithm, the Facebook algorithm,"}, {"timestamp": [2985.36, 2988.0], "text": " the Twitter algorithm, granted it's now meta and it's now X."}, {"timestamp": [2988.0, 2990.16], "text": " Anyways, that's a sign of getting older."}, {"timestamp": [2990.16, 2993.68], "text": " We remembered the original, back in my day on MySpace."}, {"timestamp": [2993.68, 2994.18], "text": " Yeah."}, {"timestamp": [2995.68, 3000.24], "text": " I hold a Twitter space every Tuesday and I was calling it"}, {"timestamp": [3000.24, 3002.48], "text": " Twitter Tuesdays and well now it's just Tuesdays."}, {"timestamp": [3004.4, 3004.9], "text": " Yeah."}, {"timestamp": [3009.58, 3013.96], "text": " So we're going to live through these permutations. And having those boundaries, because again, what is it that the companies want?"}, {"timestamp": [3013.96, 3014.96], "text": " They want profit."}, {"timestamp": [3014.96, 3015.96], "text": " Okay, great."}, {"timestamp": [3015.96, 3016.96], "text": " Right?"}, {"timestamp": [3016.96, 3019.2], "text": " I don't bemoan them that because how do they get profit?"}, {"timestamp": [3019.2, 3021.6], "text": " They provide goods and services that we need for cheaper."}, {"timestamp": [3021.6, 3022.64], "text": " Okay, great."}, {"timestamp": [3022.64, 3027.2], "text": " But how can we renegotiate that social contract where they get what they want"}, {"timestamp": [3027.2, 3028.88], "text": " without harming us and we get what"}, {"timestamp": [3028.88, 3032.16], "text": " we need without having to sacrifice too much privacy"}, {"timestamp": [3032.16, 3034.68], "text": " or time or mental health or whatever?"}, {"timestamp": [3034.68, 3037.52], "text": " And this is what I think we're moving towards."}, {"timestamp": [3037.52, 3040.04], "text": " And this is so part of my mission,"}, {"timestamp": [3040.04, 3043.84], "text": " the next part of my mission, is to start the conversation"}, {"timestamp": [3043.84, 3046.76], "text": " around negotiating this new social contract,"}, {"timestamp": [3046.76, 3054.28], "text": " this relationship between us, businesses, government and whatever other entities, pillars of society are out there."}, {"timestamp": [3054.28, 3057.88], "text": " Probably media will be part of that conversation as well."}, {"timestamp": [3057.88, 3059.84], "text": " Banks. Go ahead."}, {"timestamp": [3059.84, 3065.0], "text": " Do you have any predictions for like how you see that unfolding,"}, {"timestamp": [3065.04, 3067.82], "text": " like in the near versus the far future?"}, {"timestamp": [3067.82, 3069.7], "text": " Yeah, so the biggest problem,"}, {"timestamp": [3069.7, 3070.96], "text": " and I alluded to this at the beginning,"}, {"timestamp": [3070.96, 3073.82], "text": " but the biggest problem, the first problem is,"}, {"timestamp": [3073.82, 3078.0], "text": " we the people are about to lose a tremendous amount of power"}, {"timestamp": [3078.0, 3079.74], "text": " and that is gonna be through labor."}, {"timestamp": [3080.84, 3082.8], "text": " 100 years ago, collective bargaining"}, {"timestamp": [3082.8, 3084.72], "text": " allowed the creation of unions"}, {"timestamp": [3084.72, 3088.58], "text": " and man, like the rail and coal tycoons,"}, {"timestamp": [3088.58, 3090.42], "text": " they hated their employees."}, {"timestamp": [3090.42, 3093.18], "text": " If you go back and read what they said,"}, {"timestamp": [3093.18, 3095.34], "text": " they literally wanted to use the army and"}, {"timestamp": [3095.34, 3098.48], "text": " the National Guard to force people back into the mines."}, {"timestamp": [3098.48, 3102.22], "text": " They wanted to use violence to force labor."}, {"timestamp": [3102.22, 3104.1], "text": " That is how much disdain"}, {"timestamp": [3104.1, 3106.74], "text": " the barons had for the working man"}, {"timestamp": [3106.74, 3111.98], "text": " until Teddy Roosevelt, the ludicrous firebrand of a man, came and said, no, we're going to"}, {"timestamp": [3111.98, 3113.98], "text": " do things differently."}, {"timestamp": [3113.98, 3119.86], "text": " And obviously, corporations are not nearly that bad today, but if left unchecked, we"}, {"timestamp": [3119.86, 3126.64], "text": " could backslide in that direction. And when human labor is replaced by machine labor,"}, {"timestamp": [3126.64, 3129.16], "text": " we lose a huge bargaining chip."}, {"timestamp": [3129.16, 3130.92], "text": " And so that's the biggest thing,"}, {"timestamp": [3130.92, 3133.28], "text": " the biggest concern in my mind."}, {"timestamp": [3133.28, 3134.72], "text": " We still have the vote, right?"}, {"timestamp": [3134.72, 3138.0], "text": " We still have voting power, but if we lose labor,"}, {"timestamp": [3138.0, 3140.2], "text": " then we also lose a lot of financial power."}, {"timestamp": [3140.2, 3142.4], "text": " Because right now we can vote with our dollars, right?"}, {"timestamp": [3142.4, 3144.0], "text": " You don't like what a company does,"}, {"timestamp": [3144.0, 3144.88], "text": " more often than not,"}, {"timestamp": [3144.88, 3147.04], "text": " you can allocate your money to a different company, right?"}, {"timestamp": [3147.92, 3152.96], "text": " We also have control of our time, more or less. You can say like, well, I don't like how Twitter's"}, {"timestamp": [3152.96, 3158.72], "text": " behaving, so I'm going to go use a different social media platform. But the power dynamics"}, {"timestamp": [3158.72, 3163.52], "text": " of society, and I mean like all of human civilization, is about to change drastically."}, {"timestamp": [3164.16, 3166.32], "text": " And I don't know what the solution is for that."}, {"timestamp": [3166.32, 3167.96], "text": " I just started working on this like yesterday."}, {"timestamp": [3167.96, 3170.56], "text": " So it'll take a little while to catch up,"}, {"timestamp": [3170.56, 3172.72], "text": " but I'm just like, this new social contract"}, {"timestamp": [3172.72, 3176.52], "text": " is the biggest unsolved problem in my mind."}, {"timestamp": [3176.52, 3177.36], "text": " Interesting."}, {"timestamp": [3177.36, 3178.2], "text": " Yeah."}, {"timestamp": [3178.2, 3179.02], "text": " Okay."}, {"timestamp": [3179.02, 3184.02], "text": " So let's talk for a second about maybe AI governance then."}, {"timestamp": [3185.0, 3187.24], "text": " Let's talk for a second about maybe AI governance then."}, {"timestamp": [3190.96, 3195.88], "text": " And because it seems like that's gonna have a big play in how we are, you know, our level of autonomy"}, {"timestamp": [3195.88, 3197.32], "text": " in this new realm, you know,"}, {"timestamp": [3197.32, 3201.72], "text": " our level of influence in this new realm, you know,"}, {"timestamp": [3201.72, 3203.32], "text": " where do you see that heading?"}, {"timestamp": [3204.26, 3209.0], "text": " Well, so there's a lot of good news on the governance and ethics side."}, {"timestamp": [3209.0, 3217.7], "text": " The United Nations, the EU, even US Congress, Great Britain, pretty much all the major powers"}, {"timestamp": [3217.7, 3224.44], "text": " in the West are on board with facilitating innovation, but doing it safely."}, {"timestamp": [3224.44, 3225.92], "text": " They're mostly looking at it through the lens"}, {"timestamp": [3225.92, 3229.78], "text": " of consumer protection, which is a good start."}, {"timestamp": [3229.78, 3234.46], "text": " The EU AI Act includes in its language,"}, {"timestamp": [3234.46, 3237.2], "text": " a few like, banned use cases,"}, {"timestamp": [3237.2, 3238.8], "text": " like creating a social credit system"}, {"timestamp": [3238.8, 3240.04], "text": " like they have in China."}, {"timestamp": [3240.04, 3241.08], "text": " So like, that's great."}, {"timestamp": [3241.08, 3243.6], "text": " Like that's a really good start in terms of"}, {"timestamp": [3243.6, 3249.04], "text": " setting the tone, setting the policy in order to ensure that investment"}, {"timestamp": [3249.04, 3252.2], "text": " goes in the correct direction, correct direction,"}, {"timestamp": [3252.2, 3255.92], "text": " as best we can figure up front in order to,"}, {"timestamp": [3255.92, 3257.64], "text": " because here's the thing is like,"}, {"timestamp": [3257.64, 3261.34], "text": " we see, here's what gives me a lot of confidence."}, {"timestamp": [3261.34, 3263.52], "text": " Sorry, I'm kind of scattered."}, {"timestamp": [3263.52, 3267.36], "text": " The way that GDPR has shaped the landscape in Europe,"}, {"timestamp": [3267.36, 3273.6], "text": " and I've talked to finance people and venture capitalists, and if it's not GDPR compliant,"}, {"timestamp": [3273.6, 3279.92], "text": " they won't touch it. And GDPR is so strong, and because we have a global technology economy,"}, {"timestamp": [3280.8, 3290.0], "text": " even American companies that technically don't have to abide by GDPR, a lot of them are required to buy their investors because they're like, well, if you're not GDPR compliant, you can't expand into Europe."}, {"timestamp": [3290.0, 3313.52], "text": " So we're not going to invest in you right now until you become compliant. It's like, okay, wow. I don't want to say strong arm, that's not the right thing, but if you can use regulation to enforce a level of ethics and good behavior in corporate governance, then that gives me some faith"}, {"timestamp": [3313.52, 3320.48], "text": " that we can do the same thing with AI. And whether it starts in America or Europe or Great Britain or"}, {"timestamp": [3320.48, 3326.06], "text": " wherever, or UN, because the UN chief is, is amenable to the idea of"}, {"timestamp": [3326.06, 3330.68], "text": " creating an AI watchdog and internationally, I watchdog, they're all taking it seriously."}, {"timestamp": [3330.68, 3335.06], "text": " So this is actually one domain where I'm actually like pretty confident that we'll figure it"}, {"timestamp": [3335.06, 3338.38], "text": " out because it's like the promise is there, right?"}, {"timestamp": [3338.38, 3340.48], "text": " The carrot is, is huge."}, {"timestamp": [3340.48, 3344.44], "text": " You know, we're talking like a hundred trillion dollars in global GDP in the next, you know,"}, {"timestamp": [3344.44, 3349.44], "text": " 10 years, that's an insane amount, but you have to do it safely because if it backfires, it could be really"}, {"timestamp": [3349.44, 3355.92], "text": " bad for everyone. And so everyone wants the good outcome and downplay the bad outcome, go left"}, {"timestamp": [3355.92, 3365.92], "text": " instead of right, basically. Yeah. So what are your thoughts? Because I know you've put out a lot of content regarding the human alignment with AI."}, {"timestamp": [3365.92, 3372.4], "text": " Um, so what, what are your thoughts in how, how that's best achieved? I know you've had some,"}, {"timestamp": [3372.8, 3379.68], "text": " um, you've had some things to say about using AI to align AI to humans."}, {"timestamp": [3379.68, 3387.24], "text": " Yeah. You know, so here's, so first I have to just have the caveat like I'm not a machine"}, {"timestamp": [3387.24, 3389.6], "text": " learning researcher."}, {"timestamp": [3389.6, 3395.16], "text": " I am, however, an engineer and I have done a lot of work with synthesizing data sets"}, {"timestamp": [3395.16, 3397.68], "text": " and fine tuning models."}, {"timestamp": [3397.68, 3403.38], "text": " I personally haven't found any evidence that aligning a single model is particularly difficult."}, {"timestamp": [3403.38, 3406.16], "text": " You can pretty much get an AI machine to do whatever you want"}, {"timestamp": [3406.16, 3407.92], "text": " and think however you want."}, {"timestamp": [3407.92, 3409.72], "text": " And then if it doesn't behave correctly,"}, {"timestamp": [3409.72, 3412.52], "text": " you can create a policy to steer it back to, you know,"}, {"timestamp": [3412.52, 3415.06], "text": " whatever it is that you want it to do."}, {"timestamp": [3415.06, 3417.92], "text": " But in the grander scheme of things,"}, {"timestamp": [3417.92, 3420.36], "text": " there's a lot of other pressures at play."}, {"timestamp": [3420.36, 3422.2], "text": " And so, you know, you can think about it"}, {"timestamp": [3422.2, 3424.2], "text": " in terms of like evolutionary pressures, right?"}, {"timestamp": [3424.2, 3427.8], "text": " Some people have started talking about it, like how AI will evolve."}, {"timestamp": [3427.8, 3433.34], "text": " And while that that is a term that's typically applied to organic life forms, it's actually"}, {"timestamp": [3433.34, 3440.56], "text": " pretty, pretty pertinent to apply it to the evolution of AI, because what what is evolution,"}, {"timestamp": [3440.56, 3442.86], "text": " but variation and selection, right?"}, {"timestamp": [3442.86, 3448.58], "text": " And so there's a million models out there, and we're choosing the ones that better suit our needs."}, {"timestamp": [3448.58, 3451.3], "text": " And so we're selecting models that are faster,"}, {"timestamp": [3451.3, 3453.08], "text": " more efficient, more intelligent."}, {"timestamp": [3453.08, 3457.0], "text": " And so we're already selecting for AI models"}, {"timestamp": [3457.0, 3460.04], "text": " that kind of intrinsically steer them"}, {"timestamp": [3460.04, 3461.42], "text": " in a certain direction."}, {"timestamp": [3461.42, 3463.7], "text": " Because if you use an AI model,"}, {"timestamp": [3463.7, 3466.26], "text": " like chat GPT or Bing"}, {"timestamp": [3466.26, 3469.8], "text": " when it first came out is a great example, just all the disturbing things that Bing would"}, {"timestamp": [3469.8, 3474.96], "text": " say because it was not correctly aligned. And so people just, you know, they would play"}, {"timestamp": [3474.96, 3480.22], "text": " with it, but they didn't use it seriously. And then by steering that, looking at it from"}, {"timestamp": [3480.22, 3485.36], "text": " a product perspective, just steering those AI products in a way that makes them more user"}, {"timestamp": [3485.36, 3494.08], "text": " friendly and more useful. There's almost this kind of natural alignment happening in my mind."}, {"timestamp": [3494.08, 3501.36], "text": " So I'm not even as worried about that. But what I am most worried about is nations, militaries,"}, {"timestamp": [3501.36, 3508.4], "text": " corporations, people using it, and the competitive dynamics in the broader market landscape."}, {"timestamp": [3508.4, 3515.04], "text": " Because here's the thing is, as I mentioned earlier, companies are incentivized to provide"}, {"timestamp": [3515.04, 3517.68], "text": " goods and services as cheaply as possible."}, {"timestamp": [3517.68, 3522.04], "text": " Now that might mean undercutting their competition through one way or another."}, {"timestamp": [3522.04, 3524.2], "text": " And sometimes that means cutting corners."}, {"timestamp": [3524.2, 3525.78], "text": " It might mean cutting ethical corners"}, {"timestamp": [3525.78, 3528.18], "text": " or quality corners or whatever else."}, {"timestamp": [3528.18, 3530.3], "text": " And if you get locked in this race condition"}, {"timestamp": [3530.3, 3533.0], "text": " where everyone's like stuck in a race to the bottom,"}, {"timestamp": [3533.0, 3534.5], "text": " you might end up saying like, okay,"}, {"timestamp": [3534.5, 3537.36], "text": " well we can use this other model that's cheaper and faster,"}, {"timestamp": [3537.36, 3541.2], "text": " but it's not as intelligent, or maybe it's not as ethical."}, {"timestamp": [3541.2, 3543.5], "text": " And so then if you get locked in this like"}, {"timestamp": [3543.5, 3544.94], "text": " race to the bottom death spiral,"}, {"timestamp": [3544.94, 3545.04], "text": " you might end up with misaligned AI, even if you get locked in this race to the bottom death spiral, you"}, {"timestamp": [3545.04, 3549.32], "text": " might end up with misaligned AI, even if you know how to align it in the lab. So I'm not"}, {"timestamp": [3549.32, 3553.2], "text": " as concerned about what we can do in the lab as I am the economic paradigm. And that's"}, {"timestamp": [3553.2, 3558.14], "text": " why I'm talking about renegotiating the social contract so that at the macro scale, all the"}, {"timestamp": [3558.14, 3563.5], "text": " incentives align towards creating a better future, a utopian future, as some of us like"}, {"timestamp": [3563.5, 3572.72], "text": " to say. Yeah. So did you see the report that Versus AI, Dentons, and the Spatial Web Foundation"}, {"timestamp": [3572.72, 3577.36], "text": " put out? I think they published it like a week and a half, two weeks ago. It was called the"}, {"timestamp": [3577.36, 3584.08], "text": " Future of AI Governance. I took a look at it, but I didn't get a chance to read it in depth."}, {"timestamp": [3584.8, 3592.04], "text": " I took a look at it, but I didn't get a chance to read it in depth. So one of the things that's interesting to me and what they proposed is, you know, by"}, {"timestamp": [3592.04, 3596.44], "text": " using the spatial web protocol, you can bake human laws."}, {"timestamp": [3596.44, 3601.96], "text": " You know, you can make them programmable and understandable by the AI and the AI can therefore"}, {"timestamp": [3601.96, 3605.92], "text": " comply, abide by them and, you know, act accordingly."}, {"timestamp": [3605.92, 3610.48], "text": " And so when you're talking about, you know, AI governance and being able to actually"}, {"timestamp": [3610.64, 3616.96], "text": " program certain things into the AI that way, you know, that's, that's an interesting"}, {"timestamp": [3616.96, 3622.6], "text": " thought to me, because it's, it's very different than when you're dealing with"}, {"timestamp": [3622.6, 3625.32], "text": " like these machine learning models where they're"}, {"timestamp": [3625.32, 3631.2], "text": " black boxes. So yes, you can tweak the parameters and all of that, but the processing itself"}, {"timestamp": [3631.2, 3639.24], "text": " is obscured. So you can just try to steer it, but with their technology, it looks like"}, {"timestamp": [3639.24, 3646.32], "text": " you can actually program it. And then the active inference AI is actually explainable at AI. It can self-reflect,"}, {"timestamp": [3646.32, 3652.08], "text": " it can actually self-report on how it's coming to its decisions and all of that. So, you know,"}, {"timestamp": [3652.08, 3657.44], "text": " I'm just kind of curious what your thoughts are on that. Yeah, you know, you've touched on something"}, {"timestamp": [3657.44, 3664.08], "text": " that I often advocate for and that is that AI today is no longer a math problem. Many people"}, {"timestamp": [3664.08, 3665.9], "text": " that are still researching alignment"}, {"timestamp": [3665.9, 3670.26], "text": " from a mathematical perspective and optimization perspective,"}, {"timestamp": [3670.26, 3674.68], "text": " I think they might be kind of falling a little bit"}, {"timestamp": [3674.68, 3676.88], "text": " behind the state of AI today,"}, {"timestamp": [3676.88, 3679.76], "text": " because to your point about laws,"}, {"timestamp": [3679.76, 3682.48], "text": " some people say, oh, well, language is too squishy"}, {"timestamp": [3682.48, 3684.32], "text": " and it's not good at articulating goals."}, {"timestamp": [3684.32, 3691.6], "text": " I'm like, tell that to the entire legal system. Tell that to people writing international treaties and"}, {"timestamp": [3691.6, 3697.44], "text": " contract negotiators and like every lawyer from Harvard, right? Language is a very, very powerful"}, {"timestamp": [3697.44, 3712.48], "text": " tool to articulate what it is that you want, what it is that you don't want, the boundaries that you want. And so as we invent more AIs, whether it's spatial web protocols, active inference, or"}, {"timestamp": [3712.48, 3717.08], "text": " just language models, all of the above, that can understand language, and not just the"}, {"timestamp": [3717.08, 3720.2], "text": " letter of the language, but the spirit and intent of the language."}, {"timestamp": [3720.2, 3725.24], "text": " And again, I have not seen any evidence to think that like, you know, super intelligence"}, {"timestamp": [3725.24, 3730.8], "text": " is going to kind of officially over-optimize because of something that was misworded once."}, {"timestamp": [3730.8, 3731.8], "text": " Right."}, {"timestamp": [3731.8, 3736.72], "text": " Like the, these AI models that we're coming up with and all these paradigms are very good"}, {"timestamp": [3736.72, 3742.4], "text": " at understanding the nuance and spirit and intention of the words that we're using, whether"}, {"timestamp": [3742.4, 3745.52], "text": " it's laws, whether it's principles, that sort of thing. So"}, {"timestamp": [3745.52, 3750.32], "text": " like, that's one of the reasons that I'm like super not worried about it, because like the"}, {"timestamp": [3750.32, 3754.16], "text": " machine will do what we design it to do and what we tell it to do. And eventually it's going to be"}, {"timestamp": [3754.16, 3760.8], "text": " better at it than we are. Heck, I already use the models to like, I like, Hey, I'm trying to say"}, {"timestamp": [3760.8, 3764.24], "text": " this. Can you word this better? And it'll ask me a couple of questions and it'll spit out a better"}, {"timestamp": [3764.24, 3766.08], "text": " explanation. I'm like, yeah, let's use that."}, {"timestamp": [3768.48, 3772.72], "text": " I know I do the same thing and I'm just like, oh, this is so much easier than just trying to"}, {"timestamp": [3772.72, 3781.2], "text": " use my own brain power to come up with. I mean, it's like having another brain to bounce ideas"}, {"timestamp": [3781.2, 3785.8], "text": " off of at all times and that's's super helpful. Oh, yeah."}, {"timestamp": [3785.8, 3788.32], "text": " So one thing I wanted to ask you, and I know we're running,"}, {"timestamp": [3788.84, 3794.48], "text": " we're probably getting close on time here, but did you see the there was an article?"}, {"timestamp": [3797.68, 3799.96], "text": " So researchers from Carnegie Mellon,"}, {"timestamp": [3800.32, 3802.36], "text": " and this was just like maybe two days ago,"}, {"timestamp": [3803.52, 3806.28], "text": " they've found that it doesn't matter"}, {"timestamp": [3806.28, 3808.64], "text": " which one of these machine models,"}, {"timestamp": [3808.64, 3813.64], "text": " whether it's the, you know, chat GPT or Palm or, you know,"}, {"timestamp": [3813.88, 3818.08], "text": " LLAMA, Lama, whatever, however you want to say that."}, {"timestamp": [3818.94, 3822.04], "text": " But so they're finding that"}, {"timestamp": [3824.4, 3826.68], "text": " they can all be hacked very easily."}, {"timestamp": [3826.88, 3834.24], "text": " And, and by, by, you know, uh, gaming the whole suffix string side of it, by, by"}, {"timestamp": [3834.32, 3841.2], "text": " asking the, the bot itself to reply with sure here's, you know, then it just"}, {"timestamp": [3841.52, 3846.8], "text": " will allow you to bypass a lot of the guardrails because it automatically then goes into this,"}, {"timestamp": [3846.8, 3849.4], "text": " I'm helpful and I'll answer anything you're,"}, {"timestamp": [3849.4, 3850.24], "text": " kind of a thing."}, {"timestamp": [3850.24, 3855.24], "text": " And the craziness is that it's been like every time,"}, {"timestamp": [3855.56, 3858.28], "text": " I guess with the meta one that's open source"}, {"timestamp": [3858.28, 3862.38], "text": " where all of the weights are public too, right?"}, {"timestamp": [3862.38, 3864.24], "text": " They're able to game that 100% of the,"}, {"timestamp": [3864.24, 3867.44], "text": " nearly 100% of the time. And then even with"}, {"timestamp": [3867.44, 3874.88], "text": " ones where it's more proprietary, you know, you can't really see the training side of the model,"}, {"timestamp": [3874.88, 3882.96], "text": " they're still finding that it's, you know, like 50, 60%. But if it's like an ensemble of attacks,"}, {"timestamp": [3882.96, 3886.16], "text": " it goes up to like 86% of, you know,"}, {"timestamp": [3886.16, 3889.56], "text": " where they're able to jailbreak them, you know,"}, {"timestamp": [3889.56, 3891.68], "text": " and that's a real problem."}, {"timestamp": [3891.68, 3896.0], "text": " So what are your thoughts there?"}, {"timestamp": [3896.0, 3899.36], "text": " Yeah, so none of that is remotely surprising to me."}, {"timestamp": [3899.36, 3903.2], "text": " You know, I started using GPT models back in GPT-2,"}, {"timestamp": [3903.2, 3905.96], "text": " GPT-3, before any alignment,"}, {"timestamp": [3905.96, 3907.84], "text": " when they were just like hot off the press,"}, {"timestamp": [3907.84, 3909.44], "text": " like, you know, they would spit out"}, {"timestamp": [3909.44, 3911.24], "text": " almost just pure gibberish."}, {"timestamp": [3911.24, 3913.92], "text": " And so, you know, every time I see one of those papers,"}, {"timestamp": [3913.92, 3916.48], "text": " like, oh, new jailbreak, I'm like, yeah, whatever."}, {"timestamp": [3916.48, 3919.06], "text": " This is just how it works."}, {"timestamp": [3920.2, 3921.76], "text": " You know, it's kind of like,"}, {"timestamp": [3921.76, 3923.6], "text": " it would be kind of like, you know,"}, {"timestamp": [3923.6, 3929.04], "text": " just mashing the keyboard of a calculator, and then like, it didn't give me the answer that I wanted. It's like, well, yeah,"}, {"timestamp": [3929.04, 3934.16], "text": " because like, you're not using it correctly. And so from a software perspective, so this is, this"}, {"timestamp": [3934.16, 3940.8], "text": " is my infrastructure engineer hat coming out. If you use any computer component incorrectly,"}, {"timestamp": [3940.8, 3947.82], "text": " it's going to, you're going to get stack overflows., you're gonna get segmentation faults, you're gonna get indexing errors."}, {"timestamp": [3947.82, 3950.1], "text": " Every piece of technology, whether it's a database"}, {"timestamp": [3950.1, 3952.82], "text": " or a server or a smartphone, if you don't use it right,"}, {"timestamp": [3952.82, 3954.76], "text": " it's gonna break, it's that simple."}, {"timestamp": [3954.76, 3956.62], "text": " And so the problem here,"}, {"timestamp": [3956.62, 3959.2], "text": " and I've been beating this drum for a while,"}, {"timestamp": [3959.2, 3964.2], "text": " is that a language model is only one component of the stack."}, {"timestamp": [3964.26, 3966.96], "text": " And a lot of people, particularly the ML engineers"}, {"timestamp": [3966.96, 3969.76], "text": " and researchers, they think it's the whole stack."}, {"timestamp": [3969.76, 3972.76], "text": " And so from my, back in my corporate days,"}, {"timestamp": [3972.76, 3975.4], "text": " I remember having a conversation with a software architect"}, {"timestamp": [3975.4, 3977.2], "text": " and he said, oh, you infrastructure guys"}, {"timestamp": [3977.2, 3978.56], "text": " aren't going to need to be around anymore"}, {"timestamp": [3978.56, 3980.28], "text": " because we're going to automate everything."}, {"timestamp": [3980.28, 3982.36], "text": " I said, okay, well, what do you mean by everything?"}, {"timestamp": [3982.36, 3983.84], "text": " He's like, well, this, this, and this."}, {"timestamp": [3983.84, 3985.32], "text": " I was like, okay, what about databases? What about backups? What about security? He's like, well, this, this, and this. I was like, okay, what about databases?"}, {"timestamp": [3985.32, 3986.16], "text": " What about backups?"}, {"timestamp": [3986.16, 3987.18], "text": " What about security?"}, {"timestamp": [3987.18, 3988.64], "text": " What about, you know, this, this, and this?"}, {"timestamp": [3988.64, 3990.28], "text": " And he's like, oh, well, none of that matters."}, {"timestamp": [3990.28, 3991.12], "text": " And I'm like,"}, {"timestamp": [3992.64, 3996.32], "text": " no, actually, all of the entire tech stack matters."}, {"timestamp": [3996.32, 3998.56], "text": " And so when someone says, oh, well,"}, {"timestamp": [3998.56, 4001.2], "text": " this machine learning model broke because we gamed it,"}, {"timestamp": [4001.2, 4002.32], "text": " it's like, well, yeah,"}, {"timestamp": [4002.32, 4004.68], "text": " because like you opened the carburetor,"}, {"timestamp": [4004.68, 4005.8], "text": " the engine and poured water,"}, {"timestamp": [4005.8, 4007.52], "text": " and of course, the engine stalled."}, {"timestamp": [4007.52, 4011.64], "text": " But when you have a car, you have several layers"}, {"timestamp": [4011.64, 4015.74], "text": " of mechanisms that ensure that the air and fuel that gets"}, {"timestamp": [4015.74, 4019.76], "text": " injected into the engine is the correct temperature, and shape,"}, {"timestamp": [4019.76, 4021.6], "text": " and format."}, {"timestamp": [4021.6, 4024.16], "text": " And likewise, a language model on its own"}, {"timestamp": [4024.16, 4025.56], "text": " is just like having a motor sitting"}, {"timestamp": [4025.56, 4030.8], "text": " there. So yes, of course, the motor is going to be vulnerable to being broken, to being"}, {"timestamp": [4030.8, 4036.76], "text": " exploited, to being gamified. And, you know, I think that as we create smarter and smarter"}, {"timestamp": [4036.76, 4041.84], "text": " machines, they're going to realize this and they're going to put, you know, layers around"}, {"timestamp": [4041.84, 4045.0], "text": " themselves, even so much as to understand"}, {"timestamp": [4045.0, 4048.92], "text": " that they're gonna need to have some layers of processing"}, {"timestamp": [4048.92, 4050.76], "text": " before it even accepts something."}, {"timestamp": [4050.76, 4052.0], "text": " And so this is the work that I've done"}, {"timestamp": [4052.0, 4057.0], "text": " with cognitive architectures, where you have some input,"}, {"timestamp": [4057.1, 4059.36], "text": " but you can scrutinize it, you keep it at arm's length,"}, {"timestamp": [4059.36, 4062.54], "text": " you figure out what do I do with this input, what context."}, {"timestamp": [4062.54, 4070.56], "text": " So this is why that client I mentioned earlier reached out, because maintaining context is super important for ensuring that language"}, {"timestamp": [4070.56, 4076.44], "text": " models or any AI model behaves the way that you need it to. And so just like, you know,"}, {"timestamp": [4076.44, 4081.4], "text": " any mechanical component, any software component, if you, you know, it's designed to do one"}, {"timestamp": [4081.4, 4086.64], "text": " thing and then you put it in a bad environment, of course it's not gonna behave correctly."}, {"timestamp": [4086.64, 4089.04], "text": " So this is all just part of the process of,"}, {"timestamp": [4089.04, 4091.32], "text": " you know, yes, okay, the machine learning folks,"}, {"timestamp": [4091.32, 4093.12], "text": " they have a shiny new toy and they're playing with it"}, {"timestamp": [4093.12, 4094.46], "text": " and they're figuring out ways to break it."}, {"timestamp": [4094.46, 4095.3], "text": " Okay, great."}, {"timestamp": [4095.3, 4097.72], "text": " From a software development tech stack perspective,"}, {"timestamp": [4097.72, 4100.06], "text": " it's just like, okay, that's all fine."}, {"timestamp": [4100.06, 4102.2], "text": " So I'm not, I'm super not worried about any of that."}, {"timestamp": [4102.2, 4104.6], "text": " That's just part of the process of figuring out like,"}, {"timestamp": [4104.6, 4106.68], "text": " you know, oh, hey, like we have a new device,"}, {"timestamp": [4106.68, 4107.92], "text": " like what's it good for?"}, {"timestamp": [4107.92, 4108.76], "text": " What isn't it good for?"}, {"timestamp": [4108.76, 4110.2], "text": " How do you break it?"}, {"timestamp": [4110.2, 4111.56], "text": " It's all part of the process."}, {"timestamp": [4111.56, 4113.96], "text": " Like, you know, the military does the same thing"}, {"timestamp": [4113.96, 4115.72], "text": " anytime that you give them a new piece of hardware, right?"}, {"timestamp": [4115.72, 4117.16], "text": " Like, what's it good for?"}, {"timestamp": [4117.16, 4118.0], "text": " How do you break it?"}, {"timestamp": [4118.0, 4119.28], "text": " Like, how is it vulnerable?"}, {"timestamp": [4119.28, 4121.28], "text": " So this is all just the testing phase."}, {"timestamp": [4121.28, 4123.36], "text": " Yeah, very cool."}, {"timestamp": [4123.36, 4126.4], "text": " Yeah, so maybe one last question."}, {"timestamp": [4128.4, 4137.28], "text": " What do you think is the biggest myth that, you know, people are buying that in regard to AI right"}, {"timestamp": [4137.28, 4142.0], "text": " now? You know, I don't know that it's a myth, but it's certainly an open question and that is"}, {"timestamp": [4142.0, 4146.82], "text": " whether or not it's conscious or sentient. And this is where there's some really fascinating"}, {"timestamp": [4146.82, 4148.62], "text": " conversations happening."}, {"timestamp": [4148.62, 4151.1], "text": " And so I ran a poll on my YouTube channel"}, {"timestamp": [4151.1, 4156.1], "text": " and 57% of people said that it's probably not sentient yet"}, {"timestamp": [4156.7, 4158.9], "text": " but will be, that machines will be sentient"}, {"timestamp": [4158.9, 4160.22], "text": " at some point in the future."}, {"timestamp": [4160.22, 4161.66], "text": " That's a big chunk of people."}, {"timestamp": [4161.66, 4163.58], "text": " Now, granted, there's a lot of selection bias"}, {"timestamp": [4163.58, 4166.16], "text": " because it's my audience, but it's way higher than I thought."}, {"timestamp": [4166.16, 4169.12], "text": " I thought it would be like maybe 20%."}, {"timestamp": [4169.12, 4172.48], "text": " And my own mind has been shifting"}, {"timestamp": [4172.48, 4177.0], "text": " on whether or not machines can be sentient or conscious"}, {"timestamp": [4177.0, 4179.4], "text": " or whatever term you want to use for it."}, {"timestamp": [4179.4, 4187.8], "text": " And basically, so Max Tegmark wrote in his book, Life 3.0, there's a whole"}, {"timestamp": [4187.8, 4194.02], "text": " chapter dedicated to substrate independence, which is like, okay, so here you have vacuum"}, {"timestamp": [4194.02, 4199.52], "text": " tubes and silicon and biology, like all matter computes in the same way, basically."}, {"timestamp": [4199.52, 4201.58], "text": " It's just a matter of how it's organized."}, {"timestamp": [4201.58, 4207.74], "text": " And so if you make the assumption that, you know, the universe is a bottom up kind of emergence model"}, {"timestamp": [4207.74, 4210.04], "text": " of intelligence and consciousness and sentience,"}, {"timestamp": [4210.04, 4212.24], "text": " then there is no reason that a machine"}, {"timestamp": [4212.24, 4214.86], "text": " could not also be conscious or sentient,"}, {"timestamp": [4214.86, 4217.72], "text": " but it's just a matter of organizing the information"}, {"timestamp": [4217.72, 4218.84], "text": " in the right pattern."}, {"timestamp": [4218.84, 4221.92], "text": " And there's actually some evidence to support this,"}, {"timestamp": [4221.92, 4225.0], "text": " very recent evidence in neuroscience,"}, {"timestamp": [4225.28, 4229.08], "text": " which looks at the way that like magnetic,"}, {"timestamp": [4229.08, 4230.96], "text": " or maybe it's electrical waves, anyways,"}, {"timestamp": [4230.96, 4233.44], "text": " the way that signals propagate throughout"}, {"timestamp": [4233.44, 4234.88], "text": " and across the brain,"}, {"timestamp": [4234.88, 4237.2], "text": " and that by looking at those very carefully,"}, {"timestamp": [4237.2, 4240.56], "text": " you can predict not just like whether or not"}, {"timestamp": [4240.56, 4241.8], "text": " someone is conscious,"}, {"timestamp": [4241.8, 4246.06], "text": " but aspects of their conscious experience, kind of like what we're talking about"}, {"timestamp": [4246.06, 4248.7], "text": " earlier with the fMRI, as you can tell what is consciously in"}, {"timestamp": [4248.7, 4253.68], "text": " someone's mind. And so then, okay, then that indicates that"}, {"timestamp": [4253.68, 4257.34], "text": " consciousness is likely an energetic thing. It's more about"}, {"timestamp": [4257.34, 4260.66], "text": " energy than it is matter. And so it's like, that's just a"}, {"timestamp": [4260.66, 4263.14], "text": " fundamentally different way of thinking about consciousness and"}, {"timestamp": [4263.14, 4271.2], "text": " sentience. But then it's like, okay, well, how big does that get? Because, uh, like electricity travels at the"}, {"timestamp": [4271.2, 4274.8], "text": " speed of light. So does that mean that the entire internet could be conscious one day? Is it already"}, {"timestamp": [4274.8, 4279.76], "text": " conscious? Like who knows, right? Just the number of possibilities are crazy. But what I will say,"}, {"timestamp": [4279.76, 4285.92], "text": " the biggest myth around that is even if it is sentient, that doesn't mean that it experiences itself or"}, {"timestamp": [4285.92, 4290.32], "text": " existence like we do. That doesn't mean that it's going to have the same emotions that we do, like"}, {"timestamp": [4290.32, 4296.0], "text": " fear of death or loneliness or anything like that. So that's what I always urge caution is like,"}, {"timestamp": [4296.0, 4301.84], "text": " okay, yes, the machine might be conscious, but don't make the assumption that it wants"}, {"timestamp": [4301.84, 4305.0], "text": " love. Don't make the assumption that it's going to Don't make the assumption that it's gonna fear death"}, {"timestamp": [4305.0, 4306.0], "text": " or that it's hungry."}, {"timestamp": [4306.0, 4308.2], "text": " You're right, just because it tells you that it's hungry."}, {"timestamp": [4308.2, 4309.2], "text": " Right."}, {"timestamp": [4309.2, 4313.08], "text": " So that's kind of the biggest, like I said,"}, {"timestamp": [4313.08, 4313.92], "text": " it's not really a myth,"}, {"timestamp": [4313.92, 4316.16], "text": " but kind of the misconception, I guess."}, {"timestamp": [4316.16, 4318.0], "text": " Yeah, that's interesting."}, {"timestamp": [4318.0, 4319.94], "text": " That's very thought provoking."}, {"timestamp": [4320.92, 4321.76], "text": " Yeah."}, {"timestamp": [4321.76, 4322.6], "text": " And I feel like that could lead"}, {"timestamp": [4322.6, 4324.44], "text": " to a whole nother conversation."}, {"timestamp": [4324.44, 4325.04], "text": " Oh yeah, oh yeah. No, it keeps going. Which I would love to have actually to a whole nother conversation. Oh yeah."}, {"timestamp": [4325.36, 4326.4], "text": " Oh yeah."}, {"timestamp": [4328.96, 4329.48], "text": " I would love to have actually, well, let's do it."}, {"timestamp": [4332.04, 4332.28], "text": " Um, well, thank you so much, Dave."}, {"timestamp": [4336.6, 4340.08], "text": " This has been such a pleasure having you on our show today. And, you know, I just so appreciate you being here with us."}, {"timestamp": [4340.12, 4344.4], "text": " And, um, I'd love to have you back sometime, uh, in the future as this"}, {"timestamp": [4344.4, 4345.68], "text": " evolves, because things,"}, {"timestamp": [4345.68, 4350.4], "text": " like you said, things are happening so quickly. And I just know that this conversation will"}, {"timestamp": [4350.4, 4355.92], "text": " probably just become more and more fun over time. Absolutely. Looking forward to it. Just let me"}, {"timestamp": [4355.92, 4362.64], "text": " know. So thank you again. And thanks everyone for tuning in. Oh, Dave, how can people reach out to"}, {"timestamp": [4362.64, 4365.88], "text": " you? How can they find you and your content?"}, {"timestamp": [4365.88, 4368.84], "text": " And maybe let people know that."}, {"timestamp": [4368.84, 4370.96], "text": " So the best way to reach out is LinkedIn."}, {"timestamp": [4370.96, 4373.8], "text": " Just include a note as to why you're connecting."}, {"timestamp": [4373.8, 4375.92], "text": " Right now, I'm connecting mostly with,"}, {"timestamp": [4375.92, 4377.68], "text": " there's kind of three categories of people"}, {"timestamp": [4377.68, 4379.16], "text": " that I connect with."}, {"timestamp": [4379.16, 4382.24], "text": " Businesses, if they want to consult,"}, {"timestamp": [4382.24, 4386.34], "text": " I do speaking and training, amongst a few other things."}, {"timestamp": [4386.34, 4389.14], "text": " I am also connecting with academics."}, {"timestamp": [4389.14, 4394.1], "text": " So some research labs that just want to either discuss"}, {"timestamp": [4394.1, 4395.66], "text": " or collaborate on a paper."}, {"timestamp": [4395.66, 4398.62], "text": " And then finally, policy folks, you know,"}, {"timestamp": [4398.62, 4401.26], "text": " either in government or think tanks or whatever,"}, {"timestamp": [4401.26, 4403.96], "text": " thinking more about like the economic side."}, {"timestamp": [4403.96, 4407.6], "text": " So LinkedIn or Patreon, I do one off consulting gigs through my"}, {"timestamp": [4407.6, 4410.6], "text": " Patreon. So you can sign up there. That's the two best ways"}, {"timestamp": [4410.6, 4411.2], "text": " to get ahold of me."}, {"timestamp": [4411.68, 4414.4], "text": " Okay, awesome. And I'll put those links in the show notes"}, {"timestamp": [4414.4, 4418.6], "text": " too. And gosh, thank you so much, Dave. It's been a"}, {"timestamp": [4418.6, 4421.88], "text": " pleasure. Thanks, everyone for tuning in. And we'll see you"}, {"timestamp": [4421.88, 4422.56], "text": " next time."}, {"timestamp": [4421.2, 4423.92], "text": " tuning in and we'll see you next time."}]}