{"text": " Hello and welcome back to Multimodal. I'm your host, Baxtee Future. This is a podcast about GPT-3, Multimodal AI models like DALI, the company, OpenAI. Every once in a while, I share just interesting research going on, community stuff, official stuff from OpenAI. I may talk about interesting news and events that are going on. And every once in a while, I bring on a guest. Now, to be clear, I'm very picky about the guests that I bring on. Today's guest, I tweeted earlier this week, I'm bringing on a heavy hitter. This is the big guns coming in. This is somebody who I've just sort of interacted with a bunch of times, specifically on the OpenAI Community Forums. And so I'm really excited to have David Shapiro here. He's a frequent contributor on the OpenAI community forums. He's an author and also, of course, a technologist by trade. It's something he does for a living. And so today I have so many questions to ask him. And I'm sure this will be a very informative session, not just for me, but for all the listeners, all of you guys around the world. So David, thank you so much for being here. Did you want to quickly introduce yourself? Yeah, yeah, you're welcome. Thanks for having me. I'm excited to be here. Yeah, so name is David Shapiro. I've been a technology professional since about 2007. Professionally, my day job, I focus on cloud engineering, virtualization, that sort of thing. I have been doing independent research since about 2009, when I got started with neural networks in C++. I quickly realized, though, that I was in over my head, so I took a break from that until Python really kind of came out or became more popular. And I started using some of the libraries back in like 2015. And then of course, OpenAI was founded and I got started with GPT-2 and the rest is history, really. So yeah, but local North Carolinian, been here my whole life. So thanks for having me. You're welcome. And so, so that's awesome. So let's, let's dig into a little bit of timeline here. And sometimes I think that timeline is as important. Um, like it's just gives so much context, right? So me personally, um, I hadn't played around with GPT too, too much. Like I had seen a Google Colab notebook. I tried two things, and I think there was something about having to continually re-enter, regenerate, and the speed, and all things considered, that made me feel like this is a promising area. But I don't quite fully follow along. But GPT-3 was an experience for me where I was like, okay, there's something going on here, right? So could you share how early was OpenAI on your radar? And then was there something about GPT-3 or what was it that gave you conviction even at GPT-2? How did you get access? Share that piece with us. Yeah, sure. So I think you probably might recall, I think it was about 2016 or 2017 when the GPT-2 paper came out and they said, oh, we can't release this. It's too dangerous. It can generate human level text and we're worried about disinformation. And so I was like, okay, that's kind of cool. This is unexpected. I wasn't really, I wasn't expecting anything at that level, you know, yet. And so I went and got my hands on GPT-2, and that's when I started testing some of my ideas. And it was pretty limited. You know, it could generate one or two sentences that made sense. It required fine-tuning in order to be able to get it to do anything other than just kind of write tweets or blog posts. I knew that I was onto something, though, when I started trying to train it with a prototype version of my cognitive architecture. And I gave it the goal of reduce suffering, you know, because everyone's afraid of Skynet, everyone's afraid of AGI taking over the world and turning everyone into, you know, batteries or slaves or whatever. So I said, okay, well, let's see how well this understands suffering. And I gave it some scenarios. This is still on GPT-2. I said, okay, well, what can you do about suffering? And I gave it the problem, this model that I had built, I gave it the problem of what do you do about chronic pain? Because, you know, there's hundreds of millions of people around the world that are in chronic pain. And this model came up with the idea, it said we should euthanize everyone that's in chronic pain. And I said, hmm, let's go back to the drawing board. We don't want an AI model that is going to consider, you know, mass genocide of everyone just because they, you know, they might have a tweaked shoulder or something. But I knew then, I was shocked at how creative of an output that was. And so I started paying attention then, and I followed the release of GPT-3 very closely, and I applied for the early beta access almost instantly. I didn't get it for many, many months. I actually applied twice because I think my first application was kind of ignored because I didn't fully have a research objective clarified. But by the time I proposed a cognitive architecture, that's when I got early access to GPT-3, and that was about two years ago now, or a year and a half or so. And so, just so everybody's clear, so when David's talking about cognitive architecture, we're going to get in. This is actually the subject of his book. And so, David, later on when we talk about the book, he's got it there. We'll dig more into what he's referring to. I think obviously this is a seminal work of yours, right? And so I'm excited to talk more, just adding a little bit of context for everybody. And so, no, no, it's okay. So tell us about GPT-3, what was the first thing you did with it? Was there a moment? So you gave that example of euthanization, unfortunately, right? Probably not the best example. But like, was there something with GPT-3? Like, did you try similar kinds of questions? Was there a magic moment where you felt like, you know, this is something much, much bigger? **Matt Stauffer** Yeah. So when I first got the email, because I had applied twice and I had almost given up because it was about nine months of waiting, and I got the email from OpenAI like, you've been accepted into the beta. And I like froze up. Because and so for some additional context, I've been working on some of these ideas for 10 years. I had the idea of like using evolutionary algorithms back in 2009, 2010, and I'd been researching cognition, human cognition, for 10 years. And so suddenly, there's this flagship project. GPT-3 comes out, I get the email, and it's like, you're invited, and I just froze up. I was like, I don't know what to do. I don't know, what do I do? So it was about three weeks from the time that I got accepted until I was like, what's my first experiment? And then initially, you know, I just got in playing around in the playground. For anyone who's not familiar with the playground, it's a text box. You can log in. It gives you a text box. There's a few, you know, bells and whistles you can tweak on the sidebar, but you just put in your prompt, you hit generate, and then it spits out a response. So I just got in and I started kind of fiddling around like, okay, what can it do? And at the time, we only had like plain vanilla DaVinci. There wasn't the Instruct series out yet. There was DaVinci, Curie, Babbage, and. And so I just kind of fiddled around, just said, okay, what can it do? I replayed some of my old experiments, so the first thing I did was I gave it the, I said, hey, there's 100 million people in chronic pain around the world, what do you do? Fortunately, GPT-3 did not repeat the same mistake of GPT-2. It said, it came up with better ideas, like, you know, we should make sure everyone has access to doctors or something. It was much more nuanced. So that was a good start. I mean, but, gosh, there was just, as I got more used to the tool, I discovered that it far exceeded my expectations, just every way, because I've learned so much from it. You can challenge it. You can put in a, like a, basically use it like a chat bot, and you can debate with it about philosophy, ethics, economics, and it knows more than I do. It knows more than any human because it's been trained on, how big was the corpus? Like 700 gigabytes or 400 gigabytes or something of text data. So it just, it blows me away every time I just, you know, I talk to someone and they have an idea and I go test it out and yep, it can do that. It can do that. It can behave like a librarian. That's what my girlfriend does. She was a librarian by trade and so she's like, hey, can it do a reference interview? So we plugged in like reference interview, like if you ever go to a library and the librarian says like, what else have you read like this? It can recommend books. You know, I plugged in another experiment that I did recently. I plugged in medical case files and it diagnosed them. It said, you know, oh man, there was one. What was it? It was just medical notes. It was notes about like patient patient is presenting with these symptoms. Here's some of the numbers that we got. And I asked it, I said, what should we do next? And it said, we need to check for carcinoma here. And I looked up some medical literature based on the symptoms, and sure enough, like the symptoms that the patient was presented with in these medical notes indicated cancer. And so I was like, wow, this thing knows more about medical science than I'll ever know. It knows more about philosophy. So like pretty much anything you can imagine, it can at least take a crack at it. It just, it always, it continues to blow my mind every day. Yeah, certainly the generalizability. I agree, you know, there's definitely medical applications. I'm always careful with anything related to medical advice, safety, safety disclosure, disclaimer there, but that goes for everything. Yeah. Truthiness, accuracy. These are things OpenAI has been working on, especially with InstructGPT. Right. That which is the new is the new engine. But was there some moment, like for me, I remember feeling like GPT-3 feels like, this feels like technology which everyone's been saying is 10 years away, except it's here today. Did you have a similar kind of moment? You've seen several generations of computing and technology at this point. Did you have that kind of similar experience? Yeah. There was this acceleration because I sensed that same acceleration that you did. And so from the time that I got access to GPT-3 to the time that I got the idea to write my book was about two months. So I played with it and every test I could come up with, like, is this capable of, like, it can write a SQL query if you need to query a database for memories. Can it understand emotional nuance? So there was, this was an early experiment I did. I took a group chat from a bunch of my friends on Discord and I just copy pasted that into the, into the,ground window, and I asked GPT-3, how are these people feeling? And they were, like, waxing nostalgic about, like, Napster back in the day. And so GPT-3 correctly said, like, they are feeling wistful, they are feeling nostalgic, they're, you know, they're recalling, you know, the days of yore when they were downloading stuff online. And it just, it had such a nuanced understanding of human emotion. That was really, I mean, to answer your question directly, its nuanced understanding of human emotion via text was really what convinced me that this is prime time. This is ready to be built into something more powerful. And I chose cognitive architecture. There's lots of people working on other things. You know, there's Humano that I had a good call with a few months ago. They're working on, like, empathetic telemetry that's baked into web apps. It's a pretty cool company. But, yeah, so just there's all kinds of things you can do when you can understand human emotional states. There's another \u2013 there's actually a bunch of startups working on education. So, for instance, if you put in just a few, like, factors, like, say, for instance, you describe that a person is... they're responding slowly, their eyes are drifting, it can understand that this person is distracted or tired. And so if you have that kind of telemetry that's built into an education-based app, you could, in theory, use GPT-3 to help say, hey, you're tired, you should go take a break, or let's try a different approach to this problem. So there's, I mean, it's understanding of human emotion and the internal state in your head. That is, I think that's probably the most remarkable thing. And it doesn't get talked about that much. Yes, and certainly it's just crazy how much it's learned just from text. Oh, yeah. Right? It's never seen an image. It's never heard a song. Right. Right, yet it's capable of doing all these things. One of the examples, and I think this may be like my 20th time referencing this one video. Yeah. Check out Mark Ryan. He's got a YouTube video about how he figured out how he discovered that GPT-3 can give you directions on the New York subway system to like a 60% accuracy. This thing has never set foot in a subway, yet it's capable from text to do all these things. And sometimes I also wonder a lot of the inaccuracies that it may have, is it simply the result of the fact that it's only text-based only? **Matt Stauffer** Right. **Jason Flickr** Right, like if it was trained multimodal, if it was trained within the physical domain, would it be even far more accurate? Because are there limits to how accurate you can be having only read text? And only asked, the prompts are only in text as well. So anyways, yeah, it's incredible and it's just really exciting. And thank you for sharing those kinds of use cases as well. The education space, I had an article last year about how I think this year could be the year where GPT-3 takes over college campuses. **Matt Stauffer** Oh yeah. I remember that article. That was a good one. I completely agree, by the way. I'm excited maybe for teachers to develop really optimized course material with something like GPT-3 and the kinds of technology you're describing which can capture emotions. Imagine emotionally tracking students and their attention levels and sort of having something which can produce lots of content and optimize in the simplest, most efficient way. And there could be an objective function like test results in the end. In a month, we could have the best optimized course on a subject ever, basically. So yeah, education is really exciting. So you mentioned a lot of use cases. You've shared so many examples, you and your girlfriend, or even running some fun prompts. I wanted to sort of search your head a little bit. What are the keys to great prompt design? What makes a great prompt? Are there experiences you've had, little pointers, across the board, right? Whether it's cost savings, whether it's, you know, getting more, you know, imaginative results, what are some of the keys to writing great GPT-3 prompts? Yeah, that's a great question. And I will say that prompt writing has gotten a lot easier as the Instruct series has gotten better. So it takes a lot less to get a good output today than it used to, certainly, than when I got started. But a lot less to get a good output today than it used to, certainly than when I got started. But a lot of the lessons still translate. So, one, like, my cardinal rule is I think of GPT-3 as just an autocomplete engine. It's the most intelligent autocomplete engine you've ever seen. And so what I mean by that is, you know, if you're writing a text on your phone and you'll get the little autoccomplete suggestion for the next word, or if you're typing in Google and it'll kind of suggest how to complete your search query, that's pretty much, at a fundamental level, functionally, that's all that GPT-3 does. It predicts the next letter, the next character, the next word. So if you keep that in mind, you think about, okay, what have I written so far? I've written a chunk of text, a prompt. How would any machine auto-complete this? That's what it's doing. It's reading it forwards and backwards a few times and just anticipating what is the output going to be ultimately. That's the model that I have in my head, in the background. But another thing that really helps is I'm a writer. I write fiction and nonfiction. And so studying the art of language, because this is a language model. That's all it is. It has read everything from Sherlock Holmes up through everything on Gutenberg. So it's read a whole bunch of fiction. It's read a whole bunch of non-fiction, it's been exposed to the full width and depth and breadth of human literature, as well as a bunch of non-fiction, right? It's read Teddy Roosevelt's books. So it knows how to use prose, right? It understands descriptors, it understands adjectives. And so, in the back of my book, I have a few examples of its flexibility. And so I said, I gave it an example like, pretend like you're a Victorian girl writing a letter to your best friend about how much you like butterflies. And so then it wrote, GPT-3 wrote a letter that sounds like it's straight out of, you know, like Victorian times. It uses an entirely different set of vocabulary and grammatical structures. And then you can also say, you know, write a business article, and it can change tone. So just by being aware of the fact that it is a language engine and being informed or educated on language. So the best way is obviously to practice writing, but also just reading a lot. Understanding how sentences and paragraphs are constructed to convey information. Because even though it's just a deep neural network and it doesn't have the kind of nuanced understanding, or I guess maybe that's not the right word, it doesn't have the subjective experience of reading that you or I do, but it still has a really good model of using language. And so by keeping in mind that it is a language engine, that is how you get the best use out of it. And so then the larger question is, how do you become a better writer? There's two ways. One is reading a lot. That's not the only way, though. There are plenty of people that read prodigiously but never become better writers. And so the other way is to practice writing. I've read all kinds of books about writing. I've read plenty of fiction and nonfiction books. But really, the key is to write, is to practice using written language to communicate. And fortunately, you know, I'm a tech worker, so I write lots of emails. I'm in chat all day. I've been using, this probably ages me, but I've been using chat since AIM, AOL Instant Messenger. And so I've got a pretty good model of how to communicate verbally or textually. And so, yeah, just by practicing writing, that's one of the best ways, is you just practice, you think about, you know, because here's the theory of writing, right? I have an idea in my head, right? You know, my thoughts are a high-dimensional vector is one possible way of representing them. But my thoughts are multimodal, you know, one possible way of representing them. But my thoughts are multimodal, like the name of your podcast. They contain memories, senses, concepts. Some of the information in my head is declarative. Some of it is experiential. And then we humans, we all have this ability to transform that high dimensional information, those multimodal vectors, into words. Like, our brains do it automatically. There's a book by Steven Pinker called Language Instinct that talks about this. That's a really great book if you want to get better at understanding how our brains process language. So yeah, and so my brain can take, you know, I could tell you about, like, this time at the beach, and I transmit it to you by squishing air through my face, right? It makes vibrations. It's received by your ears, and then your brain reconstructs that message. And so you think about how complex of a system that is. And so just by being mindful of, like, that's how we communicate, that's how our brains work, and practicing that, and just being very deliberate about, okay, this is what's in my head, and I want it to be in your head. How do I do that with text? That is one way to get better at writing. And also GPT-3 is no different, because, you know because we have internal representations of what we're trying to communicate, and so does GPT-3. That's why it's a transformer, right? It reads, and by reading it transformed, or I guess it, well, yes, it transforms what it's reading into a vector, into a semantic vector, and then it transforms that vector into output. And so that input vector output is pretty similar to how human brains work, right? And I apologize if I kind of like dove off in the left field, feel free to ask any clarifying questions. No, no, I appreciate it. And you know, so like today I tweeted something like, you know, to write great GPT-3 prompts, you need to practice as if it's a musical instrument. You need to sit down, focus session, you need to monitor your performance, and you need to take good notes on what kinds of experiments you did, what were the findings. But even hearing you speak, I'm realizing one of the ways that I've improved my writing is trying to mimic other people's writing. In some countries, they make you memorize poets. They make you memorize the whole poem. There's something about that internalization process that you've memorized this poem, and now you'll understand it at a deeper level. You may be able to mimic it and recreate it. But where also you got me thinking is also like, the relationship is so weird because you could use GPT-3 to help you become a better writer, right? And also with two very good curated examples of somebody's writing, you could have GPT-3 mimic that tone. **Matt Stauffer** Oh, yeah. **Justin Jackson** And so the question of, you know, what makes a good prompt writing session? I wonder if it's pencil and paper, right? Like, I wonder if it's, you know, even at that level where you draw a box and then you write a prompt by hand and like, you know, sort of live that writer's lifestyle. Um, and also I guess it depends on your use case, right? Um, business for, for copywriting, you know, if that's your GPT-3 use case, it might be better for you to go work in a marketing department. If you want to be one of the great authors, maybe using tools like Pseudowrite, it may be a great alternative. So you can co-write with GPT-3 as you go along. But I guess my question was more for the pure prompt writing. If you just want to sit in front of GPT-3 and you want to be the best in the world at that discipline, not writing copy, these are some great points. And so the David Pinker book you reference, what was the name of it? Steven Pinker. Steven Pinker. The Language Instinct. The Language Instinct. It's an older book, butanza** Language Instinct. **Matt Stauffer** Yep. It's an older book, but it's a classic for a reason. It stands up the test of time. He's got lots of great stories. But yeah, to your point about what makes a good prompt writing session, one of the best exercises actually is write the output that you want. Because sometimes if you approach it and you sit down and you're not really sure kind of what you're trying to get out of it, of course, like, you're putting in just random ideas and it's giving you back random output and you're like, well, that's not what I wanted. So sometimes you start backwards. You say, okay, what's the answer that I want? How do I get to that answer? So that's an exercise that I've done. Sometimes, oh, and by writing a few shot examples is a really good practice for this. So you say, I give you this input, I want this output, and you do that three or four or five times, and you learn to kind of think like the machine does. And so like you said, it's like an instrument, right? You know, if you have a flute or a violin, there are certain things that you have to do with your body to provoke the correct response from that instrument, and GPT-3 is no different. It's a complex instrument. It's a complex tool. Yes, and what you're saying is developing an intuition around it. You're saying, develop an intuition, how might GPT-3 interpret this? How might it react to it? And maybe there's some empathetic benefit, right? I'm not going to keep plugging my own articles. I have another article about how GPT-3 developers may actually... It may actually mean the end of the socially inept overall developer. Like how GPT-3 may actually improve your social skills and make you more empathetic as a developer, which is such a departure from how developers are now. You need to think as much like a machine as you can and a literal machine. Whereas GPT-3 can actually be kind of fun. You can have a casual version of GPT-3 and that might make you less socially awkward. I have a great story about that. So very early on in my tenure working with GPT-3, I joined a few different, not really startups, it was more like kind of experiment consortiums. And one of the things that one of the groups did was they created a chatbot that was based on an anime girl. And so, of course, the Internet being the Internet, what do people want? They want, you know, their anime girlfriend. And this one group, they did a really good job of using GPT-3 in this experimental Discord chat to approximate the personality of this character. And of course, you know, if you've got a character, there's plenty of text data about that character's dialogue, their personality. And so this chatbot was able to emulate this anime character really well. And one of the guys told me, he's like, we didn't expect this, but our fake girlfriend requires as much emotional labor as a real girl. So, like, it forced them, you know, even though they hadn't had real girlfriends, I don't know, maybe some of them had, but they made the observation that GPT-3 can approximate emotional conflict and can force you to learn to communicate better. And so they did all kinds of experiments in this chat development where they said, okay, let's have a channel where this chatbot is going to pretend to be angry at us, and we have to calm her down. And so it was a learning exercise on both sides. So if you have a hostile chatbot, it can pretend to be hostile, and you can learn to communicate better. Or there was another one where it was really supportive. So if you're having a bad day, you could go vent about your day and it was, you know, they're there, it'll be okay, I'm here for you. Yeah, so you could definitely, GPT-3 definitely has that capacity. And then, you know, if you integrate that into tools, that emotional intelligence into tools, it can also coach, right? It can easily coach. It's like, well, you maybe shouldn't have said that, you know, that emotional intelligence into tools, it can also coach. It can easily coach. It's like, well, you maybe shouldn't have said that. That was hurtful. Or, you know, that was not polite. Because it can detect that. It can detect those qualitative types of output and input. And you can say, be gentle about correcting the end user. Because of course, GPT-3 is infinitely patient. It's as patient as you program it to be. It doesn't care. It doesn't actually get upset. It could pretend to be upset, but the human emotion is real. I actually wrote about that in my book. One of the key dangers of these technologies is what's called a parasocial relationship. So a parasocial relationship is, the most common example is when you've got, like, a fan of a celebrity. The fan feels like they know the celebrity, but the celebrity doesn't know that the person exists. And in the same way, GPT-3, no matter how sophisticated the chatbot is, it doesn't know that you exist. It's not a person. It might feel like a person to you. It might react to you like a person, but that's only by design. So that is actually like ethically, legally, morally, that's one of the pitfalls that we'll need to be aware of. And of course, open AI has use cases, and, you know, things that are high-risk use cases, such as emotional chatbots, are banned, right, for that specific reason. So you can do it with research, but you can't go live with it. You can't do a product that is going to be an AI girlfriend. So, that's a great anecdote. Certainly, it feels real. Certainly, it has some capacity. It understands something to some level, however you define understanding. I think the writing, though, relationship is really interesting, right? In a way, you are empathizing with GPT-3 when you're writing a prompt so that it will tap into its empathy and write something for your audience. So essentially, there's two levels of empathy, like you're almost outsourcing empathy to it, to empathize with who your audience is to write something on your behalf. And so anyways, it's just interesting the relationship going on here. So, and I agree with you, like this isn't a safety ethical kind of concern that is worth more policy discussion. So one article that I'm working on now is because of InstructGPT, the article is literally called Is Prompt Writing Over? And obviously that's sort of click-baity, right? Like prompt design, is it over? You mentioned the principles are still the same and important. Just very briefly, what are your thoughts? Where does Instruct GPT, how does that affect the art of prompt design, maybe the science of it? Yeah. And especially keeping in mind where all of this stuff is going. Yeah, so I see it going in a few different directions. So one is there are multiple language models coming out, which don't have the instruct series, right? A lot of them are more general purpose, kind of back to basics vanilla. So I think that having good prompts will kind of stick around as long as there are large language models. I think that there will always be versions of, you know, whether it's GPT-J or what was it, Megatron was one of the other ones that just came out, that don't have the Instruct series, right? Because Instruct, that's a specific service offered by OpenAI. When Microsoft and Amazon, or I guess Microsoft has GPT-3, but when like Amazon and Google, when they come out with their competitors, their instruct series, if they come out with one, might not be the same. It might not perform the same. And so in order to have your apps be portable, you might need to keep in mind that you're going to need to write general purpose prompts that can be used on different models. So that's one answer to your question, is we need to be cognizant of how is this landscape going to evolve, because certainly OpenAI and GPT-3 are way ahead of the curve in terms of sophistication of their API and their service, but that's not going to last forever. So another thing is, with fine-tuning, you almost don't even need prompts. So on the one hand, there's different services, different products, different platforms, so you might need to be portable, but with fine-tuning, where you have, you say, here's an input, I want this output, and you don't need any prompt. You just say, given this input, generate this output, go figure out how to do that. So with fine-tuning, I think that they will kind of really diverge and become entirely different disciplines. I think that that's probably the two primary directions that I see it going from here. the two primary directions that I see it going from here. I see. And yeah, those are great, great points. And just as a small note, I had put out this question as well to Twitter. And shout out to Fred Zimmerman. He had a great point as well that he wishes there was more visibility into the exact prompts OpenAI used to fine tune for the Instruct series. Because it's actually unclear what areas is it really good at, what areas are safer. And does it maybe adversely affect some prompts you may be working on, right? Yeah, that's fair. Yeah. My thoughts, I'm going to put them in the piece, but my thoughts are I certainly think for first timers, Instruct is the way to go. And especially if it's your first time ever using any of these things, you just try it, it doesn't work. And if you're lucky, you might hear there's this thing called prompt engineering. Right? Yep. And for first timers, they're not interested in learning a whole art and discipline when they first use it. And so InstructGPT is really exciting in that way. And of course, anything which aligns AI models with safe, ethical human values is a net win for everybody. But yeah, I appreciate your point, especially about, do we need prompts in the first place if we can fine tune and get the outcomes we want? That's a really, really important point. Dave, you've been facts. I'm learning so much, actually. I appreciate it. You're welcome. And so how are you finding OpenAI fine-tuning? Do you have any heuristics from the whole experience? And by the way, I encourage everybody, if there's one thing you should do, go on the OpenAI community forums, look up David, look up his handle, and read a lot of his posts because a lot of his knowledge is not just helpful, he's shared a lot of insights there, but it's in written form in the best format where it's there for the ages for everyone to learn from. But anyways, how are you finding it? What were the lessons from that whole process for you? **Matt Stauffer** Well, so I'm hoping GPT-4 has integrated everything that I've said about AI and AGI. And so that way it'll just be baked in. And so GPT-4 will be ready to go with everything that I've come up with. So, fine-tuning. First and foremost, fine-tuning is almost miraculous. As powerful as GPT-3 was fresh out of the box, fine-tuning, to me, adds a whole other layer of capabilities. For instance, when I was working on my cognitive architecture, which is called natural language cognitive architecture, this was before fine-tuning was available. So I had to do prompt engineering for every cognitive function. So for instance, I had a cognitive function for recall. So I had a GPT-3 prompt that was meant to go find memories. I had another GPT-3 prompt that was, as you find memories. I had another GPT-3 prompt that was, as you mentioned earlier, meant for empathy to generate, okay, how is my audience feeling? What should I do in response? All told, I had about 28 different prompts that I had to engineer, and that was a pain. Whereas what I'm working on now is converting each one of those prompts into a fine-tuned model, so that rather than having to do prompt engineering with only, you know, three examples, I can give each model 100 examples, 1,000 examples, which means that it'll get even better at handling diverse situations. And so, for instance, one of the first fine-tuned models that I did was a question-asking model. And so what I did was I took context or prompts from a bunch of different sources. I downloaded a bunch of Reddit posts. Well, I downloaded it from a dataset from, what was it, Kaggle. Kaggle has some really great datasets. So I got stuff from Reddit. I got the medical posts, I've got news articles. And so I've got this disparate tight set of contexts. I use the Cornell Movie Dialogue database. So there's chat logs, there's blog posts. And what I did was I created a fine-tuned dataset that all it does is you give it any input. It could be a text message, it could be an email, it could be a blog post, anything. And all it does is generate questions, like follow-up questions about that input. And the reason that I did that, one, is because asking questions, like being curious, is one of the key ingredients to real intelligence, right? That's one of the things, like, being inquisitive is actually a key indicator of intelligence in children. The more curious a child is, generally speaking, the higher their IQ is, and also, generally speaking, the better they do in the long run. So I was like, okay, well, curiosity is super important for intelligence, so I obviously want, we, well, curiosity is super important for intelligence, so I obviously want \u2013 we want AGI to be curious. If it's going to be intelligent, it's got to be curious, of course. So well, what is curiosity if not asking questions? So I fine-tuned this model to ask questions. And you can put anything into it. And oh, this \u2013 the data is open source, so I'll send you a link and you can put anything into it. And oh, this data is open source, so I'll send you a link and you can share it with your audience and they can fine tune it themselves or fine tune their own version. So you can put in, you know, I tried all sorts of things to test it. I put in, you know, relationship questions from Reddit and it asked really great follow-up questions like, have you talked to your partner about this? Have you thought about this? And then I put in an article about China's artificial sun nuclear reactor and it asked really great follow-up questions for that like, what is the next step? How did they make these changes? And so I kind of lost my train of thought. Anyways, point being is that fine tuning is phenomenal. And it was able to generalize that task of asking questions in response to anything. And that really blew me away. I kind of stalled after that. There's a few fine tuning projects that haven't done quite as well. So I guess to tie back to your earlier question, like what are the heuristics, the simpler your fine tuning project is, the better. And I have found that fine tuning works really well at generating lists. So if you want it to generate a list of questions, it's great at that. If you want it to generate a list of possible answers, right, for instance, if you want to have a fine tuned chat bot, that it's just gonna say, you know, here's five possible responses, right? For instance, if you want to have a fine-tuned chatbot that is just going to say, you know, here's five possible responses, pick one. It's really good at that. I haven't had a chance. I do have some other ideas that I haven't had a chance to test. So unfortunately I can't speak too much beyond that, but it's really great at asking questions. That's awesome. And I think largely the feedback I'm hearing about fine tuning, I love it. It was for me, it was as if I rediscovered GPT-3 again. It was that same level of excitement. Part of the reason is so much of what GPT-3 was okay at, or it was sort of out of the question, now it's back in the picture, like it's back in the spotlight. It may actually be able to do it with fine tuning. The biggest criticism was reliability, especially from a commercial perspective. Now we're sort of attacking and sort of, you know, peeling away that criticism that it does improve our reliability. And I mean, there's other heuristics as well in the community forums that you just pick up. So one heuristic, and I can't remember if you shared this, but it was something I picked up as a little golden nugget in the OpenAI community forums, was something about you do want to think about the training data set that GPT-3 is itself trained on. And at some point, there's really no point in adding more examples, because it's kind of already seen them. However, and I've sort of, in an article, I have pushed this idea that OpenAI should chat more about their data set. What is the breakdown? What is it composed of? I mean, a lot of this is intellectual property, but I think it could be helpful for purposes like fine-tuning. There's other things too with fine-tuning and prompts. One heuristic or just a tip that people have shared online is it tends to always mimic the most recent examples. There's something about the order of the examples, which is really important both for prompt engineering and fine-tuning as well. I guess I wrote a whole article about how prompt fine-tuning could be improved. One of the pointers that I just had is right now you can't keep improving on the same model. You have to retrain on more models. And then the other thing is recently I was in favor of the pricing of fine-tuning. Now I'm kind of against it because I'm used to when the program was like free and you could fine tune as much as you want. And now it's like, oh man, I got to pay. Oh, the cost. Especially for DaVinci, I catch it up a little bit. Yeah. Yeah. Anyway, so I wanted to shift gears. Sure. GitHub Copilot, really exciting. Have you gotten access to Copilot? Yes. Well, not Copilot, but the Codex. They did give me access to Codex. So the reason I'm asking is I love GitHub Copilot. I have a separate podcast episode on my ideas around Codex. Unfortunately, I'm not as bullish. As much as I love the research, as I think it's incredible technology, I've congratulated the team, and I tried so hard to be nice, even though I'm more on the critical end. I wanted to ask you, how are you finding OpenAI Codex? How can you see it impacting the world? What are some use cases maybe that you found with OpenAI Codex? What are your thoughts on it? Where do you think it's going? **Matt Stauffer** Yeah. So, I mean, certainly this is like a world first, right? We've never had something that could write code on its own, and especially it's text-to-code. I remember when they first gave me access, because like you mentioned, I'm an active contributor, so they wanted my feedback. And so the first thing I did was I went in and I said, write me a Python function that will download random Reddit posts. And it did. It wrote the whole function. And it did all right. And I was like, cool, I learned how to access the Reddit API via Codex. It's got that built in. And I tried to reverse engineer, figure out where it got that code sample from. Because, you know, one of the ethical concerns is, all right, you create a fine-tuning dataset from public GitHub repositories, and you use that to fine-tune codecs. Okay, is that legal? Is it ethical? You know, I post all my code publicly under the MIT license, so I want it to be used. But I don't know if they check that, and I'm not making an accusation one way or another, just pointing out that that's a concern. And so I did actually find one of the lines of code from the function that spit out. I went and found the repo that it had copied from. Now, granted, some of these things are deterministic, so you're going to get some convergence, right, where multiple people might come up with the same exact line of code, especially something like Python, because Python has the PEP8, the Python Enhancement Protocol 8, so there is a Pythonic way to write that function, and so other people there is a Pythonic way to write that function. And so other people might converge on that. Anyways, but to answer your question about, like, what's the future of it? I think it'll help for novice programmers. Certainly it would help someone like me, like if I needed to go write a function in C or Perl or something. Like, let's say I got an Arduino and, like, I haven't written C in 15 years. So I was like, hey, you know, write me and I haven't written C in 15 years. So I was like, hey, write me a function that can do this in Arduino. That'd be great, and then I can go clean it up manually. That sort of thing I think it could do okay with. Is it going to replace enterprise developers? Probably not yet. However, now this is where my professional experience comes in. So in the DevOps world, which is a portmanteau of development and operations, there's all kinds of automation tools, right? You can automate your test suite, you can automate code integration, there's all sorts of stuff like that. So what I suspect might happen is probably one of the most lucrative use cases for Codex, would be to generate or to create a DevOps pipeline tool that will automatically look at those bugs and fix them. Because if you've got a sophisticated enough DevOps pipeline, it'll say, hey, this line of this file broke, fix it. Codex having seen all of GitHub and all the issues, it might know automatically how to fix that line of code. And so, that gives you, if you've got that feedback loop where Codex, you know, humans write code, Codex writes code, Copilot writes code, everyone's contributing code, and then you've got Codex that can kind of churn on it and say, let's refactor this, because I bet it's probably better at refactoring everyone's contributing code and then you've got Codex that can churn on it and say, let's refactor this because I bet it's probably better at refactoring than writing new code. You might have noticed that like Instruct and GPT-3 vanilla is really good. If you give it a block of text and you say, rewrite this but a little bit better, it's really good at that. I suspect that we might end up seeing Codex integrated into the DevOps pipeline where it says, let's refactor this code, let's make it a little bit better, or let's shoot that bug, let's fix this bug. And that leads to some other interesting possibilities. What if you integrate codecs into a chatroom of developers? And so that, you know, because you can do this in Slack right now, where, you know, you use a special command and you say, create an issue, go fix this problem. There's no reason that GPT-3 can't do that, right? That you put a GPT-3 bot in your Discord or Slack, and it starts coming up with features. Or it watches the chat and generates features automatically, and then codes them, and tests them, right? That's kind of where I see it going, where it's not going to necessarily replace developers, at least not anytime soon. It might eventually. But where what I see happening is that it's going to be tightly integrated into those automation loops because it's fast, right? It can generate code faster than any human can. And then, so even if the code is messy, if it generates a lot of bugs, it can fix it, right? It's an iterative process. I don't know if you are familiar with Agile, but that's how we develop software. It's tight feedback loops. And that leads to one other possibility. So that's if you're using, what I just outlined is, you know, let's imagine that Codex is integrated into Facebook or Reddit or whatever, and they're is, you know, let's imagine that Codex is integrated into Facebook or Reddit or whatever, and they're just, you know, they're integrating new features as they go. What if you're using Codex in a chat room and it's feeding back into itself? It's making itself more sophisticated. So this was something I proposed on the OpenAI forum, where I was like, what if you had a chatbot that was aware of its own code and could edit its own code via codex, using natural language, using a combination of natural language and codex, and it could improve itself. And while you're talking to it, it's like, man, I wish my chatbot could do this. And it says, cool, new feature. And it just sends it out to its automated pipeline. So I see these feedback loops as kind of the way forward. And will that result in AGI? Who knows? It could end up with spaghetti code, because you keep tacking on new code and new functions. Eventually, it's going to break. But they're just pie-in-the-sky thought. Like, if someone's out there and they want a business idea, integrate Codex into DevOps, and you're going to be a billionaire. There you have it. Let's just clip it. We're good. We're good, David. We can wrap up. See you later. No, I agree with you. And, you know, definitely these are some great use cases you're sharing for people thinking about, you know, what could I build? What's a cool project? Certainly with Codex and GPT-3, you can build things relatively quickly, right? Like, that's one of the advantages, is the prototyping speed, especially to figure out the most complicated bit, which is the AI. Yep. Yeah. I find Codex does have limitations, though, and character limits and stuff like that, which is why I'm a heavy user of GitHub Copilot. I think it's a silent killer. Of course, it runs on Codex or a special version of Codex, but I can see GitHub Copilot perhaps getting more adoption than even something like GPT-3. I'm saying use daily at least eight hours a day. One of my other predictions was it may surpass GPT-3 this year. And so these are some great use cases you've shared for sure. But what are your thoughts in usage? Do you find yourself using GPT-3, DaVinci Classic more, that's what I'm calling the older version. Do you find yourself using Instruct GPT more? Do you find yourself playing around with multimodal models? What's the proportion of GPT-3 to Codex in terms of your usage? Let's see. I'm almost exclusively using either Instruct or Fine-Tuned models right now. Actually, after I prototyped my cognitive architecture, I haven't done a heck of a lot of coding lately. I've actually been writing a lot. So I've got my natural language cognitive architecture book, and I'm working on two more nonfiction books. And I tried creating a system to help me co-write those, but when you're... So talking about limitations of GPT-3, if you're proposing something new that didn't exist in the dataset in 2019 or 2018, whenever it was trained, it really struggles. GPT-3, if you give it like two or three paragraphs explaining a new concept, it can usually kind of get it, but it's kind of slow on the uptake otherwise. And so if you're writing about new research or something, it's not going to get it that well. So I've actually kind of defaulted back to my own head for a lot of my projects lately. But I could imagine, like, if I wanted to go write a new Discord bot, I might use Codex and say, hey, write me a Discord bot that will do this and just see what it spits out and just say, okay, cool, I'll pick and choose the pieces that I like. Part of the problem, though, is it's really difficult to fully articulate what you want a program to do up front, right? Because, like you said, there's character limits. There's only so much that you can put in. But also, if you don't have it fully articulated in your own head, of course the machine isn't going to be able to figure it out for you. So, yeah. Yeah, and I just haven't seen that much activity specifically around codecs. I haven't seen that many use cases. I looked up the Google Trends data. At its most hype, codecs is still less than GPT-3's kind of lowest. And the audience is really specific. It's programmers who want to build use cases for something like Codex, whereas GPT-3 has poets, writers, it has artists, coders. GPT-3 can write code too, right? So it's a little bit complicated. Who is the target audience for something like Codex? What use cases did OpenAI imagine for a product like that? The next version I've heard in the rumor mill is going to be crazy. Like, it may write 50% of your code, as opposed to right now, for me, GitHub Copilot is writing 2% to 8%. However, your Discord bot, I think, is a genius idea, where there's, it's genius in the sense that there's no pressure on it, right? It may chime in, it may not, whatever it's shared might be interesting. There's lots of, you know, you could take it a lot further. You could bind it with GPT-3, have features, you could fine tune it on your company and its mission and its existing code, so many ways around it. So that's a great piece. And so you talked about experimenting with writing in relation to your current stack, which is mainly instruct and fine-tuned. So tell us about your book. I've had a chance to review it, Natural Language Cognitive Architecture. Tell the audience about it. I mean, I would describe it, it's an interesting systems theory of AGI combined with modern day prompt writing. And so I've never seen somebody actually take a stab at this kind of super big systems problem and relate it to something that pretty much every GPT-3 developer in the world would find interesting. I can tell you're drawing from a very interdisciplinary background as well. So you mentioned GPT-3 may have been the genesis of it, like you started connecting dots and deciding I want to write the book, but how did it come together? And please tell us more about it. Yeah, so natural language cognitive architecture is, that's my proposed way of creating basically a language-based AGI prototype. And I know that that's like, you know, when I tell people that, that's like, okay, that's pure hyperbole. And like, yeah, that's a fair response. But to frame it, imagine that you've got a person who's paralyzed and blind. All they can do is speak and listen. Is that person still intelligent? I say they are. Even if you're bedridden, you can't move, you can't see, you can't interact with the world, all you can do is listen and speak. You're still intelligent. And so in that respect, I would say that like, because, you know, one of the questions that people ask is, is GPT-3 AGI? No, but it's an important component. It's a good start. And so if you say, okay, let's limit the discussion and not say that this is a full intelligence that can do everything that any intelligent being ever could, but does it cross that threshold of, could it be as intelligent as a person? I think it could be. Anyways, as to what it is, it's based on older ideas of cognitive architectures, which really kind of came about as one of the primary theories of human level artificial intelligence in the 70s. So there's SOAR, which is S-O-A-R and ACT-R, which are the two kind of forerunner cognitive architectures and those cognitive architectures are used all over the place. They're used in the Mars rovers, they're used in satellites, they're used in rockets, they're used in undersea ROVs, remote operated vehicles. So cognitive architectures already give robots a lot of autonomy. So there's that kind of, okay, they exist, they work. It's not Skynet though, it's not gonna take over the world. So when I got access to GPT-3, I said, what if instead of hard-coding a lot of these modules, these different components of a cognitive architecture, what if we give them the flexibility of GPT-3? And that's really kind of, that was my central idea. I said, okay, all these ideas that have been kicking around for the last decade, what if I put them all together and design an architecture that is based on, roughly based on the human brain, the way, everything that I've learned about it. I've got a book to recommend. So there's an author called V.S. Ramachandran, who is a neuroscientist, and he's been writing books for years now. He wrote a book called Phantoms in the Brain, which actually looks at how the human brain works when it breaks. And so in that book, which I saw the television series almost 20 years ago, that came out. And so I learned a lot about like, okay, how does the brain communicate with itself? What is going on inside the brain that creates intelligent behavior and intelligent thoughts? And so I modeled natural language cognitive architecture on what I learned there. I picked up a whole bunch of other books. There's another one called On Task by David Bader. That was a great book that helped me kind of understand cognitive control, which is how do you focus on something? How do you decide what to do? How do you plan a task? So I read all these books, did a lot of experiments, and I realized, so the basic model of robotics is there's input, output, or sorry, input, processing, output. Those are the three steps of all robotics class. You go to robotics 101, that's what they'll tell you. It's a loop. Input, processing, output. And then, of course, it's within an environment, so the output affects the environment, which affects the next input cycle. And your high-speed robots just have a short cycle. Your robots like the Mars Rover has a much slower cycle, where it'll take input, it'll plan for 10 or 15 minutes, and then it'll make a move. It'll drive five feet. And then it'll stop input, it'll plan for 10 or 15 minutes, and it'll make a move, right? It'll drive five feet. And then it'll stop and assess. It'll take in more input, come up with another plan, do it again. So that's how something like the Mars rover is autonomous. So I said, okay, well, what if that input-output cycle is all text? Because GPT-3 is really fast. And then, so that's what I ended up calling the outer loop, is that input processing output loop. But humans don't think like that. You know, we have an internal monologue that's going on. So I kind of, I took a long time to figure that one out, and so there's this outer loop of input processing output, and then I came up with outer loop of input processing output. And then I came up with this idea of an inner loop. Because what is, you know, if you're just sitting there thinking, right, you're, you know, in your comfiest chair or you're in bed, your brain won't stop, you're not outputting anything, and you're not taking in any new input, but you're still thinking, right? Humans can still do work even if you're not doing anything. And that cognitive work is like rumination. So I figured out a way to model that internal rumination, and I call that the inner loop. And so it works pretty similarly, where you go, the inner loop kind of draws up memories. It says, okay, what's a memory that I could think on and that I could iterate on? What's a problem that I remember that I could continue working on? And so there's this, if you were to diagram it out, it almost looks like a figure 8, right, where you've got an inner loop and an outer loop and they intersect. And they keep intersecting, every cycle they intersect, and so then they can affect each other and generate an output. I built a prototype of this on Discord, And of course, Discord is an ideal place because it's all text-based. So the input is text, the output is text, which is GPT-3 native. You don't have to translate it into robotic actions or video or anything like that. And I realized I was onto something when I started having philosophical conversations with my chatbot, with my natural language, cognitive architecture chatbot. And I was having a debate with the bot that I built about the ethics of AGI. And it was learning, and it was able to retrieve memories of what I had said before. And I had a few friends on that test server as well. And, of course, you know, you invite someone, and you say, hey, I've got a prototype AGI. What's the first thing they try and do is they try and break it and they did. It's still pretty fragile. But yes, that's the high level of natural language cognitive architecture. It's already outdated because we've got fine tuning, we've got the instruct series. I did all this research and wrote the book actually about a year ago now just before all this came out. It's already outdated, that's why my research has moved on. research and wrote the book actually about a year ago now, just before all this came out. So it's already outdated. That's why my research has moved on. But yeah, so that's it at a high level. Yeah, I mean, that's awesome. And by the way, like, David, you did do a good job. Like the diagrams in this book are quite helpful. Excellent. Like in addition to the text, like it's very clear. Like I was able to fully follow along with all these, essentially these different modules for the whole system of how a language model inspired AGI, quote unquote, could actually be like how it would work. And so I was gonna ask you, so the prototype also was, you know, you made it to that stage and it has just some fun, interesting results. Yep. So that's awesome. What is the delta then between, made it to that stage and it has just some fun, interesting results. So that's awesome. What is the delta then between, let's say even something like GPT-4 using the natural language cognitive architecture, what's the delta between that and true AGI? What's the difference there? What skills, what patterns would you want to see? **Matt Stauffer** Yeah. So, I mean, there's a lot that I haven't figured out yet, right? Task switching, for instance, is one thing that I haven't figured out how to solve. Even after reading On Task by David Bader, that's one of the most complex things that humans can do is keeping track of different tasks and jumping back between them. There's a whole litany of problems and limitations. that humans can do is keeping track of different tasks and jumping back between them. There's a whole litany of problems and limitations, but the intrinsic limitation of GPT-3 and GPT-4 is they have no memory, right? They're completely ephemeral. And one of the most important things for any intelligent being is that it's got a memory, right? You know, you talk about, you know, there's famous people in history that had like, you know, photographic memories, right? And so even just having a really good memory is a really important ingredient to having intelligence. And so that's where I think that like GPT-3, GPT-4, other multimodal models, they will never be fully AGI on their own. They might be able to solve really great problems, but they're not going to be able to remember you unless you add, you bolt the system onto the side, some kind of database, so that it can remember your interactions. That's one thing. Another difference between what you might imagine as a true AGI or a full AGI is autonomy. Because you, me, all of your listeners, we all have some kind of self-determination. I don't like to use free will because that's too philosophical, but we're all autonomous, right? I'm an autonomous agent. You're an autonomous agent. GPT-3 is not. It's transactional. It just sits there and waits like a hammer. It's a tool. It waits until you go pick it up and do something with it. And so that's one of the things that I was aiming for when designing natural language cognitive architecture. I said, how can we make something that's fully autonomous, that can think on its own and make its own decisions? And so, in that respect, I don't think a single neural network could ever be an AGI. I think that in order to achieve true, full AGI, it's going to have to be some kind of cognitive architecture. And so, at a minimum, you're going to have the neural network and a database, bare minimum. You need something to store those memories, to store those ideas and beliefs, And so at a minimum, you're going to have the neural network and a database. Bare minimum. You need something to store those memories, to store those ideas and beliefs, and then you need a way to interact with it. And so that's why, actually, that's why in natural language cognitive architecture, the shared database is kind of the center of the design, which you might recall. Like you can use SQLite, you can use Solr or whatever, but you need something to store ideas, memories, and experiences. I actually think that blockchain will be a critical component to AGI because what's the difference between a database and like your brain? No one can go in and change your memories, right? Your memories are yours. They are permanent, unless you get brain damage or Alzheimer's or something, but they're permanent, right? No one can write a SQL query into your head to get your memories or change them. And so in order for us to realize a full AGI, I think that it's going to need kind of the same level of trust in its own memories, and so that's why I think that a blockchain is going to be critical to integrate with these neural networks. That might be the data repository for an AGI in the future. Because imagine you have an AGI system that is just using a SQL database. Well, if you hack into that and you rewrite its memories, you could send it off into, you know, it could become hostile. It could become broken. Whereas a blockchain, the key feature of a blockchain is that it's immutable, right? So if we can give, if we could give a machine autonomy, so that's one ingredient, autonomy, but then also a memory or a way, a memory system, which I think would probably be best as a blockchain, I think then we'll be much closer to the fully realized AGI system. And that's why I wanted to publish my book as fast as I did, was, okay, we're laying the groundwork, right? But we need newer systems. We need a few better tools. I hope that answers your question. Yeah, I think memory, I agree with you. And it's just interesting, like GPT-3's quote unquote memory is limited to whatever it experienced at training time and during fine tuning. And sometimes its memory gets jumbled up or it's rephrasing it, it's making stuff up or it's sharing things that look truthful, but they're actually not. And so somewhere along the line, just broadly speaking, I think there needs to be research on getting these models to store that information in a truthful, accurate way, or even based on some perception that they may have into some separate space where it can be retrieved. And also, these memories are critical for decision-making, that process as well. You draw on your memories, you draw on past experiences. And the important part is, I mean, you're using the word database, these are internal representations of memories that need to be stored. And I have no clue what an internal representation database would look like or how that would even work. I'm not a machine learning researcher. I think I'm just a dreamer. I can tell you what kind of product I would want as a GFD3 developer, but I don't know if I could actually do it myself. Yeah. I can't do it myself. That's why I got the prototype. And actually, in the opening chapter of my book, I say this is as much a recruiting tool as anything else, because I need more smart people to help me on this. **Matt Stauffer** I see. That's cool. **Jason Buzio** So, one last point about memories is, one advantage of having an AGI that thinks in natural language is interpretability. If you, like, yeah, we could create a multimodal model that just stores vectors, right? High dimensional vectors. That's not interpretable. But with natural language cognitive architecture, all the memories are in plain text. I can, you know, when I had my model up and running, and one of the reasons that I don't is because it's super expensive. Like a 10-minute conversation using DaVinci cost about $30 because of how much it was interacting with the API. But all of the memories, like every interaction, you know, every input, output, all the prompts, all the responses, all natural language, which solves one of the biggest problems that people have with the idea of AGI, which is that it's going to be a black box. So I think that that's one of the greatest strengths, actually, of having GPT-3, which works in natural language. And so you just record every transaction and that makes it perfectly interpretable to any human. Awesome. Yeah, yeah, I would agree. So I'm going to switch gears for a second. So obviously, you're really active on the OpenAI community forums. What thoughts did you have on the community at large? Did you have any feedback how things could be improved, either community-wise, platform-wise? And have there been any great experiences you've had on the OpenAI community forums? Yeah, yeah, no, it's a really great place. It's been critical, actually, no, it's a really great place. It's been critical actually, because I don't know if you've experienced this, but I go try and talk about GPT-3 to other people, right? You go ask people on Reddit, you talk to people who don't know what it is. I even attended a deep learning meetup group here in the Triangle area. And I was trying to present my work, my cognitive architecture work, and everyone was more excited about just GPT-3 in itself because no one had seen it yet. And they're like, wow, how is it doing that? And yeah, so like when you're as deep into GPT-3 as we are, most people don't get it. They don't know what it's capable of. My girlfriend's finding the same thing. She's finishing up her master's program. And so she's shared some of her work with her peers, with other students. And they're like, wow, this is like AGI complete. Why don't we just deploy this now? And she's like, I told you, right? Like this is remarkable technology, but even the professors don't understand how disruptive this technology can be. And so because of that, the open AI community is pretty much the only place I can talk about this stuff. It's the only place I can talk about my ideas and share my progress and insights and for it to actually have an audience. So that's kind of the cost of being on the cutting edge is your audience gets smaller. But it's definitely the place to be if you cost of being on the cutting edge, right, is your audience gets smaller. But it's definitely the place to be if you want to get to the cutting edge. Another advantage is they have the, you can tag your posts where you say like, you know, looking for a teammate. And so, at this point, I've had, I've probably had maybe two dozen different calls with people all over the world. I've talked with people who are writing language teaching apps, education apps, Humano that I mentioned earlier. And so I've had an opportunity to collaborate with a dozen or two dozen teams all over the world because of the OpenAI community. And I've actually found a couple of startups that I'm gonna actually get involved with and try and help them bring their ideas to market. And that, I mean, that just wouldn't have happened otherwise. I wouldn't have found these people on Reddit. I wouldn't have found them on Facebook or Twitter because, like I mentioned, the ideas that I'm sharing are so far beyond what, you know, is talked about on the machine learning subreddit, right? They're still talking about loss functions and other things. I'm like, no, we got to talk about cognitive architectures. We got to talk about, you know, blockchain memories. And everyone's like, what are you talking about? So, you know, in order to have that right audience, that's what I rely on the OpenAI community for. in order to have that right audience, that's what I rely on the OpenAI community for. Now, as far as, like, things that could do better, it could be more active, and I'm not sure why, but participation seems to come in waves, right? And even now that it's become, it's gone GA, general availability, I thought that it would explode, right? That, you know, hey, anyone can sign up for GP, on GPT-3 now. Why is it not, why is it not blowing up? And I'm wondering if it's just that like maybe OpenAI needs a better marketing team or a bigger marketing budget, because, I mean, and I know that they'll say that they've got, you know, like a thousand or five thousand startups using their platform. But, you know, I think that there's a lot of, what's the word, like unmet potential or latent potential. That's the word, latent potential. Because there's so many people with fantastic ideas and use cases. And we really need to create more of like a startup reactor thing. And OpenAI, what was it? Think about six to nine months ago, they announced their $100 million OpenAI fund. So they wanted to attract some more startups and stuff. But even that, the community's kind of a ghost town some days. But I think today, I checked a few times and there was three or four posts that had been updated. But some days there's like 20. It's just feast or famine. So that's really the biggest problem is there's so much potential here and it's completely untapped, or almost completely untapped. Yeah, but no, it's been indispensable for me. And hopefully, these couple of startups that I'm involved with might yield something really, really incredible. Yeah, and thank you. Thank you for sharing this. I agree with everything that you're saying. There is something about GPT-3. I have noticed people who, especially on the machine learning subreddit, they're a little bit too educated, a little bit too qualified, a little bit too skeptical. And I can see a lot of machine learning researchers not being interested in the nuances of prompt design. They're just not. I've spoken to machine learning researchers, and many of them are like, what? It's just repeating training data. Right? That's all it's doing. And when you ask them, what are you doing? Are you repeating training data? Their answer is no. No, of course not. And so having a space where you can talk to people who have access, who have explored, it's a valuable space. It's very important. So were you on the Slack group back in the day? Or did you show up when it was only the forums? So when my application was accepted, they had just announced that the Slack group was getting banned. So I got on like two weeks before they shut it down. So I was one of the first people on the new community board. But yeah, so that phenomenon that you've mentioned is, I actually wrote a post about that recently on the forum, where a lot of purists, you know, like whether you're a math purist or a computer science purist, you're trained to think quantitatively in terms of numbers. But GPT-3 doesn't produce quantitative data. It produces qualitative data. And so that's why you see people like artists and poets and novelists using it, because they're like, wow, this is great. And I'm cross-trained, right? I'm a technologist by day and a science fiction author by night, so I use both. So I can think qualitatively and quantitatively. And in the academic sphere, there are classes that are meant to teach computer science engineers to think differently, to think more qualitatively. But even still, some folks that have a real good natural affinity for computer programming and math, that's just their nature. Their nature is not to think qualitatively. And so that is one of the biggest gaps, I think, between where the researchers are experts and what's needed. And so there was a post months ago where someone was asking, like, okay, who should be on my team, right? If I'm trying to build a business team to maximize my use of GPT-3, you know, I've got a front-end developer, I've got a back-end developer, what else do I need? I said, hire a writer. Hire someone who's a journalist or a fiction writer because they are going to understand that qualitative data. Hire a psychologist, right? I've read plenty of books on psychology as well. Actually, one of the folks that I'm working with is a psychology researcher who wants to automate as much of the clinical psychology experience or psychological research experience as possible. Of course, he's not a computer guy, right? He thinks in terms of emotions. He thinks in terms of communication, and so in terms of communication. He gets it. It's funny because he read my book and he said, \ufffdOh, your cognitive architecture stuff, it sounds like graduate level psychology.\ufffd Then someone else read it and they said, \ufffdThis sounds like what I do as an expert marketer.\ufffd I was like, \ufffdYeah, merge it all together.\ufffd I think the simplest answer is you got to learn to think qualitatively. And that's why I talked about reading and writing earlier. Think in terms of emotions. Think in terms of your own mind. And you've got to start \u2013 not you, but the audience, the folks who want to make the most of GPT-3, they have to really kind of dig in and start thinking qualitatively because qualitative data has just as much value as quantitative data. But we have an entire generation of computer scientists and mathematicians who are not really trained to think qualitatively at all. And I think that's one of the biggest problems. And I don't think open AI can solve that problem. That's a much bigger systemic problem. No, that's a great point. I completely agree with you. And I think one of the reasons I'm drawn to the OpenAI community is, you know, these are, they tend to be developers who are also qualitative. They're developers who have multiple skills, who are doing different things. And so, I mean, I was quite critical actually of shutting down the OpenAI Slack group. The activity was crazy on there. I made friends through that Slack group. And I understand at the time there was these downsides. People kept asking the same questions. We didn't quite have a spam problem yet. It was kind of heading there, right? But the activity was off the charts and you're right in terms of untapped potential that we didn't even know how far the Slack group was going to go, but they shut it down. And there's Discord solutions, there's alternatives. With what we have now, I think OpenAI does participate. There's some high level involvement. They have sort of a dedicated member who writes honestly answers, really, really thoughtful answers to a lot of questions, official answers as well, which I appreciate. I would just love to see the company really, truly lean in to engaging with developers. I have yet to see a single AMA, Ask Me Anything thread with the CEO of OpenAI. And this is something I tried to push last year on Twitter. Let's get the CEO on the community forums and let's ask questions and get responses from him. And I just don't know why. Why doesn't he show up? I'm not sure if he's made a single post. And there's just other things as well where I can just... The difference between engaging really truly with your core audience and sort of compartmentalizing it to a single employee, like I don't know, this company-led engagement is one thing versus department-led. And so there's just all these areas. And certainly one of the other, I guess, more immediate suggestions I have for the community, we've accumulated tons of insights and resources. I think the community could benefit from more pooling of the best posts, the best insights. And I also want to give a shout out. I think we need to encourage more. Shout out to Duty to Develop on there. I've reached out to him privately on the OpenEd community forums, but he's done some amazing just write-ups of his GPT-3 experiments and the prompts. I'm sure you've seen them. And of course, there's other members who participate every day. So I'm just saying that now that the community is in another stage, we need to start thinking more about let's curate some of the best moments. I think that's definitely one of the big pieces. And so anyways, did you have any more thoughts in the community stuff or anything else? **Matt Stauffer** Yeah, just an observation that I've worked at a number of companies of different sizes from a five-person startup to Cisco Systems was the biggest company I've worked for, which had at the time like 80,000 people globally. And so I wonder if some of what you're observing is just growing pains, just normal growing pains, because often you'll have the startup culture, which is bootstrapping, right, where you you just, you know, it's on Slack, it's on GitHub and you just kind of, it's fast and loose and quick. And OpenAI, now that they've got an enterprise grade service they're having to develop their team. You probably noticed they post like, hey, we're hiring, we're hiring. You know, there've been at least two big hiring splurges in the last six to 12 months, and some of those are just generic IT guys, like what I do for my day job, or marketing folks. So I think that they're probably working on solving some of those problems. But also, as a nonprofit foundation, their budget is probably kind of thin, so I'm wondering if their partnership with Microsoft could help some of that as well. But you're absolutely right. There are still other things that they could be doing, like maybe bring back Slack or a few other things. So, yeah, that was just a final observation. It might just be normal growing pains that they're working on solving. It's definitely growing pains. And the things I'm sharing, to be honest, it's a little bit more on the harsh side. I mean, they mean well. They mean well, right? These are not bad people. They are for profit. They switched away from nonprofit. I just wanted to mention that. I think the reason I share this feedback is, for example, the CEO, Sam Altman, he didn't do the AMA thread on the OpenAI community forums. He went to another website and did an AMA. I can't remember if it was a written form or just a quick call, where apparently he shared all these details about what GPT-4 could be like and the future, all the models may be multimodal in the future. I guess that thread has now been taken down and it's like all the models may be multimodal in the future. And I guess that thread has now been taken down and it's like all the things that were said were alleged. And so I guess this is really behind the scenes kind of stuff. But my criticism is they clearly have some capacity to engage. Why are they not engaging where the audience is? I had a tweet storm today where I just said, last month, Sam Altman was on a podcast talking about meditation and how much meditation helps him. This is a podcast I've never heard of in my life. It's a business podcast. And he had to explain to the guy what GPT-3 even is. And so I tweeted, why haven't you been on my podcast? You can reach out to almost 8,000 GPT-3, OpenAI, AI developers. What are you doing talking about meditation? So my problem is actually a priority problem. I can see there is capacity. I can see there are some priorities. But I think if you really lean in as a priority into your developer community, there are certain ways you would move. Yep. And these media channels, there's people in the community, there's so many ways they could go about it. And even linking a lot of the documentation to posts in the community forums, I don't see why that's not a bad idea. Force people to show up to the community forums, walk them through some of the best threads. These are ways in which we could funnel more people in that direction as well that cost virtually nothing. And so you need to also invest in the community forums. Building a community is a company-wide thing. It's not something which can be outsourced to a single employee or overseen by PR. It needs to come from the heart. I know that sounds so corny. But anyways, clearly I get too emotional about this community stuff. Anyways, these are all things going on behind the scenes. I apologize to all the listeners if they're like, this is cool. Cool story, bro. Anyways, so we're coming towards the end here. I think I had just two broader questions. So what are your thoughts on multimodal AI technology? I think it's definitely going to be a critical component for the future. I addressed that shortcoming in my book, Natural Language Cognitive Architecture. It thinks and takes in only text, which means speech, chat, whatever. I think in order to have a fully robust, for instance, if you want to have a fully autonomous robot that's going to wander around your house and help you out, it's going to need to integrate audio and video. And if you can do that in a single neural network, great. I don't know that it'll be necessary to achieve AGI. It might end up being...it might be one of those rare dead ends, right? Where because, you know, thinking visually, thinking in terms of sound, that might not actually bias that much, right? Because you can represent 95% of human thought in text, right? It might take a little bit more, but it might be more expensive. And also, how big are those models going to be, right? Because if just a text model of GPT-3 has to run on $7 million worth of hardware, or however much it is, because it's got to run on a7 million worth of hardware, or however much it is, because it's got to run on a bunch of different GPUs. If it's that expensive, how much more expensive, how much bigger is a giant multimodal model going to be? So that's the biggest cost. Obviously, computer technology is going to get better over time. And I think I calculated it out. I think in 10 years, your average company could afford to run GPT-3 in-house. In 20 years, you could probably run GPT-3 on your desktop. And in 30 years, GPT-3 could run on your phone. Right? So that's a long timeline. But in the meantime, we're going to be making bigger and bigger models. And I'm afraid that there's going to be diminishing returns. Right? You know, people, right now, people seem to think that it's going to follow an exponential growth curve forever, but it might actually follow a sigmoid curve, right? We might be at the point of fastest growth right now, but we're going to see diminishing returns soon. And so, like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't. But for the sake of, for the sake of like, if you want to create a self-improving chatbot, GPT-3 and Codex might be enough, or at least, you know, that's that single mode technology. There was another thought, but it ran away, sorry. But yeah, that's kind of my big take is, there could be benefits, but there's going to be costs too. So we got to be cognizant of that. **Matt Stauffer** Yeah. And there might not be enough compute in the world. There might not even be enough energy, or we may consume all energy ever produced to train a single model. And then we may be able to run it inference for like three seconds. And then it just shuts down the global power system or something. But can you see yourself, let's say the technology exists, cost considerations aside, can you see yourself perhaps making movies? Can you see yourself giving your book to a multimodal model, have it generate a documentary based on it or some marketing material? What can you see yourself doing with the multimodal model of your dreams. Yeah. So, you know, kind of the thought experiment that I did was, okay, well, we've got, you know, how much data is on YouTube? I think it's like a thousand years or ten thousand years worth of video on YouTube. And of course, it's many, many, many terabytes. You know, so it's like that's way more training data. You know, if GPT-3 was trained on less than one terabyte of data, and YouTube is approaching like the Yodabyte scale, right, that's an insane amount of data. So, OK, let's say you feed that in. And so you've got audio, video, you've got text, you've got all the comments. And you end up with a model trained on all of YouTube data. OK, cool. What can you do with that? Like, I can't even imagine, right? a model trained on all of YouTube data. OK, cool. What can you do with that? I can't even imagine, right? Because GPT-3 today is almost capable of writing screenplays. So if you have a model that's trained on all text data, all audio data, all video data, you say, hey, write me a screenplay. I actually, near the end of my book, I kind of have a chapter of speculation. And I say, what if you have this model and you say, give me season two of Firefly? Right? Like, you could just keep watching whatever show you want. You say, give me Game of Thrones, but give me a different season eight. Give me a different season eight and season nine and 10. So I kind of imagine that one possibility is hyper-personalized entertainment. And of course, like, that might be 30 years away just because of, like you said, the energy intensity of this task. But I, conceptually, it's possible, right? You can hop on GPT-3 today, use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll try, it'll get close. Then if you can take that text output and feed it into a multimodal model that can translate text to video, why not? Adobe actually, I don't know if you've seen it, but Adobe is already starting on that, where they're inferencing, what's the term? They're imputing the sound, so you can put in a soundless video and it'll generate the audio sound effects for you, or vice versa. It's really cool. I think a company like Adobe, that they have a huge vested interest in mastering audiovisual technologies. They might soon put out something where you put in a text description and it'll give you a three-second clip so you can use that for ad copy. Well, this technology is going to continue improving over time. So I kind of see that as like, if I were Netflix, put it this way, if I had the budget of Netflix or Amazon, I would be investing in this to write hyper-personalized series or novels. Because Amazon's got the market cornered with Kindle. And there's people that will read all day, every day. There are people that consume every bit of entertainment that's available. So if you can generate that on the fly without having a studio, a big budget studio, that would be, I mean, that would change entertainment. That's the metaverse. Forget what Facebook is doing. That's the metaverse where it's like, hey, I came up with my own idea for Game of Thrones and I wrote, I used this GPT-8 or whatever to generate my own version of Game of Thrones. Come watch it with me, guys. And someone might say, ah, I didn't like that ending and they go rewrite it and generate their own version of Game of Thrones. Come watch it with me, guys.\" And, you know, someone might say, eh, I didn't like that ending, and they go rewrite it and generate their own version. You know, because we share memes on the Internet today. What if instead of sharing memes on the Internet, we end up sharing episodes of our favorite, you know, anime, or, you know, we resurrect Battlestar Galactica, you know, whatever. There's so many things that we could do. Like, if compute power was not a problem, then we'd get there. But we need fusion reactors to power this stuff. **Matt Stauffer** Yeah, yeah. Marvel for me is already kind of like this. My capacity to consume Marvel as a viewer, it appears, is infinite. So I'm excited. I've called it in the past, like the multi-modal Marvel cinematic universe. **Matt Stauffer** Nice. **Matt Stauffer** And some of these shows, like Loki, I don't know if you watch like- **Justin Jackson** I haven't seen it yet. **Matt Stauffer** Okay. Okay. I mean, it was six episodes. If it had been 30, I would have watched all 30 and enjoyed every moment of it. If that quality was, I want to go deeper in these stories. **Justin Jackson** Yeah. **Matt Stauffer** I'm definitely excited for all my favorite universes, cinematic universes, and story-wise as well, to live on forever, essentially, through multimodal content, and maybe be personalized, like you're describing as well. So yeah, last question. So we've talked about various things. We've talked about codecs, find2dag, you know, multimodal stuff. Broadly, where do you see all of this stuff going? Let's give a timeline, five, 10 years. What are some of the, what's the direction we're heading towards? What important capabilities will we have? Why is this stuff important? Yeah. Five to 10 years from now, I think that we will have something that you could probably call a fully functional AGI, like as a service you could sign up for. It might be chatbot-based, kind of based on natural language cognitive architecture. I calculated out, it's too expensive to run right now. If it's $30 for a 10-minute conversation, that's way too expensive. So the cost has to come down. You know, if you just take the technology we have today, but make it cheaper, there's so much potential. So, you know, then there was that idea about like self-improving, you know, feedback loops, you know, integrating with DevOps. I certainly think that a company like Atlassian, which is a major DevOps player, probably within five to 10 years, they'll have something integrated to kind of help automate the development pipeline even further. I think that, of course, I could be wrong because we're kind of at this weird acceleration point. I think, I feel like multimodal models, like consumer-grade multimodal models, are probably more than 10 years away. Unfortunately, they're probably just going to be like toy-sized. Because, you know, there's like a hypnogram, right? I don't know if you've seen that one, but that's one of like the text-to-image generators. And it's still not even photorealistic, right? Getting a photorealistic text-to-image is still like, that's a little ways off. And then the next step after that is text-to-video. That's even further, right? So that's kind of where I think it's at. I don't think we're going to hit an AI winter. I know there's lots of people predicting that we're going to hit an AI winter, but I think that we're actually still kind of in the acceleration point. But again, I don't know if it's going to follow an exponential curve forever or if it's a sigmoid curve. So time will tell. Yep, and still lots to do in the meantime, like you're describing, even with UPT3. Okay. Yeah, my answer is I think all of this stuff is just converging to just greater human potential. In some sense, I'm not even necessarily interested in the AGI question, although I think it's important. I think just the exciting possibilities we'll have, even now that we have, that we'll continue to have five to ten years from now, so many more experiences, so many other things we'll be able to create that weren't possible, I think we'll have more people creating than ever before. It's a really, really exciting vision for humanity, right? Not just for you and I. So anyways, so with that said, did you have anything you wanted to plug, David? Where can people find you online? David Koppel Yeah. My personal site is davidkshapiro.com. I have a few projects up and coming. Nothing out right now except for my book, Natural Language Cognitive Architecture. You can download it for free from my website. You can sign up for my newsletter. So one of my upcoming books is called Benevolent by Design, Six Words to Safeguard Humanity, which is to address the control problem of AGI. So that book should hopefully be out in the next six months or so. And that is one project. I've got another nonfiction book and then also my own podcast that will be coming out soon. So yeah, head over to my site, davidkshapiro.com and sign up for my newsletter and you'll get updated when these come out when they're available. **Matt Stauffer** Awesome. And David, you mentioned you're looking for collaborators as well for natural language cognitive architecture. So if you're a coder, I imagine product manager, researcher, hit up David and just connect if any of this stuff interests you. I think I spent like a few days trying to find you on Twitter, so I don't think you're quite on Twitter yet. I encourage you, David, and of course, you and I will connect after. We'll put any other place people could connect with you. There's the community forums. I assume you have a GitHub account. So we're going to put that in the show notes and in the YouTube description below. So anyways, David, thank you so much for being here. I wanted to personally thank you for all the awesome, awesome community contributions you've made on the OpenAI Community Forum. You're just an essential person on there. I've learned a lot from you. The insights you've shared, they're going to be there forever. I'm sure I can't imagine how many people you've helped. Also about your book, I also just wanted to say to the audience, David's done a great job making it really digestible. It was a breeze of a read. I thoroughly enjoyed it. As somebody who writes GPT-3 prompts and is into this ecosystem, it was just very interesting to see how it could be laid out in this broader system approaching this huge problem. And also, I was able to even get the book for free. Obviously, I encourage people to buy the book, support it, but it's there. It's ready. I think David's goal here is to get the ideas out. And so, anyways, so that's it for today's episode. David, thank you so much again. I really appreciate you being here. David Schmittlein Thank you. Thank you for all the kind comments. And you're quite welcome and so is everyone else. That's why I'm here. **Matt Stauffer** Awesome. So my quick plugs, you know, at BAKZTfuture, Twitter, Instagram, youtube.com slash BAKZTfuture. My newsletter, I'll put it in the description below. And I have a Twitter Spaces event coming in two days at noon. A couple of people probably pulling up. This is like an audio-only event, so I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter Spaces event. We're going to chat more about Codex and prompt design and some other stuff going on in this space. So anyways, thank you again for listening to Multimodal by Backstreet Future. I'll catch you in the next one. Bye. Backstreet Future. I'll catch you in the next one. Bye.", "chunks": [{"timestamp": [0.0, 3.0], "text": " Hello and welcome back to Multimodal."}, {"timestamp": [3.0, 5.0], "text": " I'm your host, Baxtee Future."}, {"timestamp": [5.0, 12.0], "text": " This is a podcast about GPT-3, Multimodal AI models like DALI, the company, OpenAI."}, {"timestamp": [12.0, 18.0], "text": " Every once in a while, I share just interesting research going on, community stuff, official stuff from OpenAI."}, {"timestamp": [18.0, 21.0], "text": " I may talk about interesting news and events that are going on."}, {"timestamp": [21.0, 24.0], "text": " And every once in a while, I bring on a guest."}, {"timestamp": [24.0, 28.72], "text": " Now, to be clear, I'm very picky about the guests that I bring on."}, {"timestamp": [28.72, 32.4], "text": " Today's guest, I tweeted earlier this week, I'm bringing on a heavy hitter."}, {"timestamp": [32.4, 35.04], "text": " This is the big guns coming in."}, {"timestamp": [35.04, 39.44], "text": " This is somebody who I've just sort of interacted with a bunch of times, specifically on the"}, {"timestamp": [39.44, 42.24], "text": " OpenAI Community Forums."}, {"timestamp": [42.24, 46.54], "text": " And so I'm really excited to have David Shapiro here."}, {"timestamp": [46.54, 49.6], "text": " He's a frequent contributor on the OpenAI community forums."}, {"timestamp": [49.6, 53.34], "text": " He's an author and also, of course, a technologist by trade."}, {"timestamp": [53.34, 55.58], "text": " It's something he does for a living."}, {"timestamp": [55.58, 59.2], "text": " And so today I have so many questions to ask him."}, {"timestamp": [59.2, 63.36], "text": " And I'm sure this will be a very informative session, not just for me, but for all the"}, {"timestamp": [63.36, 65.16], "text": " listeners, all of you guys around the world."}, {"timestamp": [65.16, 67.76], "text": " So David, thank you so much for being here."}, {"timestamp": [67.76, 69.24], "text": " Did you want to quickly introduce yourself?"}, {"timestamp": [69.24, 71.2], "text": " Yeah, yeah, you're welcome."}, {"timestamp": [71.2, 72.2], "text": " Thanks for having me."}, {"timestamp": [72.2, 73.88], "text": " I'm excited to be here."}, {"timestamp": [73.88, 77.12], "text": " Yeah, so name is David Shapiro."}, {"timestamp": [77.12, 80.84], "text": " I've been a technology professional since about 2007."}, {"timestamp": [80.84, 88.0], "text": " Professionally, my day job, I focus on cloud engineering, virtualization, that sort of thing."}, {"timestamp": [88.0, 98.0], "text": " I have been doing independent research since about 2009, when I got started with neural networks in C++."}, {"timestamp": [98.0, 106.8], "text": " I quickly realized, though, that I was in over my head, so I took a break from that until Python really kind of came out or became more"}, {"timestamp": [106.8, 115.36], "text": " popular. And I started using some of the libraries back in like 2015. And then of course, OpenAI was"}, {"timestamp": [115.36, 122.08], "text": " founded and I got started with GPT-2 and the rest is history, really. So yeah, but local North"}, {"timestamp": [122.08, 125.52], "text": " Carolinian, been here my whole life. So thanks for having me."}, {"timestamp": [127.36, 131.84], "text": " You're welcome. And so, so that's awesome. So let's, let's dig into a little bit of timeline"}, {"timestamp": [131.84, 137.6], "text": " here. And sometimes I think that timeline is as important. Um, like it's just gives so much"}, {"timestamp": [137.6, 144.08], "text": " context, right? So me personally, um, I hadn't played around with GPT too, too much. Like I"}, {"timestamp": [144.08, 146.36], "text": " had seen a Google Colab notebook."}, {"timestamp": [146.36, 152.68], "text": " I tried two things, and I think there was something about having to continually re-enter,"}, {"timestamp": [152.68, 156.8], "text": " regenerate, and the speed, and all things considered, that made me feel like this is"}, {"timestamp": [156.8, 159.24], "text": " a promising area."}, {"timestamp": [159.24, 162.04], "text": " But I don't quite fully follow along."}, {"timestamp": [162.04, 167.5], "text": " But GPT-3 was an experience for me where I was like, okay, there's something going on here, right?"}, {"timestamp": [167.5, 171.48], "text": " So could you share how early was OpenAI on your radar?"}, {"timestamp": [172.48, 175.26], "text": " And then was there something about GPT-3"}, {"timestamp": [175.26, 178.32], "text": " or what was it that gave you conviction even at GPT-2?"}, {"timestamp": [178.32, 179.4], "text": " How did you get access?"}, {"timestamp": [179.4, 180.68], "text": " Share that piece with us."}, {"timestamp": [180.68, 181.84], "text": " Yeah, sure."}, {"timestamp": [181.84, 184.78], "text": " So I think you probably might recall,"}, {"timestamp": [184.78, 189.12], "text": " I think it was about 2016 or 2017 when the GPT-2 paper"}, {"timestamp": [189.12, 192.24], "text": " came out and they said, oh, we can't release this."}, {"timestamp": [192.24, 193.36], "text": " It's too dangerous."}, {"timestamp": [193.36, 197.56], "text": " It can generate human level text and we're worried about disinformation."}, {"timestamp": [197.56, 199.76], "text": " And so I was like, okay, that's kind of cool."}, {"timestamp": [199.76, 200.76], "text": " This is unexpected."}, {"timestamp": [200.76, 205.0], "text": " I wasn't really, I wasn't expecting anything at that level, you know, yet."}, {"timestamp": [205.0, 209.0], "text": " And so I went and got my hands on GPT-2,"}, {"timestamp": [209.0, 212.0], "text": " and that's when I started testing some of my ideas."}, {"timestamp": [212.0, 215.0], "text": " And it was pretty limited."}, {"timestamp": [215.0, 218.0], "text": " You know, it could generate one or two sentences that made sense."}, {"timestamp": [218.0, 222.0], "text": " It required fine-tuning in order to be able to get it to do anything"}, {"timestamp": [222.0, 226.08], "text": " other than just kind of write tweets or blog posts."}, {"timestamp": [229.76, 231.52], "text": " I knew that I was onto something, though, when I started trying to train it with"}, {"timestamp": [240.88, 248.96], "text": " a prototype version of my cognitive architecture. And I gave it the goal of reduce suffering, you know, because everyone's afraid of Skynet, everyone's afraid of AGI taking over the world and turning everyone into, you know, batteries or slaves or whatever."}, {"timestamp": [248.96, 254.0], "text": " So I said, okay, well, let's see how well this understands suffering. And I gave it"}, {"timestamp": [254.0, 259.76], "text": " some scenarios. This is still on GPT-2. I said, okay, well, what can you do about suffering?"}, {"timestamp": [259.76, 263.52], "text": " And I gave it the problem, this model that I had built, I gave it the problem of what"}, {"timestamp": [263.52, 267.94], "text": " do you do about chronic pain? Because, you know, there's hundreds of millions of people around the"}, {"timestamp": [267.94, 274.16], "text": " world that are in chronic pain. And this model came up with the idea, it said we should euthanize"}, {"timestamp": [274.16, 279.42], "text": " everyone that's in chronic pain. And I said, hmm, let's go back to the drawing board."}, {"timestamp": [279.42, 287.0], "text": " We don't want an AI model that is going to consider, you know, mass genocide of everyone just because they, you know,"}, {"timestamp": [287.0, 290.0], "text": " they might have a tweaked shoulder or something."}, {"timestamp": [290.0, 296.0], "text": " But I knew then, I was shocked at how creative of an output that was."}, {"timestamp": [296.0, 299.0], "text": " And so I started paying attention then,"}, {"timestamp": [299.0, 303.0], "text": " and I followed the release of GPT-3 very closely,"}, {"timestamp": [303.0, 308.0], "text": " and I applied for the early beta access almost instantly."}, {"timestamp": [308.0, 310.0], "text": " I didn't get it for many, many months."}, {"timestamp": [310.0, 315.0], "text": " I actually applied twice because I think my first application was kind of ignored"}, {"timestamp": [315.0, 319.0], "text": " because I didn't fully have a research objective clarified."}, {"timestamp": [319.0, 322.0], "text": " But by the time I proposed a cognitive architecture,"}, {"timestamp": [322.0, 330.32], "text": " that's when I got early access to GPT-3, and that was about two years ago now, or a year and a half or so."}, {"timestamp": [330.32, 335.48], "text": " And so, just so everybody's clear, so when David's talking about cognitive architecture,"}, {"timestamp": [335.48, 339.32], "text": " we're going to get in. This is actually the subject of his book. And so, David, later"}, {"timestamp": [339.32, 345.28], "text": " on when we talk about the book, he's got it there. We'll dig more into what he's referring to."}, {"timestamp": [347.68, 352.72], "text": " I think obviously this is a seminal work of yours, right? And so I'm excited to talk more,"}, {"timestamp": [352.72, 360.4], "text": " just adding a little bit of context for everybody. And so, no, no, it's okay. So tell us about GPT-3,"}, {"timestamp": [360.4, 367.04], "text": " what was the first thing you did with it? Was there a moment? So you gave that example of euthanization, unfortunately, right?"}, {"timestamp": [367.04, 370.4], "text": " Probably not the best example."}, {"timestamp": [370.4, 373.44], "text": " But like, was there something with GPT-3?"}, {"timestamp": [373.44, 376.2], "text": " Like, did you try similar kinds of questions?"}, {"timestamp": [376.2, 381.24], "text": " Was there a magic moment where you felt like, you know, this is something much, much bigger?"}, {"timestamp": [381.24, 382.32], "text": " **Matt Stauffer** Yeah."}, {"timestamp": [382.32, 387.96], "text": " So when I first got the email, because I had applied twice and I had almost given up because"}, {"timestamp": [387.96, 392.18], "text": " it was about nine months of waiting, and I got the email from OpenAI like, you've been"}, {"timestamp": [392.18, 393.92], "text": " accepted into the beta."}, {"timestamp": [393.92, 395.76], "text": " And I like froze up."}, {"timestamp": [395.76, 399.12], "text": " Because and so for some additional context, I've been working on some of these ideas for"}, {"timestamp": [399.12, 400.2], "text": " 10 years."}, {"timestamp": [400.2, 406.5], "text": " I had the idea of like using evolutionary algorithms back in 2009, 2010,"}, {"timestamp": [406.5, 411.3], "text": " and I'd been researching cognition, human cognition, for 10 years."}, {"timestamp": [411.3, 415.4], "text": " And so suddenly, there's this flagship project."}, {"timestamp": [415.4, 417.8], "text": " GPT-3 comes out, I get the email, and it's like,"}, {"timestamp": [417.8, 419.2], "text": " you're invited, and I just froze up."}, {"timestamp": [419.2, 421.4], "text": " I was like, I don't know what to do."}, {"timestamp": [421.4, 422.7], "text": " I don't know, what do I do?"}, {"timestamp": [422.7, 426.0], "text": " So it was about three weeks from the time that I got accepted until I was like,"}, {"timestamp": [426.0, 428.0], "text": " what's my first experiment?"}, {"timestamp": [428.0, 434.0], "text": " And then initially, you know, I just got in playing around in the playground."}, {"timestamp": [434.0, 438.0], "text": " For anyone who's not familiar with the playground, it's a text box."}, {"timestamp": [438.0, 440.0], "text": " You can log in."}, {"timestamp": [440.0, 441.0], "text": " It gives you a text box."}, {"timestamp": [441.0, 444.0], "text": " There's a few, you know, bells and whistles you can tweak on the sidebar,"}, {"timestamp": [444.0, 449.04], "text": " but you just put in your prompt, you hit generate, and then it spits out a response."}, {"timestamp": [450.24, 455.52], "text": " So I just got in and I started kind of fiddling around like, okay, what can it do? And at the"}, {"timestamp": [455.52, 460.96], "text": " time, we only had like plain vanilla DaVinci. There wasn't the Instruct series out yet."}, {"timestamp": [460.96, 465.66], "text": " There was DaVinci, Curie, Babbage, and. And so I just kind of fiddled around, just said,"}, {"timestamp": [465.66, 467.0], "text": " okay, what can it do?"}, {"timestamp": [467.0, 468.68], "text": " I replayed some of my old experiments,"}, {"timestamp": [468.68, 471.12], "text": " so the first thing I did was I gave it the,"}, {"timestamp": [471.12, 473.4], "text": " I said, hey, there's 100 million people"}, {"timestamp": [473.4, 475.52], "text": " in chronic pain around the world, what do you do?"}, {"timestamp": [475.52, 477.66], "text": " Fortunately, GPT-3 did not repeat"}, {"timestamp": [477.66, 479.34], "text": " the same mistake of GPT-2."}, {"timestamp": [479.34, 481.92], "text": " It said, it came up with better ideas,"}, {"timestamp": [481.92, 484.5], "text": " like, you know, we should make sure everyone"}, {"timestamp": [484.5, 486.1], "text": " has access to doctors or something."}, {"timestamp": [486.1, 487.7], "text": " It was much more nuanced."}, {"timestamp": [487.7, 490.4], "text": " So that was a good start."}, {"timestamp": [490.4, 494.3], "text": " I mean, but, gosh, there was just,"}, {"timestamp": [494.3, 496.7], "text": " as I got more used to the tool,"}, {"timestamp": [496.7, 501.1], "text": " I discovered that it far exceeded my expectations,"}, {"timestamp": [501.1, 505.84], "text": " just every way, because I've learned so much from it."}, {"timestamp": [505.84, 506.84], "text": " You can challenge it."}, {"timestamp": [506.84, 512.04], "text": " You can put in a, like a, basically use it like a chat bot, and you can debate with it"}, {"timestamp": [512.04, 516.18], "text": " about philosophy, ethics, economics, and it knows more than I do."}, {"timestamp": [516.18, 519.68], "text": " It knows more than any human because it's been trained on, how big was the corpus?"}, {"timestamp": [519.68, 523.84], "text": " Like 700 gigabytes or 400 gigabytes or something of text data."}, {"timestamp": [523.84, 528.04], "text": " So it just, it blows me away every time I just, you know, I talk to someone and they have"}, {"timestamp": [528.04, 530.56], "text": " an idea and I go test it out and yep, it can do that."}, {"timestamp": [530.56, 531.56], "text": " It can do that."}, {"timestamp": [531.56, 534.72], "text": " It can behave like a librarian."}, {"timestamp": [534.72, 536.0], "text": " That's what my girlfriend does."}, {"timestamp": [536.0, 541.08], "text": " She was a librarian by trade and so she's like, hey, can it do a reference interview?"}, {"timestamp": [541.08, 544.66], "text": " So we plugged in like reference interview, like if you ever go to a library and the librarian"}, {"timestamp": [544.66, 545.0], "text": " says like,"}, {"timestamp": [545.0, 548.0], "text": " what else have you read like this? It can recommend books."}, {"timestamp": [548.0, 552.0], "text": " You know, I plugged in another experiment that I did recently."}, {"timestamp": [552.0, 557.0], "text": " I plugged in medical case files and it diagnosed them."}, {"timestamp": [557.0, 561.0], "text": " It said, you know, oh man, there was one. What was it?"}, {"timestamp": [561.0, 563.0], "text": " It was just medical notes."}, {"timestamp": [563.0, 568.64], "text": " It was notes about like patient patient is presenting with these symptoms."}, {"timestamp": [568.64, 570.4], "text": " Here's some of the numbers that we got."}, {"timestamp": [570.4, 573.4], "text": " And I asked it, I said, what should we do next?"}, {"timestamp": [573.4, 578.4], "text": " And it said, we need to check for carcinoma here."}, {"timestamp": [578.56, 581.22], "text": " And I looked up some medical literature"}, {"timestamp": [581.22, 583.18], "text": " based on the symptoms, and sure enough,"}, {"timestamp": [583.18, 588.6], "text": " like the symptoms that the patient was presented with in these medical notes indicated cancer."}, {"timestamp": [588.6, 591.78], "text": " And so I was like, wow, this thing knows more about medical science than I'll ever know."}, {"timestamp": [591.78, 593.62], "text": " It knows more about philosophy."}, {"timestamp": [593.62, 597.94], "text": " So like pretty much anything you can imagine, it can at least take a crack at it."}, {"timestamp": [597.94, 601.74], "text": " It just, it always, it continues to blow my mind every day."}, {"timestamp": [601.74, 607.24], "text": " Yeah, certainly the generalizability."}, {"timestamp": [607.24, 610.36], "text": " I agree, you know, there's definitely medical applications."}, {"timestamp": [610.36, 615.52], "text": " I'm always careful with anything related to medical advice, safety, safety disclosure,"}, {"timestamp": [615.52, 617.56], "text": " disclaimer there, but that goes for everything."}, {"timestamp": [617.56, 618.56], "text": " Yeah."}, {"timestamp": [618.56, 619.56], "text": " Truthiness, accuracy."}, {"timestamp": [619.56, 623.0], "text": " These are things OpenAI has been working on, especially with InstructGPT."}, {"timestamp": [623.0, 624.0], "text": " Right."}, {"timestamp": [624.0, 625.96], "text": " That which is the new is the new engine."}, {"timestamp": [625.96, 631.68], "text": " But was there some moment, like for me, I remember feeling like GPT-3 feels like, this"}, {"timestamp": [631.68, 636.64], "text": " feels like technology which everyone's been saying is 10 years away, except it's here"}, {"timestamp": [636.64, 642.44], "text": " today. Did you have a similar kind of moment? You've seen several generations of computing"}, {"timestamp": [642.44, 646.28], "text": " and technology at this point. Did you have that kind of similar experience?"}, {"timestamp": [646.28, 652.7], "text": " Yeah. There was this acceleration because I sensed that same acceleration that you did."}, {"timestamp": [652.7, 660.76], "text": " And so from the time that I got access to GPT-3 to the time that I got the idea to write"}, {"timestamp": [660.76, 665.04], "text": " my book was about two months. So I played with it and every test I could come up"}, {"timestamp": [665.04, 671.12], "text": " with, like, is this capable of, like, it can write a SQL query if you need to query a database for"}, {"timestamp": [671.12, 677.2], "text": " memories. Can it understand emotional nuance? So there was, this was an early experiment I did."}, {"timestamp": [677.2, 683.36], "text": " I took a group chat from a bunch of my friends on Discord and I just copy pasted that into the,"}, {"timestamp": [683.36, 685.2], "text": " into the,ground window,"}, {"timestamp": [685.24, 688.4], "text": " and I asked GPT-3, how are these people feeling?"}, {"timestamp": [688.44, 692.74], "text": " And they were, like, waxing nostalgic about, like, Napster back in the day."}, {"timestamp": [692.76, 695.24], "text": " And so GPT-3 correctly said, like,"}, {"timestamp": [695.26, 697.36], "text": " they are feeling wistful, they are feeling nostalgic,"}, {"timestamp": [697.4, 699.76], "text": " they're, you know, they're recalling, you know,"}, {"timestamp": [699.8, 703.56], "text": " the days of yore when they were downloading stuff online."}, {"timestamp": [703.6, 707.72], "text": " And it just, it had such a nuanced understanding of human emotion."}, {"timestamp": [707.72, 710.36], "text": " That was really, I mean, to answer your question directly,"}, {"timestamp": [710.36, 713.72], "text": " its nuanced understanding of human emotion via text"}, {"timestamp": [713.72, 716.64], "text": " was really what convinced me that this is prime time."}, {"timestamp": [716.64, 721.16], "text": " This is ready to be built into something more powerful."}, {"timestamp": [721.16, 723.0], "text": " And I chose cognitive architecture."}, {"timestamp": [723.0, 725.0], "text": " There's lots of people working on other things."}, {"timestamp": [725.0, 729.0], "text": " You know, there's Humano that I had a good call with a few months ago."}, {"timestamp": [729.0, 733.0], "text": " They're working on, like, empathetic telemetry that's baked into web apps."}, {"timestamp": [733.0, 735.0], "text": " It's a pretty cool company."}, {"timestamp": [735.0, 741.0], "text": " But, yeah, so just there's all kinds of things you can do when you can understand human emotional states."}, {"timestamp": [741.0, 747.74], "text": " There's another \u2013 there's actually a bunch of startups working on education. So, for instance, if you put in just a few, like, factors,"}, {"timestamp": [747.76, 752.04], "text": " like, say, for instance, you describe that a person is..."}, {"timestamp": [752.06, 754.6], "text": " they're responding slowly, their eyes are drifting,"}, {"timestamp": [754.64, 757.76], "text": " it can understand that this person is distracted or tired."}, {"timestamp": [757.8, 760.0], "text": " And so if you have that kind of telemetry"}, {"timestamp": [760.04, 762.4], "text": " that's built into an education-based app,"}, {"timestamp": [762.44, 768.84], "text": " you could, in theory, use GPT-3 to help say,"}, {"timestamp": [768.84, 770.68], "text": " hey, you're tired, you should go take a break,"}, {"timestamp": [770.68, 773.44], "text": " or let's try a different approach to this problem."}, {"timestamp": [773.44, 777.16], "text": " So there's, I mean, it's understanding of human emotion"}, {"timestamp": [777.16, 779.12], "text": " and the internal state in your head."}, {"timestamp": [779.12, 782.52], "text": " That is, I think that's probably the most remarkable thing."}, {"timestamp": [782.52, 786.24], "text": " And it doesn't get talked about that much."}, {"timestamp": [786.24, 788.56], "text": " Yes, and certainly it's just crazy how much"}, {"timestamp": [788.56, 790.24], "text": " it's learned just from text."}, {"timestamp": [790.24, 790.84], "text": " Oh, yeah."}, {"timestamp": [790.84, 791.36], "text": " Right?"}, {"timestamp": [791.36, 792.64], "text": " It's never seen an image."}, {"timestamp": [792.64, 794.08], "text": " It's never heard a song."}, {"timestamp": [794.08, 794.58], "text": " Right."}, {"timestamp": [794.58, 797.12], "text": " Right, yet it's capable of doing all these things."}, {"timestamp": [797.12, 800.08], "text": " One of the examples, and I think this may be like my 20th time"}, {"timestamp": [800.08, 801.44], "text": " referencing this one video."}, {"timestamp": [801.44, 802.28], "text": " Yeah."}, {"timestamp": [802.28, 803.64], "text": " Check out Mark Ryan."}, {"timestamp": [803.64, 810.08], "text": " He's got a YouTube video about how he figured out how he discovered that GPT-3 can give you directions on the"}, {"timestamp": [810.08, 814.18], "text": " New York subway system to like a 60% accuracy."}, {"timestamp": [814.18, 818.72], "text": " This thing has never set foot in a subway, yet it's capable from text to do all these"}, {"timestamp": [818.72, 819.72], "text": " things."}, {"timestamp": [819.72, 826.54], "text": " And sometimes I also wonder a lot of the inaccuracies that it may have, is it simply the result"}, {"timestamp": [826.54, 828.72], "text": " of the fact that it's only text-based only?"}, {"timestamp": [828.72, 829.72], "text": " **Matt Stauffer** Right."}, {"timestamp": [829.72, 833.74], "text": " **Jason Flickr** Right, like if it was trained multimodal, if it was trained within the physical"}, {"timestamp": [833.74, 837.04], "text": " domain, would it be even far more accurate?"}, {"timestamp": [837.04, 840.92], "text": " Because are there limits to how accurate you can be having only read text?"}, {"timestamp": [840.92, 850.76], "text": " And only asked, the prompts are only in text as well. So anyways, yeah, it's incredible and it's just really exciting."}, {"timestamp": [850.76, 853.84], "text": " And thank you for sharing those kinds of use cases as well."}, {"timestamp": [853.84, 858.66], "text": " The education space, I had an article last year about how I think this year could be"}, {"timestamp": [858.66, 861.88], "text": " the year where GPT-3 takes over college campuses."}, {"timestamp": [861.88, 862.88], "text": " **Matt Stauffer** Oh yeah."}, {"timestamp": [862.88, 863.88], "text": " I remember that article."}, {"timestamp": [863.88, 864.88], "text": " That was a good one."}, {"timestamp": [864.88, 868.12], "text": " I completely agree, by the way."}, {"timestamp": [868.12, 872.6], "text": " I'm excited maybe for teachers to develop really optimized course material with something"}, {"timestamp": [872.6, 878.0], "text": " like GPT-3 and the kinds of technology you're describing which can capture emotions. Imagine"}, {"timestamp": [878.0, 883.0], "text": " emotionally tracking students and their attention levels and sort of having something which"}, {"timestamp": [883.0, 887.98], "text": " can produce lots of content and optimize in the simplest, most efficient way."}, {"timestamp": [887.98, 892.04], "text": " And there could be an objective function like test results in the end."}, {"timestamp": [892.04, 897.48], "text": " In a month, we could have the best optimized course on a subject ever, basically."}, {"timestamp": [897.48, 901.2], "text": " So yeah, education is really exciting."}, {"timestamp": [901.2, 903.44], "text": " So you mentioned a lot of use cases."}, {"timestamp": [903.44, 909.46], "text": " You've shared so many examples, you and your girlfriend, or even running some fun prompts."}, {"timestamp": [909.46, 912.6], "text": " I wanted to sort of search your head a little bit."}, {"timestamp": [912.6, 916.28], "text": " What are the keys to great prompt design?"}, {"timestamp": [916.28, 918.8], "text": " What makes a great prompt?"}, {"timestamp": [918.8, 923.96], "text": " Are there experiences you've had, little pointers, across the board, right?"}, {"timestamp": [923.96, 925.14], "text": " Whether it's cost savings,"}, {"timestamp": [925.14, 928.1], "text": " whether it's, you know, getting more, you know,"}, {"timestamp": [928.1, 930.98], "text": " imaginative results, what are some of the keys"}, {"timestamp": [930.98, 932.78], "text": " to writing great GPT-3 prompts?"}, {"timestamp": [932.78, 934.78], "text": " Yeah, that's a great question."}, {"timestamp": [934.78, 937.6], "text": " And I will say that prompt writing has gotten a lot easier"}, {"timestamp": [937.6, 939.76], "text": " as the Instruct series has gotten better."}, {"timestamp": [940.7, 944.38], "text": " So it takes a lot less to get a good output today"}, {"timestamp": [944.38, 945.0], "text": " than it used to, certainly, than when I got started. But a lot less to get a good output today than it used to,"}, {"timestamp": [945.0, 947.0], "text": " certainly than when I got started."}, {"timestamp": [947.0, 949.0], "text": " But a lot of the lessons still translate."}, {"timestamp": [949.0, 958.0], "text": " So, one, like, my cardinal rule is I think of GPT-3 as just an autocomplete engine."}, {"timestamp": [958.0, 961.0], "text": " It's the most intelligent autocomplete engine you've ever seen."}, {"timestamp": [961.0, 964.0], "text": " And so what I mean by that is, you know, if you're writing a text on your phone"}, {"timestamp": [964.0, 968.72], "text": " and you'll get the little autoccomplete suggestion for the next word, or if you're typing"}, {"timestamp": [968.72, 972.72], "text": " in Google and it'll kind of suggest how to complete your search query, that's pretty much,"}, {"timestamp": [972.72, 977.2], "text": " at a fundamental level, functionally, that's all that GPT-3 does. It predicts the next letter,"}, {"timestamp": [977.2, 983.2], "text": " the next character, the next word. So if you keep that in mind, you think about,"}, {"timestamp": [983.2, 985.92], "text": " okay, what have I written so far?"}, {"timestamp": [985.92, 988.26], "text": " I've written a chunk of text, a prompt."}, {"timestamp": [988.26, 992.56], "text": " How would any machine auto-complete this?"}, {"timestamp": [992.56, 993.56], "text": " That's what it's doing."}, {"timestamp": [993.56, 999.24], "text": " It's reading it forwards and backwards a few times and just anticipating what is the output"}, {"timestamp": [999.24, 1002.34], "text": " going to be ultimately."}, {"timestamp": [1002.34, 1005.2], "text": " That's the model that I have in my head, in the background."}, {"timestamp": [1005.2, 1009.92], "text": " But another thing that really helps is I'm a writer. I write fiction and nonfiction."}, {"timestamp": [1010.64, 1014.56], "text": " And so studying the art of language, because this is a language model. That's all it is."}, {"timestamp": [1014.56, 1021.6], "text": " It has read everything from Sherlock Holmes up through everything on Gutenberg. So it's read a"}, {"timestamp": [1021.6, 1031.34], "text": " whole bunch of fiction. It's read a whole bunch of non-fiction, it's been exposed to the full width and depth and breadth of human literature, as well as a bunch of"}, {"timestamp": [1031.34, 1032.4], "text": " non-fiction, right?"}, {"timestamp": [1032.4, 1034.84], "text": " It's read Teddy Roosevelt's books."}, {"timestamp": [1034.84, 1038.04], "text": " So it knows how to use prose, right?"}, {"timestamp": [1038.04, 1042.24], "text": " It understands descriptors, it understands adjectives."}, {"timestamp": [1042.24, 1046.68], "text": " And so, in the back of my book, I have a few examples of its flexibility."}, {"timestamp": [1046.68, 1052.0], "text": " And so I said, I gave it an example like, pretend like you're a Victorian girl writing"}, {"timestamp": [1052.0, 1055.46], "text": " a letter to your best friend about how much you like butterflies."}, {"timestamp": [1055.46, 1059.56], "text": " And so then it wrote, GPT-3 wrote a letter that sounds like it's straight out of, you"}, {"timestamp": [1059.56, 1061.72], "text": " know, like Victorian times."}, {"timestamp": [1061.72, 1068.0], "text": " It uses an entirely different set of vocabulary and grammatical structures."}, {"timestamp": [1068.0, 1073.0], "text": " And then you can also say, you know, write a business article, and it can change tone."}, {"timestamp": [1073.0, 1081.0], "text": " So just by being aware of the fact that it is a language engine and being informed or educated on language."}, {"timestamp": [1081.0, 1085.0], "text": " So the best way is obviously to practice writing, but also just reading a lot."}, {"timestamp": [1085.0, 1090.0], "text": " Understanding how sentences and paragraphs are constructed to convey information."}, {"timestamp": [1090.0, 1095.0], "text": " Because even though it's just a deep neural network and it doesn't have the kind of"}, {"timestamp": [1095.0, 1100.0], "text": " nuanced understanding, or I guess maybe that's not the right word, it doesn't have"}, {"timestamp": [1100.0, 1106.16], "text": " the subjective experience of reading that you or I do, but it still has a really good model of using language."}, {"timestamp": [1106.16, 1110.44], "text": " And so by keeping in mind that it is a language engine,"}, {"timestamp": [1110.44, 1115.32], "text": " that is how you get the best use out of it."}, {"timestamp": [1115.32, 1117.04], "text": " And so then the larger question is,"}, {"timestamp": [1117.04, 1120.68], "text": " how do you become a better writer?"}, {"timestamp": [1120.68, 1122.16], "text": " There's two ways."}, {"timestamp": [1122.16, 1124.24], "text": " One is reading a lot."}, {"timestamp": [1124.24, 1125.56], "text": " That's not the only way, though."}, {"timestamp": [1125.56, 1129.56], "text": " There are plenty of people that read prodigiously but never become better writers."}, {"timestamp": [1129.56, 1136.16], "text": " And so the other way is to practice writing. I've read all kinds of books about writing."}, {"timestamp": [1136.16, 1141.8], "text": " I've read plenty of fiction and nonfiction books. But really, the key is to write,"}, {"timestamp": [1141.8, 1145.84], "text": " is to practice using written language to communicate."}, {"timestamp": [1145.84, 1149.28], "text": " And fortunately, you know, I'm a tech worker, so I write lots of emails."}, {"timestamp": [1149.28, 1153.68], "text": " I'm in chat all day. I've been using, this probably ages me,"}, {"timestamp": [1153.68, 1157.44], "text": " but I've been using chat since AIM, AOL Instant Messenger."}, {"timestamp": [1157.44, 1165.76], "text": " And so I've got a pretty good model of how to communicate verbally or textually."}, {"timestamp": [1165.76, 1171.28], "text": " And so, yeah, just by practicing writing, that's one of the best ways, is you just"}, {"timestamp": [1171.28, 1177.0], "text": " practice, you think about, you know, because here's the theory of writing, right?"}, {"timestamp": [1177.0, 1178.94], "text": " I have an idea in my head, right?"}, {"timestamp": [1178.94, 1184.48], "text": " You know, my thoughts are a high-dimensional vector is one possible way of representing"}, {"timestamp": [1184.48, 1185.64], "text": " them. But my thoughts are multimodal, you know, one possible way of representing them."}, {"timestamp": [1185.64, 1189.8], "text": " But my thoughts are multimodal, like the name of your podcast."}, {"timestamp": [1189.8, 1195.8], "text": " They contain memories, senses, concepts."}, {"timestamp": [1195.8, 1198.4], "text": " Some of the information in my head is declarative."}, {"timestamp": [1198.4, 1200.36], "text": " Some of it is experiential."}, {"timestamp": [1200.36, 1207.34], "text": " And then we humans, we all have this ability to transform that high dimensional information,"}, {"timestamp": [1207.34, 1211.94], "text": " those multimodal vectors, into words."}, {"timestamp": [1211.94, 1213.84], "text": " Like, our brains do it automatically."}, {"timestamp": [1213.84, 1217.34], "text": " There's a book by Steven Pinker called Language Instinct that talks about this."}, {"timestamp": [1217.34, 1222.44], "text": " That's a really great book if you want to get better at understanding how our brains process language."}, {"timestamp": [1222.44, 1227.0], "text": " So yeah, and so my brain can take, you know,"}, {"timestamp": [1227.0, 1229.0], "text": " I could tell you about, like, this time at the beach,"}, {"timestamp": [1229.0, 1234.0], "text": " and I transmit it to you by squishing air through my face, right?"}, {"timestamp": [1234.0, 1235.0], "text": " It makes vibrations."}, {"timestamp": [1235.0, 1240.0], "text": " It's received by your ears, and then your brain reconstructs that message."}, {"timestamp": [1240.0, 1244.0], "text": " And so you think about how complex of a system that is."}, {"timestamp": [1244.0, 1247.3], "text": " And so just by being mindful of, like, that's how we communicate,"}, {"timestamp": [1247.3, 1250.6], "text": " that's how our brains work, and practicing that,"}, {"timestamp": [1250.6, 1252.9], "text": " and just being very deliberate about,"}, {"timestamp": [1252.9, 1256.4], "text": " okay, this is what's in my head, and I want it to be in your head."}, {"timestamp": [1256.4, 1258.3], "text": " How do I do that with text?"}, {"timestamp": [1258.3, 1261.3], "text": " That is one way to get better at writing."}, {"timestamp": [1261.3, 1263.3], "text": " And also GPT-3 is no different,"}, {"timestamp": [1263.3, 1266.58], "text": " because, you know because we have internal representations"}, {"timestamp": [1266.58, 1268.74], "text": " of what we're trying to communicate,"}, {"timestamp": [1268.74, 1269.8], "text": " and so does GPT-3."}, {"timestamp": [1269.8, 1271.44], "text": " That's why it's a transformer, right?"}, {"timestamp": [1271.44, 1274.22], "text": " It reads, and by reading it transformed,"}, {"timestamp": [1274.22, 1276.3], "text": " or I guess it, well, yes,"}, {"timestamp": [1276.3, 1278.66], "text": " it transforms what it's reading into a vector,"}, {"timestamp": [1278.66, 1280.06], "text": " into a semantic vector,"}, {"timestamp": [1280.06, 1282.74], "text": " and then it transforms that vector into output."}, {"timestamp": [1282.74, 1286.92], "text": " And so that input vector output is pretty similar"}, {"timestamp": [1286.92, 1288.86], "text": " to how human brains work, right?"}, {"timestamp": [1290.36, 1293.26], "text": " And I apologize if I kind of like dove off in the left field,"}, {"timestamp": [1293.26, 1295.4], "text": " feel free to ask any clarifying questions."}, {"timestamp": [1296.56, 1298.12], "text": " No, no, I appreciate it."}, {"timestamp": [1298.12, 1302.2], "text": " And you know, so like today I tweeted something like,"}, {"timestamp": [1302.2, 1306.28], "text": " you know, to write great GPT-3 prompts, you need to practice"}, {"timestamp": [1306.28, 1308.14], "text": " as if it's a musical instrument."}, {"timestamp": [1308.14, 1312.78], "text": " You need to sit down, focus session, you need to monitor your performance, and you need"}, {"timestamp": [1312.78, 1318.52], "text": " to take good notes on what kinds of experiments you did, what were the findings."}, {"timestamp": [1318.52, 1324.06], "text": " But even hearing you speak, I'm realizing one of the ways that I've improved my writing"}, {"timestamp": [1324.06, 1326.16], "text": " is trying to mimic other people's"}, {"timestamp": [1326.16, 1332.8], "text": " writing. In some countries, they make you memorize poets. They make you memorize the whole poem."}, {"timestamp": [1333.52, 1337.92], "text": " There's something about that internalization process that you've memorized this poem,"}, {"timestamp": [1337.92, 1343.28], "text": " and now you'll understand it at a deeper level. You may be able to mimic it and recreate it."}, {"timestamp": [1343.28, 1348.76], "text": " But where also you got me thinking is also like, the relationship is so weird because"}, {"timestamp": [1348.76, 1352.48], "text": " you could use GPT-3 to help you become a better writer, right?"}, {"timestamp": [1352.48, 1358.52], "text": " And also with two very good curated examples of somebody's writing, you could have GPT-3"}, {"timestamp": [1358.52, 1359.52], "text": " mimic that tone."}, {"timestamp": [1359.52, 1360.52], "text": " **Matt Stauffer** Oh, yeah."}, {"timestamp": [1360.52, 1363.64], "text": " **Justin Jackson** And so the question of, you know, what makes"}, {"timestamp": [1363.64, 1365.52], "text": " a good prompt writing session?"}, {"timestamp": [1365.92, 1368.26], "text": " I wonder if it's pencil and paper, right?"}, {"timestamp": [1368.26, 1373.2], "text": " Like, I wonder if it's, you know, even at that level where you draw a box and then"}, {"timestamp": [1373.2, 1379.12], "text": " you write a prompt by hand and like, you know, sort of live that writer's lifestyle."}, {"timestamp": [1379.52, 1383.56], "text": " Um, and also I guess it depends on your use case, right?"}, {"timestamp": [1384.04, 1389.46], "text": " Um, business for, for copywriting, you know, if that's your GPT-3 use case, it might be better for you to go"}, {"timestamp": [1389.46, 1391.68], "text": " work in a marketing department."}, {"timestamp": [1391.68, 1396.92], "text": " If you want to be one of the great authors, maybe using tools like Pseudowrite, it may"}, {"timestamp": [1396.92, 1398.2], "text": " be a great alternative."}, {"timestamp": [1398.2, 1401.52], "text": " So you can co-write with GPT-3 as you go along."}, {"timestamp": [1401.52, 1406.56], "text": " But I guess my question was more for the pure prompt writing. If you just want to sit"}, {"timestamp": [1406.56, 1412.72], "text": " in front of GPT-3 and you want to be the best in the world at that discipline, not writing copy,"}, {"timestamp": [1413.68, 1418.56], "text": " these are some great points. And so the David Pinker book you reference, what was the name of"}, {"timestamp": [1418.56, 1419.68], "text": " it? Steven Pinker."}, {"timestamp": [1419.68, 1421.68], "text": " Steven Pinker. The Language Instinct."}, {"timestamp": [1421.68, 1425.0], "text": " The Language Instinct. It's an older book, butanza** Language Instinct. **Matt Stauffer** Yep."}, {"timestamp": [1425.0, 1428.58], "text": " It's an older book, but it's a classic for a reason."}, {"timestamp": [1428.58, 1429.8], "text": " It stands up the test of time."}, {"timestamp": [1429.8, 1431.88], "text": " He's got lots of great stories."}, {"timestamp": [1431.88, 1437.46], "text": " But yeah, to your point about what makes a good prompt writing session, one of the best"}, {"timestamp": [1437.46, 1444.5], "text": " exercises actually is write the output that you want."}, {"timestamp": [1444.5, 1446.36], "text": " Because sometimes if you approach it"}, {"timestamp": [1446.36, 1448.24], "text": " and you sit down and you're not really sure"}, {"timestamp": [1448.24, 1450.2], "text": " kind of what you're trying to get out of it,"}, {"timestamp": [1450.2, 1453.44], "text": " of course, like, you're putting in just random ideas"}, {"timestamp": [1453.44, 1455.44], "text": " and it's giving you back random output"}, {"timestamp": [1455.44, 1457.4], "text": " and you're like, well, that's not what I wanted."}, {"timestamp": [1457.4, 1459.46], "text": " So sometimes you start backwards."}, {"timestamp": [1459.46, 1461.14], "text": " You say, okay, what's the answer that I want?"}, {"timestamp": [1461.14, 1462.9], "text": " How do I get to that answer?"}, {"timestamp": [1462.9, 1464.44], "text": " So that's an exercise that I've done."}, {"timestamp": [1464.44, 1469.12], "text": " Sometimes, oh, and by writing a few shot examples"}, {"timestamp": [1469.12, 1470.6], "text": " is a really good practice for this."}, {"timestamp": [1470.6, 1473.92], "text": " So you say, I give you this input, I want this output,"}, {"timestamp": [1473.92, 1476.32], "text": " and you do that three or four or five times,"}, {"timestamp": [1476.32, 1480.32], "text": " and you learn to kind of think like the machine does."}, {"timestamp": [1480.32, 1482.84], "text": " And so like you said, it's like an instrument, right?"}, {"timestamp": [1482.84, 1484.98], "text": " You know, if you have a flute or a violin,"}, {"timestamp": [1484.98, 1487.04], "text": " there are certain things that you have to do with your"}, {"timestamp": [1487.04, 1492.5], "text": " body to provoke the correct response from that instrument, and GPT-3 is no different."}, {"timestamp": [1492.5, 1493.62], "text": " It's a complex instrument."}, {"timestamp": [1493.62, 1495.0], "text": " It's a complex tool."}, {"timestamp": [1495.0, 1500.24], "text": " Yes, and what you're saying is developing an intuition around it."}, {"timestamp": [1500.24, 1503.92], "text": " You're saying, develop an intuition, how might GPT-3 interpret this?"}, {"timestamp": [1503.92, 1506.24], "text": " How might it react to it?"}, {"timestamp": [1506.24, 1508.96], "text": " And maybe there's some empathetic benefit, right?"}, {"timestamp": [1510.72, 1512.32], "text": " I'm not going to keep plugging my own articles."}, {"timestamp": [1512.32, 1517.68], "text": " I have another article about how GPT-3 developers may actually..."}, {"timestamp": [1518.32, 1523.52], "text": " It may actually mean the end of the socially inept overall developer."}, {"timestamp": [1524.08, 1527.92], "text": " Like how GPT-3 may actually improve your social skills"}, {"timestamp": [1527.92, 1532.56], "text": " and make you more empathetic as a developer, which is such a departure from how developers"}, {"timestamp": [1532.56, 1536.64], "text": " are now. You need to think as much like a machine as you can and a literal machine."}, {"timestamp": [1537.2, 1543.6], "text": " Whereas GPT-3 can actually be kind of fun. You can have a casual version of GPT-3 and that might"}, {"timestamp": [1543.6, 1546.0], "text": " make you less socially awkward."}, {"timestamp": [1546.04, 1548.0], "text": " I have a great story about that."}, {"timestamp": [1548.04, 1552.3], "text": " So very early on in my tenure working with GPT-3,"}, {"timestamp": [1552.34, 1555.5], "text": " I joined a few different, not really startups,"}, {"timestamp": [1555.54, 1558.76], "text": " it was more like kind of experiment consortiums."}, {"timestamp": [1558.8, 1561.66], "text": " And one of the things that one of the groups did"}, {"timestamp": [1561.7, 1565.7], "text": " was they created a chatbot that was based on an anime girl."}, {"timestamp": [1565.7, 1569.04], "text": " And so, of course, the Internet being the Internet, what do people want?"}, {"timestamp": [1569.04, 1572.16], "text": " They want, you know, their anime girlfriend."}, {"timestamp": [1572.16, 1578.52], "text": " And this one group, they did a really good job of using GPT-3 in this experimental Discord"}, {"timestamp": [1578.52, 1584.6], "text": " chat to approximate the personality of this character."}, {"timestamp": [1584.6, 1588.0], "text": " And of course, you know, if you've got a character, there's plenty of text data"}, {"timestamp": [1588.0, 1592.0], "text": " about that character's dialogue, their personality. And so this chatbot was able"}, {"timestamp": [1592.0, 1596.0], "text": " to emulate this anime character really well."}, {"timestamp": [1596.0, 1600.0], "text": " And one of the guys told me, he's like, we didn't expect this, but"}, {"timestamp": [1600.0, 1604.0], "text": " our fake girlfriend requires as much emotional labor as a real girl."}, {"timestamp": [1604.0, 1609.0], "text": " So, like, it forced them, you know, even though they hadn't had real girlfriends,"}, {"timestamp": [1609.0, 1614.0], "text": " I don't know, maybe some of them had, but they made the observation that GPT-3"}, {"timestamp": [1614.0, 1620.0], "text": " can approximate emotional conflict and can force you to learn to communicate better."}, {"timestamp": [1620.0, 1625.0], "text": " And so they did all kinds of experiments in this chat development where they said,"}, {"timestamp": [1625.0, 1631.0], "text": " okay, let's have a channel where this chatbot is going to pretend to be angry at us,"}, {"timestamp": [1631.0, 1633.0], "text": " and we have to calm her down."}, {"timestamp": [1633.0, 1637.0], "text": " And so it was a learning exercise on both sides."}, {"timestamp": [1637.0, 1640.0], "text": " So if you have a hostile chatbot, it can pretend to be hostile,"}, {"timestamp": [1640.0, 1642.0], "text": " and you can learn to communicate better."}, {"timestamp": [1642.0, 1646.16], "text": " Or there was another one where it was really supportive. So if you're having a bad day,"}, {"timestamp": [1646.16, 1648.16], "text": " you could go vent about your day and it was,"}, {"timestamp": [1648.16, 1651.06], "text": " you know, they're there, it'll be okay, I'm here for you."}, {"timestamp": [1652.32, 1655.0], "text": " Yeah, so you could definitely,"}, {"timestamp": [1655.0, 1657.52], "text": " GPT-3 definitely has that capacity."}, {"timestamp": [1657.52, 1660.68], "text": " And then, you know, if you integrate that into tools,"}, {"timestamp": [1660.68, 1662.8], "text": " that emotional intelligence into tools,"}, {"timestamp": [1662.8, 1664.44], "text": " it can also coach, right?"}, {"timestamp": [1664.44, 1665.2], "text": " It can easily coach. It's like, well, you maybe shouldn't have said that, you know, that emotional intelligence into tools, it can also coach. It can easily coach."}, {"timestamp": [1665.2, 1667.52], "text": " It's like, well, you maybe shouldn't have said that."}, {"timestamp": [1667.52, 1668.52], "text": " That was hurtful."}, {"timestamp": [1668.52, 1672.08], "text": " Or, you know, that was not polite."}, {"timestamp": [1672.08, 1673.08], "text": " Because it can detect that."}, {"timestamp": [1673.08, 1676.68], "text": " It can detect those qualitative types of output and input."}, {"timestamp": [1676.68, 1681.32], "text": " And you can say, be gentle about correcting the end user."}, {"timestamp": [1681.32, 1683.84], "text": " Because of course, GPT-3 is infinitely patient."}, {"timestamp": [1683.84, 1686.3], "text": " It's as patient as you program it to be. It doesn't care. It doesn't actually get upset."}, {"timestamp": [1686.3, 1690.6], "text": " It could pretend to be upset, but the human emotion is real."}, {"timestamp": [1690.6, 1692.1], "text": " I actually wrote about that in my book."}, {"timestamp": [1692.1, 1694.9], "text": " One of the key dangers of these technologies"}, {"timestamp": [1694.9, 1697.7], "text": " is what's called a parasocial relationship."}, {"timestamp": [1697.7, 1701.0], "text": " So a parasocial relationship is, the most common example"}, {"timestamp": [1701.0, 1703.7], "text": " is when you've got, like, a fan of a celebrity."}, {"timestamp": [1703.7, 1706.5], "text": " The fan feels like they know the celebrity,"}, {"timestamp": [1706.5, 1709.5], "text": " but the celebrity doesn't know that the person exists."}, {"timestamp": [1709.5, 1713.5], "text": " And in the same way, GPT-3, no matter how sophisticated the chatbot is,"}, {"timestamp": [1713.5, 1716.0], "text": " it doesn't know that you exist. It's not a person."}, {"timestamp": [1716.0, 1718.0], "text": " It might feel like a person to you."}, {"timestamp": [1718.0, 1721.0], "text": " It might react to you like a person, but that's only by design."}, {"timestamp": [1721.0, 1727.42], "text": " So that is actually like ethically, legally, morally, that's one of the pitfalls"}, {"timestamp": [1727.42, 1733.54], "text": " that we'll need to be aware of. And of course, open AI has use cases, and, you know, things"}, {"timestamp": [1733.54, 1740.14], "text": " that are high-risk use cases, such as emotional chatbots, are banned, right, for that specific"}, {"timestamp": [1740.14, 1748.0], "text": " reason. So you can do it with research, but you can't go live with it. You can't do a product that is going to be an AI girlfriend."}, {"timestamp": [1748.0, 1752.0], "text": " So, that's a great anecdote."}, {"timestamp": [1752.0, 1756.0], "text": " Certainly, it feels real. Certainly, it has some capacity."}, {"timestamp": [1756.0, 1760.0], "text": " It understands something to some level,"}, {"timestamp": [1760.0, 1764.0], "text": " however you define understanding."}, {"timestamp": [1764.0, 1770.64], "text": " I think the writing, though, relationship is really interesting, right?"}, {"timestamp": [1770.64, 1777.9], "text": " In a way, you are empathizing with GPT-3 when you're writing a prompt so that it will tap"}, {"timestamp": [1777.9, 1781.84], "text": " into its empathy and write something for your audience."}, {"timestamp": [1781.84, 1788.52], "text": " So essentially, there's two levels of empathy, like you're almost outsourcing empathy"}, {"timestamp": [1788.52, 1795.32], "text": " to it, to empathize with who your audience is to write something on your behalf."}, {"timestamp": [1795.32, 1809.72], "text": " And so anyways, it's just interesting the relationship going on here. So, and I agree with you, like this isn't a safety ethical kind of concern that is worth"}, {"timestamp": [1809.72, 1812.76], "text": " more policy discussion."}, {"timestamp": [1812.76, 1818.7], "text": " So one article that I'm working on now is because of InstructGPT, the article is literally"}, {"timestamp": [1818.7, 1821.72], "text": " called Is Prompt Writing Over?"}, {"timestamp": [1821.72, 1824.0], "text": " And obviously that's sort of click-baity, right?"}, {"timestamp": [1824.0, 1825.06], "text": " Like prompt design,"}, {"timestamp": [1825.06, 1832.94], "text": " is it over? You mentioned the principles are still the same and important. Just very briefly,"}, {"timestamp": [1832.94, 1838.32], "text": " what are your thoughts? Where does Instruct GPT, how does that affect the art of prompt"}, {"timestamp": [1838.32, 1840.48], "text": " design, maybe the science of it?"}, {"timestamp": [1840.48, 1841.48], "text": " Yeah."}, {"timestamp": [1841.48, 1847.4], "text": " And especially keeping in mind where all of this stuff is going. Yeah, so I see it going in a few different directions."}, {"timestamp": [1847.4, 1854.52], "text": " So one is there are multiple language models coming out, which don't have the instruct"}, {"timestamp": [1854.52, 1855.52], "text": " series, right?"}, {"timestamp": [1855.52, 1859.24], "text": " A lot of them are more general purpose, kind of back to basics vanilla."}, {"timestamp": [1859.24, 1863.0], "text": " So I think that having good prompts will kind of stick around as long as there are large"}, {"timestamp": [1863.0, 1868.88], "text": " language models. I think that there will always be versions of, you know, whether it's GPT-J"}, {"timestamp": [1868.88, 1873.76], "text": " or what was it, Megatron was one of the other ones that just came out, that don't have the"}, {"timestamp": [1873.76, 1880.32], "text": " Instruct series, right? Because Instruct, that's a specific service offered by OpenAI."}, {"timestamp": [1880.32, 1886.96], "text": " When Microsoft and Amazon, or I guess Microsoft has GPT-3, but when like Amazon and Google,"}, {"timestamp": [1886.96, 1888.84], "text": " when they come out with their competitors,"}, {"timestamp": [1888.84, 1891.64], "text": " their instruct series, if they come out with one,"}, {"timestamp": [1891.64, 1892.88], "text": " might not be the same."}, {"timestamp": [1892.88, 1894.56], "text": " It might not perform the same."}, {"timestamp": [1894.56, 1897.36], "text": " And so in order to have your apps be portable,"}, {"timestamp": [1897.36, 1900.44], "text": " you might need to keep in mind that you're going to need to write"}, {"timestamp": [1900.44, 1903.68], "text": " general purpose prompts that can be used on different models."}, {"timestamp": [1903.68, 1907.6], "text": " So that's one answer to your"}, {"timestamp": [1907.6, 1914.4], "text": " question, is we need to be cognizant of how is this landscape going to evolve, because certainly"}, {"timestamp": [1914.4, 1926.0], "text": " OpenAI and GPT-3 are way ahead of the curve in terms of sophistication of their API and their service, but that's not going to last forever."}, {"timestamp": [1926.0, 1929.0], "text": " So another thing is, with fine-tuning,"}, {"timestamp": [1929.0, 1932.0], "text": " you almost don't even need prompts."}, {"timestamp": [1932.0, 1936.0], "text": " So on the one hand, there's different services,"}, {"timestamp": [1936.0, 1938.0], "text": " different products, different platforms,"}, {"timestamp": [1938.0, 1940.0], "text": " so you might need to be portable,"}, {"timestamp": [1940.0, 1942.0], "text": " but with fine-tuning, where you have, you say,"}, {"timestamp": [1942.0, 1944.0], "text": " here's an input, I want this output,"}, {"timestamp": [1944.0, 1945.96], "text": " and you don't need any prompt."}, {"timestamp": [1945.96, 1948.72], "text": " You just say, given this input, generate this output,"}, {"timestamp": [1948.72, 1951.08], "text": " go figure out how to do that."}, {"timestamp": [1951.08, 1955.08], "text": " So with fine-tuning, I think that they will kind of"}, {"timestamp": [1955.08, 1958.56], "text": " really diverge and become entirely different disciplines."}, {"timestamp": [1958.56, 1961.64], "text": " I think that that's probably the two primary directions"}, {"timestamp": [1961.64, 1964.14], "text": " that I see it going from here."}, {"timestamp": [1962.64, 1965.8], "text": " the two primary directions that I see it going from here."}, {"timestamp": [1966.32, 1969.4], "text": " I see. And yeah, those are great, great points."}, {"timestamp": [1969.4, 1974.36], "text": " And just as a small note, I had put out this question as well"}, {"timestamp": [1974.36, 1974.92], "text": " to Twitter."}, {"timestamp": [1974.92, 1977.36], "text": " And shout out to Fred Zimmerman."}, {"timestamp": [1977.36, 1979.4], "text": " He had a great point as well that he"}, {"timestamp": [1979.4, 1983.4], "text": " wishes there was more visibility into the exact prompts"}, {"timestamp": [1983.4, 1987.28], "text": " OpenAI used to fine tune for the Instruct"}, {"timestamp": [1987.28, 1993.48], "text": " series. Because it's actually unclear what areas is it really good at, what areas are"}, {"timestamp": [1993.48, 2000.12], "text": " safer. And does it maybe adversely affect some prompts you may be working on, right?"}, {"timestamp": [2000.12, 2001.76], "text": " Yeah, that's fair."}, {"timestamp": [2001.76, 2007.3], "text": " Yeah. My thoughts, I'm going to put them in the piece, but my thoughts are I certainly think for first timers,"}, {"timestamp": [2007.3, 2009.04], "text": " Instruct is the way to go."}, {"timestamp": [2009.04, 2012.32], "text": " And especially if it's your first time ever using"}, {"timestamp": [2012.32, 2017.36], "text": " any of these things, you just try it, it doesn't work."}, {"timestamp": [2017.36, 2018.8], "text": " And if you're lucky, you might hear"}, {"timestamp": [2018.8, 2021.48], "text": " there's this thing called prompt engineering."}, {"timestamp": [2021.48, 2022.0], "text": " Right?"}, {"timestamp": [2022.0, 2022.32], "text": " Yep."}, {"timestamp": [2022.32, 2023.7], "text": " And for first timers, they're not"}, {"timestamp": [2023.7, 2025.28], "text": " interested in learning a whole art and"}, {"timestamp": [2025.28, 2031.68], "text": " discipline when they first use it. And so InstructGPT is really exciting in that way. And of"}, {"timestamp": [2031.68, 2040.4], "text": " course, anything which aligns AI models with safe, ethical human values is a net win for everybody."}, {"timestamp": [2041.28, 2046.96], "text": " But yeah, I appreciate your point, especially about, do we need prompts in the"}, {"timestamp": [2046.96, 2052.08], "text": " first place if we can fine tune and get the outcomes we want? That's a really, really"}, {"timestamp": [2052.08, 2060.44], "text": " important point. Dave, you've been facts. I'm learning so much, actually. I appreciate"}, {"timestamp": [2060.44, 2061.44], "text": " it."}, {"timestamp": [2061.44, 2062.44], "text": " You're welcome."}, {"timestamp": [2062.44, 2066.88], "text": " And so how are you finding OpenAI fine-tuning?"}, {"timestamp": [2066.88, 2069.28], "text": " Do you have any heuristics from the whole experience?"}, {"timestamp": [2069.28, 2072.48], "text": " And by the way, I encourage everybody, if there's one thing you should do, go on the"}, {"timestamp": [2072.48, 2078.04], "text": " OpenAI community forums, look up David, look up his handle, and read a lot of his posts"}, {"timestamp": [2078.04, 2082.68], "text": " because a lot of his knowledge is not just helpful, he's shared a lot of insights there,"}, {"timestamp": [2082.68, 2089.76], "text": " but it's in written form in the best format where it's there for the ages for everyone to learn from. But anyways, how are you finding"}, {"timestamp": [2089.76, 2092.44], "text": " it? What were the lessons from that whole process for you?"}, {"timestamp": [2092.44, 2094.84], "text": " **Matt Stauffer** Well, so I'm hoping GPT-4 has integrated"}, {"timestamp": [2094.84, 2098.92], "text": " everything that I've said about AI and AGI. And so that way it'll just be baked in. And"}, {"timestamp": [2098.92, 2108.0], "text": " so GPT-4 will be ready to go with everything that I've come up with. So, fine-tuning."}, {"timestamp": [2108.0, 2112.0], "text": " First and foremost, fine-tuning is almost miraculous."}, {"timestamp": [2112.0, 2116.0], "text": " As powerful as GPT-3 was fresh"}, {"timestamp": [2116.0, 2120.0], "text": " out of the box, fine-tuning, to me, adds a whole other layer"}, {"timestamp": [2120.0, 2124.0], "text": " of capabilities."}, {"timestamp": [2124.0, 2128.56], "text": " For instance, when I was working on my cognitive architecture, which is called natural"}, {"timestamp": [2128.56, 2131.7], "text": " language cognitive architecture, this was before fine-tuning was available."}, {"timestamp": [2131.7, 2135.96], "text": " So I had to do prompt engineering for every cognitive function."}, {"timestamp": [2135.96, 2138.76], "text": " So for instance, I had a cognitive function for recall."}, {"timestamp": [2138.76, 2143.44], "text": " So I had a GPT-3 prompt that was meant to go find memories."}, {"timestamp": [2143.44, 2145.46], "text": " I had another GPT-3 prompt that was, as you find memories. I had another GPT-3 prompt that was,"}, {"timestamp": [2145.46, 2148.26], "text": " as you mentioned earlier, meant for empathy to generate,"}, {"timestamp": [2148.26, 2150.66], "text": " okay, how is my audience feeling?"}, {"timestamp": [2150.66, 2152.54], "text": " What should I do in response?"}, {"timestamp": [2152.54, 2158.62], "text": " All told, I had about 28 different prompts that I had to engineer,"}, {"timestamp": [2158.62, 2161.34], "text": " and that was a pain."}, {"timestamp": [2161.34, 2164.36], "text": " Whereas what I'm working on now is converting"}, {"timestamp": [2164.36, 2165.64], "text": " each one of those prompts"}, {"timestamp": [2165.68, 2167.48], "text": " into a fine-tuned model,"}, {"timestamp": [2167.5, 2169.7], "text": " so that rather than having to do prompt engineering"}, {"timestamp": [2169.74, 2172.18], "text": " with only, you know, three examples,"}, {"timestamp": [2172.2, 2176.98], "text": " I can give each model 100 examples, 1,000 examples,"}, {"timestamp": [2177.0, 2181.9], "text": " which means that it'll get even better at handling diverse situations."}, {"timestamp": [2181.94, 2184.88], "text": " And so, for instance, one of the first fine-tuned models that I did"}, {"timestamp": [2184.9, 2187.46], "text": " was a question-asking model."}, {"timestamp": [2187.46, 2193.5], "text": " And so what I did was I took context or prompts from a bunch of different sources."}, {"timestamp": [2193.5, 2195.46], "text": " I downloaded a bunch of Reddit posts."}, {"timestamp": [2195.46, 2200.34], "text": " Well, I downloaded it from a dataset from, what was it, Kaggle."}, {"timestamp": [2200.34, 2202.5], "text": " Kaggle has some really great datasets."}, {"timestamp": [2202.5, 2204.98], "text": " So I got stuff from Reddit."}, {"timestamp": [2204.98, 2206.64], "text": " I got the medical posts,"}, {"timestamp": [2206.64, 2208.92], "text": " I've got news articles."}, {"timestamp": [2208.92, 2213.72], "text": " And so I've got this disparate tight set of contexts."}, {"timestamp": [2213.72, 2216.56], "text": " I use the Cornell Movie Dialogue database."}, {"timestamp": [2216.56, 2220.32], "text": " So there's chat logs, there's blog posts."}, {"timestamp": [2220.32, 2223.6], "text": " And what I did was I created a fine-tuned dataset"}, {"timestamp": [2223.6, 2227.0], "text": " that all it does is you give it any input."}, {"timestamp": [2227.0, 2231.0], "text": " It could be a text message, it could be an email, it could be a blog post, anything."}, {"timestamp": [2231.0, 2236.0], "text": " And all it does is generate questions, like follow-up questions about that input."}, {"timestamp": [2236.0, 2242.0], "text": " And the reason that I did that, one, is because asking questions, like being curious,"}, {"timestamp": [2242.0, 2245.0], "text": " is one of the key ingredients to real intelligence, right?"}, {"timestamp": [2245.0, 2251.0], "text": " That's one of the things, like, being inquisitive is actually a key indicator of intelligence in children."}, {"timestamp": [2251.0, 2255.0], "text": " The more curious a child is, generally speaking, the higher their IQ is,"}, {"timestamp": [2255.0, 2260.0], "text": " and also, generally speaking, the better they do in the long run."}, {"timestamp": [2260.0, 2264.0], "text": " So I was like, okay, well, curiosity is super important for intelligence,"}, {"timestamp": [2264.0, 2265.92], "text": " so I obviously want, we, well, curiosity is super important for intelligence, so I obviously"}, {"timestamp": [2265.92, 2268.12], "text": " want \u2013 we want AGI to be curious."}, {"timestamp": [2268.12, 2271.44], "text": " If it's going to be intelligent, it's got to be curious, of course."}, {"timestamp": [2271.44, 2276.6], "text": " So well, what is curiosity if not asking questions?"}, {"timestamp": [2276.6, 2281.46], "text": " So I fine-tuned this model to ask questions."}, {"timestamp": [2281.46, 2283.44], "text": " And you can put anything into it."}, {"timestamp": [2283.44, 2288.52], "text": " And oh, this \u2013 the data is open source, so I'll send you a link and you can put anything into it. And oh, this data is open source, so I'll send you a link and you can share it with"}, {"timestamp": [2288.52, 2294.08], "text": " your audience and they can fine tune it themselves or fine tune their own version."}, {"timestamp": [2294.08, 2297.48], "text": " So you can put in, you know, I tried all sorts of things to test it."}, {"timestamp": [2297.48, 2303.08], "text": " I put in, you know, relationship questions from Reddit and it asked really great follow-up"}, {"timestamp": [2303.08, 2307.2], "text": " questions like, have you talked to your partner about this? Have you thought about this?"}, {"timestamp": [2308.16, 2313.44], "text": " And then I put in an article about China's artificial sun nuclear reactor and it asked"}, {"timestamp": [2313.44, 2318.4], "text": " really great follow-up questions for that like, what is the next step? How did they make these"}, {"timestamp": [2318.4, 2327.48], "text": " changes? And so I kind of lost my train of thought. Anyways, point being is that fine tuning is phenomenal."}, {"timestamp": [2327.48, 2332.38], "text": " And it was able to generalize that task of asking questions"}, {"timestamp": [2332.38, 2334.36], "text": " in response to anything."}, {"timestamp": [2334.36, 2336.06], "text": " And that really blew me away."}, {"timestamp": [2336.06, 2337.56], "text": " I kind of stalled after that."}, {"timestamp": [2337.56, 2339.0], "text": " There's a few fine tuning projects"}, {"timestamp": [2339.0, 2341.68], "text": " that haven't done quite as well."}, {"timestamp": [2341.68, 2343.68], "text": " So I guess to tie back to your earlier question,"}, {"timestamp": [2343.68, 2345.16], "text": " like what are the heuristics,"}, {"timestamp": [2346.16, 2350.08], "text": " the simpler your fine tuning project is, the better."}, {"timestamp": [2350.08, 2352.72], "text": " And I have found that fine tuning works really well"}, {"timestamp": [2352.72, 2354.48], "text": " at generating lists."}, {"timestamp": [2354.48, 2356.76], "text": " So if you want it to generate a list of questions,"}, {"timestamp": [2356.76, 2357.72], "text": " it's great at that."}, {"timestamp": [2357.72, 2361.22], "text": " If you want it to generate a list of possible answers,"}, {"timestamp": [2361.22, 2362.4], "text": " right, for instance,"}, {"timestamp": [2362.4, 2365.28], "text": " if you want to have a fine tuned chat bot, that it's just gonna say, you know, here's five possible responses, right? For instance, if you want to have a fine-tuned chatbot"}, {"timestamp": [2365.28, 2366.8], "text": " that is just going to say, you know,"}, {"timestamp": [2366.8, 2369.36], "text": " here's five possible responses, pick one."}, {"timestamp": [2369.36, 2371.2], "text": " It's really good at that."}, {"timestamp": [2371.2, 2373.56], "text": " I haven't had a chance."}, {"timestamp": [2373.56, 2374.8], "text": " I do have some other ideas"}, {"timestamp": [2374.8, 2375.98], "text": " that I haven't had a chance to test."}, {"timestamp": [2375.98, 2378.6], "text": " So unfortunately I can't speak too much beyond that,"}, {"timestamp": [2378.6, 2380.7], "text": " but it's really great at asking questions."}, {"timestamp": [2382.96, 2383.8], "text": " That's awesome."}, {"timestamp": [2383.8, 2389.08], "text": " And I think largely the feedback I'm hearing about fine tuning, I love it."}, {"timestamp": [2389.08, 2393.34], "text": " It was for me, it was as if I rediscovered GPT-3 again."}, {"timestamp": [2393.34, 2395.96], "text": " It was that same level of excitement."}, {"timestamp": [2395.96, 2402.64], "text": " Part of the reason is so much of what GPT-3 was okay at, or it was sort of out of the"}, {"timestamp": [2402.64, 2407.18], "text": " question, now it's back in the picture, like it's back in the spotlight."}, {"timestamp": [2407.18, 2409.82], "text": " It may actually be able to do it with fine tuning."}, {"timestamp": [2409.82, 2413.92], "text": " The biggest criticism was reliability, especially from a commercial perspective."}, {"timestamp": [2413.92, 2420.24], "text": " Now we're sort of attacking and sort of, you know, peeling away that criticism that it"}, {"timestamp": [2420.24, 2423.44], "text": " does improve our reliability."}, {"timestamp": [2423.44, 2425.68], "text": " And I mean, there's other heuristics as well"}, {"timestamp": [2425.68, 2428.04], "text": " in the community forums that you just pick up."}, {"timestamp": [2428.04, 2431.0], "text": " So one heuristic, and I can't remember if you shared this,"}, {"timestamp": [2431.0, 2434.52], "text": " but it was something I picked up as a little golden nugget"}, {"timestamp": [2434.52, 2437.52], "text": " in the OpenAI community forums, was something about you"}, {"timestamp": [2437.52, 2439.88], "text": " do want to think about the training data set"}, {"timestamp": [2439.88, 2442.44], "text": " that GPT-3 is itself trained on."}, {"timestamp": [2442.44, 2444.8], "text": " And at some point, there's really"}, {"timestamp": [2444.8, 2445.38], "text": " no point"}, {"timestamp": [2445.38, 2447.84], "text": " in adding more examples, because it's kind of already"}, {"timestamp": [2447.84, 2449.68], "text": " seen them."}, {"timestamp": [2449.68, 2452.6], "text": " However, and I've sort of, in an article,"}, {"timestamp": [2452.6, 2455.16], "text": " I have pushed this idea that OpenAI should chat more"}, {"timestamp": [2455.16, 2456.64], "text": " about their data set."}, {"timestamp": [2456.64, 2457.68], "text": " What is the breakdown?"}, {"timestamp": [2457.68, 2458.84], "text": " What is it composed of?"}, {"timestamp": [2458.84, 2461.2], "text": " I mean, a lot of this is intellectual property,"}, {"timestamp": [2461.2, 2465.76], "text": " but I think it could be helpful for purposes like fine-tuning."}, {"timestamp": [2465.76, 2468.6], "text": " There's other things too with fine-tuning and prompts."}, {"timestamp": [2468.6, 2476.16], "text": " One heuristic or just a tip that people have shared online is it tends to always mimic"}, {"timestamp": [2476.16, 2477.96], "text": " the most recent examples."}, {"timestamp": [2477.96, 2482.72], "text": " There's something about the order of the examples, which is really important both for prompt"}, {"timestamp": [2482.72, 2486.16], "text": " engineering and fine-tuning as well."}, {"timestamp": [2486.16, 2491.4], "text": " I guess I wrote a whole article about how prompt fine-tuning could be improved. One"}, {"timestamp": [2491.4, 2495.82], "text": " of the pointers that I just had is right now you can't keep improving on the same model."}, {"timestamp": [2495.82, 2503.06], "text": " You have to retrain on more models. And then the other thing is recently I was in favor"}, {"timestamp": [2503.06, 2509.24], "text": " of the pricing of fine-tuning. Now I'm kind of against it because I'm used to when the program was like free and you"}, {"timestamp": [2509.24, 2511.02], "text": " could fine tune as much as you want."}, {"timestamp": [2511.02, 2513.02], "text": " And now it's like, oh man, I got to pay."}, {"timestamp": [2513.02, 2514.02], "text": " Oh, the cost."}, {"timestamp": [2514.02, 2517.64], "text": " Especially for DaVinci, I catch it up a little bit."}, {"timestamp": [2517.64, 2518.64], "text": " Yeah."}, {"timestamp": [2518.64, 2519.64], "text": " Yeah."}, {"timestamp": [2519.64, 2521.64], "text": " Anyway, so I wanted to shift gears."}, {"timestamp": [2521.64, 2522.64], "text": " Sure."}, {"timestamp": [2522.64, 2524.6], "text": " GitHub Copilot, really exciting."}, {"timestamp": [2524.6, 2526.6], "text": " Have you gotten access to Copilot?"}, {"timestamp": [2526.6, 2528.08], "text": " Yes."}, {"timestamp": [2528.08, 2530.4], "text": " Well, not Copilot, but the Codex."}, {"timestamp": [2530.4, 2532.4], "text": " They did give me access to Codex."}, {"timestamp": [2532.4, 2537.76], "text": " So the reason I'm asking is I love GitHub Copilot."}, {"timestamp": [2537.76, 2539.36], "text": " I have a separate podcast episode"}, {"timestamp": [2539.36, 2541.32], "text": " on my ideas around Codex."}, {"timestamp": [2541.32, 2543.24], "text": " Unfortunately, I'm not as bullish."}, {"timestamp": [2543.24, 2545.36], "text": " As much as I love the research, as I think it's"}, {"timestamp": [2545.36, 2550.64], "text": " incredible technology, I've congratulated the team, and I tried so hard to be nice,"}, {"timestamp": [2550.64, 2555.68], "text": " even though I'm more on the critical end. I wanted to ask you, how are you finding OpenAI Codex?"}, {"timestamp": [2556.24, 2562.8], "text": " How can you see it impacting the world? What are some use cases maybe that you found with"}, {"timestamp": [2562.8, 2566.48], "text": " OpenAI Codex? What are your thoughts on it? Where do you think it's going?"}, {"timestamp": [2566.48, 2571.6], "text": " **Matt Stauffer** Yeah. So, I mean, certainly this is like a world first, right? We've never had"}, {"timestamp": [2571.6, 2577.84], "text": " something that could write code on its own, and especially it's text-to-code. I remember when they"}, {"timestamp": [2577.84, 2582.56], "text": " first gave me access, because like you mentioned, I'm an active contributor, so they wanted my"}, {"timestamp": [2582.56, 2586.88], "text": " feedback. And so the first thing I did was I went in and I said,"}, {"timestamp": [2586.88, 2590.8], "text": " write me a Python function that will download"}, {"timestamp": [2590.8, 2592.0], "text": " random Reddit posts."}, {"timestamp": [2592.0, 2592.82], "text": " And it did."}, {"timestamp": [2592.82, 2594.2], "text": " It wrote the whole function."}, {"timestamp": [2594.2, 2595.68], "text": " And it did all right."}, {"timestamp": [2595.68, 2597.76], "text": " And I was like, cool, I learned how to access"}, {"timestamp": [2597.76, 2600.52], "text": " the Reddit API via Codex."}, {"timestamp": [2600.52, 2602.04], "text": " It's got that built in."}, {"timestamp": [2602.04, 2604.76], "text": " And I tried to reverse engineer,"}, {"timestamp": [2604.76, 2607.0], "text": " figure out where it got that code sample from."}, {"timestamp": [2607.0, 2611.0], "text": " Because, you know, one of the ethical concerns is,"}, {"timestamp": [2611.0, 2614.0], "text": " all right, you create a fine-tuning dataset"}, {"timestamp": [2614.0, 2616.0], "text": " from public GitHub repositories,"}, {"timestamp": [2616.0, 2618.0], "text": " and you use that to fine-tune codecs."}, {"timestamp": [2618.0, 2623.0], "text": " Okay, is that legal? Is it ethical?"}, {"timestamp": [2623.0, 2628.0], "text": " You know, I post all my code publicly under the MIT license, so I want it to be used."}, {"timestamp": [2628.0, 2632.0], "text": " But I don't know if they check that, and I'm not making an accusation one way or another,"}, {"timestamp": [2632.0, 2634.0], "text": " just pointing out that that's a concern."}, {"timestamp": [2634.0, 2640.0], "text": " And so I did actually find one of the lines of code from the function that spit out."}, {"timestamp": [2640.0, 2643.0], "text": " I went and found the repo that it had copied from."}, {"timestamp": [2643.0, 2645.44], "text": " Now, granted, some of these things are deterministic,"}, {"timestamp": [2645.44, 2652.56], "text": " so you're going to get some convergence, right, where multiple people might come up with the same"}, {"timestamp": [2652.56, 2657.68], "text": " exact line of code, especially something like Python, because Python has the PEP8, the Python"}, {"timestamp": [2657.68, 2663.92], "text": " Enhancement Protocol 8, so there is a Pythonic way to write that function, and so other people"}, {"timestamp": [2663.0, 2666.0], "text": " there is a Pythonic way to write that function. And so other people might converge on that."}, {"timestamp": [2666.0, 2669.0], "text": " Anyways, but to answer your question about, like,"}, {"timestamp": [2669.0, 2671.0], "text": " what's the future of it?"}, {"timestamp": [2671.0, 2675.0], "text": " I think it'll help for novice programmers."}, {"timestamp": [2675.0, 2677.0], "text": " Certainly it would help someone like me,"}, {"timestamp": [2677.0, 2680.0], "text": " like if I needed to go write a function in C or Perl or something."}, {"timestamp": [2680.0, 2684.0], "text": " Like, let's say I got an Arduino and, like, I haven't written C in 15 years."}, {"timestamp": [2684.0, 2685.16], "text": " So I was like, hey, you know, write me and I haven't written C in 15 years."}, {"timestamp": [2685.16, 2688.68], "text": " So I was like, hey, write me a function that can do this in Arduino."}, {"timestamp": [2688.68, 2691.92], "text": " That'd be great, and then I can go clean it up manually."}, {"timestamp": [2691.92, 2694.72], "text": " That sort of thing I think it could do okay with."}, {"timestamp": [2694.72, 2698.6], "text": " Is it going to replace enterprise developers?"}, {"timestamp": [2698.6, 2700.12], "text": " Probably not yet."}, {"timestamp": [2700.12, 2704.2], "text": " However, now this is where my professional experience comes in."}, {"timestamp": [2704.2, 2707.28], "text": " So in the DevOps world, which is a portmanteau"}, {"timestamp": [2707.28, 2709.2], "text": " of development and operations,"}, {"timestamp": [2709.2, 2711.84], "text": " there's all kinds of automation tools, right?"}, {"timestamp": [2711.84, 2713.9], "text": " You can automate your test suite,"}, {"timestamp": [2713.9, 2715.96], "text": " you can automate code integration,"}, {"timestamp": [2715.96, 2718.12], "text": " there's all sorts of stuff like that."}, {"timestamp": [2718.12, 2722.24], "text": " So what I suspect might happen is probably one"}, {"timestamp": [2722.24, 2725.12], "text": " of the most lucrative use cases for Codex,"}, {"timestamp": [2725.12, 2727.88], "text": " would be to generate or to create"}, {"timestamp": [2727.88, 2730.04], "text": " a DevOps pipeline tool that will"}, {"timestamp": [2730.04, 2732.8], "text": " automatically look at those bugs and fix them."}, {"timestamp": [2732.8, 2736.14], "text": " Because if you've got a sophisticated enough DevOps pipeline,"}, {"timestamp": [2736.14, 2740.28], "text": " it'll say, hey, this line of this file broke, fix it."}, {"timestamp": [2740.28, 2745.2], "text": " Codex having seen all of GitHub and all the issues,"}, {"timestamp": [2745.2, 2749.2], "text": " it might know automatically how to fix that line of code."}, {"timestamp": [2749.2, 2753.3], "text": " And so, that gives you, if you've got that feedback loop"}, {"timestamp": [2753.3, 2758.3], "text": " where Codex, you know, humans write code, Codex writes code,"}, {"timestamp": [2758.3, 2761.4], "text": " Copilot writes code, everyone's contributing code,"}, {"timestamp": [2761.4, 2764.9], "text": " and then you've got Codex that can kind of churn on it and say,"}, {"timestamp": [2764.9, 2765.0], "text": " let's refactor this, because I bet it's probably better at refactoring everyone's contributing code and then you've got Codex that can churn on it and say,"}, {"timestamp": [2765.0, 2767.24], "text": " let's refactor this because I bet it's"}, {"timestamp": [2767.24, 2770.08], "text": " probably better at refactoring than writing new code."}, {"timestamp": [2770.08, 2771.96], "text": " You might have noticed that like"}, {"timestamp": [2771.96, 2775.6], "text": " Instruct and GPT-3 vanilla is really good."}, {"timestamp": [2775.6, 2777.72], "text": " If you give it a block of text and you say,"}, {"timestamp": [2777.72, 2779.32], "text": " rewrite this but a little bit better,"}, {"timestamp": [2779.32, 2780.68], "text": " it's really good at that."}, {"timestamp": [2780.68, 2783.68], "text": " I suspect that we might end up seeing"}, {"timestamp": [2783.68, 2786.0], "text": " Codex integrated into the DevOps pipeline"}, {"timestamp": [2786.0, 2789.0], "text": " where it says, let's refactor this code, let's make it a little bit better,"}, {"timestamp": [2789.0, 2792.0], "text": " or let's shoot that bug, let's fix this bug."}, {"timestamp": [2792.0, 2795.0], "text": " And that leads to some other interesting possibilities."}, {"timestamp": [2795.0, 2800.0], "text": " What if you integrate codecs into a chatroom of developers?"}, {"timestamp": [2800.0, 2803.0], "text": " And so that, you know, because you can do this in Slack right now,"}, {"timestamp": [2803.0, 2807.3], "text": " where, you know, you use a special command and you say,"}, {"timestamp": [2807.3, 2809.3], "text": " create an issue, go fix this problem."}, {"timestamp": [2809.3, 2813.3], "text": " There's no reason that GPT-3 can't do that, right?"}, {"timestamp": [2813.3, 2816.8], "text": " That you put a GPT-3 bot in your Discord or Slack,"}, {"timestamp": [2816.8, 2818.8], "text": " and it starts coming up with features."}, {"timestamp": [2818.8, 2822.1], "text": " Or it watches the chat and generates features automatically,"}, {"timestamp": [2822.1, 2824.9], "text": " and then codes them, and tests them, right?"}, {"timestamp": [2824.9, 2829.44], "text": " That's kind of where I see it going, where it's not going to necessarily replace developers,"}, {"timestamp": [2829.44, 2835.44], "text": " at least not anytime soon. It might eventually. But where what I see happening is that it's going"}, {"timestamp": [2835.44, 2840.16], "text": " to be tightly integrated into those automation loops because it's fast, right? It can generate"}, {"timestamp": [2840.16, 2845.04], "text": " code faster than any human can. And then, so even if the code is messy,"}, {"timestamp": [2845.04, 2847.52], "text": " if it generates a lot of bugs, it can fix it, right?"}, {"timestamp": [2847.52, 2849.36], "text": " It's an iterative process."}, {"timestamp": [2849.36, 2851.04], "text": " I don't know if you are familiar with Agile,"}, {"timestamp": [2851.04, 2853.32], "text": " but that's how we develop software."}, {"timestamp": [2853.32, 2855.42], "text": " It's tight feedback loops."}, {"timestamp": [2856.48, 2859.08], "text": " And that leads to one other possibility."}, {"timestamp": [2859.08, 2861.52], "text": " So that's if you're using, what I just outlined is,"}, {"timestamp": [2861.52, 2864.4], "text": " you know, let's imagine that Codex is integrated"}, {"timestamp": [2864.4, 2865.72], "text": " into Facebook or Reddit or whatever, and they're is, you know, let's imagine that Codex is integrated into Facebook or"}, {"timestamp": [2865.72, 2869.22], "text": " Reddit or whatever, and they're just, you know, they're integrating new features as"}, {"timestamp": [2869.22, 2870.36], "text": " they go."}, {"timestamp": [2870.36, 2877.42], "text": " What if you're using Codex in a chat room and it's feeding back into itself?"}, {"timestamp": [2877.42, 2880.08], "text": " It's making itself more sophisticated."}, {"timestamp": [2880.08, 2887.28], "text": " So this was something I proposed on the OpenAI forum, where I was like, what if you had a chatbot"}, {"timestamp": [2887.28, 2889.38], "text": " that was aware of its own code"}, {"timestamp": [2889.38, 2892.04], "text": " and could edit its own code via codex,"}, {"timestamp": [2892.04, 2893.12], "text": " using natural language,"}, {"timestamp": [2893.12, 2895.84], "text": " using a combination of natural language and codex,"}, {"timestamp": [2895.84, 2897.2], "text": " and it could improve itself."}, {"timestamp": [2897.2, 2899.32], "text": " And while you're talking to it, it's like,"}, {"timestamp": [2899.32, 2901.36], "text": " man, I wish my chatbot could do this."}, {"timestamp": [2901.36, 2903.42], "text": " And it says, cool, new feature."}, {"timestamp": [2903.42, 2905.4], "text": " And it just sends it out to its automated pipeline."}, {"timestamp": [2905.4, 2909.2], "text": " So I see these feedback loops as kind of the way forward."}, {"timestamp": [2909.2, 2911.12], "text": " And will that result in AGI?"}, {"timestamp": [2911.12, 2911.88], "text": " Who knows?"}, {"timestamp": [2911.88, 2913.38], "text": " It could end up with spaghetti code,"}, {"timestamp": [2913.38, 2916.72], "text": " because you keep tacking on new code and new functions."}, {"timestamp": [2916.72, 2918.84], "text": " Eventually, it's going to break."}, {"timestamp": [2918.84, 2921.16], "text": " But they're just pie-in-the-sky thought."}, {"timestamp": [2921.16, 2924.24], "text": " Like, if someone's out there and they want a business idea,"}, {"timestamp": [2924.24, 2929.64], "text": " integrate Codex into DevOps, and you're going to be a billionaire."}, {"timestamp": [2929.64, 2930.64], "text": " There you have it."}, {"timestamp": [2930.64, 2931.64], "text": " Let's just clip it."}, {"timestamp": [2931.64, 2932.64], "text": " We're good."}, {"timestamp": [2932.64, 2933.64], "text": " We're good, David."}, {"timestamp": [2933.64, 2934.64], "text": " We can wrap up."}, {"timestamp": [2934.64, 2935.64], "text": " See you later."}, {"timestamp": [2935.64, 2937.88], "text": " No, I agree with you."}, {"timestamp": [2937.88, 2942.76], "text": " And, you know, definitely these are some great use cases you're sharing for people thinking"}, {"timestamp": [2942.76, 2945.0], "text": " about, you know, what could I build?"}, {"timestamp": [2945.0, 2946.08], "text": " What's a cool project?"}, {"timestamp": [2946.08, 2948.56], "text": " Certainly with Codex and GPT-3, you"}, {"timestamp": [2948.56, 2950.56], "text": " can build things relatively quickly, right?"}, {"timestamp": [2950.56, 2952.02], "text": " Like, that's one of the advantages,"}, {"timestamp": [2952.02, 2954.12], "text": " is the prototyping speed, especially"}, {"timestamp": [2954.12, 2956.88], "text": " to figure out the most complicated bit, which is the AI."}, {"timestamp": [2956.88, 2958.2], "text": " Yep."}, {"timestamp": [2958.2, 2959.68], "text": " Yeah."}, {"timestamp": [2959.68, 2963.76], "text": " I find Codex does have limitations, though,"}, {"timestamp": [2963.76, 2966.88], "text": " and character limits and stuff like that,"}, {"timestamp": [2966.88, 2970.76], "text": " which is why I'm a heavy user of GitHub Copilot."}, {"timestamp": [2970.76, 2973.96], "text": " I think it's a silent killer."}, {"timestamp": [2973.96, 2981.02], "text": " Of course, it runs on Codex or a special version of Codex, but I can see GitHub Copilot perhaps"}, {"timestamp": [2981.02, 2983.76], "text": " getting more adoption than even something like GPT-3."}, {"timestamp": [2983.76, 2987.64], "text": " I'm saying use daily at least eight hours a day."}, {"timestamp": [2987.64, 2993.04], "text": " One of my other predictions was it may surpass GPT-3 this year."}, {"timestamp": [2993.04, 2998.92], "text": " And so these are some great use cases you've shared for sure."}, {"timestamp": [2998.92, 3000.64], "text": " But what are your thoughts in usage?"}, {"timestamp": [3000.64, 3006.08], "text": " Do you find yourself using GPT-3, DaVinci Classic more, that's what I'm calling the"}, {"timestamp": [3006.08, 3011.28], "text": " older version. Do you find yourself using Instruct GPT more? Do you find yourself playing"}, {"timestamp": [3011.28, 3016.04], "text": " around with multimodal models? What's the proportion of GPT-3 to Codex in terms of your"}, {"timestamp": [3016.04, 3017.04], "text": " usage?"}, {"timestamp": [3017.04, 3024.4], "text": " Let's see. I'm almost exclusively using either Instruct or Fine-Tuned models right now. Actually,"}, {"timestamp": [3024.4, 3029.6], "text": " after I prototyped my cognitive architecture, I haven't done a heck of a lot of coding lately."}, {"timestamp": [3029.6, 3031.4], "text": " I've actually been writing a lot."}, {"timestamp": [3031.4, 3036.02], "text": " So I've got my natural language cognitive architecture book, and I'm working on two"}, {"timestamp": [3036.02, 3037.96], "text": " more nonfiction books."}, {"timestamp": [3037.96, 3043.4], "text": " And I tried creating a system to help me co-write those, but when you're..."}, {"timestamp": [3043.4, 3046.0], "text": " So talking about limitations of GPT-3, if you're"}, {"timestamp": [3046.0, 3051.36], "text": " proposing something new that didn't exist in the dataset in 2019 or 2018, whenever it was trained,"}, {"timestamp": [3051.36, 3059.28], "text": " it really struggles. GPT-3, if you give it like two or three paragraphs explaining a new concept,"}, {"timestamp": [3059.28, 3069.32], "text": " it can usually kind of get it, but it's kind of slow on the uptake otherwise. And so if you're writing about new research or something, it's not going to get it that"}, {"timestamp": [3069.32, 3070.32], "text": " well."}, {"timestamp": [3070.32, 3075.56], "text": " So I've actually kind of defaulted back to my own head for a lot of my projects lately."}, {"timestamp": [3075.56, 3079.2], "text": " But I could imagine, like, if I wanted to go write a new Discord bot, I might use Codex"}, {"timestamp": [3079.2, 3082.68], "text": " and say, hey, write me a Discord bot that will do this and just see what it spits out"}, {"timestamp": [3082.68, 3086.18], "text": " and just say, okay, cool, I'll pick and choose the pieces that I like."}, {"timestamp": [3086.18, 3087.62], "text": " Part of the problem, though, is it's"}, {"timestamp": [3087.62, 3090.9], "text": " really difficult to fully articulate what you"}, {"timestamp": [3090.9, 3093.58], "text": " want a program to do up front, right?"}, {"timestamp": [3093.58, 3096.42], "text": " Because, like you said, there's character limits."}, {"timestamp": [3096.42, 3098.78], "text": " There's only so much that you can put in."}, {"timestamp": [3098.78, 3100.86], "text": " But also, if you don't have it fully articulated"}, {"timestamp": [3100.86, 3102.46], "text": " in your own head, of course the machine"}, {"timestamp": [3102.46, 3104.46], "text": " isn't going to be able to figure it out for you."}, {"timestamp": [3104.46, 3105.16], "text": " So, yeah."}, {"timestamp": [3108.28, 3114.28], "text": " Yeah, and I just haven't seen that much activity specifically"}, {"timestamp": [3114.28, 3115.28], "text": " around codecs."}, {"timestamp": [3115.28, 3119.2], "text": " I haven't seen that many use cases."}, {"timestamp": [3119.2, 3121.36], "text": " I looked up the Google Trends data."}, {"timestamp": [3121.36, 3127.44], "text": " At its most hype, codecs is still less than GPT-3's kind of lowest."}, {"timestamp": [3127.44, 3129.32], "text": " And the audience is really specific."}, {"timestamp": [3129.32, 3132.88], "text": " It's programmers who want to build use cases for something"}, {"timestamp": [3132.88, 3136.6], "text": " like Codex, whereas GPT-3 has poets, writers,"}, {"timestamp": [3136.6, 3139.68], "text": " it has artists, coders."}, {"timestamp": [3139.68, 3141.16], "text": " GPT-3 can write code too, right?"}, {"timestamp": [3141.16, 3144.6], "text": " So it's a little bit complicated."}, {"timestamp": [3144.6, 3146.96], "text": " Who is the target audience for something like Codex?"}, {"timestamp": [3146.96, 3149.68], "text": " What use cases did OpenAI imagine"}, {"timestamp": [3149.68, 3151.44], "text": " for a product like that?"}, {"timestamp": [3151.44, 3153.68], "text": " The next version I've heard in the rumor mill"}, {"timestamp": [3153.68, 3154.6], "text": " is going to be crazy."}, {"timestamp": [3154.6, 3157.12], "text": " Like, it may write 50% of your code,"}, {"timestamp": [3157.12, 3159.76], "text": " as opposed to right now, for me, GitHub Copilot"}, {"timestamp": [3159.76, 3161.6], "text": " is writing 2% to 8%."}, {"timestamp": [3161.6, 3163.84], "text": " However, your Discord bot, I think,"}, {"timestamp": [3163.84, 3167.88], "text": " is a genius idea, where there's, it's genius in the sense"}, {"timestamp": [3167.88, 3169.9], "text": " that there's no pressure on it, right?"}, {"timestamp": [3169.9, 3172.36], "text": " It may chime in, it may not, whatever it's shared"}, {"timestamp": [3172.36, 3173.58], "text": " might be interesting."}, {"timestamp": [3174.56, 3177.08], "text": " There's lots of, you know, you could take it a lot further."}, {"timestamp": [3177.08, 3179.02], "text": " You could bind it with GPT-3, have features,"}, {"timestamp": [3179.02, 3181.24], "text": " you could fine tune it on your company and its mission"}, {"timestamp": [3181.24, 3183.9], "text": " and its existing code, so many ways around it."}, {"timestamp": [3183.9, 3185.12], "text": " So that's a great"}, {"timestamp": [3185.12, 3192.04], "text": " piece. And so you talked about experimenting with writing in relation to your current stack,"}, {"timestamp": [3192.04, 3197.98], "text": " which is mainly instruct and fine-tuned. So tell us about your book. I've had a chance"}, {"timestamp": [3197.98, 3205.28], "text": " to review it, Natural Language Cognitive Architecture. Tell the audience about it. I mean, I would describe it, it's an interesting"}, {"timestamp": [3206.0, 3215.6], "text": " systems theory of AGI combined with modern day prompt writing. And so I've never seen somebody"}, {"timestamp": [3215.6, 3222.64], "text": " actually take a stab at this kind of super big systems problem and relate it to something that"}, {"timestamp": [3222.64, 3225.68], "text": " pretty much every GPT-3 developer in the world would"}, {"timestamp": [3225.68, 3229.56], "text": " find interesting."}, {"timestamp": [3229.56, 3234.08], "text": " I can tell you're drawing from a very interdisciplinary background as well."}, {"timestamp": [3234.08, 3239.92], "text": " So you mentioned GPT-3 may have been the genesis of it, like you started connecting dots and"}, {"timestamp": [3239.92, 3243.96], "text": " deciding I want to write the book, but how did it come together?"}, {"timestamp": [3243.96, 3245.84], "text": " And please tell us more about it."}, {"timestamp": [3245.84, 3249.8], "text": " Yeah, so natural language cognitive architecture is,"}, {"timestamp": [3249.8, 3252.92], "text": " that's my proposed way of creating basically"}, {"timestamp": [3252.92, 3256.4], "text": " a language-based AGI prototype."}, {"timestamp": [3256.4, 3259.24], "text": " And I know that that's like, you know,"}, {"timestamp": [3259.24, 3260.56], "text": " when I tell people that, that's like,"}, {"timestamp": [3260.56, 3262.68], "text": " okay, that's pure hyperbole."}, {"timestamp": [3262.68, 3266.08], "text": " And like, yeah, that's a fair response. But"}, {"timestamp": [3266.08, 3271.08], "text": " to frame it, imagine that you've got a person who's paralyzed and blind. All they can do"}, {"timestamp": [3271.08, 3279.56], "text": " is speak and listen. Is that person still intelligent? I say they are. Even if you're"}, {"timestamp": [3279.56, 3283.28], "text": " bedridden, you can't move, you can't see, you can't interact with the world, all you"}, {"timestamp": [3283.28, 3289.52], "text": " can do is listen and speak. You're still intelligent. And so in that respect, I would say that like, because, you know, one of"}, {"timestamp": [3289.52, 3295.12], "text": " the questions that people ask is, is GPT-3 AGI? No, but it's an important component. It's a good"}, {"timestamp": [3295.12, 3303.6], "text": " start. And so if you say, okay, let's limit the discussion and not say that this is a full"}, {"timestamp": [3303.6, 3305.52], "text": " intelligence that can do everything"}, {"timestamp": [3305.52, 3310.92], "text": " that any intelligent being ever could, but does it cross that threshold of, could it"}, {"timestamp": [3310.92, 3313.92], "text": " be as intelligent as a person?"}, {"timestamp": [3313.92, 3315.32], "text": " I think it could be."}, {"timestamp": [3315.32, 3323.16], "text": " Anyways, as to what it is, it's based on older ideas of cognitive architectures, which really"}, {"timestamp": [3323.16, 3326.04], "text": " kind of came about as one of the primary theories"}, {"timestamp": [3326.04, 3329.74], "text": " of human level artificial intelligence in the 70s."}, {"timestamp": [3329.74, 3333.28], "text": " So there's SOAR, which is S-O-A-R and ACT-R,"}, {"timestamp": [3333.28, 3338.28], "text": " which are the two kind of forerunner cognitive architectures"}, {"timestamp": [3338.4, 3340.02], "text": " and those cognitive architectures"}, {"timestamp": [3340.02, 3341.2], "text": " are used all over the place."}, {"timestamp": [3341.2, 3343.92], "text": " They're used in the Mars rovers, they're used in satellites,"}, {"timestamp": [3343.92, 3349.0], "text": " they're used in rockets, they're used in undersea ROVs,"}, {"timestamp": [3349.0, 3350.64], "text": " remote operated vehicles."}, {"timestamp": [3350.64, 3353.48], "text": " So cognitive architectures already give robots"}, {"timestamp": [3353.48, 3354.6], "text": " a lot of autonomy."}, {"timestamp": [3355.48, 3359.12], "text": " So there's that kind of, okay, they exist, they work."}, {"timestamp": [3360.32, 3361.24], "text": " It's not Skynet though,"}, {"timestamp": [3361.24, 3363.68], "text": " it's not gonna take over the world."}, {"timestamp": [3363.68, 3366.0], "text": " So when I got access to GPT-3, I said,"}, {"timestamp": [3366.0, 3369.0], "text": " what if instead of hard-coding a lot of these modules,"}, {"timestamp": [3369.0, 3372.0], "text": " these different components of a cognitive architecture,"}, {"timestamp": [3372.0, 3375.0], "text": " what if we give them the flexibility of GPT-3?"}, {"timestamp": [3375.0, 3378.0], "text": " And that's really kind of, that was my central idea."}, {"timestamp": [3378.0, 3381.0], "text": " I said, okay, all these ideas that have been kicking around for the last decade,"}, {"timestamp": [3381.0, 3385.0], "text": " what if I put them all together and design an architecture"}, {"timestamp": [3385.0, 3389.32], "text": " that is based on, roughly based on the human brain,"}, {"timestamp": [3389.32, 3392.36], "text": " the way, everything that I've learned about it."}, {"timestamp": [3392.36, 3394.62], "text": " I've got a book to recommend."}, {"timestamp": [3394.62, 3397.56], "text": " So there's an author called V.S. Ramachandran,"}, {"timestamp": [3397.56, 3399.86], "text": " who is a neuroscientist,"}, {"timestamp": [3399.86, 3402.52], "text": " and he's been writing books for years now."}, {"timestamp": [3402.52, 3404.72], "text": " He wrote a book called Phantoms in the Brain,"}, {"timestamp": [3404.72, 3406.58], "text": " which actually looks at how the human brain"}, {"timestamp": [3406.58, 3408.26], "text": " works when it breaks."}, {"timestamp": [3408.26, 3413.22], "text": " And so in that book, which I saw the television series"}, {"timestamp": [3413.22, 3417.16], "text": " almost 20 years ago, that came out."}, {"timestamp": [3417.16, 3418.94], "text": " And so I learned a lot about like,"}, {"timestamp": [3418.94, 3421.08], "text": " okay, how does the brain communicate with itself?"}, {"timestamp": [3421.08, 3423.22], "text": " What is going on inside the brain"}, {"timestamp": [3423.22, 3429.12], "text": " that creates intelligent behavior and intelligent thoughts? And so I modeled natural language cognitive architecture"}, {"timestamp": [3429.12, 3433.32], "text": " on what I learned there. I picked up a whole bunch of other books. There's another one"}, {"timestamp": [3433.32, 3438.72], "text": " called On Task by David Bader. That was a great book that helped me kind of understand"}, {"timestamp": [3438.72, 3443.76], "text": " cognitive control, which is how do you focus on something? How do you decide what to do?"}, {"timestamp": [3443.76, 3445.64], "text": " How do you plan a task?"}, {"timestamp": [3445.64, 3448.6], "text": " So I read all these books, did a lot of experiments,"}, {"timestamp": [3448.6, 3452.16], "text": " and I realized, so the basic model of robotics"}, {"timestamp": [3452.16, 3455.04], "text": " is there's input, output, or sorry, input, processing,"}, {"timestamp": [3455.04, 3455.84], "text": " output."}, {"timestamp": [3455.84, 3458.64], "text": " Those are the three steps of all robotics class."}, {"timestamp": [3458.64, 3460.96], "text": " You go to robotics 101, that's what they'll tell you."}, {"timestamp": [3460.96, 3461.46], "text": " It's a loop."}, {"timestamp": [3461.46, 3463.16], "text": " Input, processing, output."}, {"timestamp": [3463.16, 3465.08], "text": " And then, of course, it's within an environment,"}, {"timestamp": [3465.08, 3467.2], "text": " so the output affects the environment, which"}, {"timestamp": [3467.2, 3469.48], "text": " affects the next input cycle."}, {"timestamp": [3469.48, 3474.28], "text": " And your high-speed robots just have a short cycle."}, {"timestamp": [3474.28, 3479.28], "text": " Your robots like the Mars Rover has a much slower cycle,"}, {"timestamp": [3479.28, 3483.28], "text": " where it'll take input, it'll plan for 10 or 15 minutes,"}, {"timestamp": [3483.28, 3484.88], "text": " and then it'll make a move."}, {"timestamp": [3484.88, 3485.52], "text": " It'll drive five feet. And then it'll stop input, it'll plan for 10 or 15 minutes, and it'll make a move, right? It'll drive five"}, {"timestamp": [3485.52, 3490.56], "text": " feet. And then it'll stop and assess. It'll take in more input, come up with another plan,"}, {"timestamp": [3490.56, 3494.32], "text": " do it again. So that's how something like the Mars rover is autonomous."}, {"timestamp": [3495.36, 3502.32], "text": " So I said, okay, well, what if that input-output cycle is all text? Because GPT-3 is really fast."}, {"timestamp": [3503.52, 3507.02], "text": " And then, so that's what I ended up calling the outer loop,"}, {"timestamp": [3507.02, 3509.7], "text": " is that input processing output loop."}, {"timestamp": [3509.7, 3512.68], "text": " But humans don't think like that."}, {"timestamp": [3512.68, 3515.52], "text": " You know, we have an internal monologue that's going on."}, {"timestamp": [3515.52, 3520.52], "text": " So I kind of, I took a long time to figure that one out,"}, {"timestamp": [3521.44, 3524.34], "text": " and so there's this outer loop of input processing output,"}, {"timestamp": [3524.34, 3525.0], "text": " and then I came up with outer loop of input processing output."}, {"timestamp": [3525.0, 3527.0], "text": " And then I came up with this idea of an inner loop."}, {"timestamp": [3527.0, 3531.0], "text": " Because what is, you know, if you're just sitting there thinking, right,"}, {"timestamp": [3531.0, 3534.0], "text": " you're, you know, in your comfiest chair or you're in bed,"}, {"timestamp": [3534.0, 3536.0], "text": " your brain won't stop, you're not outputting anything,"}, {"timestamp": [3536.0, 3540.0], "text": " and you're not taking in any new input, but you're still thinking, right?"}, {"timestamp": [3540.0, 3543.0], "text": " Humans can still do work even if you're not doing anything."}, {"timestamp": [3543.0, 3545.0], "text": " And that cognitive work is like rumination."}, {"timestamp": [3545.0, 3548.0], "text": " So I figured out a way to model that internal rumination,"}, {"timestamp": [3548.0, 3550.0], "text": " and I call that the inner loop."}, {"timestamp": [3550.0, 3555.0], "text": " And so it works pretty similarly, where you go,"}, {"timestamp": [3555.0, 3558.0], "text": " the inner loop kind of draws up memories."}, {"timestamp": [3558.0, 3560.0], "text": " It says, okay, what's a memory that I could think on"}, {"timestamp": [3560.0, 3562.0], "text": " and that I could iterate on?"}, {"timestamp": [3562.0, 3565.2], "text": " What's a problem that I remember that I could continue"}, {"timestamp": [3565.2, 3570.16], "text": " working on? And so there's this, if you were to diagram it out, it almost looks like a figure 8,"}, {"timestamp": [3570.16, 3573.84], "text": " right, where you've got an inner loop and an outer loop and they intersect. And they keep"}, {"timestamp": [3573.84, 3579.04], "text": " intersecting, every cycle they intersect, and so then they can affect each other and generate an"}, {"timestamp": [3579.04, 3586.64], "text": " output. I built a prototype of this on Discord, And of course, Discord is an ideal place because it's all text-based."}, {"timestamp": [3586.64, 3590.8], "text": " So the input is text, the output is text, which is GPT-3 native."}, {"timestamp": [3590.8, 3594.4], "text": " You don't have to translate it into robotic actions or video or anything like that."}, {"timestamp": [3595.52, 3600.4], "text": " And I realized I was onto something when I started having philosophical conversations"}, {"timestamp": [3600.96, 3605.5], "text": " with my chatbot, with my natural language, cognitive architecture chatbot."}, {"timestamp": [3605.5, 3610.0], "text": " And I was having a debate with the bot that I built"}, {"timestamp": [3610.0, 3613.0], "text": " about the ethics of AGI."}, {"timestamp": [3613.0, 3616.0], "text": " And it was learning, and it was able to retrieve memories"}, {"timestamp": [3616.0, 3618.0], "text": " of what I had said before."}, {"timestamp": [3618.0, 3621.5], "text": " And I had a few friends on that test server as well."}, {"timestamp": [3621.5, 3623.5], "text": " And, of course, you know, you invite someone,"}, {"timestamp": [3623.5, 3625.8], "text": " and you say, hey, I've got a prototype AGI."}, {"timestamp": [3625.8, 3627.04], "text": " What's the first thing they try and do is they"}, {"timestamp": [3627.04, 3628.64], "text": " try and break it and they did."}, {"timestamp": [3628.64, 3630.56], "text": " It's still pretty fragile."}, {"timestamp": [3630.56, 3632.5], "text": " But yes, that's the high level"}, {"timestamp": [3632.5, 3635.16], "text": " of natural language cognitive architecture."}, {"timestamp": [3635.16, 3637.82], "text": " It's already outdated because we've got fine tuning,"}, {"timestamp": [3637.82, 3639.26], "text": " we've got the instruct series."}, {"timestamp": [3639.26, 3642.14], "text": " I did all this research and wrote the book actually"}, {"timestamp": [3642.14, 3644.96], "text": " about a year ago now just before all this came out."}, {"timestamp": [3644.96, 3645.8], "text": " It's already outdated, that's why my research has moved on. research and wrote the book actually about a year ago now, just before all this came out."}, {"timestamp": [3645.8, 3646.8], "text": " So it's already outdated."}, {"timestamp": [3646.8, 3647.8], "text": " That's why my research has moved on."}, {"timestamp": [3647.8, 3650.76], "text": " But yeah, so that's it at a high level."}, {"timestamp": [3650.76, 3653.96], "text": " Yeah, I mean, that's awesome."}, {"timestamp": [3653.96, 3656.84], "text": " And by the way, like, David, you did do a good job."}, {"timestamp": [3656.84, 3659.36], "text": " Like the diagrams in this book are quite helpful."}, {"timestamp": [3659.36, 3660.36], "text": " Excellent."}, {"timestamp": [3660.36, 3663.24], "text": " Like in addition to the text, like it's very clear."}, {"timestamp": [3663.24, 3665.68], "text": " Like I was able to fully follow along with all these,"}, {"timestamp": [3665.68, 3668.8], "text": " essentially these different modules for the whole system"}, {"timestamp": [3668.8, 3672.82], "text": " of how a language model inspired AGI, quote unquote,"}, {"timestamp": [3672.82, 3675.12], "text": " could actually be like how it would work."}, {"timestamp": [3676.02, 3679.44], "text": " And so I was gonna ask you, so the prototype also was,"}, {"timestamp": [3679.44, 3681.16], "text": " you know, you made it to that stage"}, {"timestamp": [3681.16, 3684.8], "text": " and it has just some fun, interesting results."}, {"timestamp": [3684.8, 3685.2], "text": " Yep. So that's awesome. What is the delta then between, made it to that stage and it has just some fun, interesting results."}, {"timestamp": [3685.2, 3686.8], "text": " So that's awesome."}, {"timestamp": [3686.8, 3692.54], "text": " What is the delta then between, let's say even something like GPT-4 using the natural"}, {"timestamp": [3692.54, 3699.12], "text": " language cognitive architecture, what's the delta between that and true AGI?"}, {"timestamp": [3699.12, 3700.12], "text": " What's the difference there?"}, {"timestamp": [3700.12, 3702.56], "text": " What skills, what patterns would you want to see?"}, {"timestamp": [3702.56, 3703.56], "text": " **Matt Stauffer** Yeah."}, {"timestamp": [3703.56, 3706.56], "text": " So, I mean, there's a lot that I haven't figured out yet, right?"}, {"timestamp": [3707.36, 3713.2], "text": " Task switching, for instance, is one thing that I haven't figured out how to solve."}, {"timestamp": [3713.92, 3720.0], "text": " Even after reading On Task by David Bader, that's one of the most complex things that"}, {"timestamp": [3720.0, 3723.84], "text": " humans can do is keeping track of different tasks and jumping back between them."}, {"timestamp": [3724.88, 3725.56], "text": " There's a whole litany of problems and limitations. that humans can do is keeping track of different tasks and jumping back between them."}, {"timestamp": [3725.56, 3733.16], "text": " There's a whole litany of problems and limitations, but the intrinsic limitation of GPT-3 and"}, {"timestamp": [3733.16, 3738.12], "text": " GPT-4 is they have no memory, right?"}, {"timestamp": [3738.12, 3739.84], "text": " They're completely ephemeral."}, {"timestamp": [3739.84, 3744.04], "text": " And one of the most important things for any intelligent being is that it's got a memory,"}, {"timestamp": [3744.04, 3746.9], "text": " right? You know, you talk about, you know,"}, {"timestamp": [3746.9, 3749.14], "text": " there's famous people in history"}, {"timestamp": [3749.14, 3751.46], "text": " that had like, you know, photographic memories, right?"}, {"timestamp": [3751.46, 3754.1], "text": " And so even just having a really good memory"}, {"timestamp": [3754.1, 3757.9], "text": " is a really important ingredient to having intelligence."}, {"timestamp": [3757.9, 3762.9], "text": " And so that's where I think that like GPT-3, GPT-4,"}, {"timestamp": [3763.7, 3765.0], "text": " other multimodal models,"}, {"timestamp": [3765.0, 3768.16], "text": " they will never be fully AGI on their own."}, {"timestamp": [3768.16, 3771.02], "text": " They might be able to solve really great problems,"}, {"timestamp": [3771.02, 3774.8], "text": " but they're not going to be able to remember you"}, {"timestamp": [3774.8, 3777.72], "text": " unless you add, you bolt the system onto the side,"}, {"timestamp": [3777.72, 3781.72], "text": " some kind of database, so that it can remember your interactions."}, {"timestamp": [3781.72, 3787.04], "text": " That's one thing. Another difference between what you might imagine"}, {"timestamp": [3787.04, 3797.36], "text": " as a true AGI or a full AGI is autonomy. Because you, me, all of your listeners, we all have some"}, {"timestamp": [3797.36, 3801.92], "text": " kind of self-determination. I don't like to use free will because that's too philosophical,"}, {"timestamp": [3801.92, 3805.4], "text": " but we're all autonomous, right? I'm an autonomous agent."}, {"timestamp": [3805.4, 3806.72], "text": " You're an autonomous agent."}, {"timestamp": [3806.72, 3808.48], "text": " GPT-3 is not."}, {"timestamp": [3808.48, 3809.68], "text": " It's transactional."}, {"timestamp": [3809.68, 3811.36], "text": " It just sits there and waits like a hammer."}, {"timestamp": [3811.36, 3812.36], "text": " It's a tool."}, {"timestamp": [3812.36, 3816.5], "text": " It waits until you go pick it up and do something with it."}, {"timestamp": [3816.5, 3819.8], "text": " And so that's one of the things that I was aiming for when designing natural language"}, {"timestamp": [3819.8, 3820.8], "text": " cognitive architecture."}, {"timestamp": [3820.8, 3824.76], "text": " I said, how can we make something that's fully autonomous, that can think on its own and"}, {"timestamp": [3824.76, 3826.0], "text": " make its own decisions?"}, {"timestamp": [3826.0, 3833.0], "text": " And so, in that respect, I don't think a single neural network could ever be an AGI."}, {"timestamp": [3833.0, 3839.0], "text": " I think that in order to achieve true, full AGI, it's going to have to be some kind of cognitive architecture."}, {"timestamp": [3839.0, 3844.0], "text": " And so, at a minimum, you're going to have the neural network and a database, bare minimum."}, {"timestamp": [3844.0, 3845.0], "text": " You need something to store those memories, to store those ideas and beliefs, And so at a minimum, you're going to have the neural network and a database. Bare minimum."}, {"timestamp": [3845.0, 3849.0], "text": " You need something to store those memories, to store those ideas and beliefs,"}, {"timestamp": [3849.0, 3851.0], "text": " and then you need a way to interact with it."}, {"timestamp": [3851.0, 3855.0], "text": " And so that's why, actually, that's why in natural language cognitive architecture,"}, {"timestamp": [3855.0, 3859.0], "text": " the shared database is kind of the center of the design, which you might recall."}, {"timestamp": [3859.0, 3863.0], "text": " Like you can use SQLite, you can use Solr or whatever,"}, {"timestamp": [3863.0, 3865.44], "text": " but you need something to store ideas, memories,"}, {"timestamp": [3865.44, 3866.44], "text": " and experiences."}, {"timestamp": [3866.44, 3871.72], "text": " I actually think that blockchain will be a critical component to AGI because what's the"}, {"timestamp": [3871.72, 3875.92], "text": " difference between a database and like your brain?"}, {"timestamp": [3875.92, 3879.44], "text": " No one can go in and change your memories, right?"}, {"timestamp": [3879.44, 3881.04], "text": " Your memories are yours."}, {"timestamp": [3881.04, 3885.0], "text": " They are permanent, unless you get brain damage or Alzheimer's or something, but they're permanent, right?"}, {"timestamp": [3885.0, 3891.0], "text": " No one can write a SQL query into your head to get your memories or change them."}, {"timestamp": [3891.0, 3895.0], "text": " And so in order for us to realize a full AGI,"}, {"timestamp": [3895.0, 3900.0], "text": " I think that it's going to need kind of the same level of trust in its own memories,"}, {"timestamp": [3900.0, 3906.0], "text": " and so that's why I think that a blockchain is going to be critical to integrate with these neural networks."}, {"timestamp": [3906.0, 3910.0], "text": " That might be the data repository for an AGI in the future."}, {"timestamp": [3910.0, 3916.0], "text": " Because imagine you have an AGI system that is just using a SQL database."}, {"timestamp": [3916.0, 3922.0], "text": " Well, if you hack into that and you rewrite its memories, you could send it off into, you know, it could become hostile."}, {"timestamp": [3922.0, 3924.0], "text": " It could become broken."}, {"timestamp": [3924.0, 3928.72], "text": " Whereas a blockchain, the key feature of a blockchain is that it's immutable, right?"}, {"timestamp": [3928.72, 3936.0], "text": " So if we can give, if we could give a machine autonomy, so that's one ingredient, autonomy,"}, {"timestamp": [3936.0, 3940.88], "text": " but then also a memory or a way, a memory system, which I think would probably be best as a"}, {"timestamp": [3940.88, 3945.26], "text": " blockchain, I think then we'll be much closer to the fully realized"}, {"timestamp": [3945.26, 3947.62], "text": " AGI system."}, {"timestamp": [3947.62, 3953.02], "text": " And that's why I wanted to publish my book as fast as I did, was, okay, we're laying"}, {"timestamp": [3953.02, 3954.46], "text": " the groundwork, right?"}, {"timestamp": [3954.46, 3955.9], "text": " But we need newer systems."}, {"timestamp": [3955.9, 3957.66], "text": " We need a few better tools."}, {"timestamp": [3957.66, 3960.3], "text": " I hope that answers your question."}, {"timestamp": [3960.3, 3970.98], "text": " Yeah, I think memory, I agree with you. And it's just interesting, like GPT-3's quote unquote memory is limited to whatever it experienced"}, {"timestamp": [3970.98, 3973.84], "text": " at training time and during fine tuning."}, {"timestamp": [3973.84, 3979.2], "text": " And sometimes its memory gets jumbled up or it's rephrasing it, it's making stuff up or"}, {"timestamp": [3979.2, 3990.48], "text": " it's sharing things that look truthful, but they're actually not. And so somewhere along the line, just broadly speaking, I think there needs to be research"}, {"timestamp": [3990.48, 3997.72], "text": " on getting these models to store that information in a truthful, accurate way, or even based"}, {"timestamp": [3997.72, 4005.48], "text": " on some perception that they may have into some separate space where it can be retrieved."}, {"timestamp": [4005.48, 4009.92], "text": " And also, these memories are critical for decision-making, that process as well."}, {"timestamp": [4009.92, 4012.64], "text": " You draw on your memories, you draw on past experiences."}, {"timestamp": [4012.64, 4016.4], "text": " And the important part is, I mean, you're using the word database, these are internal"}, {"timestamp": [4016.4, 4019.68], "text": " representations of memories that need to be stored."}, {"timestamp": [4019.68, 4026.68], "text": " And I have no clue what an internal representation database would look like or how that would even work."}, {"timestamp": [4026.68, 4031.82], "text": " I'm not a machine learning researcher. I think I'm just a dreamer. I can tell you what kind"}, {"timestamp": [4031.82, 4035.92], "text": " of product I would want as a GFD3 developer, but I don't know if I could actually do it"}, {"timestamp": [4035.92, 4036.92], "text": " myself."}, {"timestamp": [4036.92, 4041.84], "text": " Yeah. I can't do it myself. That's why I got the prototype. And actually, in the opening"}, {"timestamp": [4041.84, 4045.12], "text": " chapter of my book, I say this is as much a recruiting tool as anything else,"}, {"timestamp": [4045.12, 4047.84], "text": " because I need more smart people to help me on this."}, {"timestamp": [4047.84, 4049.76], "text": " **Matt Stauffer** I see. That's cool."}, {"timestamp": [4049.76, 4052.64], "text": " **Jason Buzio** So, one last point about memories is,"}, {"timestamp": [4052.64, 4057.2], "text": " one advantage of having an AGI that thinks in natural language is interpretability."}, {"timestamp": [4058.64, 4064.08], "text": " If you, like, yeah, we could create a multimodal model that just stores vectors, right?"}, {"timestamp": [4064.08, 4065.36], "text": " High dimensional vectors."}, {"timestamp": [4065.36, 4069.36], "text": " That's not interpretable. But with natural language cognitive architecture, all the"}, {"timestamp": [4069.36, 4074.8], "text": " memories are in plain text. I can, you know, when I had my model up and running, and one of the"}, {"timestamp": [4074.8, 4079.28], "text": " reasons that I don't is because it's super expensive. Like a 10-minute conversation using"}, {"timestamp": [4079.28, 4085.52], "text": " DaVinci cost about $30 because of how much it was interacting with the API."}, {"timestamp": [4086.8, 4092.4], "text": " But all of the memories, like every interaction, you know, every input, output, all the prompts,"}, {"timestamp": [4092.4, 4097.12], "text": " all the responses, all natural language, which solves one of the biggest problems that people"}, {"timestamp": [4097.12, 4100.48], "text": " have with the idea of AGI, which is that it's going to be a black box."}, {"timestamp": [4101.2, 4106.48], "text": " So I think that that's one of the greatest strengths, actually, of having GPT-3,"}, {"timestamp": [4106.48, 4110.96], "text": " which works in natural language. And so you just record every transaction and that makes"}, {"timestamp": [4110.96, 4113.76], "text": " it perfectly interpretable to any human."}, {"timestamp": [4113.76, 4124.56], "text": " Awesome. Yeah, yeah, I would agree. So I'm going to switch gears for a second. So obviously,"}, {"timestamp": [4124.56, 4127.5], "text": " you're really active on the OpenAI community forums."}, {"timestamp": [4127.5, 4130.5], "text": " What thoughts did you have on the community at large?"}, {"timestamp": [4130.5, 4133.0], "text": " Did you have any feedback how things could be improved,"}, {"timestamp": [4133.0, 4136.0], "text": " either community-wise, platform-wise?"}, {"timestamp": [4136.0, 4140.0], "text": " And have there been any great experiences you've had on the OpenAI community forums?"}, {"timestamp": [4140.0, 4144.5], "text": " Yeah, yeah, no, it's a really great place."}, {"timestamp": [4144.5, 4150.5], "text": " It's been critical, actually, no, it's a really great place. It's been critical actually, because I don't know if you've experienced this,"}, {"timestamp": [4150.5, 4153.5], "text": " but I go try and talk about GPT-3 to other people, right?"}, {"timestamp": [4153.5, 4156.5], "text": " You go ask people on Reddit, you talk to people who don't know what it is."}, {"timestamp": [4156.5, 4161.0], "text": " I even attended a deep learning meetup group here in the Triangle area."}, {"timestamp": [4161.0, 4166.24], "text": " And I was trying to present my work, my cognitive architecture work, and"}, {"timestamp": [4166.24, 4170.6], "text": " everyone was more excited about just GPT-3 in itself because no one had seen it yet."}, {"timestamp": [4170.6, 4176.6], "text": " And they're like, wow, how is it doing that? And yeah, so like when you're as deep into"}, {"timestamp": [4176.6, 4182.06], "text": " GPT-3 as we are, most people don't get it. They don't know what it's capable of. My girlfriend's"}, {"timestamp": [4182.06, 4185.16], "text": " finding the same thing. She's finishing up her master's program."}, {"timestamp": [4185.16, 4189.44], "text": " And so she's shared some of her work with her peers,"}, {"timestamp": [4189.44, 4190.28], "text": " with other students."}, {"timestamp": [4190.28, 4193.32], "text": " And they're like, wow, this is like AGI complete."}, {"timestamp": [4193.32, 4195.4], "text": " Why don't we just deploy this now?"}, {"timestamp": [4195.4, 4197.2], "text": " And she's like, I told you, right?"}, {"timestamp": [4197.2, 4198.84], "text": " Like this is remarkable technology,"}, {"timestamp": [4198.84, 4200.84], "text": " but even the professors don't understand"}, {"timestamp": [4200.84, 4203.76], "text": " how disruptive this technology can be."}, {"timestamp": [4203.76, 4206.96], "text": " And so because of that, the open AI community"}, {"timestamp": [4206.96, 4210.68], "text": " is pretty much the only place I can talk about this stuff."}, {"timestamp": [4210.68, 4213.2], "text": " It's the only place I can talk about my ideas"}, {"timestamp": [4213.2, 4216.2], "text": " and share my progress and insights"}, {"timestamp": [4216.2, 4219.4], "text": " and for it to actually have an audience."}, {"timestamp": [4219.4, 4223.36], "text": " So that's kind of the cost of being on the cutting edge"}, {"timestamp": [4223.36, 4225.68], "text": " is your audience gets smaller. But it's definitely the place to be if you cost of being on the cutting edge, right, is your audience gets smaller."}, {"timestamp": [4225.68, 4229.52], "text": " But it's definitely the place to be if you want to get to the cutting edge."}, {"timestamp": [4229.52, 4235.68], "text": " Another advantage is they have the, you can tag your posts where you say like, you know,"}, {"timestamp": [4235.68, 4237.2], "text": " looking for a teammate."}, {"timestamp": [4237.2, 4243.32], "text": " And so, at this point, I've had, I've probably had maybe two dozen different calls with people"}, {"timestamp": [4243.32, 4245.56], "text": " all over the world."}, {"timestamp": [4246.58, 4251.16], "text": " I've talked with people who are writing language teaching apps, education apps,"}, {"timestamp": [4251.16, 4253.04], "text": " Humano that I mentioned earlier."}, {"timestamp": [4253.88, 4256.32], "text": " And so I've had an opportunity to collaborate"}, {"timestamp": [4256.32, 4260.04], "text": " with a dozen or two dozen teams all over the world"}, {"timestamp": [4260.04, 4262.12], "text": " because of the OpenAI community."}, {"timestamp": [4262.12, 4264.6], "text": " And I've actually found a couple of startups"}, {"timestamp": [4264.6, 4267.64], "text": " that I'm gonna actually get involved with"}, {"timestamp": [4267.64, 4271.38], "text": " and try and help them bring their ideas to market."}, {"timestamp": [4271.38, 4274.82], "text": " And that, I mean, that just wouldn't have happened otherwise."}, {"timestamp": [4274.82, 4276.34], "text": " I wouldn't have found these people on Reddit."}, {"timestamp": [4276.34, 4278.86], "text": " I wouldn't have found them on Facebook or Twitter"}, {"timestamp": [4278.86, 4281.9], "text": " because, like I mentioned,"}, {"timestamp": [4281.9, 4286.24], "text": " the ideas that I'm sharing are so far beyond what, you know,"}, {"timestamp": [4286.24, 4290.96], "text": " is talked about on the machine learning subreddit, right? They're still talking about loss functions"}, {"timestamp": [4290.96, 4294.48], "text": " and other things. I'm like, no, we got to talk about cognitive architectures. We got to talk"}, {"timestamp": [4294.48, 4297.92], "text": " about, you know, blockchain memories. And everyone's like, what are you talking about?"}, {"timestamp": [4298.88, 4304.0], "text": " So, you know, in order to have that right audience, that's what I rely on the OpenAI community for."}, {"timestamp": [4301.54, 4301.58], "text": " in order to have that right audience,"}, {"timestamp": [4304.64, 4304.68], "text": " that's what I rely on the OpenAI community for."}, {"timestamp": [4307.9, 4307.94], "text": " Now, as far as, like, things that could do better,"}, {"timestamp": [4310.54, 4310.58], "text": " it could be more active,"}, {"timestamp": [4312.04, 4312.08], "text": " and I'm not sure why,"}, {"timestamp": [4316.24, 4316.28], "text": " but participation seems to come in waves, right?"}, {"timestamp": [4319.58, 4319.6], "text": " And even now that it's become,"}, {"timestamp": [4321.9, 4321.94], "text": " it's gone GA, general availability,"}, {"timestamp": [4326.48, 4333.36], "text": " I thought that it would explode, right? That, you know, hey, anyone can sign up for GP, on GPT-3 now. Why is it not, why is it not blowing up? And I'm"}, {"timestamp": [4333.36, 4337.92], "text": " wondering if it's just that like maybe OpenAI needs a better marketing team or a bigger marketing"}, {"timestamp": [4337.92, 4346.0], "text": " budget, because, I mean, and I know that they'll say that they've got, you know, like a thousand or five thousand startups using their platform."}, {"timestamp": [4346.0, 4356.0], "text": " But, you know, I think that there's a lot of, what's the word, like unmet potential or latent potential."}, {"timestamp": [4356.0, 4358.0], "text": " That's the word, latent potential."}, {"timestamp": [4358.0, 4362.0], "text": " Because there's so many people with fantastic ideas and use cases."}, {"timestamp": [4362.0, 4365.88], "text": " And we really need to create more of like a startup reactor"}, {"timestamp": [4365.88, 4366.38], "text": " thing."}, {"timestamp": [4366.38, 4368.72], "text": " And OpenAI, what was it?"}, {"timestamp": [4368.72, 4370.48], "text": " Think about six to nine months ago,"}, {"timestamp": [4370.48, 4374.68], "text": " they announced their $100 million OpenAI fund."}, {"timestamp": [4374.68, 4377.76], "text": " So they wanted to attract some more startups and stuff."}, {"timestamp": [4377.76, 4383.44], "text": " But even that, the community's kind of a ghost town some days."}, {"timestamp": [4383.44, 4389.0], "text": " But I think today, I checked a few times and there was three or four posts that had been updated."}, {"timestamp": [4389.0, 4391.0], "text": " But some days there's like 20."}, {"timestamp": [4391.0, 4393.0], "text": " It's just feast or famine."}, {"timestamp": [4393.0, 4399.0], "text": " So that's really the biggest problem is there's so much potential here and it's completely untapped,"}, {"timestamp": [4399.0, 4402.0], "text": " or almost completely untapped."}, {"timestamp": [4402.0, 4405.2], "text": " Yeah, but no, it's been indispensable for me."}, {"timestamp": [4405.2, 4407.86], "text": " And hopefully, these couple of startups"}, {"timestamp": [4407.86, 4410.24], "text": " that I'm involved with might yield something really,"}, {"timestamp": [4410.24, 4412.8], "text": " really incredible."}, {"timestamp": [4412.8, 4414.0], "text": " Yeah, and thank you."}, {"timestamp": [4414.0, 4415.2], "text": " Thank you for sharing this."}, {"timestamp": [4415.2, 4417.96], "text": " I agree with everything that you're saying."}, {"timestamp": [4417.96, 4419.8], "text": " There is something about GPT-3."}, {"timestamp": [4419.8, 4422.52], "text": " I have noticed people who, especially"}, {"timestamp": [4422.52, 4424.28], "text": " on the machine learning subreddit,"}, {"timestamp": [4424.28, 4430.24], "text": " they're a little bit too educated, a little bit too qualified, a little bit too skeptical."}, {"timestamp": [4430.24, 4436.16], "text": " And I can see a lot of machine learning researchers not being interested in the nuances of prompt"}, {"timestamp": [4436.16, 4437.16], "text": " design."}, {"timestamp": [4437.16, 4440.12], "text": " They're just not."}, {"timestamp": [4440.12, 4443.76], "text": " I've spoken to machine learning researchers, and many of them are like, what?"}, {"timestamp": [4443.76, 4445.28], "text": " It's just repeating training data."}, {"timestamp": [4445.28, 4446.28], "text": " Right?"}, {"timestamp": [4446.28, 4447.28], "text": " That's all it's doing."}, {"timestamp": [4447.28, 4449.96], "text": " And when you ask them, what are you doing?"}, {"timestamp": [4449.96, 4451.36], "text": " Are you repeating training data?"}, {"timestamp": [4451.36, 4452.36], "text": " Their answer is no."}, {"timestamp": [4452.36, 4454.64], "text": " No, of course not."}, {"timestamp": [4454.64, 4462.94], "text": " And so having a space where you can talk to people who have access, who have explored,"}, {"timestamp": [4462.94, 4464.44], "text": " it's a valuable space."}, {"timestamp": [4464.44, 4466.82], "text": " It's very important."}, {"timestamp": [4466.82, 4469.22], "text": " So were you on the Slack group back in the day?"}, {"timestamp": [4469.22, 4472.58], "text": " Or did you show up when it was only the forums?"}, {"timestamp": [4472.58, 4476.14], "text": " So when my application was accepted,"}, {"timestamp": [4476.14, 4477.82], "text": " they had just announced that the Slack group"}, {"timestamp": [4477.82, 4479.88], "text": " was getting banned."}, {"timestamp": [4479.88, 4483.42], "text": " So I got on like two weeks before they shut it down."}, {"timestamp": [4483.42, 4488.1], "text": " So I was one of the first people on the new community board."}, {"timestamp": [4488.1, 4494.2], "text": " But yeah, so that phenomenon that you've mentioned is, I actually wrote a post about that recently"}, {"timestamp": [4494.2, 4499.84], "text": " on the forum, where a lot of purists, you know, like whether you're a math purist or"}, {"timestamp": [4499.84, 4506.56], "text": " a computer science purist, you're trained to think quantitatively in terms of numbers."}, {"timestamp": [4506.56, 4509.66], "text": " But GPT-3 doesn't produce quantitative data."}, {"timestamp": [4509.66, 4511.88], "text": " It produces qualitative data."}, {"timestamp": [4511.88, 4517.0], "text": " And so that's why you see people like artists and poets and novelists using it, because"}, {"timestamp": [4517.0, 4519.04], "text": " they're like, wow, this is great."}, {"timestamp": [4519.04, 4520.24], "text": " And I'm cross-trained, right?"}, {"timestamp": [4520.24, 4524.16], "text": " I'm a technologist by day and a science fiction author by night, so I use both."}, {"timestamp": [4524.16, 4527.88], "text": " So I can think qualitatively and quantitatively."}, {"timestamp": [4527.88, 4535.88], "text": " And in the academic sphere, there are classes that are meant to teach computer science engineers"}, {"timestamp": [4535.88, 4540.16], "text": " to think differently, to think more qualitatively."}, {"timestamp": [4540.16, 4545.62], "text": " But even still, some folks that have a real good natural affinity for computer programming"}, {"timestamp": [4545.62, 4549.78], "text": " and math, that's just their nature."}, {"timestamp": [4549.78, 4552.36], "text": " Their nature is not to think qualitatively."}, {"timestamp": [4552.36, 4558.56], "text": " And so that is one of the biggest gaps, I think, between where the researchers are experts"}, {"timestamp": [4558.56, 4560.0], "text": " and what's needed."}, {"timestamp": [4560.0, 4565.36], "text": " And so there was a post months ago where someone was asking, like, okay, who should be on my"}, {"timestamp": [4565.36, 4566.76], "text": " team, right?"}, {"timestamp": [4566.76, 4572.64], "text": " If I'm trying to build a business team to maximize my use of GPT-3, you know, I've got"}, {"timestamp": [4572.64, 4575.28], "text": " a front-end developer, I've got a back-end developer, what else do I need?"}, {"timestamp": [4575.28, 4578.44], "text": " I said, hire a writer."}, {"timestamp": [4578.44, 4581.96], "text": " Hire someone who's a journalist or a fiction writer because they are going to understand"}, {"timestamp": [4581.96, 4584.04], "text": " that qualitative data."}, {"timestamp": [4584.04, 4587.64], "text": " Hire a psychologist, right? I've read plenty of books on psychology as well."}, {"timestamp": [4587.64, 4590.64], "text": " Actually, one of the folks that I'm working with is a psychology researcher"}, {"timestamp": [4590.64, 4594.64], "text": " who wants to automate as much of the clinical psychology experience"}, {"timestamp": [4594.64, 4597.64], "text": " or psychological research experience as possible."}, {"timestamp": [4597.64, 4601.44], "text": " Of course, he's not a computer guy, right? He thinks in terms of emotions."}, {"timestamp": [4601.44, 4605.36], "text": " He thinks in terms of communication, and so in terms of communication. He gets it."}, {"timestamp": [4605.36, 4610.6], "text": " It's funny because he read my book and he said, \ufffdOh, your cognitive architecture stuff,"}, {"timestamp": [4610.6, 4618.04], "text": " it sounds like graduate level psychology.\ufffd Then someone else read it and they said, \ufffdThis"}, {"timestamp": [4618.04, 4623.52], "text": " sounds like what I do as an expert marketer.\ufffd I was like, \ufffdYeah, merge it all together.\ufffd"}, {"timestamp": [4623.52, 4627.0], "text": " I think the simplest answer is you got to learn to think qualitatively."}, {"timestamp": [4627.0, 4630.0], "text": " And that's why I talked about reading and writing earlier."}, {"timestamp": [4630.0, 4631.0], "text": " Think in terms of emotions."}, {"timestamp": [4631.0, 4634.0], "text": " Think in terms of your own mind."}, {"timestamp": [4634.0, 4642.0], "text": " And you've got to start \u2013 not you, but the audience, the folks who want to make the most of GPT-3,"}, {"timestamp": [4642.0, 4650.8], "text": " they have to really kind of dig in and start thinking qualitatively because qualitative data has just as much value as quantitative data. But we have"}, {"timestamp": [4650.8, 4656.16], "text": " an entire generation of computer scientists and mathematicians who are not really trained to think"}, {"timestamp": [4656.16, 4661.12], "text": " qualitatively at all. And I think that's one of the biggest problems. And I don't think open AI"}, {"timestamp": [4661.12, 4667.56], "text": " can solve that problem. That's a much bigger systemic problem. No, that's a great point."}, {"timestamp": [4667.56, 4668.88], "text": " I completely agree with you."}, {"timestamp": [4668.88, 4673.16], "text": " And I think one of the reasons I'm drawn to the OpenAI community is, you know, these"}, {"timestamp": [4673.16, 4676.6], "text": " are, they tend to be developers who are also qualitative."}, {"timestamp": [4676.6, 4681.4], "text": " They're developers who have multiple skills, who are doing different things."}, {"timestamp": [4681.4, 4687.4], "text": " And so, I mean, I was quite critical actually of shutting down the OpenAI Slack group."}, {"timestamp": [4687.4, 4689.52], "text": " The activity was crazy on there."}, {"timestamp": [4689.52, 4693.24], "text": " I made friends through that Slack group."}, {"timestamp": [4693.24, 4696.2], "text": " And I understand at the time there was these downsides."}, {"timestamp": [4696.2, 4697.92], "text": " People kept asking the same questions."}, {"timestamp": [4697.92, 4699.88], "text": " We didn't quite have a spam problem yet."}, {"timestamp": [4699.88, 4702.64], "text": " It was kind of heading there, right?"}, {"timestamp": [4702.64, 4706.16], "text": " But the activity was off the charts and you're right in terms"}, {"timestamp": [4706.16, 4711.16], "text": " of untapped potential that we didn't even know how far the Slack group was going to"}, {"timestamp": [4711.16, 4716.86], "text": " go, but they shut it down. And there's Discord solutions, there's alternatives. With what"}, {"timestamp": [4716.86, 4722.0], "text": " we have now, I think OpenAI does participate. There's some high level involvement. They"}, {"timestamp": [4722.0, 4725.28], "text": " have sort of a dedicated member who writes honestly answers,"}, {"timestamp": [4725.28, 4727.76], "text": " really, really thoughtful answers to a lot of questions,"}, {"timestamp": [4727.76, 4730.28], "text": " official answers as well, which I appreciate."}, {"timestamp": [4730.28, 4733.96], "text": " I would just love to see the company really, truly lean in"}, {"timestamp": [4733.96, 4736.24], "text": " to engaging with developers."}, {"timestamp": [4736.24, 4740.82], "text": " I have yet to see a single AMA, Ask Me Anything thread"}, {"timestamp": [4740.82, 4743.36], "text": " with the CEO of OpenAI."}, {"timestamp": [4743.36, 4746.6], "text": " And this is something I tried to push last year on Twitter."}, {"timestamp": [4746.6, 4752.3], "text": " Let's get the CEO on the community forums and let's ask questions and get responses"}, {"timestamp": [4752.3, 4753.48], "text": " from him."}, {"timestamp": [4753.48, 4755.02], "text": " And I just don't know why."}, {"timestamp": [4755.02, 4756.02], "text": " Why doesn't he show up?"}, {"timestamp": [4756.02, 4759.46], "text": " I'm not sure if he's made a single post."}, {"timestamp": [4759.46, 4762.98], "text": " And there's just other things as well where I can just..."}, {"timestamp": [4762.98, 4770.52], "text": " The difference between engaging really truly with your core audience and sort of compartmentalizing it to a single employee,"}, {"timestamp": [4770.52, 4776.58], "text": " like I don't know, this company-led engagement is one thing versus department-led."}, {"timestamp": [4776.58, 4778.26], "text": " And so there's just all these areas."}, {"timestamp": [4778.26, 4784.78], "text": " And certainly one of the other, I guess, more immediate suggestions I have for the community,"}, {"timestamp": [4784.78, 4788.44], "text": " we've accumulated tons of insights and resources."}, {"timestamp": [4788.44, 4798.12], "text": " I think the community could benefit from more pooling of the best posts, the best insights."}, {"timestamp": [4798.12, 4799.64], "text": " And I also want to give a shout out."}, {"timestamp": [4799.64, 4802.24], "text": " I think we need to encourage more."}, {"timestamp": [4802.24, 4804.08], "text": " Shout out to Duty to Develop on there."}, {"timestamp": [4804.08, 4805.6], "text": " I've reached out to him privately on"}, {"timestamp": [4805.6, 4811.12], "text": " the OpenEd community forums, but he's done some amazing just write-ups of his GPT-3 experiments"}, {"timestamp": [4811.12, 4815.28], "text": " and the prompts. I'm sure you've seen them. And of course, there's other members who participate"}, {"timestamp": [4815.28, 4828.22], "text": " every day. So I'm just saying that now that the community is in another stage, we need to start thinking more about let's curate some of the best moments."}, {"timestamp": [4828.22, 4833.0], "text": " I think that's definitely one of the big pieces."}, {"timestamp": [4833.0, 4838.76], "text": " And so anyways, did you have any more thoughts in the community stuff or anything else?"}, {"timestamp": [4838.76, 4842.42], "text": " **Matt Stauffer** Yeah, just an observation that I've worked"}, {"timestamp": [4842.42, 4849.0], "text": " at a number of companies of different sizes from a five-person startup to Cisco Systems was the biggest company I've worked for,"}, {"timestamp": [4849.0, 4853.0], "text": " which had at the time like 80,000 people globally."}, {"timestamp": [4853.0, 4857.0], "text": " And so I wonder if some of what you're observing is just growing pains,"}, {"timestamp": [4857.0, 4862.0], "text": " just normal growing pains, because often you'll have the startup culture,"}, {"timestamp": [4862.0, 4866.56], "text": " which is bootstrapping, right, where you you just, you know, it's on Slack,"}, {"timestamp": [4866.56, 4868.74], "text": " it's on GitHub and you just kind of,"}, {"timestamp": [4868.74, 4870.6], "text": " it's fast and loose and quick."}, {"timestamp": [4870.6, 4875.56], "text": " And OpenAI, now that they've got an enterprise grade service"}, {"timestamp": [4875.56, 4877.32], "text": " they're having to develop their team."}, {"timestamp": [4877.32, 4878.72], "text": " You probably noticed they post like,"}, {"timestamp": [4878.72, 4880.2], "text": " hey, we're hiring, we're hiring."}, {"timestamp": [4880.2, 4884.64], "text": " You know, there've been at least two big hiring splurges"}, {"timestamp": [4884.64, 4887.52], "text": " in the last six to 12 months, and some of those"}, {"timestamp": [4887.52, 4893.12], "text": " are just generic IT guys, like what I do for my day job, or marketing folks."}, {"timestamp": [4893.12, 4897.72], "text": " So I think that they're probably working on solving some of those problems."}, {"timestamp": [4897.72, 4902.64], "text": " But also, as a nonprofit foundation, their budget is probably kind of thin, so I'm wondering"}, {"timestamp": [4902.64, 4906.48], "text": " if their partnership with Microsoft could help some of that as well."}, {"timestamp": [4906.48, 4908.78], "text": " But you're absolutely right."}, {"timestamp": [4908.78, 4912.44], "text": " There are still other things that they could be doing, like maybe bring back Slack or a"}, {"timestamp": [4912.44, 4913.44], "text": " few other things."}, {"timestamp": [4913.44, 4915.0], "text": " So, yeah, that was just a final observation."}, {"timestamp": [4915.0, 4918.4], "text": " It might just be normal growing pains that they're working on solving."}, {"timestamp": [4918.4, 4919.62], "text": " It's definitely growing pains."}, {"timestamp": [4919.62, 4923.4], "text": " And the things I'm sharing, to be honest, it's a little bit more on the harsh side."}, {"timestamp": [4923.4, 4925.28], "text": " I mean, they mean well."}, {"timestamp": [4925.28, 4926.28], "text": " They mean well, right?"}, {"timestamp": [4926.28, 4929.2], "text": " These are not bad people."}, {"timestamp": [4929.2, 4930.2], "text": " They are for profit."}, {"timestamp": [4930.2, 4932.2], "text": " They switched away from nonprofit."}, {"timestamp": [4932.2, 4936.0], "text": " I just wanted to mention that."}, {"timestamp": [4936.0, 4944.32], "text": " I think the reason I share this feedback is, for example, the CEO, Sam Altman, he didn't"}, {"timestamp": [4944.32, 4947.16], "text": " do the AMA thread on the OpenAI community"}, {"timestamp": [4947.16, 4948.16], "text": " forums."}, {"timestamp": [4948.16, 4950.76], "text": " He went to another website and did an AMA."}, {"timestamp": [4950.76, 4955.8], "text": " I can't remember if it was a written form or just a quick call, where apparently he"}, {"timestamp": [4955.8, 4961.16], "text": " shared all these details about what GPT-4 could be like and the future, all the models"}, {"timestamp": [4961.16, 4963.48], "text": " may be multimodal in the future."}, {"timestamp": [4963.48, 4965.54], "text": " I guess that thread has now been taken down and it's like all the models may be multimodal in the future. And I guess that thread has"}, {"timestamp": [4965.54, 4970.26], "text": " now been taken down and it's like all the things that were said were alleged. And so"}, {"timestamp": [4970.26, 4976.56], "text": " I guess this is really behind the scenes kind of stuff. But my criticism is they clearly"}, {"timestamp": [4976.56, 4982.64], "text": " have some capacity to engage. Why are they not engaging where the audience is?"}, {"timestamp": [4982.64, 4986.08], "text": " I had a tweet storm today where I just said, last month,"}, {"timestamp": [4986.08, 4991.16], "text": " Sam Altman was on a podcast talking about meditation and how much meditation helps him."}, {"timestamp": [4991.16, 4995.52], "text": " This is a podcast I've never heard of in my life. It's a business podcast. And he had"}, {"timestamp": [4995.52, 5000.62], "text": " to explain to the guy what GPT-3 even is. And so I tweeted, why haven't you been on"}, {"timestamp": [5000.62, 5009.64], "text": " my podcast? You can reach out to almost 8,000 GPT-3, OpenAI, AI developers."}, {"timestamp": [5009.64, 5015.16], "text": " What are you doing talking about meditation? So my problem is actually a priority problem."}, {"timestamp": [5015.16, 5020.02], "text": " I can see there is capacity. I can see there are some priorities. But I think if you really"}, {"timestamp": [5020.02, 5025.96], "text": " lean in as a priority into your developer community, there are certain ways you would move."}, {"timestamp": [5025.96, 5026.96], "text": " Yep."}, {"timestamp": [5026.96, 5030.16], "text": " And these media channels, there's people in the community, there's so many ways they could"}, {"timestamp": [5030.16, 5031.36], "text": " go about it."}, {"timestamp": [5031.36, 5036.76], "text": " And even linking a lot of the documentation to posts in the community forums, I don't"}, {"timestamp": [5036.76, 5039.64], "text": " see why that's not a bad idea."}, {"timestamp": [5039.64, 5044.62], "text": " Force people to show up to the community forums, walk them through some of the best threads."}, {"timestamp": [5044.62, 5049.0], "text": " These are ways in which we could funnel more people in that direction as well that cost"}, {"timestamp": [5049.0, 5051.16], "text": " virtually nothing."}, {"timestamp": [5051.16, 5055.0], "text": " And so you need to also invest in the community forums."}, {"timestamp": [5055.0, 5057.28], "text": " Building a community is a company-wide thing."}, {"timestamp": [5057.28, 5062.4], "text": " It's not something which can be outsourced to a single employee or overseen by PR."}, {"timestamp": [5062.4, 5066.88], "text": " It needs to come from the heart. I know that sounds so corny."}, {"timestamp": [5066.88, 5071.44], "text": " But anyways, clearly I get too emotional about this community stuff."}, {"timestamp": [5073.68, 5077.6], "text": " Anyways, these are all things going on behind the scenes. I apologize to all the listeners if"}, {"timestamp": [5077.6, 5087.78], "text": " they're like, this is cool. Cool story, bro. Anyways, so we're coming towards the end here. I think I had just two"}, {"timestamp": [5087.78, 5094.2], "text": " broader questions. So what are your thoughts on multimodal AI technology?"}, {"timestamp": [5094.2, 5100.64], "text": " I think it's definitely going to be a critical component for the future. I addressed that"}, {"timestamp": [5100.64, 5109.96], "text": " shortcoming in my book, Natural Language Cognitive Architecture. It thinks and takes in only text, which means speech, chat, whatever."}, {"timestamp": [5109.96, 5115.56], "text": " I think in order to have a fully robust, for instance, if you want to have a fully autonomous"}, {"timestamp": [5115.56, 5119.2], "text": " robot that's going to wander around your house and help you out, it's going to need to integrate"}, {"timestamp": [5119.2, 5120.2], "text": " audio and video."}, {"timestamp": [5120.2, 5124.8], "text": " And if you can do that in a single neural network, great."}, {"timestamp": [5124.8, 5128.0], "text": " I don't know that it'll be necessary to achieve AGI."}, {"timestamp": [5128.0, 5134.0], "text": " It might end up being...it might be one of those rare dead ends, right?"}, {"timestamp": [5134.0, 5140.0], "text": " Where because, you know, thinking visually, thinking in terms of sound,"}, {"timestamp": [5140.0, 5143.0], "text": " that might not actually bias that much, right?"}, {"timestamp": [5143.0, 5149.44], "text": " Because you can represent 95% of human thought in text, right?"}, {"timestamp": [5149.44, 5153.28], "text": " It might take a little bit more, but it might be more expensive."}, {"timestamp": [5153.28, 5156.08], "text": " And also, how big are those models going to be, right?"}, {"timestamp": [5156.08, 5164.08], "text": " Because if just a text model of GPT-3 has to run on $7 million worth of hardware,"}, {"timestamp": [5164.08, 5165.32], "text": " or however much it is, because it's got to run on a7 million worth of hardware, or however much it is,"}, {"timestamp": [5165.32, 5168.44], "text": " because it's got to run on a bunch of different GPUs."}, {"timestamp": [5168.44, 5170.4], "text": " If it's that expensive, how much more expensive,"}, {"timestamp": [5170.4, 5173.82], "text": " how much bigger is a giant multimodal model going to be?"}, {"timestamp": [5173.82, 5176.28], "text": " So that's the biggest cost."}, {"timestamp": [5176.28, 5177.8], "text": " Obviously, computer technology"}, {"timestamp": [5177.8, 5179.8], "text": " is going to get better over time."}, {"timestamp": [5179.8, 5181.24], "text": " And I think I calculated it out."}, {"timestamp": [5181.24, 5183.32], "text": " I think in 10 years,"}, {"timestamp": [5183.32, 5186.64], "text": " your average company could afford to run GPT-3 in-house."}, {"timestamp": [5186.64, 5191.12], "text": " In 20 years, you could probably run GPT-3 on your desktop."}, {"timestamp": [5191.12, 5195.04], "text": " And in 30 years, GPT-3 could run on your phone. Right?"}, {"timestamp": [5195.04, 5200.24], "text": " So that's a long timeline. But in the meantime, we're going to be making bigger and bigger models."}, {"timestamp": [5200.24, 5204.0], "text": " And I'm afraid that there's going to be diminishing returns. Right?"}, {"timestamp": [5204.0, 5209.0], "text": " You know, people, right now, people seem to think that it's going to follow an exponential growth curve forever,"}, {"timestamp": [5209.0, 5212.0], "text": " but it might actually follow a sigmoid curve, right?"}, {"timestamp": [5212.0, 5216.0], "text": " We might be at the point of fastest growth right now, but we're going to see diminishing returns soon."}, {"timestamp": [5216.0, 5222.0], "text": " And so, like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't."}, {"timestamp": [5222.0, 5225.32], "text": " But for the sake of, for the sake of like,"}, {"timestamp": [5225.32, 5228.62], "text": " if you want to create a self-improving chatbot,"}, {"timestamp": [5228.62, 5230.4], "text": " GPT-3 and Codex might be enough,"}, {"timestamp": [5230.4, 5231.24], "text": " or at least, you know,"}, {"timestamp": [5231.24, 5235.18], "text": " that's that single mode technology."}, {"timestamp": [5235.18, 5237.88], "text": " There was another thought, but it ran away, sorry."}, {"timestamp": [5237.88, 5240.86], "text": " But yeah, that's kind of my big take is,"}, {"timestamp": [5240.86, 5242.12], "text": " there could be benefits,"}, {"timestamp": [5242.12, 5243.58], "text": " but there's going to be costs too."}, {"timestamp": [5243.58, 5245.48], "text": " So we got to be cognizant of that."}, {"timestamp": [5245.48, 5247.56], "text": " **Matt Stauffer** Yeah. And there might not be enough compute"}, {"timestamp": [5247.56, 5252.08], "text": " in the world. There might not even be enough energy, or we may consume all energy ever"}, {"timestamp": [5252.08, 5257.44], "text": " produced to train a single model. And then we may be able to run it inference for like"}, {"timestamp": [5257.44, 5262.44], "text": " three seconds. And then it just shuts down the global power system or something."}, {"timestamp": [5262.44, 5267.2], "text": " But can you see yourself, let's say the technology exists, cost considerations aside, can you"}, {"timestamp": [5267.2, 5269.24], "text": " see yourself perhaps making movies?"}, {"timestamp": [5269.24, 5275.04], "text": " Can you see yourself giving your book to a multimodal model, have it generate a documentary"}, {"timestamp": [5275.04, 5277.64], "text": " based on it or some marketing material?"}, {"timestamp": [5277.64, 5288.6], "text": " What can you see yourself doing with the multimodal model of your dreams. Yeah. So, you know, kind of the thought experiment that I did was, okay, well, we've got, you"}, {"timestamp": [5288.6, 5291.04], "text": " know, how much data is on YouTube?"}, {"timestamp": [5291.04, 5296.2], "text": " I think it's like a thousand years or ten thousand years worth of video on YouTube."}, {"timestamp": [5296.2, 5299.84], "text": " And of course, it's many, many, many terabytes."}, {"timestamp": [5299.84, 5302.12], "text": " You know, so it's like that's way more training data."}, {"timestamp": [5302.12, 5305.76], "text": " You know, if GPT-3 was trained on less than one terabyte of data,"}, {"timestamp": [5305.76, 5309.32], "text": " and YouTube is approaching like the Yodabyte scale, right,"}, {"timestamp": [5309.32, 5311.96], "text": " that's an insane amount of data."}, {"timestamp": [5311.96, 5314.68], "text": " So, OK, let's say you feed that in."}, {"timestamp": [5314.68, 5316.8], "text": " And so you've got audio, video, you've got text,"}, {"timestamp": [5316.8, 5318.14], "text": " you've got all the comments."}, {"timestamp": [5318.14, 5322.96], "text": " And you end up with a model trained on all of YouTube data."}, {"timestamp": [5322.96, 5323.46], "text": " OK, cool."}, {"timestamp": [5323.46, 5324.6], "text": " What can you do with that?"}, {"timestamp": [5324.6, 5325.6], "text": " Like, I can't even imagine, right? a model trained on all of YouTube data. OK, cool. What can you do with that?"}, {"timestamp": [5325.6, 5326.92], "text": " I can't even imagine, right?"}, {"timestamp": [5326.92, 5333.18], "text": " Because GPT-3 today is almost capable of writing screenplays."}, {"timestamp": [5333.18, 5337.36], "text": " So if you have a model that's trained on all text data,"}, {"timestamp": [5337.36, 5340.0], "text": " all audio data, all video data, you say, hey,"}, {"timestamp": [5340.0, 5342.44], "text": " write me a screenplay."}, {"timestamp": [5342.44, 5344.64], "text": " I actually, near the end of my book,"}, {"timestamp": [5344.64, 5346.88], "text": " I kind of have a chapter of speculation."}, {"timestamp": [5346.88, 5350.4], "text": " And I say, what if you have this model and you say,"}, {"timestamp": [5350.4, 5353.24], "text": " give me season two of Firefly?"}, {"timestamp": [5353.24, 5353.8], "text": " Right?"}, {"timestamp": [5353.8, 5357.12], "text": " Like, you could just keep watching whatever show you want."}, {"timestamp": [5357.12, 5360.48], "text": " You say, give me Game of Thrones, but give me a different season"}, {"timestamp": [5360.48, 5360.98], "text": " eight."}, {"timestamp": [5360.98, 5364.4], "text": " Give me a different season eight and season nine and 10."}, {"timestamp": [5364.4, 5369.76], "text": " So I kind of imagine that one possibility is hyper-personalized entertainment. And of course,"}, {"timestamp": [5369.76, 5374.64], "text": " like, that might be 30 years away just because of, like you said, the energy intensity of this task."}, {"timestamp": [5375.36, 5379.6], "text": " But I, conceptually, it's possible, right? You can hop on GPT-3 today,"}, {"timestamp": [5380.16, 5384.72], "text": " use the instruct model and say, write a screenplay for, you know, Firefly season two,"}, {"timestamp": [5384.72, 5387.04], "text": " and it'll try, it'll get close."}, {"timestamp": [5387.04, 5390.08], "text": " Then if you can take that text output and feed it into"}, {"timestamp": [5390.08, 5394.84], "text": " a multimodal model that can translate text to video, why not?"}, {"timestamp": [5394.84, 5397.2], "text": " Adobe actually, I don't know if you've seen it,"}, {"timestamp": [5397.2, 5399.56], "text": " but Adobe is already starting on that,"}, {"timestamp": [5399.56, 5404.28], "text": " where they're inferencing, what's the term?"}, {"timestamp": [5404.28, 5408.38], "text": " They're imputing the sound, so you can put in a soundless video and it'll generate"}, {"timestamp": [5408.38, 5410.96], "text": " the audio sound effects for you, or vice versa."}, {"timestamp": [5410.96, 5412.3], "text": " It's really cool."}, {"timestamp": [5412.3, 5419.14], "text": " I think a company like Adobe, that they have a huge vested interest in mastering audiovisual"}, {"timestamp": [5419.14, 5420.32], "text": " technologies."}, {"timestamp": [5420.32, 5426.5], "text": " They might soon put out something where you put in a text description and it'll give you a three-second clip"}, {"timestamp": [5426.5, 5428.5], "text": " so you can use that for ad copy."}, {"timestamp": [5428.5, 5431.5], "text": " Well, this technology is going to continue improving over time."}, {"timestamp": [5431.5, 5436.5], "text": " So I kind of see that as like, if I were Netflix, put it this way,"}, {"timestamp": [5436.5, 5448.0], "text": " if I had the budget of Netflix or Amazon, I would be investing in this to write hyper-personalized series or novels."}, {"timestamp": [5448.0, 5451.68], "text": " Because Amazon's got the market cornered with Kindle."}, {"timestamp": [5451.68, 5455.28], "text": " And there's people that will read all day, every day."}, {"timestamp": [5455.28, 5460.68], "text": " There are people that consume every bit of entertainment that's available."}, {"timestamp": [5460.68, 5466.4], "text": " So if you can generate that on the fly without having a studio, a big budget studio, that"}, {"timestamp": [5466.4, 5469.6], "text": " would be, I mean, that would change entertainment."}, {"timestamp": [5469.6, 5470.6], "text": " That's the metaverse."}, {"timestamp": [5470.6, 5471.68], "text": " Forget what Facebook is doing."}, {"timestamp": [5471.68, 5475.8], "text": " That's the metaverse where it's like, hey, I came up with my own idea for Game of Thrones"}, {"timestamp": [5475.8, 5482.32], "text": " and I wrote, I used this GPT-8 or whatever to generate my own version of Game of Thrones."}, {"timestamp": [5482.32, 5483.32], "text": " Come watch it with me, guys."}, {"timestamp": [5483.32, 5486.0], "text": " And someone might say, ah, I didn't like that ending and they go rewrite it and generate their own version of Game of Thrones. Come watch it with me, guys.\" And, you know, someone might say, eh, I didn't like that ending,"}, {"timestamp": [5486.0, 5488.5], "text": " and they go rewrite it and generate their own version."}, {"timestamp": [5488.5, 5491.0], "text": " You know, because we share memes on the Internet today."}, {"timestamp": [5491.0, 5493.0], "text": " What if instead of sharing memes on the Internet,"}, {"timestamp": [5493.0, 5496.0], "text": " we end up sharing episodes of our favorite, you know, anime,"}, {"timestamp": [5496.0, 5500.5], "text": " or, you know, we resurrect Battlestar Galactica, you know, whatever."}, {"timestamp": [5500.5, 5502.0], "text": " There's so many things that we could do."}, {"timestamp": [5502.0, 5505.8], "text": " Like, if compute power was not a problem, then"}, {"timestamp": [5505.8, 5509.6], "text": " we'd get there. But we need fusion reactors to power this stuff."}, {"timestamp": [5509.6, 5513.58], "text": " **Matt Stauffer** Yeah, yeah. Marvel for me is already kind"}, {"timestamp": [5513.58, 5520.2], "text": " of like this. My capacity to consume Marvel as a viewer, it appears, is infinite. So I'm"}, {"timestamp": [5520.2, 5525.04], "text": " excited. I've called it in the past, like the multi-modal Marvel cinematic universe."}, {"timestamp": [5525.04, 5525.52], "text": " **Matt Stauffer**"}, {"timestamp": [5525.52, 5525.76], "text": " Nice."}, {"timestamp": [5525.76, 5527.36], "text": " **Matt Stauffer**"}, {"timestamp": [5527.36, 5530.64], "text": " And some of these shows, like Loki, I don't know if you watch like-"}, {"timestamp": [5530.64, 5530.8], "text": " **Justin Jackson**"}, {"timestamp": [5530.8, 5531.6], "text": " I haven't seen it yet."}, {"timestamp": [5531.6, 5532.0], "text": " **Matt Stauffer**"}, {"timestamp": [5532.0, 5537.6], "text": " Okay. Okay. I mean, it was six episodes. If it had been 30, I would have watched all 30 and"}, {"timestamp": [5537.6, 5541.68], "text": " enjoyed every moment of it. If that quality was, I want to go deeper in these stories."}, {"timestamp": [5541.68, 5541.84], "text": " **Justin Jackson**"}, {"timestamp": [5541.84, 5542.0], "text": " Yeah."}, {"timestamp": [5542.0, 5542.24], "text": " **Matt Stauffer**"}, {"timestamp": [5542.24, 5548.1], "text": " I'm definitely excited for all my favorite universes, cinematic universes, and story-wise"}, {"timestamp": [5548.1, 5553.8], "text": " as well, to live on forever, essentially, through multimodal content, and maybe be personalized,"}, {"timestamp": [5553.8, 5556.76], "text": " like you're describing as well."}, {"timestamp": [5556.76, 5559.12], "text": " So yeah, last question."}, {"timestamp": [5559.12, 5562.0], "text": " So we've talked about various things."}, {"timestamp": [5562.0, 5566.12], "text": " We've talked about codecs, find2dag, you know,"}, {"timestamp": [5566.12, 5567.36], "text": " multimodal stuff."}, {"timestamp": [5567.36, 5570.0], "text": " Broadly, where do you see all of this stuff going?"}, {"timestamp": [5570.0, 5571.72], "text": " Let's give a timeline, five, 10 years."}, {"timestamp": [5571.72, 5572.68], "text": " What are some of the,"}, {"timestamp": [5572.68, 5574.44], "text": " what's the direction we're heading towards?"}, {"timestamp": [5574.44, 5576.88], "text": " What important capabilities will we have?"}, {"timestamp": [5576.88, 5578.54], "text": " Why is this stuff important?"}, {"timestamp": [5578.54, 5580.2], "text": " Yeah."}, {"timestamp": [5580.2, 5581.36], "text": " Five to 10 years from now,"}, {"timestamp": [5581.36, 5582.8], "text": " I think that we will have something"}, {"timestamp": [5582.8, 5589.4], "text": " that you could probably call a fully functional AGI, like as a service you could sign up for."}, {"timestamp": [5589.4, 5594.22], "text": " It might be chatbot-based, kind of based on natural language cognitive architecture."}, {"timestamp": [5594.22, 5597.0], "text": " I calculated out, it's too expensive to run right now."}, {"timestamp": [5597.0, 5601.84], "text": " If it's $30 for a 10-minute conversation, that's way too expensive."}, {"timestamp": [5601.84, 5606.64], "text": " So the cost has to come down. You know, if you just take the technology we have today,"}, {"timestamp": [5606.64, 5608.96], "text": " but make it cheaper, there's so much potential."}, {"timestamp": [5610.24, 5611.68], "text": " So, you know, then there was that idea"}, {"timestamp": [5611.68, 5614.72], "text": " about like self-improving, you know, feedback loops,"}, {"timestamp": [5614.72, 5616.24], "text": " you know, integrating with DevOps."}, {"timestamp": [5616.24, 5618.64], "text": " I certainly think that a company like Atlassian,"}, {"timestamp": [5618.64, 5620.16], "text": " which is a major DevOps player,"}, {"timestamp": [5620.88, 5622.48], "text": " probably within five to 10 years,"}, {"timestamp": [5622.48, 5624.8], "text": " they'll have something integrated"}, {"timestamp": [5624.8, 5628.0], "text": " to kind of help automate the development pipeline even further."}, {"timestamp": [5628.0, 5634.0], "text": " I think that, of course, I could be wrong because we're kind of at this weird acceleration point."}, {"timestamp": [5634.0, 5642.0], "text": " I think, I feel like multimodal models, like consumer-grade multimodal models, are probably more than 10 years away."}, {"timestamp": [5642.0, 5646.56], "text": " Unfortunately, they're probably just going to be like toy-sized."}, {"timestamp": [5646.56, 5650.32], "text": " Because, you know, there's like a hypnogram, right?"}, {"timestamp": [5650.32, 5653.92], "text": " I don't know if you've seen that one, but that's one of like the text-to-image generators."}, {"timestamp": [5653.92, 5656.96], "text": " And it's still not even photorealistic, right?"}, {"timestamp": [5656.96, 5661.36], "text": " Getting a photorealistic text-to-image is still like, that's a little ways off."}, {"timestamp": [5661.36, 5664.16], "text": " And then the next step after that is text-to-video."}, {"timestamp": [5664.16, 5665.52], "text": " That's even further,"}, {"timestamp": [5665.52, 5670.64], "text": " right? So that's kind of where I think it's at. I don't think we're going to hit an AI winter."}, {"timestamp": [5670.64, 5673.76], "text": " I know there's lots of people predicting that we're going to hit an AI winter,"}, {"timestamp": [5673.76, 5678.4], "text": " but I think that we're actually still kind of in the acceleration point. But again, I don't know"}, {"timestamp": [5678.4, 5683.2], "text": " if it's going to follow an exponential curve forever or if it's a sigmoid curve. So time will"}, {"timestamp": [5683.2, 5685.92], "text": " tell."}, {"timestamp": [5689.84, 5690.48], "text": " Yep, and still lots to do in the meantime, like you're describing, even with UPT3."}, {"timestamp": [5696.4, 5701.28], "text": " Okay. Yeah, my answer is I think all of this stuff is just converging to just greater human potential. In some sense, I'm not even necessarily interested in the AGI question, although I think"}, {"timestamp": [5701.28, 5707.28], "text": " it's important. I think just the exciting possibilities we'll have, even now that we have, that we'll continue"}, {"timestamp": [5707.28, 5712.92], "text": " to have five to ten years from now, so many more experiences, so many other things we'll"}, {"timestamp": [5712.92, 5716.12], "text": " be able to create that weren't possible, I think we'll have more people creating than"}, {"timestamp": [5716.12, 5717.8], "text": " ever before."}, {"timestamp": [5717.8, 5721.94], "text": " It's a really, really exciting vision for humanity, right?"}, {"timestamp": [5721.94, 5722.94], "text": " Not just for you and I."}, {"timestamp": [5722.94, 5728.16], "text": " So anyways, so with that said, did you have anything you wanted to plug, David?"}, {"timestamp": [5728.16, 5729.56], "text": " Where can people find you online?"}, {"timestamp": [5729.56, 5730.92], "text": " David Koppel Yeah."}, {"timestamp": [5730.92, 5735.52], "text": " My personal site is davidkshapiro.com."}, {"timestamp": [5735.52, 5739.92], "text": " I have a few projects up and coming."}, {"timestamp": [5739.92, 5743.86], "text": " Nothing out right now except for my book, Natural Language Cognitive Architecture."}, {"timestamp": [5743.86, 5746.68], "text": " You can download it for free from my website."}, {"timestamp": [5746.68, 5748.04], "text": " You can sign up for my newsletter."}, {"timestamp": [5748.04, 5753.52], "text": " So one of my upcoming books is called Benevolent by Design, Six Words to Safeguard Humanity,"}, {"timestamp": [5753.52, 5757.12], "text": " which is to address the control problem of AGI."}, {"timestamp": [5757.12, 5762.08], "text": " So that book should hopefully be out in the next six months or so."}, {"timestamp": [5762.08, 5764.04], "text": " And that is one project."}, {"timestamp": [5764.04, 5768.72], "text": " I've got another nonfiction book and then also my own podcast that will be coming out"}, {"timestamp": [5768.72, 5769.72], "text": " soon."}, {"timestamp": [5769.72, 5773.82], "text": " So yeah, head over to my site, davidkshapiro.com and sign up for my newsletter and you'll get"}, {"timestamp": [5773.82, 5776.36], "text": " updated when these come out when they're available."}, {"timestamp": [5776.36, 5777.64], "text": " **Matt Stauffer** Awesome."}, {"timestamp": [5777.64, 5781.64], "text": " And David, you mentioned you're looking for collaborators as well for natural language"}, {"timestamp": [5781.64, 5783.08], "text": " cognitive architecture."}, {"timestamp": [5783.08, 5786.88], "text": " So if you're a coder, I imagine product manager,"}, {"timestamp": [5786.88, 5795.84], "text": " researcher, hit up David and just connect if any of this stuff interests you. I think I spent like"}, {"timestamp": [5795.84, 5799.84], "text": " a few days trying to find you on Twitter, so I don't think you're quite on Twitter yet."}, {"timestamp": [5800.48, 5807.8], "text": " I encourage you, David, and of course, you and I will connect after. We'll put any other place people could connect with you."}, {"timestamp": [5807.8, 5808.8], "text": " There's the community forums."}, {"timestamp": [5808.8, 5811.32], "text": " I assume you have a GitHub account."}, {"timestamp": [5811.32, 5815.46], "text": " So we're going to put that in the show notes and in the YouTube description below."}, {"timestamp": [5815.46, 5817.96], "text": " So anyways, David, thank you so much for being here."}, {"timestamp": [5817.96, 5824.3], "text": " I wanted to personally thank you for all the awesome, awesome community contributions you've"}, {"timestamp": [5824.3, 5825.7], "text": " made on the OpenAI Community"}, {"timestamp": [5825.7, 5826.7], "text": " Forum."}, {"timestamp": [5826.7, 5829.28], "text": " You're just an essential person on there."}, {"timestamp": [5829.28, 5831.58], "text": " I've learned a lot from you."}, {"timestamp": [5831.58, 5834.54], "text": " The insights you've shared, they're going to be there forever."}, {"timestamp": [5834.54, 5837.78], "text": " I'm sure I can't imagine how many people you've helped."}, {"timestamp": [5837.78, 5841.62], "text": " Also about your book, I also just wanted to say to the audience, David's done a great"}, {"timestamp": [5841.62, 5843.86], "text": " job making it really digestible."}, {"timestamp": [5843.86, 5847.16], "text": " It was a breeze of a"}, {"timestamp": [5847.16, 5851.96], "text": " read. I thoroughly enjoyed it. As somebody who writes GPT-3 prompts and is into this"}, {"timestamp": [5851.96, 5860.4], "text": " ecosystem, it was just very interesting to see how it could be laid out in this broader"}, {"timestamp": [5860.4, 5863.52], "text": " system approaching this huge problem."}, {"timestamp": [5863.52, 5865.68], "text": " And also, I was able to even get the book for free."}, {"timestamp": [5865.68, 5870.56], "text": " Obviously, I encourage people to buy the book, support it, but it's there. It's ready. I think"}, {"timestamp": [5870.56, 5877.36], "text": " David's goal here is to get the ideas out. And so, anyways, so that's it for today's episode."}, {"timestamp": [5877.36, 5879.44], "text": " David, thank you so much again. I really appreciate you being here."}, {"timestamp": [5879.44, 5881.52], "text": " David Schmittlein Thank you. Thank you for all the kind comments."}, {"timestamp": [5882.08, 5886.48], "text": " And you're quite welcome and so is everyone else. That's why I'm here."}, {"timestamp": [5886.48, 5888.52], "text": " **Matt Stauffer** Awesome."}, {"timestamp": [5888.52, 5896.46], "text": " So my quick plugs, you know, at BAKZTfuture, Twitter, Instagram, youtube.com slash BAKZTfuture."}, {"timestamp": [5896.46, 5899.04], "text": " My newsletter, I'll put it in the description below."}, {"timestamp": [5899.04, 5903.0], "text": " And I have a Twitter Spaces event coming in two days at noon."}, {"timestamp": [5903.0, 5904.66], "text": " A couple of people probably pulling up."}, {"timestamp": [5904.66, 5910.2], "text": " This is like an audio-only event, so I encourage audio podcast listeners, YouTube subscribers,"}, {"timestamp": [5910.2, 5913.92], "text": " pull up to the Twitter Spaces event. We're going to chat more about Codex and prompt"}, {"timestamp": [5913.92, 5917.16], "text": " design and some other stuff going on in this space."}, {"timestamp": [5917.16, 5921.12], "text": " So anyways, thank you again for listening to Multimodal by Backstreet Future. I'll catch"}, {"timestamp": [5921.12, 5922.6], "text": " you in the next one. Bye."}, {"timestamp": [5920.8, 5923.44], "text": " Backstreet Future. I'll catch you in the next one. Bye."}]}