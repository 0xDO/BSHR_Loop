{"text": " Good morning everybody. David Shapiro here with another video. So I mentioned a few videos ago, you may or may not have seen it, I mentioned the concept of polymorphic applications and a bunch of you were like, what is this concept? Tell me more. And some people are like, yeah, whatever, that's not possible. So this video is here to teach you a new paradigm of software development and software architecture. So it's, I call it polymorphic applications or basically self-changing applications. And we'll get right into it. But first, I just wanted to say I am available. I used to do consulting on Patreon. I still have a few Patreon clients there, but I'm kind of moving to more involved engagements, including strategic and technical consulting, teaching and coaching, as well as speaking engagements. So reach out to me on LinkedIn if you're interested in any of these services. And then with all that, back to the show. So a couple of days ago, Microsoft CEO Satya Nadella at the Microsoft Inspire 2023 keynote basically said, AI is here and these are the new capabilities because of language technology. So the two primary components that he outlined were natural language interface. So that's the front end right voice chat that sort of thing emails and then in the back end you have what's called a reasoning engine. And so what is a reasoning engine in this case a reasoning engine is instead of thinking about the language models as something that just generates text right. A lot of people pick the low hanging fruit. Let's use it to write, let's use it to write fiction. You know, that's the obvious use case. But what's less obvious, and this is what I have been advocating for for literally years, when I wrote my initial book on cognitive architecture, natural language cognitive architecture, I said, we have invented a cognitive engine or a reasoning engine. He probably didn't use cognitive engine because that's very anthropomorphic, but it's a reasoning engine. And so what does that mean? Language models, they're capable of brainstorming and planning. They're capable of problem solving. They're capable of coding and testing code and interpreting bugs. They're also capable of choosing and deciding or making decisions. And so these are all elements of cognition. And this is why I have been focusing on cognitive architecture for the last four years. So also the keynote was really good. You should watch it if you haven't, especially if you're whether you're an investor in this space or a startup founder or a product owner. If you are doing anything with generative AI, you need to watch that keynote speech. Okay, so I promised polymorphic applications, but really one thing that you need to become more familiar with is what I call mission-oriented programming. So this is the next paradigm beyond object-oriented programming. Instead of thinking of objects, we now have to think of missions as the primary organizing feature of this new software paradigm. And so let me just read the definition. Mission-oriented programming is a paradigm in software development where applications are primarily designed and built around a clear, measurable, and purposeful mission. I'll give you some examples in a little bit. Don't panic. The mission serves as the core driver of the software's behavior, adaptation, and evolution. These applications, often referred to as polymorphic applications, are not just tools but active agents in pursuing their respective mission. And so then there's a few components of this. One, mission-centric design. Number two, autonomy and agency, the ability to make decisions in pursuit of that mission. There needs to be adaptability and flexibility, hence the polymorphic aspect. Dynamic tool creation. If it needs a tool, it can make the tool. And then on the lower level, how do you actually allow the application to change itself? You use micro frameworks, configurable frontends, and configurable backends. Okay, cool. So I just threw a lot at you. You're probably like, what? What are you talking about? You know, think about it this way. This is how we got here. LLMs can write code. They can solve problems. They can make decisions. They can write unit tests. They can write user stories. So extrapolate that out as far as possible. You have an LLM that can do all of these things one at a time. What if you stack them all together? So another way to think about it, a simpler way to think about it, is generative AI or LLMs or whatever, they're just a type of automation. So if you're an infrastructure engineer, an IT guy, or a software dev, or a software architect, just think about generative AI as just another kind of automation. That's it. Automation, automation, automation. That's all you need to think about it. It gives you some new capabilities in the automation name of the game, but it's still just fundamentally a kind of automation. It's that simple. Okay, so when we talk about this automation engine or this reasoning engine, like what things can it automate? So first, think about how OpenAI just came out with function calling and API use. So basically that means that your LLM, your generative AI model, is universal middleware. It can talk to anything in your organization, anything in your tech stack, as long as it's got an API. So because of that, it can touch everything. It's capable of problem solving. There are many times where you just plug in like an error code or you know describe something that you're that you're getting and it can understand it. It's not always right but it can also understand when it's getting it wrong and kind of stop and you know you can use tree of thought or chain of thought or whatever. There's all kinds of problem solving techniques out there. Tree of Thought, by the way, increases problem-solving capabilities by like 900% or something. So if it can solve problems, that means it can overcome problems automatically. Decision-making. So here's the thing is, with a given objective or a mission, it can make decisions in pursuit of that mission. And this is why I say mission-oriented programming. And so then of course, planning and brainstorming, which is also required to pursue any given mission. So if you have an automation engine that is capable of these things now, what can you do with this now that it's in your toolbox? So one of the first things that you need to think about and to kind of change your mind is on-demand tools or tools that make tools, a tool factory. Because that's what, I mean, that's what GPT does with code. It generates new code. And what is code? But it's a software tool. And so, you know, you think about it like the replicator from Star Trek. Computer, I need something that does X, Y, and Z, and a few seconds later, it pops out. So if you think about the first step towards building polymorphic applications, the first step towards mission-oriented programming is the ability to synthesize any code object that you need on the fly. And so if it can write code, it can test code, it can call APIs, it can read the manual, it can write user stories, it can read epics, it can do all of that stuff. You combine all that together and you create an automated tool factory or a replicator. This is the first step in creating polymorphic applications because you can stack the blocks together, but you need something to print the blocks for you, right? It's like a 3D printer or a replicator, but for code. And so then, once all the blocks are, you've got a block factory, a tool factory, now you can just plug them all together. You aim for plug and play architecture, pluggable architecture. And from there, because it's got access to literally every API in your organization it can talk to Azure, AWS, Google Cloud, VMware, Microsoft, whatever. It can talk to everything in your organization as long as it's got an API then you build those building blocks and then you stack them together and what can you do with Legos? You can do anything with Legos as long as you've got enough blocks. So I think you're kind of getting the picture is that if you think about it in terms of modularity or individual pieces that can be kind of printed or fabricated on demand, and then you plug them together, that is the underpinning mentality of polymorphic applications. It's polymorphic because it can change shapes because all the pieces, all the underlying pieces are just plug and play. Okay, so we talked about mission earlier, but who runs the show? You've got a whole bunch of blocks, a whole bunch of building blocks, but how does the machine know what to plug into what and that sort of thing? You need a brain, right? You've got the tool factory. OK great. And at first it's going to be humans asking for tools. You know Replicator you know T Earl Gray hot. Right. But what you need to do is ultimately automate as much of that as possible. So the tool factory is automated but then you need to automate the thing that's asking for the tools and the tool and the building blocks. This is called a cognitive architecture. So remember, going back to Satya Nadella's point that we have a reasoning engine, or a cognitive engine, or an automation engine, whatever you wanna call it, it's all the same thing. Cognition, reasoning, automation, all the same thing. Now, let's dive into cognitive architecture. So, many of you that have been following my channel for a long time know that I am like the cognitive architecture guy. I wrote natural language cognitive architecture more than two years ago now. This is my shtick. This is my jam. Now what was missing from my books in the past was a more coherent framework. I had some diagrams. I had some architectural recommendations and a lot of people get a lot out of it. But this is like the OSI model but for next gen applications for polymorphic applications. So I call it the ACE model autonomous cognitive entities. So again right there in the name autonomous the idea is to create something that has a higher degree of autonomy but it also uses cognition it uses reasoning and then it's an entity and that it is somewhat self-contained. Now some people are going to be deploying generative AI in a more distributed sense and that's not what this video is about. This video is about basically the architecture to create commander data or you know BD1 or whatever. Right if you want to create C3PO or R2D2 this is the architecture the paradigm the, the framework, the conceptual framework that you need to implement. So without further ado, it is six layers. That might change, but I've been working on this for a while, so I'm pretty sure I've got it nailed down. So the first layer is the aspirational layer. This is the layer that is concerned with the mission, the values, the purpose, the ethics, and the morals of whatever it is that you're building. And like I said, we'll give you some examples of missions in just a minute so that it'll make more sense. Layer two is the global strategy layer. This is long-term thinking context. Context is the primary thing for layer two, and it's basically like a CEO. It says, okay, I have my vision, I have my morals, right, my higher order beliefs. Now, what is the biggest time horizon way to think about that mission? Layer three is agent model. So this is the self model and it's primarily concerned with capabilities. The machine has to know what it is, how it works and what it's capable of in order to make decisions. Because if you tell a machine, hey, go solve world hunger, it's like, okay, cool. Well, I'm gonna go plant a bunch of fields. It's like, well, actually, you've only got a budget of $50, right? And you've got one hand, and it's tied behind your back, right? So by understanding what the model is capable of, or your engine, or your architecture, it has to know what it is capable of and what it model is capable of, or your engine, or your architecture, it has to know what it is capable of and what it's not capable of. It also has to be capable of changing itself. So this is where the polymorphic thing comes in. If it understands that it has a limitation, such as like, well, I don't have access to that API, I need to get access to that API. How do I get access to that? Do I send a ticket to the help desk and they'll give me permission? or is it something that I can solve on my own? That sort of thing. So you need a very comprehensive agent model in order for the machine to be able to change itself in alignment with pursuing its mission. And so you've probably got some alarm bells pinging because you're like, Dave, you're basically designing Skynet, which is exactly why the aspirational layer is at the very top. Those constraints, those boundaries, the missions, the values, the purpose, this is what sets the tone, the guiding North Star of the device, so that it never fully goes off the rails, but you need something that is constantly watching. You know, it's the ideal self. And we'll dive into each of these layers in just a minute. But I wanted to give you the whole framework at a high level overview. So below the agent model is the executive function. So this is primarily concerned with planning or making plans. So that means like forecasting, like, OK, what's going to go wrong? How do I think through this? Directives, oh, and you know, the, the, the prompt engineering is already there. Let's think through this step-by-step, right? Take that out to the nth degree. That's the executive function layer. This is also concerned with resources. So if you make a plan, you have to know what resources are available to you. And these are external resources, right? The agent model in terms of capabilities, that is internal resources. This is what I am capable of in the world, but then executive function for planning is about external resources. How much compute do I need? What kind of developers need to help me? What kind of access do I need? What kind of data do I need from the outside world? And so then once you get a plan together, you get the blueprint, you get the burn-down chart, or however you want to organize it, your Kanban board, your JIRA board, right? Because that's basically what you're interacting with here at the executive function layer. Below that is cognitive control. Cognitive control is saying, okay, here's the big plan, now what? So this is about task selection and task switching. So that means choosing the correct order of operations and then, but more importantly, choosing when to switch tasks because if something isn't working or you started a task and then it's not the right order of operations or you realize that you've come into a barrier, you need to back out of that task and choose a different one. And so that is driven by frustration signals and cognitive damping, which we'll get into in just a moment. And then finally, layer six is task prosecution, which is doing one task at a time, but more importantly detecting when that task is truly successful or has failed and then reporting that information back up to the cognitive control layer, which will then adjust the frustration and damping signals. Okay, so let's get into these layers a little bit deeper. Number one, the aspirational layer. It's all about mission. I'll just read the description to you. This is the uppermost layer, which is somewhat abstracted and detached. This is the ideal self or superego version of the agent, which keeps track of the highest values, virtues, principles, vision super ego version of the agent which keeps track of the highest values virtues principles Vision and mission of the agent in other words this sets the tone for all other layers below it in other words I'm sorry I meant that I use that twice this serves as the moral compass and the guiding North Star for the autonomous cognitive entity for the Ace it provides the raison d'etre and I'm no that I didn't say that right But the reason for being. This layer also serves as the ultimate arbiter for all moral dilemmas that that thing faces. And so what I mean by that is, here's some examples of some missions that you might give. So for lawyers, this is set by the ethical standards and regulations and whatever, but the mission of a lawyer is to zealously advocate for their client. That is the highest mission of a lawyer. For doctors, achieve the best possible health outcome for their patient. So these are examples of aspirational missions, that that central mission informs every decision that they make, including all moral decisions, all ethical decisions. It also speaks to how they allocate their time, energy, and resources. And so by having a centrally organizing mission, that mission-oriented programming, you embed that at the highest level, or the heart, the core of your autonomous cognitive entity, and it will pursue that mission. And then any decisions that it makes will be with respect to and in pursuit of in honor of that mission. And that's why defining missions is really super critical. Just below that is global strategy which is all about context. So global context, world state, and long-term strategic thinking. This has the greatest time horizon and the greatest physical scope as well. So for instance, if you've got a lawyer robot or a doctor robot, it needs to not just be thinking about the hospital that it works in or the law firm that it works in, it needs to be thinking about the entire context of, okay, what's going on with medical research in the entire world? What is the status quo? What is the news? What is happening in the marketplace? What is happening in our competitors? That sort of thing. And so you think about this as the CEO of your autonomous cognitive entity. It maintains a very high-level perspective and it kind of keeps it at arm's length. So again, it's not going to get lost in the weeds just like any CEO should not be lost in the weeds. They need to be looking up and out, not down and in. So this layer of your autonomous cognitive entity is anchored to the real world because remember the aspirational layer is this is more abstract and conceptual. This is not really anchored to the real world. This is anchored to principles, values, mission, that sort of stuff. This is the first layer, the global strategy layer is the first one that is anchored to the real world but it has a global perspective and it also has a long-term perspective. It's not thinking about today and tomorrow, it's thinking about a decade from now, it's thinking about a century from now. It's also looking at the entire planet Earth and then if you know we expand beyond Earth it'll be thinking about the entire sphere of human existence, the entire universe, the entire cosmos. So you maintain that big perspective in order to you know start the start the process of zooming in. And you'll notice that each subsequent layer zooms in further and further and further. So we start super high level, super detached, and then we zoom in progressively with each layer. So this has to do with world state as well. Alright, I think you get the idea, so let's move on. Agent model. Agent model is all about capabilities. This is the ego of the machine. This is what it knows and believes about itself. Now, I know that this sounds very anthropomorphic, but again, we're talking about cognitive entities here. We're talking about reasoning engines. This is the layer that confers functional sentience. This is the first layer where your software architecture knows that it is a software architecture and it knows how it's configured, it knows what it's capable of, it understands its operational conditions, how many servers are running it, if it's on one piece of hardware it needs to know the voltage and temperature and all that other kind of stuff. So this is self-referential information. What is it? Basically the question question is, what am I capable of? How am I built? What can I do? What can't I do? What am I allowed to change about myself? What can I change about myself? What functions are autonomic? So for instance, if you have a bunch of models that are just learning online all the time, it needs to understand that and so that it can steer those models learning. You probably will use frozen models at first because you don't want the thing, you know, having a complex set of interactions and learning stuff that you weren't aware of. But in the long run, these things will be basically curating their own internal architecture, their own internal library of models, that sort of stuff. But this is fundamentally about capabilities. What am I capable of? What do I need to be capable of? What am I not capable of? And this level of zooming in is saying, okay, given my mission, my purpose, and given the global context of what's going on in the world, what am I capable of like what is within the realm of possibility for me to pursue that mission? And so by couching all this within that agent model, it's like, okay Well, you know my goal like the goal that I was given is, you know conquer the world But I'm running on you know an Intel Pentium 3 so my resources are limited So maybe I need to get more more resources and I'll have to beg borrow or steal them or whatever. Obviously don't give your agent the goal of conquering the world. We already tried that with Chaos GPT. Don't do it again. Let's stick with something a little bit more constructive. The next layer is executive function. This is primarily about planning. Once you have the mission, you've got the vision, and you've got the global context, and then you know what you're capable of, with all of that in mind, you're then able to say, okay, well, I've got access to so-so-and-so, internal resources, this is what's going on in the world. Okay, cool. Now let's come up with a plan to actually make it happen. And so you can think about this as a little miniature internal project manager or a director. Right. So it's the CEO. And then you got like director or PM or program owner or whatever. Right. But it's fundamentally about like, OK, let's think through this whole thing end to end. Let's come up with a plan to do this. And remember, large language models are great at it. If you don't believe me, go to chat GPT right now and tell it a problem. Give it everything that I just in the upper layers and say, come up with a plan and you'll see that it can do it. Okay. So once you, what goes into the plan though? So you have to think into the future, right? Okay. So you forecast like, what are the choke points, what are the points of failure, what's going to be happening, what resources do I need, and so this is really critical. Remember, I was talking about external resources. What kind of help am I going to need? How many parts, materials, money, energy, whatever, data? So this is where this is the layer where it's like, okay, let's let's scrape together a specific project plan for this exact project, whatever it's going to come up with in order to pursue that given mission. Below that is cognitive control. So cognitive control is fundamentally about focus. It says, okay, we've got the plan, we've got the mission, I know what I'm capable of, now what do I do? What is the correct order of operations here? So this is task selection and task switching. And so if you've got really bad ADHD, then your brain will switch tasks on you without your consent. And so that is an executive dysfunction that happens in humans. And as people have noticed, executive dysfunction is really common in these language models because they'll context switch without you telling it because they don't have it. So this layer is about giving it that cognitive control so that it doesn't context switch or task switch unless and until it's ready or it's supposed to. So this layer is going to be most familiar to people who are familiar with a term called finite state machines. So basically a finite state machine says you're doing one of any number of tasks. So this is really common in robotics and NPCs and video games. For an NPC you might be fighting, you might be running, you might be hiding, you might be searching. So there's a finite number of states that you can be in, or there's a finite number of tasks you can pick from from the project plan above. So whatever state the machine is in, you have to switch between those states and say, okay, right now I'm writing code, then I'm going to switch to testing code, then I'm going to switch to doing integration testing, then I'm going to switch to testing code, then I'm going to switch to doing integration testing, then I'm going to switch to deployment. So this is what cognitive control is about. It is about task switching, task selection, and then it's driven in part by, so from above it's driven by the project plan, from below it's driven by frustration and damping. So frustration is very simple. Frustration is simply just keeping track of the rate of failures versus the rate of successes. If the rate of failures is too high, then you've got the wrong plan. And that means you need to go up a layer and the executive function layer says, well, this plan isn't working, let's make a new plan. So that's where frustration comes in because if everything you're trying and 75% of it is failing, you probably got the wrong plan. Right. So that's where frustration comes in because if if everything you're trying and 75 percent of it is failing, you're probably got the wrong plan. So you need to go up and kind of re strategize and reallocate resources and feed that information back in. I cover all this in my book Symphony of Thought, by the way, where I give plenty of examples of how to achieve each of these steps with prompt engineering. Damping is, so cognitive damping is basically, okay let's pump the brakes, let's try and prove ourselves wrong, right? Like, okay let's let's think through this, let's do like what could go wrong here, let's let's think about failure points, let's think about is this is this a step that we're gonna regret if it goes wrong? Do we have a back out strategy? So cognitive damping is about slowing down and thinking through things a little bit more deliberately so that you don't make any grievous mistakes. In the context of medical robot, it's like okay if I make this incision there's no turning back right? And so cognitive damping says let's stop let's check like let's dot all of our I's and cross all of our T's to make sure that we're gonna do this right and make sure that we're ready to do this so that's what cognitive damping is and again this is all pretty easy to put into prompt engineering so those two things frustration and damping are two of the big biggest tactics for maintaining focus on what you're doing and why at any given moment. And then the very bottom layer is task prosecution. So this is tasks. This is the low-level, hands-on product task. So this could be the robotic command of go from A to B. It could be, you know, write X number of code or send an API call to, you know, there. And this is something that happens, all of these things happen in your brain and it's completely transparent, right? So imagine you go to, this happens to me all the time. I get home and I forget that I locked the door. So the task is open the door. So I go and reach for the door and it doesn't turn. Oh right, I locked it. So that's exactly what I mean by low-level tasks, success or failure. And so then it's like, okay, my brain gets the signal, door didn't open, right, let's back up a level. So my cognitive control switches from open the door to get the keys out of my pocket and then try again. So by focusing on this layer of success or failure of one task at a time, then you have that information available and you pass it back up the stack. In the case of a robotic example, it's like, okay, get from the living room to the kitchen and then it gets stuck because it's like, oh, well, there's a know laying, my dog does this all the time, lays right in the middle of the path. I can't so now now I've got a new problem right and so it's like okay well getting from the living room to the kitchen failed because there's a dog in the way so you pass that back up to cognitive control. It says oh let's let's switch switch states to now I need to ask the dog to get out of the way or ask the human to get the dog out of the way, right? Okay, so I've heard a lot of people say like, yeah, this is never happening, this is many years away and that's just not true. There are all kinds of people already building different components, but what I'm here to do is say, look, put it all together. So Langchain provides the last two, cognitive control and task prosecution. Langchain is fundamentally about workflows. One of the biggest limitations about Langchain is that it's linear, it's not cyclical. That being said, it's not that difficult to couch it in the middle of an infinite loop, but then you also need to make sure that you understand these paradigms of, you know, pass versus fail, and also when to go up another layer in order to change directions. The Ethos project, which was built by a team that came together in my Gato community, I think they got second place in their hackathon. Ethos is an example of that mission first. It is the higher order microservice that provides that alignment justification. And it's also a microservice, so it's modular. Some other examples, Baby AGI and Chaos GPT were somewhat naive attempts at the upper layers of the stack. They didn't, people were trying to like integrate everything all at once and didn't really have a clear mission and didn't have clear cognitive control or planning or executive function. Anyways, all this is available in my three primary books, Natural Language, Cognitive Architecture, Simply Enough Thought, and Benevolent by Design. One thing to keep in mind, like I said, is that this framework is newer. So you'll see bits and pieces of it, the progenitor thought in those books. You can also look at this framework that I just came up with. It's under my GitHub, Dave Schapps slash Benevolent underscore AGI. And yeah, so there you have it. This is the next generation of software development. So basically, whenever someone comes to me with a pitch, they're like, I'm doing a startup. Unless they have this level of sophistication, I'm just gonna say like, yeah, like you're off to a good start, but you need to be thinking in terms of cognitive architecture. You need to be thinking in terms of layers of abstraction and polymorphic applications. Anything less than that is just low-hanging fruit at this point. And as we've seen numerous times over the last six months, the very next iteration of chat GPT or a competitor is pretty much going to destroy any business model. Clawed, with its 100,000 tokens, destroyed a whole bunch of business models. Chat GPT with the plugins and the code interpreter destroyed a whole bunch of business models. SoGBT with the plugins and the code interpreter destroyed a whole bunch of business models. So unless you're thinking at this level of sophistication, your business probably won't work. And it's not just me saying it, Marissa Mayer of, what was it, Yahoo? She said that generative AI is gonna tear through the tech industry like a wildfire. And that this wildfire is gonna burn all the brush out, and the brush in this case is startups and smaller companies and That's gonna that's gonna be a clean sweep and we're gonna have a lot of new fresh growth after this that new fresh growth that she's talking about is cognitive architecture and polymorphic applications So thank you for watching Like I said, I am available to consult on any of this stuff reach out to me on LinkedIn link in the description Cheers. Have a good one So, thank you for watching. Like I said, I am available to consult on any of this stuff. Reach out to me on LinkedIn. Link in the description. Cheers, have a good one.", "chunks": [{"timestamp": [0.0, 4.0], "text": " Good morning everybody. David Shapiro here with another video."}, {"timestamp": [4.0, 8.0], "text": " So I mentioned a few videos ago, you may or may not have seen it,"}, {"timestamp": [8.0, 12.0], "text": " I mentioned the concept of polymorphic applications and a bunch of you"}, {"timestamp": [12.0, 16.0], "text": " were like, what is this concept? Tell me more. And some people are like, yeah,"}, {"timestamp": [16.0, 20.0], "text": " whatever, that's not possible. So this video is"}, {"timestamp": [20.0, 24.0], "text": " here to teach you a new paradigm of software"}, {"timestamp": [24.0, 26.72], "text": " development and software architecture."}, {"timestamp": [26.72, 29.52], "text": " So it's, I call it polymorphic applications"}, {"timestamp": [29.52, 32.48], "text": " or basically self-changing applications."}, {"timestamp": [32.48, 35.08], "text": " And we'll get right into it."}, {"timestamp": [35.08, 38.6], "text": " But first, I just wanted to say I am available."}, {"timestamp": [38.6, 41.16], "text": " I used to do consulting on Patreon."}, {"timestamp": [41.16, 42.64], "text": " I still have a few Patreon clients there,"}, {"timestamp": [42.64, 52.08], "text": " but I'm kind of moving to more involved engagements, including strategic and technical consulting, teaching and coaching,"}, {"timestamp": [52.08, 58.24], "text": " as well as speaking engagements. So reach out to me on LinkedIn if you're interested in any of these"}, {"timestamp": [58.24, 65.08], "text": " services. And then with all that, back to the show. So a couple of days ago,"}, {"timestamp": [65.08, 67.4], "text": " Microsoft CEO Satya Nadella"}, {"timestamp": [67.4, 71.88], "text": " at the Microsoft Inspire 2023 keynote"}, {"timestamp": [71.88, 73.56], "text": " basically said,"}, {"timestamp": [73.56, 76.76], "text": " AI is here and these are the new capabilities"}, {"timestamp": [76.76, 78.52], "text": " because of language technology."}, {"timestamp": [78.52, 81.76], "text": " So the two primary components that he outlined"}, {"timestamp": [81.76, 84.2], "text": " were natural language interface."}, {"timestamp": [84.2, 86.1], "text": " So that's the front end right voice"}, {"timestamp": [86.12, 88.12], "text": " chat that sort of thing emails"}, {"timestamp": [88.52, 90.44], "text": " and then in the back end you have"}, {"timestamp": [90.44, 92.04], "text": " what's called a reasoning engine."}, {"timestamp": [93.08, 95.2], "text": " And so what is a reasoning"}, {"timestamp": [95.2, 97.22], "text": " engine in this case a reasoning engine"}, {"timestamp": [97.24, 99.04], "text": " is instead of thinking about the"}, {"timestamp": [99.04, 101.02], "text": " language models as something that"}, {"timestamp": [101.04, 102.6], "text": " just generates text right."}, {"timestamp": [102.62, 104.58], "text": " A lot of people pick the low hanging"}, {"timestamp": [104.58, 108.28], "text": " fruit. Let's use it to write, let's use it to write fiction."}, {"timestamp": [108.28, 110.52], "text": " You know, that's the obvious use case."}, {"timestamp": [110.52, 111.56], "text": " But what's less obvious,"}, {"timestamp": [111.56, 113.32], "text": " and this is what I have been advocating for"}, {"timestamp": [113.32, 115.16], "text": " for literally years,"}, {"timestamp": [115.16, 117.84], "text": " when I wrote my initial book on cognitive architecture,"}, {"timestamp": [117.84, 119.44], "text": " natural language cognitive architecture,"}, {"timestamp": [119.44, 121.84], "text": " I said, we have invented a cognitive engine"}, {"timestamp": [121.84, 123.0], "text": " or a reasoning engine."}, {"timestamp": [123.0, 124.84], "text": " He probably didn't use cognitive engine"}, {"timestamp": [124.84, 129.28], "text": " because that's very anthropomorphic, but it's a reasoning engine. And so what does that mean?"}, {"timestamp": [130.08, 136.08], "text": " Language models, they're capable of brainstorming and planning. They're capable of problem solving."}, {"timestamp": [136.08, 141.68], "text": " They're capable of coding and testing code and interpreting bugs. They're also capable of"}, {"timestamp": [141.68, 148.3], "text": " choosing and deciding or making decisions. And so these are all elements of cognition."}, {"timestamp": [148.3, 153.74], "text": " And this is why I have been focusing on cognitive architecture for the last four years."}, {"timestamp": [153.74, 155.54], "text": " So also the keynote was really good."}, {"timestamp": [155.54, 160.3], "text": " You should watch it if you haven't, especially if you're whether you're an investor in this"}, {"timestamp": [160.3, 166.92], "text": " space or a startup founder or a product owner. If you are doing anything with generative AI,"}, {"timestamp": [166.92, 170.24], "text": " you need to watch that keynote speech."}, {"timestamp": [170.24, 175.04], "text": " Okay, so I promised polymorphic applications,"}, {"timestamp": [175.04, 176.92], "text": " but really one thing that you need"}, {"timestamp": [176.92, 178.1], "text": " to become more familiar with"}, {"timestamp": [178.1, 180.12], "text": " is what I call mission-oriented programming."}, {"timestamp": [180.12, 182.08], "text": " So this is the next paradigm"}, {"timestamp": [182.08, 184.56], "text": " beyond object-oriented programming."}, {"timestamp": [184.56, 192.32], "text": " Instead of thinking of objects, we now have to think of missions as the primary organizing"}, {"timestamp": [192.32, 194.4], "text": " feature of this new software paradigm."}, {"timestamp": [194.96, 197.28], "text": " And so let me just read the definition."}, {"timestamp": [197.28, 200.8], "text": " Mission-oriented programming is a paradigm in software development where applications"}, {"timestamp": [200.8, 210.32], "text": " are primarily designed and built around a clear, measurable, and purposeful mission. I'll give you some examples in a little bit. Don't panic. The mission serves as the core"}, {"timestamp": [210.32, 215.36], "text": " driver of the software's behavior, adaptation, and evolution. These applications, often referred to"}, {"timestamp": [215.36, 220.4], "text": " as polymorphic applications, are not just tools but active agents in pursuing their respective"}, {"timestamp": [220.4, 230.96], "text": " mission. And so then there's a few components of this. One, mission-centric design. Number two, autonomy and agency, the ability to make decisions in pursuit of that"}, {"timestamp": [230.96, 237.44], "text": " mission. There needs to be adaptability and flexibility, hence the polymorphic aspect."}, {"timestamp": [237.44, 242.44], "text": " Dynamic tool creation. If it needs a tool, it can make the tool. And then on the lower"}, {"timestamp": [242.44, 247.44], "text": " level, how do you actually allow the application to change"}, {"timestamp": [247.44, 252.16], "text": " itself? You use micro frameworks, configurable frontends, and configurable backends."}, {"timestamp": [252.16, 257.92], "text": " Okay, cool. So I just threw a lot at you. You're probably like, what? What are you talking"}, {"timestamp": [257.92, 265.56], "text": " about? You know, think about it this way. This is how we got here. LLMs can write code."}, {"timestamp": [265.56, 266.6], "text": " They can solve problems."}, {"timestamp": [266.6, 267.6], "text": " They can make decisions."}, {"timestamp": [267.6, 268.76], "text": " They can write unit tests."}, {"timestamp": [268.76, 270.44], "text": " They can write user stories."}, {"timestamp": [270.44, 272.64], "text": " So extrapolate that out as far as possible."}, {"timestamp": [272.64, 276.12], "text": " You have an LLM that can do all of these things one at a time."}, {"timestamp": [276.12, 278.48], "text": " What if you stack them all together?"}, {"timestamp": [278.48, 284.2], "text": " So another way to think about it, a simpler way to think about it, is generative AI or"}, {"timestamp": [284.2, 285.16], "text": " LLMs or whatever,"}, {"timestamp": [285.16, 289.64], "text": " they're just a type of automation. So if you're an infrastructure engineer, an IT"}, {"timestamp": [289.64, 294.44], "text": " guy, or a software dev, or a software architect, just think about"}, {"timestamp": [294.44, 299.24], "text": " generative AI as just another kind of automation. That's it. Automation,"}, {"timestamp": [299.24, 303.48], "text": " automation, automation. That's all you need to think about it. It gives you"}, {"timestamp": [303.48, 306.0], "text": " some new capabilities in the automation"}, {"timestamp": [306.0, 312.0], "text": " name of the game, but it's still just fundamentally a kind of automation. It's that simple."}, {"timestamp": [312.0, 319.0], "text": " Okay, so when we talk about this automation engine or this reasoning engine, like what things can it automate?"}, {"timestamp": [319.0, 325.72], "text": " So first, think about how OpenAI just came out with function calling and API use."}, {"timestamp": [325.72, 329.56], "text": " So basically that means that your LLM,"}, {"timestamp": [329.56, 332.32], "text": " your generative AI model, is universal middleware."}, {"timestamp": [332.32, 334.6], "text": " It can talk to anything in your organization,"}, {"timestamp": [334.6, 338.48], "text": " anything in your tech stack, as long as it's got an API."}, {"timestamp": [338.48, 341.54], "text": " So because of that, it can touch everything."}, {"timestamp": [341.54, 343.26], "text": " It's capable of problem solving."}, {"timestamp": [343.26, 366.32], "text": " There are many times where you just plug in like an error code or you know describe something that you're that you're getting and it can understand it. It's not always right but it can also understand when it's getting it wrong and kind of stop and you know you can use tree of thought or chain of thought or whatever. There's all kinds of problem solving techniques out there. Tree of Thought, by the way, increases"}, {"timestamp": [366.32, 371.68], "text": " problem-solving capabilities by like 900% or something. So if it can solve problems, that"}, {"timestamp": [371.68, 377.68], "text": " means it can overcome problems automatically. Decision-making. So here's the thing is, with"}, {"timestamp": [377.68, 388.1], "text": " a given objective or a mission, it can make decisions in pursuit of that mission. And this is why I say mission-oriented programming."}, {"timestamp": [388.1, 390.18], "text": " And so then of course, planning and brainstorming,"}, {"timestamp": [390.18, 394.9], "text": " which is also required to pursue any given mission."}, {"timestamp": [394.9, 396.3], "text": " So if you have an automation engine"}, {"timestamp": [396.3, 398.46], "text": " that is capable of these things now,"}, {"timestamp": [398.46, 402.1], "text": " what can you do with this now that it's in your toolbox?"}, {"timestamp": [402.1, 405.0], "text": " So one of the first things that you need to think about"}, {"timestamp": [405.0, 408.96], "text": " and to kind of change your mind is on-demand tools"}, {"timestamp": [408.96, 411.52], "text": " or tools that make tools, a tool factory."}, {"timestamp": [411.52, 415.34], "text": " Because that's what, I mean, that's what GPT does with code."}, {"timestamp": [415.34, 417.04], "text": " It generates new code."}, {"timestamp": [417.04, 417.88], "text": " And what is code?"}, {"timestamp": [417.88, 419.76], "text": " But it's a software tool."}, {"timestamp": [419.76, 421.78], "text": " And so, you know, you think about it"}, {"timestamp": [421.78, 423.68], "text": " like the replicator from Star Trek."}, {"timestamp": [423.68, 426.12], "text": " Computer, I need something that does X, Y, and Z,"}, {"timestamp": [426.12, 428.88], "text": " and a few seconds later, it pops out."}, {"timestamp": [428.88, 431.64], "text": " So if you think about the first step"}, {"timestamp": [431.64, 434.52], "text": " towards building polymorphic applications,"}, {"timestamp": [434.52, 436.84], "text": " the first step towards mission-oriented programming"}, {"timestamp": [436.84, 440.2], "text": " is the ability to synthesize any code object"}, {"timestamp": [440.2, 442.48], "text": " that you need on the fly."}, {"timestamp": [442.48, 444.68], "text": " And so if it can write code, it can test code,"}, {"timestamp": [444.68, 447.0], "text": " it can call APIs, it can read the manual,"}, {"timestamp": [447.0, 450.0], "text": " it can write user stories, it can read epics,"}, {"timestamp": [450.0, 452.0], "text": " it can do all of that stuff."}, {"timestamp": [452.0, 454.0], "text": " You combine all that together and you create an automated"}, {"timestamp": [454.0, 456.0], "text": " tool factory or a replicator."}, {"timestamp": [456.0, 459.0], "text": " This is the first step in creating polymorphic applications"}, {"timestamp": [459.0, 463.0], "text": " because you can stack the blocks together,"}, {"timestamp": [463.0, 465.96], "text": " but you need something to print the blocks for you, right?"}, {"timestamp": [465.96, 469.88], "text": " It's like a 3D printer or a replicator, but for code."}, {"timestamp": [469.88, 473.04], "text": " And so then, once all the blocks are,"}, {"timestamp": [473.04, 475.16], "text": " you've got a block factory, a tool factory,"}, {"timestamp": [475.16, 477.24], "text": " now you can just plug them all together."}, {"timestamp": [477.24, 479.76], "text": " You aim for plug and play architecture,"}, {"timestamp": [479.76, 481.52], "text": " pluggable architecture."}, {"timestamp": [481.52, 484.1], "text": " And from there, because it's got access"}, {"timestamp": [484.1, 490.9], "text": " to literally every API in your organization it can talk to Azure, AWS, Google Cloud, VMware,"}, {"timestamp": [490.9, 494.34], "text": " Microsoft, whatever. It can talk to everything in your organization as long"}, {"timestamp": [494.34, 499.1], "text": " as it's got an API then you build those building blocks and then you stack them"}, {"timestamp": [499.1, 503.86], "text": " together and what can you do with Legos? You can do anything with Legos as long"}, {"timestamp": [503.86, 505.2], "text": " as you've got enough blocks."}, {"timestamp": [505.2, 510.16], "text": " So I think you're kind of getting the picture is that if you think about it in terms of modularity"}, {"timestamp": [510.16, 515.68], "text": " or individual pieces that can be kind of printed or fabricated on demand, and then you plug them"}, {"timestamp": [515.68, 522.56], "text": " together, that is the underpinning mentality of polymorphic applications. It's polymorphic because"}, {"timestamp": [522.56, 526.86], "text": " it can change shapes because all the pieces, all the underlying pieces"}, {"timestamp": [526.86, 528.74], "text": " are just plug and play."}, {"timestamp": [528.74, 531.82], "text": " Okay, so we talked about mission earlier,"}, {"timestamp": [531.82, 533.4], "text": " but who runs the show?"}, {"timestamp": [533.4, 534.94], "text": " You've got a whole bunch of blocks,"}, {"timestamp": [534.94, 536.2], "text": " a whole bunch of building blocks,"}, {"timestamp": [536.2, 539.58], "text": " but how does the machine know what to plug into what"}, {"timestamp": [539.58, 540.92], "text": " and that sort of thing?"}, {"timestamp": [540.92, 542.82], "text": " You need a brain, right?"}, {"timestamp": [542.82, 545.08], "text": " You've got the tool factory. OK great."}, {"timestamp": [545.44, 547.44], "text": " And at first it's going to be humans asking for"}, {"timestamp": [547.44, 549.16], "text": " tools. You know Replicator you know"}, {"timestamp": [549.72, 550.8], "text": " T Earl Gray hot."}, {"timestamp": [550.88, 553.12], "text": " Right. But what"}, {"timestamp": [553.12, 555.24], "text": " you need to do is ultimately automate as much"}, {"timestamp": [555.24, 556.32], "text": " of that as possible."}, {"timestamp": [556.72, 558.92], "text": " So the tool factory is automated but then"}, {"timestamp": [558.92, 560.76], "text": " you need to automate the thing that's asking for"}, {"timestamp": [560.76, 563.08], "text": " the tools and the tool and the building blocks."}, {"timestamp": [563.6, 566.36], "text": " This is called a cognitive architecture."}, {"timestamp": [566.36, 568.34], "text": " So remember, going back to Satya Nadella's point"}, {"timestamp": [568.34, 569.88], "text": " that we have a reasoning engine,"}, {"timestamp": [569.88, 572.22], "text": " or a cognitive engine, or an automation engine,"}, {"timestamp": [572.22, 574.4], "text": " whatever you wanna call it, it's all the same thing."}, {"timestamp": [574.4, 577.42], "text": " Cognition, reasoning, automation, all the same thing."}, {"timestamp": [577.42, 579.9], "text": " Now, let's dive into cognitive architecture."}, {"timestamp": [580.82, 583.02], "text": " So, many of you that have been following my channel"}, {"timestamp": [583.02, 584.56], "text": " for a long time know that I am like"}, {"timestamp": [584.56, 587.76], "text": " the cognitive architecture guy. I wrote natural language cognitive"}, {"timestamp": [587.76, 592.12], "text": " architecture more than two years ago now. This is my shtick. This is my jam. Now"}, {"timestamp": [592.12, 597.28], "text": " what was missing from my books in the past was a more coherent framework. I had"}, {"timestamp": [597.28, 601.54], "text": " some diagrams. I had some architectural recommendations and a lot of people get a"}, {"timestamp": [601.54, 605.0], "text": " lot out of it. But this is like the OSI model"}, {"timestamp": [607.72, 609.04], "text": " but for next gen applications for polymorphic applications."}, {"timestamp": [609.04, 611.04], "text": " So I call it the ACE model autonomous"}, {"timestamp": [611.04, 612.4], "text": " cognitive entities."}, {"timestamp": [612.64, 615.12], "text": " So again right there in the name autonomous"}, {"timestamp": [615.16, 617.22], "text": " the idea is to create something that has a"}, {"timestamp": [617.22, 619.16], "text": " higher degree of autonomy"}, {"timestamp": [619.96, 621.92], "text": " but it also uses cognition it uses"}, {"timestamp": [621.92, 623.78], "text": " reasoning and then it's an entity"}, {"timestamp": [623.78, 626.56], "text": " and that it is somewhat self-contained."}, {"timestamp": [626.56, 629.48], "text": " Now some people are going to be deploying generative AI"}, {"timestamp": [629.48, 630.72], "text": " in a more distributed sense"}, {"timestamp": [630.72, 632.52], "text": " and that's not what this video is about."}, {"timestamp": [632.52, 635.16], "text": " This video is about basically the architecture"}, {"timestamp": [635.16, 639.16], "text": " to create commander data or you know BD1 or whatever."}, {"timestamp": [639.16, 642.08], "text": " Right if you want to create C3PO or R2D2"}, {"timestamp": [642.08, 647.2], "text": " this is the architecture the paradigm the, the framework, the conceptual framework"}, {"timestamp": [647.2, 648.6], "text": " that you need to implement."}, {"timestamp": [648.6, 651.96], "text": " So without further ado, it is six layers."}, {"timestamp": [651.96, 656.14], "text": " That might change, but I've been working on this for a while, so I'm pretty sure I've"}, {"timestamp": [656.14, 657.68], "text": " got it nailed down."}, {"timestamp": [657.68, 660.32], "text": " So the first layer is the aspirational layer."}, {"timestamp": [660.32, 664.94], "text": " This is the layer that is concerned with the mission, the values, the purpose, the ethics,"}, {"timestamp": [664.94, 668.4], "text": " and the morals of whatever it is that you're building."}, {"timestamp": [668.4, 673.8], "text": " And like I said, we'll give you some examples of missions in just a minute so that it'll make more sense."}, {"timestamp": [673.8, 678.4], "text": " Layer two is the global strategy layer. This is long-term thinking context."}, {"timestamp": [678.4, 682.6], "text": " Context is the primary thing for layer two, and it's basically like a CEO."}, {"timestamp": [682.6, 685.92], "text": " It says, okay, I have my vision,"}, {"timestamp": [685.92, 689.48], "text": " I have my morals, right, my higher order beliefs."}, {"timestamp": [689.48, 693.58], "text": " Now, what is the biggest time horizon way"}, {"timestamp": [693.58, 696.26], "text": " to think about that mission?"}, {"timestamp": [696.26, 697.58], "text": " Layer three is agent model."}, {"timestamp": [697.58, 699.5], "text": " So this is the self model"}, {"timestamp": [699.5, 702.06], "text": " and it's primarily concerned with capabilities."}, {"timestamp": [702.06, 704.46], "text": " The machine has to know what it is, how it works"}, {"timestamp": [704.46, 707.5], "text": " and what it's capable of in order to make decisions."}, {"timestamp": [707.5, 708.92], "text": " Because if you tell a machine,"}, {"timestamp": [708.92, 711.28], "text": " hey, go solve world hunger, it's like, okay, cool."}, {"timestamp": [711.28, 713.12], "text": " Well, I'm gonna go plant a bunch of fields."}, {"timestamp": [713.12, 713.96], "text": " It's like, well, actually,"}, {"timestamp": [713.96, 716.0], "text": " you've only got a budget of $50, right?"}, {"timestamp": [716.0, 717.52], "text": " And you've got one hand,"}, {"timestamp": [717.52, 719.98], "text": " and it's tied behind your back, right?"}, {"timestamp": [719.98, 722.6], "text": " So by understanding what the model is capable of,"}, {"timestamp": [722.6, 724.76], "text": " or your engine, or your architecture,"}, {"timestamp": [724.76, 726.64], "text": " it has to know what it is capable of and what it model is capable of, or your engine, or your architecture, it has to know what it is capable of"}, {"timestamp": [726.64, 728.22], "text": " and what it's not capable of."}, {"timestamp": [729.16, 731.68], "text": " It also has to be capable of changing itself."}, {"timestamp": [731.68, 734.32], "text": " So this is where the polymorphic thing comes in."}, {"timestamp": [734.32, 737.0], "text": " If it understands that it has a limitation,"}, {"timestamp": [737.0, 739.44], "text": " such as like, well, I don't have access to that API,"}, {"timestamp": [739.44, 741.7], "text": " I need to get access to that API."}, {"timestamp": [741.7, 743.14], "text": " How do I get access to that?"}, {"timestamp": [743.14, 749.7], "text": " Do I send a ticket to the help desk and they'll give me permission? or is it something that I can solve on my own? That sort of thing."}, {"timestamp": [749.7, 754.74], "text": " So you need a very comprehensive agent model in order for the machine to be able to change"}, {"timestamp": [754.74, 761.98], "text": " itself in alignment with pursuing its mission. And so you've probably got some alarm bells"}, {"timestamp": [761.98, 766.28], "text": " pinging because you're like, Dave, you're basically designing Skynet,"}, {"timestamp": [766.28, 768.9], "text": " which is exactly why the aspirational layer"}, {"timestamp": [768.9, 770.56], "text": " is at the very top."}, {"timestamp": [770.56, 773.26], "text": " Those constraints, those boundaries, the missions,"}, {"timestamp": [773.26, 774.96], "text": " the values, the purpose,"}, {"timestamp": [774.96, 778.5], "text": " this is what sets the tone,"}, {"timestamp": [778.5, 781.12], "text": " the guiding North Star of the device,"}, {"timestamp": [781.12, 784.24], "text": " so that it never fully goes off the rails,"}, {"timestamp": [784.24, 786.88], "text": " but you need something that is constantly watching."}, {"timestamp": [786.88, 788.48], "text": " You know, it's the ideal self."}, {"timestamp": [788.48, 790.4], "text": " And we'll dive into each of these layers in just a minute."}, {"timestamp": [790.4, 792.52], "text": " But I wanted to give you the whole framework"}, {"timestamp": [792.52, 794.44], "text": " at a high level overview."}, {"timestamp": [794.44, 797.64], "text": " So below the agent model is the executive function."}, {"timestamp": [797.64, 801.16], "text": " So this is primarily concerned with planning or making plans."}, {"timestamp": [801.16, 803.24], "text": " So that means like forecasting, like, OK,"}, {"timestamp": [803.24, 804.24], "text": " what's going to go wrong?"}, {"timestamp": [804.24, 808.32], "text": " How do I think through this? Directives, oh, and you know, the, the, the prompt engineering"}, {"timestamp": [808.32, 811.96], "text": " is already there. Let's think through this step-by-step, right? Take that out to the"}, {"timestamp": [811.96, 817.62], "text": " nth degree. That's the executive function layer. This is also concerned with resources."}, {"timestamp": [817.62, 821.72], "text": " So if you make a plan, you have to know what resources are available to you. And these"}, {"timestamp": [821.72, 827.0], "text": " are external resources, right? The agent model in terms of capabilities, that is internal resources."}, {"timestamp": [827.0, 834.0], "text": " This is what I am capable of in the world, but then executive function for planning is about external resources."}, {"timestamp": [834.0, 837.0], "text": " How much compute do I need? What kind of developers need to help me?"}, {"timestamp": [837.0, 842.0], "text": " What kind of access do I need? What kind of data do I need from the outside world?"}, {"timestamp": [842.0, 846.0], "text": " And so then once you get a plan together, you get the blueprint, you get the burn-down chart,"}, {"timestamp": [846.0, 848.0], "text": " or however you want to organize it,"}, {"timestamp": [848.0, 851.0], "text": " your Kanban board, your JIRA board, right?"}, {"timestamp": [851.0, 854.0], "text": " Because that's basically what you're interacting with here"}, {"timestamp": [854.0, 856.0], "text": " at the executive function layer."}, {"timestamp": [856.0, 857.0], "text": " Below that is cognitive control."}, {"timestamp": [857.0, 858.0], "text": " Cognitive control is saying,"}, {"timestamp": [858.0, 862.0], "text": " okay, here's the big plan, now what?"}, {"timestamp": [862.0, 867.48], "text": " So this is about task selection and task switching. So that means choosing"}, {"timestamp": [867.48, 872.84], "text": " the correct order of operations and then, but more importantly, choosing when to switch"}, {"timestamp": [872.84, 877.92], "text": " tasks because if something isn't working or you started a task and then it's not the right"}, {"timestamp": [877.92, 882.22], "text": " order of operations or you realize that you've come into a barrier, you need to back out"}, {"timestamp": [882.22, 885.88], "text": " of that task and choose a different one. And so that is driven by"}, {"timestamp": [886.5, 892.56], "text": " frustration signals and cognitive damping, which we'll get into in just a moment. And then finally, layer six is task"}, {"timestamp": [892.86, 897.86], "text": " prosecution, which is doing one task at a time, but more importantly detecting when that task is"}, {"timestamp": [898.48, 907.88], "text": " truly successful or has failed and then reporting that information back up to the cognitive control layer, which will then adjust the frustration and damping signals."}, {"timestamp": [907.88, 911.12], "text": " Okay, so let's get into these layers a little bit deeper."}, {"timestamp": [911.12, 912.96], "text": " Number one, the aspirational layer."}, {"timestamp": [912.96, 914.38], "text": " It's all about mission."}, {"timestamp": [914.38, 916.4], "text": " I'll just read the description to you."}, {"timestamp": [916.4, 917.94], "text": " This is the uppermost layer,"}, {"timestamp": [917.94, 920.18], "text": " which is somewhat abstracted and detached."}, {"timestamp": [920.18, 923.68], "text": " This is the ideal self or superego version of the agent,"}, {"timestamp": [923.68, 926.32], "text": " which keeps track of the highest values, virtues, principles, vision super ego version of the agent which keeps track of the highest values virtues principles"}, {"timestamp": [926.66, 933.02], "text": " Vision and mission of the agent in other words this sets the tone for all other layers below it in other words"}, {"timestamp": [933.02, 934.02], "text": " I'm sorry"}, {"timestamp": [934.02, 939.92], "text": " I meant that I use that twice this serves as the moral compass and the guiding North Star for the autonomous cognitive entity for the"}, {"timestamp": [939.92, 943.74], "text": " Ace it provides the raison d'etre and I'm no that I didn't say that right"}, {"timestamp": [943.74, 945.0], "text": " But the reason for being."}, {"timestamp": [945.0, 952.0], "text": " This layer also serves as the ultimate arbiter for all moral dilemmas that that thing faces."}, {"timestamp": [952.0, 957.0], "text": " And so what I mean by that is, here's some examples of some missions that you might give."}, {"timestamp": [957.0, 967.14], "text": " So for lawyers, this is set by the ethical standards and regulations and whatever, but the mission of a lawyer is to"}, {"timestamp": [967.14, 969.8], "text": " zealously advocate for their client."}, {"timestamp": [969.8, 972.08], "text": " That is the highest mission of a lawyer."}, {"timestamp": [972.08, 975.86], "text": " For doctors, achieve the best possible health outcome for their patient."}, {"timestamp": [975.86, 982.58], "text": " So these are examples of aspirational missions, that that central mission informs every decision"}, {"timestamp": [982.58, 987.6], "text": " that they make, including all moral decisions, all ethical decisions."}, {"timestamp": [987.6, 990.44], "text": " It also speaks to how they allocate their time,"}, {"timestamp": [990.44, 992.04], "text": " energy, and resources."}, {"timestamp": [992.04, 994.98], "text": " And so by having a centrally organizing mission,"}, {"timestamp": [994.98, 996.94], "text": " that mission-oriented programming,"}, {"timestamp": [996.94, 999.0], "text": " you embed that at the highest level,"}, {"timestamp": [999.0, 1003.52], "text": " or the heart, the core of your autonomous cognitive entity,"}, {"timestamp": [1003.52, 1010.68], "text": " and it will pursue that mission. And then any decisions that it makes will be with respect to and in"}, {"timestamp": [1010.68, 1016.32], "text": " pursuit of in honor of that mission. And that's why defining missions is really"}, {"timestamp": [1016.32, 1021.72], "text": " super critical. Just below that is global strategy which is all about context. So"}, {"timestamp": [1021.72, 1025.72], "text": " global context, world state, and long-term strategic thinking."}, {"timestamp": [1025.72, 1028.56], "text": " This has the greatest time horizon"}, {"timestamp": [1028.56, 1032.32], "text": " and the greatest physical scope as well."}, {"timestamp": [1032.32, 1034.68], "text": " So for instance, if you've got a lawyer robot"}, {"timestamp": [1034.68, 1037.36], "text": " or a doctor robot,"}, {"timestamp": [1037.36, 1039.64], "text": " it needs to not just be thinking about the hospital"}, {"timestamp": [1039.64, 1041.84], "text": " that it works in or the law firm that it works in,"}, {"timestamp": [1041.84, 1044.44], "text": " it needs to be thinking about the entire context"}, {"timestamp": [1044.44, 1049.94], "text": " of, okay, what's going on with medical research in the entire world?"}, {"timestamp": [1049.94, 1051.22], "text": " What is the status quo?"}, {"timestamp": [1051.22, 1052.44], "text": " What is the news?"}, {"timestamp": [1052.44, 1054.96], "text": " What is happening in the marketplace?"}, {"timestamp": [1054.96, 1057.06], "text": " What is happening in our competitors?"}, {"timestamp": [1057.06, 1058.52], "text": " That sort of thing."}, {"timestamp": [1058.52, 1062.88], "text": " And so you think about this as the CEO of your autonomous cognitive entity."}, {"timestamp": [1062.88, 1065.32], "text": " It maintains a very high-level perspective"}, {"timestamp": [1065.32, 1070.52], "text": " and it kind of keeps it at arm's length. So again, it's not going to get"}, {"timestamp": [1070.52, 1074.46], "text": " lost in the weeds just like any CEO should not be lost in the weeds. They"}, {"timestamp": [1074.46, 1080.28], "text": " need to be looking up and out, not down and in. So this layer of your autonomous"}, {"timestamp": [1080.28, 1084.62], "text": " cognitive entity is anchored to the real world because remember the"}, {"timestamp": [1084.62, 1088.5], "text": " aspirational layer is this is more abstract and conceptual. This is not"}, {"timestamp": [1088.5, 1093.9], "text": " really anchored to the real world. This is anchored to principles, values, mission,"}, {"timestamp": [1093.9, 1097.64], "text": " that sort of stuff. This is the first layer, the global strategy layer is the"}, {"timestamp": [1097.64, 1101.24], "text": " first one that is anchored to the real world but it has a global perspective"}, {"timestamp": [1101.24, 1107.52], "text": " and it also has a long-term perspective. It's not thinking about today and tomorrow, it's thinking about a decade from now, it's"}, {"timestamp": [1107.52, 1111.16], "text": " thinking about a century from now. It's also looking at the entire planet Earth"}, {"timestamp": [1111.16, 1115.12], "text": " and then if you know we expand beyond Earth it'll be thinking about the entire"}, {"timestamp": [1115.12, 1119.08], "text": " sphere of human existence, the entire universe, the entire cosmos. So you"}, {"timestamp": [1119.08, 1124.54], "text": " maintain that big perspective in order to you know start the start the process"}, {"timestamp": [1124.54, 1125.0], "text": " of zooming in."}, {"timestamp": [1125.0, 1129.0], "text": " And you'll notice that each subsequent layer zooms in further and further and further."}, {"timestamp": [1129.0, 1134.0], "text": " So we start super high level, super detached, and then we zoom in"}, {"timestamp": [1134.0, 1139.0], "text": " progressively with each layer. So this has to do with world state as well."}, {"timestamp": [1139.0, 1141.0], "text": " Alright, I think you get the idea, so let's move on."}, {"timestamp": [1141.0, 1147.12], "text": " Agent model. Agent model is all about capabilities. This is the ego of the machine."}, {"timestamp": [1147.12, 1153.04], "text": " This is what it knows and believes about itself. Now, I know that this sounds very anthropomorphic,"}, {"timestamp": [1153.04, 1157.68], "text": " but again, we're talking about cognitive entities here. We're talking about reasoning engines."}, {"timestamp": [1157.68, 1162.88], "text": " This is the layer that confers functional sentience. This is the first layer where"}, {"timestamp": [1162.88, 1166.48], "text": " your software architecture knows that it is a software"}, {"timestamp": [1166.48, 1171.44], "text": " architecture and it knows how it's configured, it knows what it's capable of, it understands its"}, {"timestamp": [1171.44, 1176.16], "text": " operational conditions, how many servers are running it, if it's on one piece of hardware it"}, {"timestamp": [1176.16, 1180.96], "text": " needs to know the voltage and temperature and all that other kind of stuff. So this is self-referential"}, {"timestamp": [1180.96, 1185.74], "text": " information. What is it? Basically the question question is, what am I capable of?"}, {"timestamp": [1185.74, 1186.88], "text": " How am I built?"}, {"timestamp": [1186.88, 1187.96], "text": " What can I do?"}, {"timestamp": [1187.96, 1189.28], "text": " What can't I do?"}, {"timestamp": [1189.28, 1191.4], "text": " What am I allowed to change about myself?"}, {"timestamp": [1191.4, 1193.18], "text": " What can I change about myself?"}, {"timestamp": [1193.18, 1194.72], "text": " What functions are autonomic?"}, {"timestamp": [1194.72, 1197.68], "text": " So for instance, if you have a bunch of models"}, {"timestamp": [1197.68, 1200.28], "text": " that are just learning online all the time,"}, {"timestamp": [1200.28, 1201.8], "text": " it needs to understand that"}, {"timestamp": [1201.8, 1204.36], "text": " and so that it can steer those models learning."}, {"timestamp": [1204.36, 1207.8], "text": " You probably will use frozen models at first because you don't want the thing,"}, {"timestamp": [1207.8, 1212.48], "text": " you know, having a complex set of interactions and learning stuff that you weren't aware of."}, {"timestamp": [1212.48, 1219.4], "text": " But in the long run, these things will be basically curating their own internal architecture,"}, {"timestamp": [1219.4, 1222.76], "text": " their own internal library of models, that sort of stuff."}, {"timestamp": [1222.76, 1225.28], "text": " But this is fundamentally about capabilities."}, {"timestamp": [1225.28, 1230.0], "text": " What am I capable of? What do I need to be capable of? What am I not capable of? And this"}, {"timestamp": [1231.84, 1238.72], "text": " level of zooming in is saying, okay, given my mission, my purpose, and given the global context"}, {"timestamp": [1238.72, 1246.74], "text": " of what's going on in the world, what am I capable of like what is within the realm of possibility for me to pursue that mission?"}, {"timestamp": [1246.74, 1251.18], "text": " And so by couching all this within that agent model, it's like, okay"}, {"timestamp": [1251.32, 1256.16], "text": " Well, you know my goal like the goal that I was given is, you know conquer the world"}, {"timestamp": [1256.16, 1261.64], "text": " But I'm running on you know an Intel Pentium 3 so my resources are limited"}, {"timestamp": [1261.64, 1269.72], "text": " So maybe I need to get more more resources and I'll have to beg borrow or steal them or whatever. Obviously don't give your agent the"}, {"timestamp": [1269.72, 1273.88], "text": " goal of conquering the world. We already tried that with Chaos GPT. Don't do it"}, {"timestamp": [1273.88, 1278.08], "text": " again. Let's stick with something a little bit more constructive. The"}, {"timestamp": [1278.08, 1281.68], "text": " next layer is executive function. This is primarily about"}, {"timestamp": [1281.68, 1288.0], "text": " planning. Once you have the mission, you've got the vision,"}, {"timestamp": [1288.0, 1289.64], "text": " and you've got the global context,"}, {"timestamp": [1289.64, 1291.4], "text": " and then you know what you're capable of,"}, {"timestamp": [1291.4, 1294.32], "text": " with all of that in mind, you're then able to say,"}, {"timestamp": [1294.32, 1298.76], "text": " okay, well, I've got access to so-so-and-so,"}, {"timestamp": [1298.76, 1301.44], "text": " internal resources, this is what's going on in the world."}, {"timestamp": [1301.44, 1302.36], "text": " Okay, cool."}, {"timestamp": [1302.36, 1305.48], "text": " Now let's come up with a plan to actually make it happen."}, {"timestamp": [1305.84, 1307.38], "text": " And so you can think about this as a"}, {"timestamp": [1307.38, 1309.14], "text": " little miniature internal project"}, {"timestamp": [1309.14, 1310.88], "text": " manager or a director."}, {"timestamp": [1310.96, 1312.64], "text": " Right. So it's the CEO."}, {"timestamp": [1312.86, 1315.04], "text": " And then you got like director or PM"}, {"timestamp": [1315.04, 1316.6], "text": " or program owner or whatever."}, {"timestamp": [1316.8, 1318.76], "text": " Right. But it's fundamentally about"}, {"timestamp": [1318.76, 1320.24], "text": " like, OK, let's think through this"}, {"timestamp": [1320.24, 1321.48], "text": " whole thing end to end."}, {"timestamp": [1321.8, 1323.38], "text": " Let's come up with a plan to do this."}, {"timestamp": [1323.4, 1324.72], "text": " And remember, large language models"}, {"timestamp": [1324.72, 1328.02], "text": " are great at it. If you don't believe me, go to chat GPT right now and tell"}, {"timestamp": [1328.02, 1332.26], "text": " it a problem. Give it everything that I just in the upper layers and say, come up with"}, {"timestamp": [1332.26, 1334.9], "text": " a plan and you'll see that it can do it."}, {"timestamp": [1334.9, 1340.54], "text": " Okay. So once you, what goes into the plan though? So you have to think into the future,"}, {"timestamp": [1340.54, 1345.52], "text": " right? Okay. So you forecast like, what are the choke points, what are the points of failure,"}, {"timestamp": [1350.32, 1354.72], "text": " what's going to be happening, what resources do I need, and so this is really critical. Remember, I was talking about external resources. What kind of help am I going to need? How many parts,"}, {"timestamp": [1354.72, 1360.08], "text": " materials, money, energy, whatever, data? So this is where this is the layer where it's like, okay,"}, {"timestamp": [1360.08, 1366.0], "text": " let's let's scrape together a specific project plan for this exact project,"}, {"timestamp": [1366.0, 1369.74], "text": " whatever it's going to come up with in order to pursue that given"}, {"timestamp": [1369.74, 1374.52], "text": " mission. Below that is cognitive control. So cognitive control is fundamentally"}, {"timestamp": [1374.52, 1378.52], "text": " about focus. It says, okay, we've got the plan, we've got the mission, I know what"}, {"timestamp": [1378.52, 1383.92], "text": " I'm capable of, now what do I do? What is the correct order of operations here? So"}, {"timestamp": [1383.92, 1386.72], "text": " this is task selection and task switching."}, {"timestamp": [1386.72, 1389.44], "text": " And so if you've got really bad ADHD,"}, {"timestamp": [1389.44, 1391.88], "text": " then your brain will switch tasks on you"}, {"timestamp": [1391.88, 1394.86], "text": " without your consent."}, {"timestamp": [1394.86, 1397.44], "text": " And so that is an executive dysfunction"}, {"timestamp": [1397.44, 1398.52], "text": " that happens in humans."}, {"timestamp": [1398.52, 1399.96], "text": " And as people have noticed,"}, {"timestamp": [1399.96, 1401.72], "text": " executive dysfunction is really common"}, {"timestamp": [1401.72, 1402.72], "text": " in these language models"}, {"timestamp": [1402.72, 1408.28], "text": " because they'll context switch without you telling it because they"}, {"timestamp": [1408.28, 1414.44], "text": " don't have it. So this layer is about giving it that cognitive control"}, {"timestamp": [1414.44, 1419.32], "text": " so that it doesn't context switch or task switch unless and until it's ready"}, {"timestamp": [1419.32, 1427.28], "text": " or it's supposed to. So this layer is going to be most familiar to people who are familiar with a term called"}, {"timestamp": [1427.28, 1433.04], "text": " finite state machines. So basically a finite state machine says you're doing one of any number of"}, {"timestamp": [1433.04, 1441.04], "text": " tasks. So this is really common in robotics and NPCs and video games. For an NPC you might be"}, {"timestamp": [1441.04, 1448.72], "text": " fighting, you might be running, you might be hiding, you might be searching. So there's a finite number of states that you can be in, or there's a finite number"}, {"timestamp": [1448.72, 1453.52], "text": " of tasks you can pick from from the project plan above."}, {"timestamp": [1453.52, 1459.0], "text": " So whatever state the machine is in, you have to switch between those states and say, okay,"}, {"timestamp": [1459.0, 1463.44], "text": " right now I'm writing code, then I'm going to switch to testing code, then I'm going"}, {"timestamp": [1463.44, 1465.48], "text": " to switch to doing integration testing, then I'm going to switch to testing code, then I'm going to switch to doing"}, {"timestamp": [1465.48, 1469.86], "text": " integration testing, then I'm going to switch to deployment. So this is"}, {"timestamp": [1469.86, 1473.98], "text": " what cognitive control is about. It is about task switching, task selection,"}, {"timestamp": [1473.98, 1479.5], "text": " and then it's driven in part by, so from above it's driven by the project plan,"}, {"timestamp": [1479.5, 1484.02], "text": " from below it's driven by frustration and damping. So frustration is very"}, {"timestamp": [1484.02, 1486.76], "text": " simple. Frustration is simply just keeping track"}, {"timestamp": [1486.76, 1489.92], "text": " of the rate of failures versus the rate of successes."}, {"timestamp": [1489.92, 1492.12], "text": " If the rate of failures is too high,"}, {"timestamp": [1492.12, 1493.76], "text": " then you've got the wrong plan."}, {"timestamp": [1493.76, 1495.78], "text": " And that means you need to go up a layer"}, {"timestamp": [1495.78, 1497.8], "text": " and the executive function layer says,"}, {"timestamp": [1497.8, 1501.24], "text": " well, this plan isn't working, let's make a new plan."}, {"timestamp": [1501.24, 1502.76], "text": " So that's where frustration comes in"}, {"timestamp": [1502.76, 1504.88], "text": " because if everything you're trying"}, {"timestamp": [1504.88, 1509.8], "text": " and 75% of it is failing, you probably got the wrong plan. Right. So that's where frustration comes in because if if everything you're trying and 75 percent of it is failing, you're probably got the wrong plan. So you need to go up and"}, {"timestamp": [1509.8, 1515.22], "text": " kind of re strategize and reallocate resources and feed that information back in. I cover"}, {"timestamp": [1515.22, 1520.46], "text": " all this in my book Symphony of Thought, by the way, where I give plenty of examples of"}, {"timestamp": [1520.46, 1526.68], "text": " how to achieve each of these steps with prompt engineering. Damping is, so"}, {"timestamp": [1526.68, 1531.4], "text": " cognitive damping is basically, okay let's pump the brakes, let's try and"}, {"timestamp": [1531.4, 1535.56], "text": " prove ourselves wrong, right? Like, okay let's let's think through this, let's do"}, {"timestamp": [1535.56, 1541.64], "text": " like what could go wrong here, let's let's think about failure points, let's"}, {"timestamp": [1541.64, 1548.56], "text": " think about is this is this a step that we're gonna regret if it goes wrong? Do we have a back out strategy? So cognitive damping"}, {"timestamp": [1548.56, 1552.12], "text": " is about slowing down and thinking through things a little bit more"}, {"timestamp": [1552.12, 1556.6], "text": " deliberately so that you don't make any grievous mistakes. In the context of"}, {"timestamp": [1556.6, 1562.52], "text": " medical robot, it's like okay if I make this incision there's no"}, {"timestamp": [1562.52, 1569.0], "text": " turning back right? And so cognitive damping says let's stop let's check like let's dot all of our I's and"}, {"timestamp": [1569.0, 1573.14], "text": " cross all of our T's to make sure that we're gonna do this right and make sure"}, {"timestamp": [1573.14, 1576.8], "text": " that we're ready to do this so that's what cognitive damping is and again this"}, {"timestamp": [1576.8, 1581.96], "text": " is all pretty easy to put into prompt engineering so those two things"}, {"timestamp": [1581.96, 1585.76], "text": " frustration and damping are two of the big biggest"}, {"timestamp": [1585.76, 1590.48], "text": " tactics for maintaining focus on what you're doing and why at any given moment."}, {"timestamp": [1590.48, 1595.48], "text": " And then the very bottom layer is task prosecution. So this is tasks. This is the"}, {"timestamp": [1595.48, 1602.0], "text": " low-level, hands-on product task. So this could be the robotic command of go from"}, {"timestamp": [1602.0, 1605.04], "text": " A to B. It could be, you know, write X number"}, {"timestamp": [1605.04, 1610.96], "text": " of code or send an API call to, you know, there. And this is something that happens,"}, {"timestamp": [1610.96, 1615.84], "text": " all of these things happen in your brain and it's completely transparent, right? So imagine"}, {"timestamp": [1615.84, 1621.08], "text": " you go to, this happens to me all the time. I get home and I forget that I locked the"}, {"timestamp": [1621.08, 1625.12], "text": " door. So the task is open the door. So I go and reach for the door"}, {"timestamp": [1625.12, 1630.24], "text": " and it doesn't turn. Oh right, I locked it. So that's exactly what I mean by low-level tasks,"}, {"timestamp": [1630.24, 1635.6], "text": " success or failure. And so then it's like, okay, my brain gets the signal, door didn't open, right,"}, {"timestamp": [1635.6, 1640.96], "text": " let's back up a level. So my cognitive control switches from open the door to get the keys out"}, {"timestamp": [1640.96, 1645.46], "text": " of my pocket and then try again. So by focusing on this"}, {"timestamp": [1645.46, 1652.36], "text": " layer of success or failure of one task at a time, then you have that information"}, {"timestamp": [1652.36, 1657.22], "text": " available and you pass it back up the stack. In the case of a robotic"}, {"timestamp": [1657.22, 1661.84], "text": " example, it's like, okay, get from the living room to the kitchen and then"}, {"timestamp": [1661.84, 1667.84], "text": " it gets stuck because it's like, oh, well, there's a know laying, my dog does this all the time, lays right in the"}, {"timestamp": [1667.84, 1672.56], "text": " middle of the path. I can't so now now I've got a new problem right and so it's"}, {"timestamp": [1672.56, 1676.02], "text": " like okay well getting from the living room to the kitchen failed because"}, {"timestamp": [1676.02, 1679.08], "text": " there's a dog in the way so you pass that back up to cognitive control. It"}, {"timestamp": [1679.08, 1683.44], "text": " says oh let's let's switch switch states to now I need to ask the dog to get out"}, {"timestamp": [1683.44, 1686.6], "text": " of the way or ask the human to get the dog out of the way, right?"}, {"timestamp": [1686.6, 1690.32], "text": " Okay, so I've heard a lot of people say like,"}, {"timestamp": [1690.32, 1692.48], "text": " yeah, this is never happening, this is many years away"}, {"timestamp": [1692.48, 1693.98], "text": " and that's just not true."}, {"timestamp": [1693.98, 1695.4], "text": " There are all kinds of people"}, {"timestamp": [1695.4, 1697.4], "text": " already building different components,"}, {"timestamp": [1697.4, 1700.64], "text": " but what I'm here to do is say, look, put it all together."}, {"timestamp": [1700.64, 1704.08], "text": " So Langchain provides the last two,"}, {"timestamp": [1704.08, 1706.6], "text": " cognitive control and task prosecution."}, {"timestamp": [1706.6, 1709.8], "text": " Langchain is fundamentally about workflows."}, {"timestamp": [1709.8, 1712.3], "text": " One of the biggest limitations about Langchain"}, {"timestamp": [1712.3, 1714.1], "text": " is that it's linear, it's not cyclical."}, {"timestamp": [1714.1, 1717.3], "text": " That being said, it's not that difficult to couch it"}, {"timestamp": [1717.3, 1719.6], "text": " in the middle of an infinite loop,"}, {"timestamp": [1719.6, 1722.9], "text": " but then you also need to make sure that you understand"}, {"timestamp": [1722.9, 1730.56], "text": " these paradigms of, you know, pass versus fail, and also when to go up another layer in order to change directions."}, {"timestamp": [1730.56, 1738.32], "text": " The Ethos project, which was built by a team that came together in my Gato community,"}, {"timestamp": [1738.32, 1741.68], "text": " I think they got second place in their hackathon."}, {"timestamp": [1741.68, 1747.16], "text": " Ethos is an example of that mission first. It is the higher order microservice"}, {"timestamp": [1747.16, 1750.92], "text": " that provides that alignment justification."}, {"timestamp": [1750.92, 1753.92], "text": " And it's also a microservice, so it's modular."}, {"timestamp": [1753.92, 1756.56], "text": " Some other examples, Baby AGI and Chaos GPT"}, {"timestamp": [1756.56, 1759.88], "text": " were somewhat naive attempts at the upper layers"}, {"timestamp": [1761.6, 1763.08], "text": " of the stack."}, {"timestamp": [1763.08, 1768.16], "text": " They didn't, people were trying to like integrate everything all at once and didn't really have"}, {"timestamp": [1768.16, 1772.56], "text": " a clear mission and didn't have clear cognitive control or planning or executive function."}, {"timestamp": [1773.12, 1778.24], "text": " Anyways, all this is available in my three primary books, Natural Language, Cognitive"}, {"timestamp": [1778.24, 1782.64], "text": " Architecture, Simply Enough Thought, and Benevolent by Design. One thing to keep in mind, like I said,"}, {"timestamp": [1782.64, 1785.08], "text": " is that this framework is newer."}, {"timestamp": [1785.08, 1788.44], "text": " So you'll see bits and pieces of it,"}, {"timestamp": [1788.44, 1791.12], "text": " the progenitor thought in those books."}, {"timestamp": [1791.12, 1792.52], "text": " You can also look at this framework"}, {"timestamp": [1792.52, 1793.4], "text": " that I just came up with."}, {"timestamp": [1793.4, 1796.2], "text": " It's under my GitHub, Dave Schapps slash Benevolent"}, {"timestamp": [1796.2, 1798.12], "text": " underscore AGI."}, {"timestamp": [1798.12, 1799.96], "text": " And yeah, so there you have it."}, {"timestamp": [1799.96, 1804.2], "text": " This is the next generation of software development."}, {"timestamp": [1804.2, 1808.04], "text": " So basically, whenever someone comes to me with a pitch,"}, {"timestamp": [1808.04, 1809.48], "text": " they're like, I'm doing a startup."}, {"timestamp": [1809.48, 1811.98], "text": " Unless they have this level of sophistication,"}, {"timestamp": [1811.98, 1813.64], "text": " I'm just gonna say like, yeah,"}, {"timestamp": [1813.64, 1815.06], "text": " like you're off to a good start,"}, {"timestamp": [1815.06, 1817.28], "text": " but you need to be thinking in terms"}, {"timestamp": [1817.28, 1819.16], "text": " of cognitive architecture."}, {"timestamp": [1819.16, 1822.4], "text": " You need to be thinking in terms of layers of abstraction"}, {"timestamp": [1822.4, 1824.24], "text": " and polymorphic applications."}, {"timestamp": [1824.24, 1827.6], "text": " Anything less than that is just low-hanging fruit at this point."}, {"timestamp": [1827.6, 1831.6], "text": " And as we've seen numerous times over the last six months,"}, {"timestamp": [1831.6, 1835.1], "text": " the very next iteration of chat GPT or a competitor"}, {"timestamp": [1835.1, 1837.7], "text": " is pretty much going to destroy any business model."}, {"timestamp": [1837.7, 1840.2], "text": " Clawed, with its 100,000 tokens,"}, {"timestamp": [1840.2, 1842.3], "text": " destroyed a whole bunch of business models."}, {"timestamp": [1842.3, 1844.9], "text": " Chat GPT with the plugins and the code interpreter"}, {"timestamp": [1844.9, 1845.04], "text": " destroyed a whole bunch of business models. SoGBT with the plugins and the code interpreter"}, {"timestamp": [1845.04, 1846.88], "text": " destroyed a whole bunch of business models."}, {"timestamp": [1846.88, 1849.82], "text": " So unless you're thinking at this level of sophistication,"}, {"timestamp": [1849.82, 1852.64], "text": " your business probably won't work."}, {"timestamp": [1852.64, 1854.04], "text": " And it's not just me saying it,"}, {"timestamp": [1854.04, 1856.76], "text": " Marissa Mayer of, what was it, Yahoo?"}, {"timestamp": [1856.76, 1859.06], "text": " She said that generative AI is gonna tear through"}, {"timestamp": [1859.06, 1861.12], "text": " the tech industry like a wildfire."}, {"timestamp": [1861.12, 1864.4], "text": " And that this wildfire is gonna burn all the brush out,"}, {"timestamp": [1864.4, 1865.8], "text": " and the brush in this case is"}, {"timestamp": [1866.32, 1868.32], "text": " startups and smaller companies and"}, {"timestamp": [1868.44, 1873.22], "text": " That's gonna that's gonna be a clean sweep and we're gonna have a lot of new fresh growth after this"}, {"timestamp": [1873.64, 1876.26], "text": " that new fresh growth that she's talking about is"}, {"timestamp": [1876.92, 1879.6], "text": " cognitive architecture and polymorphic applications"}, {"timestamp": [1879.6, 1881.6], "text": " So thank you for watching"}, {"timestamp": [1881.64, 1887.34], "text": " Like I said, I am available to consult on any of this stuff reach out to me on LinkedIn link in the description"}, {"timestamp": [1887.34, 1889.34], "text": " Cheers. Have a good one"}, {"timestamp": [1883.71, 1885.85], "text": " So, thank you for watching."}, {"timestamp": [1885.85, 1888.63], "text": " Like I said, I am available to consult on any of this stuff."}, {"timestamp": [1888.63, 1890.31], "text": " Reach out to me on LinkedIn."}, {"timestamp": [1890.31, 1891.31], "text": " Link in the description."}, {"timestamp": [1891.31, 1892.19], "text": " Cheers, have a good one."}]}