{"text": " There are exactly three kinds of prompts that exist that encapsulate literally every other kind of prompt and just a couple of foundational principles that you need to know in order to master language models such as GPT and LLAMA and others. Now a little bit of background about myself, I've been doing prompt engineering since GPT-2 and of course now we are on GPT-4. So the three fundamental operations you need start with reductive operations. So what do I mean by reductive operations? A reductive operation is where you have a larger input than you do output. So this is things like summarization. Summarization is you say the same thing with fewer words. There's a few formats that you can use. You can use list, you can use notes, you can use executive summary. Each of these have slightly different focuses. So, for instance, an executive summary gives you just a high-level overview or the core assertion without actually trying to say the same thing over again. Distillation is a very useful one, which is basically you're telling it to kind of purify the core underlying principle or fact. This removes a lot of noise. Extraction, so extraction is what a lot of people are really familiar with already because that comes from older NLP, such as answering questions, named entity extraction, getting dates and numbers, that sort of thing. Characterizing, this is one that a lot of people don't know about. You can have the language model either characterize the text or characterize the topic within the text. And so what I mean by that is you can characterize text, you can say, you know, is this, does this look like fiction? Does it look like a scientific article? Does this look like code? Or then you can, if it look like a scientific article, does this look like code? Or, if it is code, for instance, you can characterize the code within that context. You can analyze it. So basically you can do structural analysis, rhetorical analysis, that sort of thing. There's evaluations. So evaluations are measuring, grading, or judging the content. So there's several kinds of evaluations. So say for instance, you have a rubric and you say, hey, grade this, you know, grade this essay based on this rubric. Plenty of people have discovered that out there. You can also evaluate against moral frameworks. So you can say, like, is this aligned with, you know, Plato's, you know, yada yada, or is this aligned with Plato's yada yada, or is this aligned with Kantian logic or whatever. It knows those things. And then finally critiquing. So critiquing is providing critical feedback and recommendations to improve something. That's reductive operations. The second kind of operation is transformational operations. So transformational operations, you basically, the input and output put are roughly the same size, and or roughly the same meaning. So an example of a transformation operation is reformatting. So this just changes the presentation. You might say, hey, take this and turn it into bullet points. Rewrite this prose as screenplay or rewrite this screenplay as prose. Translate it from XML to JSON, that sort of thing. Refactoring, so refactoring is something from the programming world where it just basically say, okay, take this chunk of code, write something that does the same exact thing but write it better or write it differently. And you can also do the same thing with language. So for instance, any kind of structured language or constructed language, you can say, hey, you know, basically say the same thing but in a different way. And so like whether it's a legal document or a scientific document or whatever, you can refactor those as well. Language change, so you can change between natural languages such as English and Portuguese. You can also translate between coding languages like C++ to Python. Now obviously I can hear people in the background screaming like C++ is compiled and Python is interpreted and there's all kinds of things that mean that you can't translate directly from one to the other. It's true. But give it a try. You'd be surprised how well modern language models can do. So restructuring, so restructuring is different from reformatting, whereas restructuring says, okay, this is in the wrong order. We need to change the order, we need to remove sections, we need to add sections. You can optimize for logical flow, you can optimize for basically flow, you can optimize for basically any kind of priority that you want. So that's restructuring. Modification. There's all kinds of modifications you can do, but basically you rewrite copy to achieve a slightly different intention. You can change the tone, you can change the formality, you can change the level of diplomacy, you can change the style. Those are what I mean by modifications. And then finally, clarification. So clarification is where you say, hey, write this, but write it like more clear, right? Articulate it better and more clearly, which is something that is extremely useful when you're using it to edit, for instance, scientific copy, or if you're not a native English speaker, or native anything speaker, really. If I was trying to write something in French, and I would, you know, like, my grasp of French is like, I've basically taken two weeks of French, so I'd be like, rewrite this garbage so that it's actually comprehensible to a French person. That's an example of a transformational operation. And then finally, generative operations or expansion operations or magnification operations. In this case, the input is much smaller than the output. And it may or may not have the same meaning, but the idea is that you go from small to big. And so this is drafting. So drafting is where you give it a set of instructions, and you have it draft some kind of document, whether that's a code file, fiction, legal document, knowledge base, scientific document, some kind of storytelling like telling a story about data or other facts. Planning. So planning is another thing that I don't see a lot of out there that it's actually really good at, which is given a set of parameters, come up with plans. So you can have action plans, project plans, brainstorm objectives and missions. You can have it elucidate constraints and also discuss the context within that planning session. Brainstorming. So brainstorming is something that a lot of people are familiar with, especially with chat GPT, where you basically use your imagination to list out possibilities. So this is either generating ideas, exploration of possibilities, problem solving. Problem solving requires brainstorming and then hypothesizing, actually generating hypotheses. And then finally amplification, which is similar to expanding something in the transformational one, but basically instead of just saying, explain this thing better, you say, riff on this and actually expand this entire topic to fully unpack it. That's the keyword that I use there is unpacking, where it's like, hey, here's a topic that I wanna talk more about, unpack this. So those are the three operations. It's reductive, transformational, and generative or expansive. Now, the other thing that you need to know about prompt engineering and language models is latency and emergence. All right, so before we dive into this, I need to give you a really, really, really fast crash course on Bloom's Taxonomy. So Bloom studied all kinds of stuff in education. So if you look him up, did a lot of stuff with education like Bloom's Two Sigma Problem, but Bloom's Taxonomy, as someone who is a big fan of frameworks, this is really critical to understand how humans learn in order to really see what language models are capable of. So at the bottom layer you have remember, followed by understand, followed by apply, analyze, evaluate, and create. And so let's really quickly unpack these and you will see that language models have already attained most, if not all, of Bloom's Taxonomy. So remembering is just recalling facts and concepts. Yes, it can regurgitate stuff, no problem. Understanding, so understanding is defined as explaining the ideas and concepts, connecting the words to meanings. Yes, language models can absolutely do that. And so whenever I see someone in the comments say like, but language models don't truly understand anything. I'm like well Yes, they do by this definition But then if you want to dive into the epistemics and and philosophy of it Then I would argue that humans don't understand anything. Anyways, anyways going down a rabbit hole Applying using information in new situations. So this is the functional utility if language models couldn't apply Information they wouldn't be useful. But they are useful, and so we see that they can apply in functional settings. Analyzing. So drawing connections among ideas. If you don't believe me, go into ChatGPT and ask it to do a rhetorical analysis on your own emails or whatever. It absolutely can analyze stuff. Number or the next one, it's one, I didn't number these, evaluating. So justifying a decision or action. Now of course this is where chat GPT, you can really get it into a corner because it will definitely justify itself. It will explain and articulate and defend something, whether or not it's right, but again, doing it in one shot is not necessarily going to produce the best results, because if you have a person, if you back a person into a corner and ask them to justify themselves, well, people will often do the same things, especially pathological liars. And then finally, creating, producing new or original work, generating something that did not previously exist. Pretty much everything that a language model spits out is something that did not previously exist. Pretty much everything that a language model spits out is something that did not previously exist. So by this definition, by this model, language models are already at the full stack of Bloom's taxonomy in terms of mental capabilities that humans are. Now, you might argue, it's like, well, okay, it's not quite as smart as a human in some respects, and it makes kinds of mistakes. Okay, sure, whatever. I'm not gonna argue about the details. The point here is that Bloom's taxonomy is a good model for understanding what language models are capable of. Okay, so latency, latent content, or the latent space. This is the knowledge, facts, concepts, information, and other capabilities that are embedded in the model, but they must be activated by the correct prompting. It's like buried treasure, hence the graphic here. So training data. The latent content only originates from the training data, which means that it's not going to magically know something that wasn't in the training data. Now that being said, it can connect dots and synthesize new information. This is what I mean by this, is creating, is if you activate the right set of patterns with the right words and prompts, the language model will be able to make novel connections. World knowledge, so general facts and understanding about the world, this is all inferred and imbibed from the training data, scientific information, cultural knowledge, historical knowledge, and languages. All of this comes from the training data because whether or not you realize it, these language models are trained on many, many, many terabytes worth of internet data, which means through either explicit or implicit connections, it understands all of these things. Emergent capabilities. So, the larger a language model is, the more emergent capabilities we're finding. And so several examples of these emergent capabilities are theory of mind. The theory of mind is the ability to understand and keep track of the content of other people's minds. Basically, it's read enough Reddit to look at how people think to understand how humans think. Now, you might say this is highly controversial because it doesn't have mirror neurons and other kinds of stuff. I really don't care. Mathematically speaking, it has read enough human-generated content to be able to model a human's mind. Implied cognition, the second one. So basically thinking with the right prompting. So what I mean by this is that in, because all that these language models are doing is they're trained to predict the next token. In order to accurately predict the next token, it had to learn to think. It's as simple as that is, okay, well, it's just a mathematical model to predict the next token. Yes, but it's also a black box on the inside, and it's doing a lot of embedded operations and using some of those emergent capabilities in order to accurately predict whatever the next token is going to be. Logical reasoning. So, inductive and deductive reasoning, basically triangulating principles from general observations or vice versa. Again, if you tell it, you know, analyze this with a specific framework, use inductive reasoning, use deductive reasoning, it will surprise you with what it's capable of doing. And finally, in-context learning. The fact that language models are able to use entirely novel information that are outside of the training distribution, and it can effectively and accurately use that information, is very similar to humans' ability to improvise. And so the fact that one of the emergent capabilities is this improvisational ability or in-context learning, means that these models are actually very, very intelligent. And then finally, hallucination equals creativity. A lot of people say, oh well it's not actually creative but it's hallucinating. I'm like, you can't have it both ways because these are cognitively the same exact behavior. There is functionally no difference between hallucination, which is making stuff up, and creativity, which is creating new things. The difference is whether or not you as the user recognize that hallucination is actually a superpower, that you cannot have creativity without the ability to imagine things that don't exist. This actually comes from human evolution, which is when when mentally modern humans started emerging, we started drawing things on cave walls that didn't exist, like hybrid animals and hybrid humans and other things. So my point here is that the emergent ability to be creative, to hallucinate, is actually required in order to achieve these things. And so you cannot separate hallucination from creativity. Now, what you can do is there's mechanisms that you can use in order to ground that. So, you can recognize it as this is a feature, not a bug. It is a cognitive behavior, but then it's a matter of labeling and keeping track of what's fictitious versus real. So, for instance, I was talking to some of my lawyer friends who like using Claude, or they experiment with Claude. I don't think they use it in their business. But you know, one thing that one thing that models do, and you've seen this in the news, particularly with law, is they'll just make up cases that don't exist. And that's like, oh, well, this is problematic. And I'm like, no, it's not. What it's doing is it's imagining if there was a case that did this, then it would prove this point. And so the way that human brains use this is that because there are mental disorders and brain injuries that basically make it so that you cannot discern your imagination from reality. But this is a necessary function because how do you know what to go look for? You imagine it first. You say, hey, wouldn't it be great if there was a legal case out there that proved this point? And so, rather than pathologizing and say, oh, the thing hallucinated this, you just didn't take it a far enough step to say, hey, let's go find a case like this one. We imagined a possibly useful case. So now let's go ground that in reality using case text to go search. It's also very context dependent. So basically, like, when is it good and useful to hallucinate and imagine and brainstorm? So, now that you know all these things, you are equipped with pretty much everything that I know from the last four years of prompt engineering and working with language models on a daily basis. So you are set to go forth and automate everything in the universe. Have a good one. Thanks.", "chunks": [{"timestamp": [0.0, 8.0], "text": " There are exactly three kinds of prompts that exist that encapsulate literally every other kind of prompt"}, {"timestamp": [8.0, 14.8], "text": " and just a couple of foundational principles that you need to know in order to master language models"}, {"timestamp": [14.8, 17.8], "text": " such as GPT and LLAMA and others."}, {"timestamp": [17.8, 22.6], "text": " Now a little bit of background about myself, I've been doing prompt engineering since GPT-2"}, {"timestamp": [22.6, 25.26], "text": " and of course now we are on GPT-4."}, {"timestamp": [25.26, 28.0], "text": " So the three fundamental operations you need"}, {"timestamp": [28.0, 30.72], "text": " start with reductive operations."}, {"timestamp": [30.72, 32.96], "text": " So what do I mean by reductive operations?"}, {"timestamp": [32.96, 36.72], "text": " A reductive operation is where you have a larger input"}, {"timestamp": [36.72, 38.42], "text": " than you do output."}, {"timestamp": [38.42, 41.16], "text": " So this is things like summarization."}, {"timestamp": [41.16, 44.42], "text": " Summarization is you say the same thing with fewer words."}, {"timestamp": [44.42, 46.0], "text": " There's a few formats that you can use."}, {"timestamp": [46.0, 49.0], "text": " You can use list, you can use notes, you can use executive summary."}, {"timestamp": [49.0, 51.0], "text": " Each of these have slightly different focuses."}, {"timestamp": [51.0, 56.0], "text": " So, for instance, an executive summary gives you just a high-level overview"}, {"timestamp": [56.0, 61.0], "text": " or the core assertion without actually trying to say the same thing over again."}, {"timestamp": [61.0, 67.02], "text": " Distillation is a very useful one, which is basically you're telling it to kind of purify"}, {"timestamp": [67.86, 72.04], "text": " the core underlying principle or fact."}, {"timestamp": [72.04, 74.28], "text": " This removes a lot of noise."}, {"timestamp": [74.28, 77.14], "text": " Extraction, so extraction is what a lot of people"}, {"timestamp": [77.14, 78.46], "text": " are really familiar with already"}, {"timestamp": [78.46, 80.54], "text": " because that comes from older NLP,"}, {"timestamp": [80.54, 84.06], "text": " such as answering questions, named entity extraction,"}, {"timestamp": [84.06, 86.24], "text": " getting dates and numbers, that sort of thing."}, {"timestamp": [86.24, 88.1], "text": " Characterizing, this is one that a lot of people"}, {"timestamp": [88.1, 89.0], "text": " don't know about."}, {"timestamp": [89.0, 91.86], "text": " You can have the language model either characterize"}, {"timestamp": [91.86, 96.04], "text": " the text or characterize the topic within the text."}, {"timestamp": [96.04, 98.48], "text": " And so what I mean by that is you can characterize text,"}, {"timestamp": [98.48, 102.2], "text": " you can say, you know, is this, does this look like fiction?"}, {"timestamp": [102.2, 104.8], "text": " Does it look like a scientific article?"}, {"timestamp": [104.8, 106.0], "text": " Does this look like code? Or then you can, if it look like a scientific article, does this look like code?"}, {"timestamp": [106.0, 112.0], "text": " Or, if it is code, for instance, you can characterize the code within that context."}, {"timestamp": [112.0, 119.0], "text": " You can analyze it. So basically you can do structural analysis, rhetorical analysis, that sort of thing."}, {"timestamp": [119.0, 127.08], "text": " There's evaluations. So evaluations are measuring, grading, or judging the content. So there's several kinds of evaluations."}, {"timestamp": [127.08, 132.48], "text": " So say for instance, you have a rubric and you say, hey, grade this, you know, grade"}, {"timestamp": [132.48, 135.72], "text": " this essay based on this rubric."}, {"timestamp": [135.72, 138.28], "text": " Plenty of people have discovered that out there."}, {"timestamp": [138.28, 140.24], "text": " You can also evaluate against moral frameworks."}, {"timestamp": [140.24, 145.0], "text": " So you can say, like, is this aligned with, you know, Plato's, you know, yada yada, or is this aligned with Plato's yada yada,"}, {"timestamp": [145.9, 149.7], "text": " or is this aligned with Kantian logic or whatever."}, {"timestamp": [149.7, 151.02], "text": " It knows those things."}, {"timestamp": [151.02, 152.58], "text": " And then finally critiquing."}, {"timestamp": [152.58, 155.4], "text": " So critiquing is providing critical feedback"}, {"timestamp": [155.4, 158.08], "text": " and recommendations to improve something."}, {"timestamp": [158.08, 160.02], "text": " That's reductive operations."}, {"timestamp": [160.02, 162.86], "text": " The second kind of operation is transformational operations."}, {"timestamp": [162.86, 165.24], "text": " So transformational operations,"}, {"timestamp": [165.24, 167.46], "text": " you basically, the input and output put"}, {"timestamp": [167.46, 169.08], "text": " are roughly the same size,"}, {"timestamp": [171.08, 173.34], "text": " and or roughly the same meaning."}, {"timestamp": [173.34, 176.6], "text": " So an example of a transformation operation is reformatting."}, {"timestamp": [176.6, 178.88], "text": " So this just changes the presentation."}, {"timestamp": [178.88, 180.68], "text": " You might say, hey, take this"}, {"timestamp": [180.68, 182.64], "text": " and turn it into bullet points."}, {"timestamp": [183.64, 185.48], "text": " Rewrite this prose as screenplay"}, {"timestamp": [185.48, 187.86], "text": " or rewrite this screenplay as prose."}, {"timestamp": [187.86, 191.1], "text": " Translate it from XML to JSON, that sort of thing."}, {"timestamp": [191.1, 193.84], "text": " Refactoring, so refactoring is something"}, {"timestamp": [193.84, 196.86], "text": " from the programming world where it just basically say,"}, {"timestamp": [196.86, 199.0], "text": " okay, take this chunk of code,"}, {"timestamp": [199.0, 200.82], "text": " write something that does the same exact thing"}, {"timestamp": [200.82, 203.74], "text": " but write it better or write it differently."}, {"timestamp": [203.74, 205.76], "text": " And you can also do the same thing with language."}, {"timestamp": [205.76, 210.96], "text": " So for instance, any kind of structured language or"}, {"timestamp": [210.96, 214.48], "text": " constructed language, you can say, hey, you know,"}, {"timestamp": [214.48, 217.48], "text": " basically say the same thing but in a different way."}, {"timestamp": [217.48, 219.84], "text": " And so like whether it's a legal document or a scientific"}, {"timestamp": [219.84, 223.44], "text": " document or whatever, you can refactor those as well."}, {"timestamp": [223.44, 228.08], "text": " Language change, so you can change between natural languages such as English and Portuguese."}, {"timestamp": [228.36, 232.76], "text": " You can also translate between coding languages like C++ to Python."}, {"timestamp": [233.04, 238.68], "text": " Now obviously I can hear people in the background screaming like C++ is compiled and Python is"}, {"timestamp": [238.68, 243.16], "text": " interpreted and there's all kinds of things that mean that you can't translate directly from one to the"}, {"timestamp": [243.16, 244.36], "text": " other. It's true."}, {"timestamp": [244.4, 245.28], "text": " But give it a try."}, {"timestamp": [245.28, 248.8], "text": " You'd be surprised how well modern language models can do."}, {"timestamp": [249.68, 251.96], "text": " So restructuring, so restructuring is different"}, {"timestamp": [251.96, 255.12], "text": " from reformatting, whereas restructuring says,"}, {"timestamp": [255.12, 257.62], "text": " okay, this is in the wrong order."}, {"timestamp": [257.62, 260.52], "text": " We need to change the order, we need to remove sections,"}, {"timestamp": [260.52, 262.32], "text": " we need to add sections."}, {"timestamp": [262.32, 264.36], "text": " You can optimize for logical flow,"}, {"timestamp": [264.36, 265.12], "text": " you can optimize for basically flow, you can optimize"}, {"timestamp": [265.12, 271.52], "text": " for basically any kind of priority that you want. So that's restructuring. Modification."}, {"timestamp": [271.52, 277.44], "text": " There's all kinds of modifications you can do, but basically you rewrite copy to achieve"}, {"timestamp": [277.44, 281.64], "text": " a slightly different intention. You can change the tone, you can change the formality, you"}, {"timestamp": [281.64, 286.06], "text": " can change the level of diplomacy, you can change the style."}, {"timestamp": [286.06, 288.0], "text": " Those are what I mean by modifications."}, {"timestamp": [288.0, 289.84], "text": " And then finally, clarification."}, {"timestamp": [289.84, 297.16], "text": " So clarification is where you say, hey, write this, but write it like more clear, right?"}, {"timestamp": [297.16, 302.2], "text": " Articulate it better and more clearly, which is something that is extremely useful when"}, {"timestamp": [302.2, 308.16], "text": " you're using it to edit, for instance, scientific copy, or if you're not a native English speaker, or native anything speaker, really."}, {"timestamp": [308.16, 312.48], "text": " If I was trying to write something in French, and I would, you know, like,"}, {"timestamp": [312.48, 316.32], "text": " my grasp of French is like, I've basically taken two weeks of French,"}, {"timestamp": [316.32, 321.6], "text": " so I'd be like, rewrite this garbage so that it's actually comprehensible to a French person."}, {"timestamp": [321.6, 325.12], "text": " That's an example of a transformational operation."}, {"timestamp": [325.12, 328.2], "text": " And then finally, generative operations or expansion"}, {"timestamp": [328.2, 330.2], "text": " operations or magnification operations."}, {"timestamp": [330.2, 334.0], "text": " In this case, the input is much smaller than the output."}, {"timestamp": [334.0, 335.88], "text": " And it may or may not have the same meaning,"}, {"timestamp": [335.88, 338.6], "text": " but the idea is that you go from small to big."}, {"timestamp": [338.6, 340.32], "text": " And so this is drafting."}, {"timestamp": [340.32, 343.08], "text": " So drafting is where you give it a set of instructions,"}, {"timestamp": [343.08, 346.14], "text": " and you have it draft some kind"}, {"timestamp": [346.14, 352.98], "text": " of document, whether that's a code file, fiction, legal document, knowledge base, scientific"}, {"timestamp": [352.98, 358.5], "text": " document, some kind of storytelling like telling a story about data or other facts."}, {"timestamp": [358.5, 359.5], "text": " Planning."}, {"timestamp": [359.5, 363.44], "text": " So planning is another thing that I don't see a lot of out there that it's actually"}, {"timestamp": [363.44, 367.72], "text": " really good at, which is given a set of parameters, come up with plans."}, {"timestamp": [367.72, 373.68], "text": " So you can have action plans, project plans, brainstorm objectives and missions."}, {"timestamp": [373.68, 381.64], "text": " You can have it elucidate constraints and also discuss the context within that planning session."}, {"timestamp": [381.64, 385.52], "text": " Brainstorming. So brainstorming is something that a lot of people are familiar with,"}, {"timestamp": [385.52, 387.16], "text": " especially with chat GPT,"}, {"timestamp": [387.16, 389.44], "text": " where you basically use your imagination"}, {"timestamp": [389.44, 391.2], "text": " to list out possibilities."}, {"timestamp": [391.2, 394.4], "text": " So this is either generating ideas,"}, {"timestamp": [394.4, 397.5], "text": " exploration of possibilities, problem solving."}, {"timestamp": [397.5, 399.2], "text": " Problem solving requires brainstorming"}, {"timestamp": [399.2, 400.76], "text": " and then hypothesizing,"}, {"timestamp": [400.76, 403.04], "text": " actually generating hypotheses."}, {"timestamp": [403.04, 404.68], "text": " And then finally amplification,"}, {"timestamp": [405.72, 408.56], "text": " which is similar to expanding something"}, {"timestamp": [409.68, 410.8], "text": " in the transformational one,"}, {"timestamp": [410.8, 412.48], "text": " but basically instead of just saying,"}, {"timestamp": [412.48, 414.32], "text": " explain this thing better, you say,"}, {"timestamp": [414.32, 418.68], "text": " riff on this and actually expand this entire topic"}, {"timestamp": [418.68, 419.8], "text": " to fully unpack it."}, {"timestamp": [419.8, 422.48], "text": " That's the keyword that I use there is unpacking,"}, {"timestamp": [422.48, 424.56], "text": " where it's like, hey, here's a topic"}, {"timestamp": [424.56, 427.52], "text": " that I wanna talk more about, unpack this."}, {"timestamp": [427.52, 428.84], "text": " So those are the three operations."}, {"timestamp": [428.84, 430.2], "text": " It's reductive, transformational,"}, {"timestamp": [430.2, 432.48], "text": " and generative or expansive."}, {"timestamp": [432.48, 434.4], "text": " Now, the other thing that you need to know"}, {"timestamp": [434.4, 437.24], "text": " about prompt engineering and language models"}, {"timestamp": [437.24, 439.88], "text": " is latency and emergence."}, {"timestamp": [439.88, 441.56], "text": " All right, so before we dive into this,"}, {"timestamp": [441.56, 444.46], "text": " I need to give you a really, really, really fast crash course"}, {"timestamp": [444.46, 446.14], "text": " on Bloom's Taxonomy."}, {"timestamp": [446.14, 449.68], "text": " So Bloom studied all kinds of stuff in education."}, {"timestamp": [449.68, 452.64], "text": " So if you look him up, did a lot of stuff"}, {"timestamp": [452.64, 455.18], "text": " with education like Bloom's Two Sigma Problem,"}, {"timestamp": [455.18, 457.76], "text": " but Bloom's Taxonomy, as someone who is a big fan"}, {"timestamp": [457.76, 462.76], "text": " of frameworks, this is really critical to understand"}, {"timestamp": [463.1, 466.44], "text": " how humans learn in order to really see what"}, {"timestamp": [466.44, 470.8], "text": " language models are capable of. So at the bottom layer you have remember, followed"}, {"timestamp": [470.8, 475.6], "text": " by understand, followed by apply, analyze, evaluate, and create. And so let's really"}, {"timestamp": [475.6, 479.4], "text": " quickly unpack these and you will see that language models have already"}, {"timestamp": [479.4, 487.82], "text": " attained most, if not all, of Bloom's Taxonomy. So remembering is just recalling facts and concepts."}, {"timestamp": [487.82, 491.0], "text": " Yes, it can regurgitate stuff, no problem."}, {"timestamp": [491.0, 493.38], "text": " Understanding, so understanding is defined"}, {"timestamp": [493.38, 495.98], "text": " as explaining the ideas and concepts,"}, {"timestamp": [495.98, 497.62], "text": " connecting the words to meanings."}, {"timestamp": [497.62, 500.38], "text": " Yes, language models can absolutely do that."}, {"timestamp": [500.38, 502.94], "text": " And so whenever I see someone in the comments say like,"}, {"timestamp": [502.94, 505.92], "text": " but language models don't truly understand anything. I'm like well"}, {"timestamp": [505.92, 507.92], "text": " Yes, they do by this definition"}, {"timestamp": [507.96, 512.04], "text": " But then if you want to dive into the epistemics and and philosophy of it"}, {"timestamp": [512.04, 516.48], "text": " Then I would argue that humans don't understand anything. Anyways, anyways going down a rabbit hole"}, {"timestamp": [517.24, 523.44], "text": " Applying using information in new situations. So this is the functional utility if language models couldn't apply"}, {"timestamp": [523.96, 526.5], "text": " Information they wouldn't be useful."}, {"timestamp": [526.5, 532.16], "text": " But they are useful, and so we see that they can apply in functional settings."}, {"timestamp": [532.16, 533.36], "text": " Analyzing."}, {"timestamp": [533.36, 535.4], "text": " So drawing connections among ideas."}, {"timestamp": [535.4, 539.5], "text": " If you don't believe me, go into ChatGPT and ask it to do a rhetorical analysis on your"}, {"timestamp": [539.5, 541.24], "text": " own emails or whatever."}, {"timestamp": [541.24, 544.36], "text": " It absolutely can analyze stuff."}, {"timestamp": [544.36, 548.0], "text": " Number or the next one, it's one, I didn't number these,"}, {"timestamp": [548.0, 552.0], "text": " evaluating. So justifying a decision or action. Now of course this is where"}, {"timestamp": [552.0, 556.0], "text": " chat GPT, you can really get it into a corner because it will definitely"}, {"timestamp": [556.0, 560.0], "text": " justify itself. It will explain and articulate and defend"}, {"timestamp": [560.0, 564.0], "text": " something, whether or not it's right, but again,"}, {"timestamp": [564.0, 565.96], "text": " doing it in one shot is not necessarily"}, {"timestamp": [565.96, 570.96], "text": " going to produce the best results, because if you have a person, if you back a person"}, {"timestamp": [570.96, 574.68], "text": " into a corner and ask them to justify themselves, well, people will often do the same things,"}, {"timestamp": [574.68, 576.92], "text": " especially pathological liars."}, {"timestamp": [576.92, 580.54], "text": " And then finally, creating, producing new or original work, generating something that"}, {"timestamp": [580.54, 582.2], "text": " did not previously exist."}, {"timestamp": [582.2, 586.0], "text": " Pretty much everything that a language model spits out is something that did not previously exist. Pretty much everything that a language model spits out is something that did not previously exist."}, {"timestamp": [586.0, 588.0], "text": " So by this"}, {"timestamp": [588.0, 590.0], "text": " definition, by this model,"}, {"timestamp": [590.0, 592.0], "text": " language models are already"}, {"timestamp": [592.0, 594.0], "text": " at the full stack of Bloom's taxonomy"}, {"timestamp": [594.0, 596.0], "text": " in terms of mental"}, {"timestamp": [596.0, 598.0], "text": " capabilities that humans are."}, {"timestamp": [598.0, 600.0], "text": " Now, you might argue, it's like, well,"}, {"timestamp": [600.0, 602.0], "text": " okay, it's not quite as smart as a human"}, {"timestamp": [602.0, 604.0], "text": " in some respects, and it makes"}, {"timestamp": [604.0, 605.72], "text": " kinds of mistakes. Okay, sure, whatever."}, {"timestamp": [605.72, 608.4], "text": " I'm not gonna argue about the details."}, {"timestamp": [608.4, 611.64], "text": " The point here is that Bloom's taxonomy is a good model"}, {"timestamp": [611.64, 613.72], "text": " for understanding what language models are capable of."}, {"timestamp": [613.72, 618.72], "text": " Okay, so latency, latent content, or the latent space."}, {"timestamp": [619.12, 621.66], "text": " This is the knowledge, facts, concepts, information,"}, {"timestamp": [621.66, 624.56], "text": " and other capabilities that are embedded in the model,"}, {"timestamp": [624.56, 631.54], "text": " but they must be activated by the correct prompting. It's like buried treasure, hence the graphic here."}, {"timestamp": [631.54, 636.32], "text": " So training data. The latent content only originates from the training data, which means"}, {"timestamp": [636.32, 641.24], "text": " that it's not going to magically know something that wasn't in the training data. Now that"}, {"timestamp": [641.24, 648.76], "text": " being said, it can connect dots and synthesize new information. This is what I mean by this, is creating, is if you"}, {"timestamp": [648.76, 652.4], "text": " activate the right set of patterns with the right words and prompts,"}, {"timestamp": [652.4, 657.24], "text": " the language model will be able to make novel connections. World knowledge, so"}, {"timestamp": [657.24, 661.24], "text": " general facts and understanding about the world, this is all inferred and"}, {"timestamp": [661.24, 665.74], "text": " imbibed from the training data, scientific information, cultural knowledge,"}, {"timestamp": [665.74, 667.84], "text": " historical knowledge, and languages."}, {"timestamp": [667.84, 669.36], "text": " All of this comes from the training data"}, {"timestamp": [669.36, 672.54], "text": " because whether or not you realize it,"}, {"timestamp": [672.54, 674.64], "text": " these language models are trained on many, many,"}, {"timestamp": [674.64, 677.0], "text": " many terabytes worth of internet data,"}, {"timestamp": [677.0, 681.84], "text": " which means through either explicit or implicit connections,"}, {"timestamp": [681.84, 684.4], "text": " it understands all of these things."}, {"timestamp": [684.4, 688.0], "text": " Emergent capabilities. So, the larger"}, {"timestamp": [688.0, 692.0], "text": " a language model is, the more emergent capabilities we're finding. And so"}, {"timestamp": [692.0, 696.0], "text": " several examples of these emergent capabilities are theory of mind."}, {"timestamp": [696.0, 700.0], "text": " The theory of mind is the ability to understand and keep track of the content of"}, {"timestamp": [700.0, 704.0], "text": " other people's minds. Basically, it's read enough Reddit to look at"}, {"timestamp": [704.0, 705.96], "text": " how people think"}, {"timestamp": [705.96, 711.36], "text": " to understand how humans think. Now, you might say this is highly controversial because it"}, {"timestamp": [711.36, 715.92], "text": " doesn't have mirror neurons and other kinds of stuff. I really don't care. Mathematically"}, {"timestamp": [715.92, 723.04], "text": " speaking, it has read enough human-generated content to be able to model a human's mind."}, {"timestamp": [723.04, 725.12], "text": " Implied cognition, the second one."}, {"timestamp": [725.12, 727.92], "text": " So basically thinking with the right prompting."}, {"timestamp": [727.92, 732.2], "text": " So what I mean by this is that in, because all that these language models are doing is"}, {"timestamp": [732.2, 736.04], "text": " they're trained to predict the next token."}, {"timestamp": [736.04, 741.48], "text": " In order to accurately predict the next token, it had to learn to think."}, {"timestamp": [741.48, 746.0], "text": " It's as simple as that is, okay, well, it's just a mathematical model to predict the next token."}, {"timestamp": [746.0, 752.0], "text": " Yes, but it's also a black box on the inside, and it's doing a lot of embedded operations"}, {"timestamp": [752.0, 760.0], "text": " and using some of those emergent capabilities in order to accurately predict whatever the next token is going to be."}, {"timestamp": [760.0, 768.0], "text": " Logical reasoning. So, inductive and deductive reasoning, basically triangulating principles from general observations or vice versa."}, {"timestamp": [768.0, 776.0], "text": " Again, if you tell it, you know, analyze this with a specific framework, use inductive reasoning, use deductive reasoning,"}, {"timestamp": [776.0, 780.0], "text": " it will surprise you with what it's capable of doing. And finally, in-context learning."}, {"timestamp": [780.0, 785.0], "text": " The fact that language models are able to use entirely novel information"}, {"timestamp": [785.0, 788.0], "text": " that are outside of the training distribution,"}, {"timestamp": [788.0, 792.0], "text": " and it can effectively and accurately use that information,"}, {"timestamp": [792.0, 796.0], "text": " is very similar to humans' ability to improvise."}, {"timestamp": [796.0, 801.0], "text": " And so the fact that one of the emergent capabilities is this improvisational ability"}, {"timestamp": [801.0, 806.0], "text": " or in-context learning, means that these models are actually very, very intelligent."}, {"timestamp": [806.0, 809.0], "text": " And then finally, hallucination equals creativity."}, {"timestamp": [809.0, 812.0], "text": " A lot of people say, oh well it's not actually creative"}, {"timestamp": [812.0, 815.0], "text": " but it's hallucinating. I'm like, you can't have it both ways"}, {"timestamp": [815.0, 818.0], "text": " because these are cognitively the same exact behavior."}, {"timestamp": [818.0, 821.0], "text": " There is functionally no difference"}, {"timestamp": [821.0, 824.0], "text": " between hallucination, which is making stuff up, and"}, {"timestamp": [824.0, 825.6], "text": " creativity, which is creating new things."}, {"timestamp": [825.92, 828.92], "text": " The difference is whether or not you as the user"}, {"timestamp": [829.72, 837.44], "text": " recognize that hallucination is actually a superpower, that you cannot have creativity without the ability to imagine things that don't exist."}, {"timestamp": [837.96, 840.28], "text": " This actually comes from human evolution, which is"}, {"timestamp": [840.84, 847.16], "text": " when when mentally modern humans started emerging, we started drawing things on cave walls that didn't exist,"}, {"timestamp": [847.16, 850.84], "text": " like hybrid animals and hybrid humans and other things."}, {"timestamp": [850.84, 855.4], "text": " So my point here is that the emergent ability"}, {"timestamp": [855.4, 857.52], "text": " to be creative, to hallucinate,"}, {"timestamp": [857.52, 860.98], "text": " is actually required in order to achieve these things."}, {"timestamp": [860.98, 864.88], "text": " And so you cannot separate hallucination from creativity."}, {"timestamp": [864.88, 870.6], "text": " Now, what you can do is there's mechanisms that you can use in order to ground that."}, {"timestamp": [870.6, 875.1], "text": " So, you can recognize it as this is a feature, not a bug."}, {"timestamp": [875.1, 880.8], "text": " It is a cognitive behavior, but then it's a matter of labeling and keeping track of what's fictitious versus real."}, {"timestamp": [880.8, 884.5], "text": " So, for instance, I was talking to some of my lawyer friends who like using Claude,"}, {"timestamp": [884.5, 888.76], "text": " or they experiment with Claude. I don't think they use it in their business. But you"}, {"timestamp": [888.76, 892.92], "text": " know, one thing that one thing that models do, and you've seen this in the news, particularly"}, {"timestamp": [892.92, 898.36], "text": " with law, is they'll just make up cases that don't exist. And that's like, oh, well, this"}, {"timestamp": [898.36, 902.52], "text": " is problematic. And I'm like, no, it's not. What it's doing is it's imagining if there"}, {"timestamp": [902.52, 905.0], "text": " was a case that did this, then it would prove this point."}, {"timestamp": [905.0, 912.0], "text": " And so the way that human brains use this is that because there are mental disorders and brain injuries"}, {"timestamp": [912.0, 918.0], "text": " that basically make it so that you cannot discern your imagination from reality."}, {"timestamp": [918.0, 925.52], "text": " But this is a necessary function because how do you know what to go look for? You imagine it first. You say,"}, {"timestamp": [925.52, 931.36], "text": " hey, wouldn't it be great if there was a legal case out there that proved this point? And so,"}, {"timestamp": [931.36, 937.28], "text": " rather than pathologizing and say, oh, the thing hallucinated this, you just didn't take it a far"}, {"timestamp": [937.28, 948.0], "text": " enough step to say, hey, let's go find a case like this one. We imagined a possibly useful case. So now let's go ground that in reality using case text to go"}, {"timestamp": [948.0, 952.0], "text": " search. It's also very context dependent."}, {"timestamp": [952.0, 956.0], "text": " So basically, like, when is it good and useful to"}, {"timestamp": [956.0, 960.0], "text": " hallucinate and imagine and brainstorm? So, now that you know all"}, {"timestamp": [960.0, 964.0], "text": " these things, you are equipped with pretty much everything"}, {"timestamp": [964.0, 965.04], "text": " that I know from the last"}, {"timestamp": [965.04, 970.6], "text": " four years of prompt engineering and working with language models on a daily basis. So"}, {"timestamp": [970.6, 977.04], "text": " you are set to go forth and automate everything in the universe. Have a good one. Thanks."}]}