{"text": " Hello everybody, David Shapiro here with another video. So today's video is about measuring machine autonomy rather than intelligence as a roadmap or set of milestones towards AGI. You know, for a long time I've been using the term autonomous cognitive entity, ACE, rather than AGI because general intelligence is one idea, but you know, general intelligence doesn't necessarily apply agency. And what we're realizing is that agency is actually very, very important to talk about and research, and it's not actually getting enough research because a lot of people say, oh, well, either it'll never happen or we shouldn't do it, but the thing is is people are doing it anyways, so we need to talk about it. Before we dive in, I just want to do a quick plug for my Patreon. I give away all my code for free, all my videos are ad-free, and that is because I am supported by a grassroots movement of support. So if you want to help keep the show alive, keep it going, and support me so that I can keep doing this work, I would prefer to do this than ever take a corporate job ever again. So jump over to Patreon all tiers get you access to the private discord server and then of course there's several higher tiers but really every little bit helps. Another quick update is the Gato community. So as a decentralized community we're right now one of the biggest things is we're developing an organizational roadmap. So basically we're setting up various milestones such as governance, community engagement, legal and financial milestones so that it can become fully autonomous and therefore not even dependent upon me. I had a good talk with some members of the community who were concerned that I'm going to be like the benevolent dictator for life, but like I am phobic of control. Like I actually don't want to control something. I want to create a system that is self-sustaining without me. I mean heck, that's what all my research does around AI. So I want to do the same thing with people, because if I can't do the same thing with people, then I sure as heck probably can't do the same thing with AI. So the goal is for Gato to ultimately be leaderless and operate by consensus as a DAO and that sort of stuff. We're working towards it. The doors are open for anyone to join, which we do have a steady trickle of people coming in. But yeah, so that's a quick update on Gato. And now back to the show. So I got this idea after talking with a few of my patrons who were saying like, well what's the roadmap towards AGI? And you know I've alluded to autonomy for quite a while, autonomous cognitive architectures, but I figured let me actually tell you guys where I got that idea. And the idea comes from levels of car autonomy. So the SAE, the international, was it the something of automotive engineers. Anyways, the SAE created the levels of car autonomy. So level zero, no driving all the way up to level five, full self-driving capability. To my knowledge, we haven't had anything get above level three yet because there are numerous problems around making executive decisions. And also there's a lot of sensory problems, like if you're driving in a whiteout blizzard, you know, if there's a fire, right? Because there's very little training data of like how to drive around a forest fire, for instance. Now, that being said, I do suspect it'll be solved eventually. Some people have recently started saying maybe you should integrate large language models into the executive function and I fully agree with that. You don't want to make all of the decisions, you want some things to be just completely robotically automated. So for instance, if you have a forward-looking radar and it detects that you're heading towards a non-moving object at 60 miles an hour, slam on the brakes regardless of whatever else is going on. There are a few things that you can do just fully automatically, but then for those higher order executive reasons, like say for instance, you hear that the occupant is screaming and gurgling and struggling to breathe, maybe the car should make a decision to go to the hospital instead of going to grandma's house. Not sure. Anyways, point being is that this is a very useful framework for kind of tracking our progress towards full self-driving cars. And I realized let's use the same thing for the path towards AGI or autonomous cognitive entities. So we need this roadmap, but there's a few problems. for the path towards AGI or autonomous cognitive entities. So we need this roadmap, but there's a few problems. So first of all, AGI means different things to different people. There's no consistent definition. A lot of people assume that it means that it has to be embodied or that it can do things that humans can't do or this, that, or the other. Also, a lot of conventional benchmarks just don't apply to artificial intelligence anymore. They're actually having to publish papers and research new benchmarks in order to measure large language models. The idea of, you know, oh well it'll be AGI once it has self-improvement, okay sure but we can already automate some of that anyways with reinforcement learning and that sort of thing. So it's like this is all really squishy. So basically how do you get from chat GPT to Skynet or something like that? Intelligence is not necessarily the best benchmark and the reason that I say that is because chat GPT or GPT-4 rather is already superhuman in a lot of respects. By any objective measure it's better at test-taking and a lot of other tasks than humans. That's also faster which means that med depending on how you measure its intelligence its IQ is like 145. Now that being said it does still make some really brain dead mistakes. Those are going to be solved if you especially if you look at the trend line modality. So another thing that a lot of people point out is that it's just text right. But text is the best kind of symbolic AI because you can literally represent pretty much anything with text. That being said, I've started to pivot and I believe that we're going to see another gigantic leap as we introduce more multimodal models. And the reason is because I got this idea when I was thinking about the fact that if you cross-train a language model on multiple languages, it gets better at all tasks. And that is because different languages have different strengths in terms of how they represent facts of the real world, and you also get broader ideas about how the world works that are embedded in language, because there are terms that just do not translate from one language to another. Likewise, I think that there's going to be some information that just does not translate from one language to another. Likewise, I think that there's gonna be some information that just does not translate from one modality to another, whether it's images, video text, spatial data, audio data, that sort of stuff. And so I think that by creating multimodal models, they're gonna have a much more nuanced understanding of everything that they're talking about. That being said, a multimodal model is still not gonna be enough, right? Necessary but not sufficient, because a model sitting on a shelf doesn't really matter. So the two primary ingredients that I see to machine autonomy, which is gonna be the best like proxy, the best benchmark is agency and dependency. And so what I mean by agency is the ability for an entity, a self-contained entity, to set goals and objectives, to task switch and implement cognitive control and pursue self-determination. Because agency or agentic behavior is the ability to just be fully self-directed and self-contained make independent decision decisions and that sort of thing. You know whenever you think of like an example of a of a robot right. You might think of the robots from iRobot where they don't really have that much agency they just kind of wait there like the physical embodiment of chat GPT until they're given an update and then they have a lot more agency. So agency is a multi-dimensional kind of proxy or benchmark for level of intelligence. And of course you can already give the reins over to like GPT-4 and stuff like that. That being said, there is a lot in the cognitive architecture that has to be figured out in order for agency to make more sense in the long run. So for instance agency implies that you remember what your purpose is and where you are and where you're going. And then dependency. So this is the other dimension and remember it's both of these. You need both of these ingredients. So basically dependency is how dependent on humans the machine is. So the more independent it is for all needs and the more decisions it can make, the better, the closer it is to full AGI. And so when I mean dependence is a need for human programming, need for hardware and physical infrastructure provided by humans, data, architecture, design patterns, and then finally solving problems and just keeping itself going and self-improving over the long run without human aid. So as agency goes up and as dependency goes down, that's how you know that we're going to be closer and closer to AGI. And we can we can easily measure those things right now because chat GPT, for instance, it has to run on gigantic data centers that are run entirely by humans, or mostly by humans rather. So these are the two primary ingredients that I think, and I basically built it into a framework very similar to the self-driving cars one. So level zero is reactive. Basically it has no agency. It's a tool. Level 1 is some autonomy, so it has a little bit of agency to make some executive decisions. Lang chain is a really good example of this where it it basically has the ability to choose between a set of tools, but that's about it. Still requires significant human oversight and is also still very very much dependent upon humans. Semi-autonomy is what a lot of people are working on with like auto GPT, where it can choose what kind of information it needs to go find, or it can also even start to rewrite some of its own code or come up with other ideas, some directives. Then high autonomy, as far as I know, has not been achieved yet anywhere in the world, which is basically that it is able to pick some of its own directives and more completely modify itself. Basically, if you were to have what baby AGI and auto GPT tried to be, which is they can rewrite their entire code base and change their own directives and are not dependent upon a whole heck of a lot of human infrastructure, that would be level three. And then level four is full autonomy, meaning they have absolutely no need for humans whatsoever. They're 100% self-determined in terms of what they do, when, where, and why, and how they do it. And then of course on a physical level, they will continue to exist in perpetuity without human intervention. All right, so, level 0, inert, reactive. Agency 0%, dependency 100%. Basically, it's a wrench. ChatGPT, as it is right now, Mid Journey and a whole bunch of other AI tools, they just sit there waiting for a human to push the button and they do their thing and then they switch back off. So these are level zero in terms of AGI score, even though they're intelligent. So again, like I said, intelligence is not necessarily a good measure of AGI. For something to be AGI or an autonomous cognitive entity it also needs agency and independence which chat GPT has none of. So even if you have GPT-5, right, that could be a billion times more intelligent than every human combined, if it doesn't have agency and independence it's not an AGI. So that's why I'm like AGI is not a good good measurement for some of these things. Level one, some autonomy. Like I said Lang chain is a really good example because it can pick and choose between a few options and not much else. A few other examples are like Roomba's, Amazon's warehouse robots, the Mars rovers, they have a little bit of autonomy but basically they mostly wait for a human command and then the human command says drive over there and it'll figure out how to get 10 feet that way on its own. Some advanced chatbots also have some autonomy. Again, anything that incorporates lang chain or similar very basic obfuscated choices, that's going to have some autonomy. Let's see, next is semi-autonomy. So semi-autonomous is where its directives still primarily come from humans, but it might be more of a mission rather than a directive or a rule. And so with a mission, the idea is that here's a general objective. You have some autonomy to figure out how to get there or how to do it on your own. And so this is the autonomous drones that militaries around the world are building where it's like, your mission is to destroy that SAM site or your mission is to get the passenger from A to B or in video games, the NPC's mission might be like, you're gonna try and capture the castle or whatever. So they still operate within a relatively constrained environment, meaning they can't change their own environment or their own fundamental operation. They still have a clearly defined, they're not general purpose, put it that way. Full self-driving car, no matter how intelligent it is, it's still just a car. A drone, no matter how intelligent it is, it's still just a car. A drone, no matter how intelligent it is, it's still just a drone. And ditto for a video game NPC. So this is kind of the midway point where anything above that, they're basically saying, okay, given these constraints and given this environment, you have free reign to do whatever it takes to get that job done. Whereas the some autonomy, they basically can only pick and choose from a very short menu of options, whereas semi-autonomy is they can figure it out themselves. Then high autonomy, so this is like Cortana in the early days, Commander Data, and the Nester Class 5 from iRobot. So they're almost entirely self-directing. Data can make up his own mind on things, but he still has a lot of limitations just due to the form factor that he's in. Likewise, Cortana, at least in the early days, is basically designed to be a weapon, a military aid, and so within those constraints, she still has a tremendous amount of autonomy and able to change the way she does things. But in both cases of Data and Cortana, they're still very much dependent on their human companions and human counterparts to continue operating. So most fictional examples of AI, at least many of the friendly ones, are what we would call high autonomy. And then full autonomy, so this is where they can completely ignore humans if they want to and are completely independent of humans. So the examples here are Skynet, Ultron, the Geth in Mass Effect, Cortana after Guardians, the Reapers from Mass Effect. So these are the kind of nightmare scenario where it's like, okay, it has no need for us anymore, so then what does it do? And that's what we're kind of most afraid of. But again, this is the work that I and other people are doing. And one, I think that this is inevitable, that it'll get to that level of level four full autonomy, but I'm also not afraid of it because I haven't seen any reason to be yet. Now that being said, I'm not saying that it is inevitable that it will be safe. No, we could absolutely do this wrong and it could kill everyone. I'm not denying that at all. And I think it's coming sooner than a lot of researchers realize. That being said, I do have a few more videos planned about, OK, if we're aiming for and building level 4 full autonomous AGI, how do we make it safe or what will it ultimately choose to do, which you've seen some of my other videos. How do we get from where we're at to level 4? Because like I said, the best that we have is we're approaching level two semi-autonomy in a few cases, right? People are experimenting with it, but there's a lot of problems. So all the work that I've done on cognitive architecture is gonna help get us there, but there's still a few other problems. So first is algorithmic breakthroughs need to happen. Namely, like I mentioned at the beginning, multimodal models I think will very, very much advance us towards that just because they're going to have a much more nuanced understanding of how to pursue any goal. They're going to have a much better world model by being able to integrate multiple kinds of information and data. Context size, parameter count, those kinds of things, MESA optimization, loss functions. That's all the math, which that's not to demean or diminish the value of mathematical researchers and the computer scientists and the data scientists who really build these new architectures, but it's kind of like Moore's law, where you can predict with a pretty regular cadence how models become more sophisticated over time. There doesn't seem to be any major blockers. If you pay attention to chip design, every year people are like, oh, well, this is going to be the end of Moore's law. But then inevitably someone figures out another way of approaching the problem. Likewise, I see the same thing, the same pattern happening with language models. And then another big thing that we're seeing is online learning, memory systems, and those sorts of things like recurrent neural networks and other ways of like managing in-context learning and that sort of stuff. But one thing that people have started noticing, for instance, is that ChatGPT, with even just over the last couple of days, or a couple of weeks, rather, because its data is two years old almost and growing, it's actually, its utility is already dropping because it's more and more out of date. And so we're realizing very quickly that you're gonna need to have continuous learning in these models so that they can stay relevant. And then there's the software architecture such as cognitive architectures, orchestrating and training millions of models. So one thing that I've started telling people is that AGI was never ever going to be a single model. It is a huge, gigantic, monumental mistake to think that one model, whether it's GPT-5 or GPT-18 or whatever, is going to be responsible for AGI. You're going to have, at a bare minimum, probably dozens if not hundreds or thousands of models required to achieve level 4 autonomy. These are models that are going to be doing things like handling vision, handling motor control, they're going to be performing task orchestration, you're going to have models that are dedicated to ethics and reasoning, long-term planning, and you're also going to have multiple models of every single kind that work in conjunction. This is called an ensemble of experts, which is an old school method of basically saying, okay, you have a dozen models that are similar, but they might be slightly different architectures, different training data, that sort of stuff. And so each one have strengths and weaknesses, and you get them all to work together, and then you overcome any flaws or faults in any single model. And so this is why I'm also really, really skeptical of any research that tries to align a single model. Like, that's kind of pointless. No, it's not pointless research. But it would be a mistake to think that aligning a single model is going to be the solution. Because any roboticist and old school ML data scientist will say, oh yeah, ensemble of experts. you know, this is very much the way. And also there's an entire book about it called A Thousand Brains by Jeff Hawkins. Yeah, so the software architecture to do all this in a fully automated way that is stable and self-sustaining that the AGI can tune and manipulate and spin up another copy of itself and test it. Self-testing and self-correction are going to be some of the hardest things to achieve with getting to level 4 full autonomy. So anyways, that's it for this video. It was pretty short. I just wanted to lay this out because I thought it was a really valuable idea to talk about like okay, how do we actually get to AGI from here. So I laid out five levels of autonomy based on agency and dependency. I hope this helps it make sense and kind of get a much clearer idea of what AGI or autonomous cognitive entities will actually look like. So thanks for watching.", "chunks": [{"timestamp": [0.0, 2.8], "text": " Hello everybody, David Shapiro here with another video."}, {"timestamp": [3.36, 11.04], "text": " So today's video is about measuring machine autonomy rather than intelligence as a roadmap"}, {"timestamp": [11.04, 17.76], "text": " or set of milestones towards AGI. You know, for a long time I've been using the term autonomous"}, {"timestamp": [17.76, 23.92], "text": " cognitive entity, ACE, rather than AGI because general intelligence is one idea, but you know,"}, {"timestamp": [24.56, 27.7], "text": " general intelligence doesn't necessarily apply agency."}, {"timestamp": [27.7, 29.86], "text": " And what we're realizing is that agency"}, {"timestamp": [29.86, 33.38], "text": " is actually very, very important to talk about"}, {"timestamp": [33.38, 36.3], "text": " and research, and it's not actually getting enough research"}, {"timestamp": [37.38, 38.42], "text": " because a lot of people say,"}, {"timestamp": [38.42, 41.02], "text": " oh, well, either it'll never happen or we shouldn't do it,"}, {"timestamp": [41.02, 43.14], "text": " but the thing is is people are doing it anyways,"}, {"timestamp": [43.14, 44.86], "text": " so we need to talk about it."}, {"timestamp": [44.86, 50.32], "text": " Before we dive in, I just want to do a quick plug for my Patreon. I give away all my code for free,"}, {"timestamp": [50.32, 57.6], "text": " all my videos are ad-free, and that is because I am supported by a grassroots movement of support."}, {"timestamp": [57.6, 63.12], "text": " So if you want to help keep the show alive, keep it going, and support me so that I can"}, {"timestamp": [63.12, 70.64], "text": " keep doing this work, I would prefer to do this than ever take a corporate job ever again. So jump over to Patreon all"}, {"timestamp": [70.64, 74.68], "text": " tiers get you access to the private discord server and then of course there's several"}, {"timestamp": [74.68, 81.04], "text": " higher tiers but really every little bit helps. Another quick update is the Gato community."}, {"timestamp": [81.04, 86.04], "text": " So as a decentralized community we're right now one of the biggest things"}, {"timestamp": [86.04, 91.04], "text": " is we're developing an organizational roadmap. So basically we're setting up various milestones"}, {"timestamp": [91.04, 96.76], "text": " such as governance, community engagement, legal and financial milestones so that it"}, {"timestamp": [96.76, 101.72], "text": " can become fully autonomous and therefore not even dependent upon me. I had a good talk"}, {"timestamp": [101.72, 109.44], "text": " with some members of the community who were concerned that I'm going to be like the benevolent dictator for life, but like I am phobic of control. Like I actually don't want"}, {"timestamp": [109.44, 114.32], "text": " to control something. I want to create a system that is self-sustaining without me. I mean heck,"}, {"timestamp": [114.32, 118.72], "text": " that's what all my research does around AI. So I want to do the same thing with people,"}, {"timestamp": [118.72, 122.24], "text": " because if I can't do the same thing with people, then I sure as heck probably can't do the same"}, {"timestamp": [122.24, 129.8], "text": " thing with AI. So the goal is for Gato to ultimately be leaderless and operate by consensus as a DAO and that"}, {"timestamp": [129.8, 130.8], "text": " sort of stuff."}, {"timestamp": [130.8, 133.2], "text": " We're working towards it."}, {"timestamp": [133.2, 136.92], "text": " The doors are open for anyone to join, which we do have a steady trickle of people coming"}, {"timestamp": [136.92, 137.92], "text": " in."}, {"timestamp": [137.92, 139.82], "text": " But yeah, so that's a quick update on Gato."}, {"timestamp": [139.82, 141.72], "text": " And now back to the show."}, {"timestamp": [141.72, 145.04], "text": " So I got this idea after talking with a few of my patrons"}, {"timestamp": [145.04, 149.16], "text": " who were saying like, well what's the roadmap towards AGI? And you know I've"}, {"timestamp": [149.16, 153.4], "text": " alluded to autonomy for quite a while, autonomous cognitive architectures,"}, {"timestamp": [153.4, 157.8], "text": " but I figured let me actually tell you guys where I got that idea. And the idea"}, {"timestamp": [157.8, 166.96], "text": " comes from levels of car autonomy. So the SAE, the international, was it the something of automotive engineers."}, {"timestamp": [166.96, 171.8], "text": " Anyways, the SAE created the levels of"}, {"timestamp": [172.44, 176.24], "text": " car autonomy. So level zero, no driving all the way up to level five,"}, {"timestamp": [176.52, 179.76], "text": " full self-driving capability. To my knowledge,"}, {"timestamp": [179.78, 184.72], "text": " we haven't had anything get above level three yet because"}, {"timestamp": [184.96, 187.4], "text": " there are numerous problems around"}, {"timestamp": [187.4, 189.76], "text": " making executive decisions."}, {"timestamp": [189.76, 194.76], "text": " And also there's a lot of sensory problems, like if you're driving in a whiteout blizzard,"}, {"timestamp": [194.76, 196.44], "text": " you know, if there's a fire, right?"}, {"timestamp": [196.44, 200.28], "text": " Because there's very little training data of like how to drive around a forest fire,"}, {"timestamp": [200.28, 201.68], "text": " for instance."}, {"timestamp": [201.68, 207.88], "text": " Now, that being said, I do suspect it'll be solved eventually. Some people have"}, {"timestamp": [207.88, 212.76], "text": " recently started saying maybe you should integrate large language models into the executive function"}, {"timestamp": [212.76, 216.28], "text": " and I fully agree with that. You don't want to make all of the decisions, you want some"}, {"timestamp": [216.28, 221.32], "text": " things to be just completely robotically automated. So for instance, if you have a forward-looking"}, {"timestamp": [221.32, 225.64], "text": " radar and it detects that you're heading towards a non-moving object"}, {"timestamp": [225.64, 229.88], "text": " at 60 miles an hour, slam on the brakes regardless of whatever else is going on."}, {"timestamp": [229.88, 234.84], "text": " There are a few things that you can do just fully automatically, but then for those higher"}, {"timestamp": [234.84, 239.92], "text": " order executive reasons, like say for instance, you hear that the occupant is screaming and"}, {"timestamp": [239.92, 244.24], "text": " gurgling and struggling to breathe, maybe the car should make a decision to go to the"}, {"timestamp": [244.24, 248.4], "text": " hospital instead of going to grandma's house."}, {"timestamp": [248.4, 249.24], "text": " Not sure."}, {"timestamp": [250.36, 254.36], "text": " Anyways, point being is that this is a very useful framework"}, {"timestamp": [254.36, 256.32], "text": " for kind of tracking our progress"}, {"timestamp": [256.32, 258.6], "text": " towards full self-driving cars."}, {"timestamp": [258.6, 259.96], "text": " And I realized let's use the same thing"}, {"timestamp": [259.96, 264.96], "text": " for the path towards AGI or autonomous cognitive entities."}, {"timestamp": [265.0, 265.84], "text": " So we need this roadmap, but there's a few problems. for the path towards AGI or autonomous cognitive entities."}, {"timestamp": [268.92, 271.22], "text": " So we need this roadmap, but there's a few problems. So first of all, AGI means different things"}, {"timestamp": [271.22, 273.04], "text": " to different people."}, {"timestamp": [273.04, 275.2], "text": " There's no consistent definition."}, {"timestamp": [275.2, 276.84], "text": " A lot of people assume that it means"}, {"timestamp": [276.84, 279.28], "text": " that it has to be embodied or that it can do things"}, {"timestamp": [279.28, 282.22], "text": " that humans can't do or this, that, or the other."}, {"timestamp": [282.22, 284.02], "text": " Also, a lot of conventional benchmarks"}, {"timestamp": [284.02, 288.96], "text": " just don't apply to artificial intelligence anymore. They're actually having to publish papers and research"}, {"timestamp": [288.96, 295.44], "text": " new benchmarks in order to measure large language models. The idea of, you know, oh well it'll"}, {"timestamp": [295.44, 301.24], "text": " be AGI once it has self-improvement, okay sure but we can already automate some of that"}, {"timestamp": [301.24, 308.36], "text": " anyways with reinforcement learning and that sort of thing. So it's like this is all really squishy. So basically how do you get from"}, {"timestamp": [308.36, 313.6], "text": " chat GPT to Skynet or something like that? Intelligence is not necessarily the"}, {"timestamp": [313.6, 317.88], "text": " best benchmark and the reason that I say that is because chat GPT or GPT-4 rather"}, {"timestamp": [317.88, 322.84], "text": " is already superhuman in a lot of respects. By any objective measure it's"}, {"timestamp": [322.84, 325.46], "text": " better at test-taking and a lot of other tasks than humans."}, {"timestamp": [325.64, 327.68], "text": " That's also faster which means that"}, {"timestamp": [327.68, 329.72], "text": " med depending on how you measure its"}, {"timestamp": [329.72, 332.12], "text": " intelligence its IQ is like 145."}, {"timestamp": [332.92, 335.06], "text": " Now that being said it does still make some really"}, {"timestamp": [335.06, 336.08], "text": " brain dead mistakes."}, {"timestamp": [336.52, 338.44], "text": " Those are going to be solved if you especially if you look"}, {"timestamp": [338.44, 340.86], "text": " at the trend line modality."}, {"timestamp": [340.88, 342.92], "text": " So another thing that a lot of people point out is"}, {"timestamp": [342.92, 344.72], "text": " that it's just text right."}, {"timestamp": [344.76, 349.8], "text": " But text is the best kind of symbolic AI because you can literally represent pretty much anything"}, {"timestamp": [349.8, 350.84], "text": " with text."}, {"timestamp": [350.84, 354.4], "text": " That being said, I've started to pivot and I believe that we're going to see another"}, {"timestamp": [354.4, 358.84], "text": " gigantic leap as we introduce more multimodal models."}, {"timestamp": [358.84, 362.62], "text": " And the reason is because I got this idea when I was thinking about the fact that if"}, {"timestamp": [362.62, 366.92], "text": " you cross-train a language model on multiple languages, it gets better at all tasks."}, {"timestamp": [366.92, 372.16], "text": " And that is because different languages have different strengths in terms of how they represent"}, {"timestamp": [372.16, 377.1], "text": " facts of the real world, and you also get broader ideas about how the world works that"}, {"timestamp": [377.1, 381.52], "text": " are embedded in language, because there are terms that just do not translate from one"}, {"timestamp": [381.52, 383.04], "text": " language to another."}, {"timestamp": [383.04, 385.96], "text": " Likewise, I think that there's going to be some information that just does not translate from one language to another. Likewise, I think that there's gonna be some information"}, {"timestamp": [385.96, 389.08], "text": " that just does not translate from one modality to another,"}, {"timestamp": [389.08, 394.08], "text": " whether it's images, video text, spatial data,"}, {"timestamp": [394.2, 395.88], "text": " audio data, that sort of stuff."}, {"timestamp": [395.88, 398.28], "text": " And so I think that by creating multimodal models,"}, {"timestamp": [398.28, 400.96], "text": " they're gonna have a much more nuanced understanding"}, {"timestamp": [400.96, 403.56], "text": " of everything that they're talking about."}, {"timestamp": [403.56, 405.3], "text": " That being said, a multimodal model"}, {"timestamp": [405.3, 407.16], "text": " is still not gonna be enough, right?"}, {"timestamp": [407.16, 408.52], "text": " Necessary but not sufficient,"}, {"timestamp": [408.52, 411.58], "text": " because a model sitting on a shelf doesn't really matter."}, {"timestamp": [413.1, 416.4], "text": " So the two primary ingredients"}, {"timestamp": [416.4, 418.18], "text": " that I see to machine autonomy,"}, {"timestamp": [418.18, 420.92], "text": " which is gonna be the best like proxy,"}, {"timestamp": [420.92, 424.4], "text": " the best benchmark is agency and dependency."}, {"timestamp": [424.4, 429.8], "text": " And so what I mean by agency is the ability for an entity, a self-contained entity, to"}, {"timestamp": [429.8, 438.56], "text": " set goals and objectives, to task switch and implement cognitive control and pursue self-determination."}, {"timestamp": [438.56, 443.76], "text": " Because agency or agentic behavior is the ability to just be fully self-directed and"}, {"timestamp": [443.76, 446.5], "text": " self-contained make independent decision decisions"}, {"timestamp": [446.7, 447.4], "text": " and that sort of thing."}, {"timestamp": [448.4, 450.8], "text": " You know whenever you think of like an example of a"}, {"timestamp": [451.0, 452.7], "text": " of a robot right."}, {"timestamp": [452.9, 455.6], "text": " You might think of the robots from iRobot"}, {"timestamp": [455.8, 457.7], "text": " where they don't really have that much agency they just kind"}, {"timestamp": [457.9, 460.4], "text": " of wait there like the physical embodiment of chat GPT"}, {"timestamp": [461.2, 463.1], "text": " until they're given an update"}, {"timestamp": [463.3, 464.7], "text": " and then they have a lot more agency."}, {"timestamp": [465.0, 473.0], "text": " So agency is a multi-dimensional kind of proxy or benchmark for level of intelligence."}, {"timestamp": [473.0, 479.0], "text": " And of course you can already give the reins over to like GPT-4 and stuff like that."}, {"timestamp": [479.0, 485.28], "text": " That being said, there is a lot in the cognitive architecture that has to be figured out in"}, {"timestamp": [485.28, 489.0], "text": " order for agency to make more sense in the long run."}, {"timestamp": [489.0, 493.52], "text": " So for instance agency implies that you remember what your purpose is and where you are and"}, {"timestamp": [493.52, 494.92], "text": " where you're going."}, {"timestamp": [494.92, 496.3], "text": " And then dependency."}, {"timestamp": [496.3, 498.76], "text": " So this is the other dimension and remember it's both of these."}, {"timestamp": [498.76, 500.72], "text": " You need both of these ingredients."}, {"timestamp": [500.72, 506.68], "text": " So basically dependency is how dependent on humans the machine is."}, {"timestamp": [506.68, 512.32], "text": " So the more independent it is for all needs and the more decisions it can make, the better,"}, {"timestamp": [512.32, 514.12], "text": " the closer it is to full AGI."}, {"timestamp": [514.12, 519.34], "text": " And so when I mean dependence is a need for human programming, need for hardware and physical"}, {"timestamp": [519.34, 526.64], "text": " infrastructure provided by humans, data, architecture, design patterns, and then finally solving problems and"}, {"timestamp": [526.64, 534.48], "text": " just keeping itself going and self-improving over the long run without human aid. So as agency goes"}, {"timestamp": [534.48, 539.68], "text": " up and as dependency goes down, that's how you know that we're going to be closer and closer to"}, {"timestamp": [539.68, 545.44], "text": " AGI. And we can we can easily measure those things right now because chat GPT, for instance, it has"}, {"timestamp": [545.44, 551.66], "text": " to run on gigantic data centers that are run entirely by humans, or mostly by humans rather."}, {"timestamp": [551.66, 558.96], "text": " So these are the two primary ingredients that I think, and I basically built it into a framework"}, {"timestamp": [558.96, 561.62], "text": " very similar to the self-driving cars one."}, {"timestamp": [561.62, 563.76], "text": " So level zero is reactive."}, {"timestamp": [563.76, 564.96], "text": " Basically it has no agency."}, {"timestamp": [564.96, 565.32], "text": " It's a tool."}, {"timestamp": [565.32, 569.48], "text": " Level 1 is some autonomy, so it has a little bit of agency to make some"}, {"timestamp": [569.48, 573.08], "text": " executive decisions. Lang chain is a really good example of this where it"}, {"timestamp": [573.08, 577.28], "text": " it basically has the ability to choose between a set of tools, but that's about"}, {"timestamp": [577.28, 582.52], "text": " it. Still requires significant human oversight and is also still very"}, {"timestamp": [582.52, 585.66], "text": " very much dependent upon humans."}, {"timestamp": [585.66, 589.7], "text": " Semi-autonomy is what a lot of people are working on with like auto GPT, where it can"}, {"timestamp": [589.7, 595.76], "text": " choose what kind of information it needs to go find, or it can also even start to rewrite"}, {"timestamp": [595.76, 602.08], "text": " some of its own code or come up with other ideas, some directives."}, {"timestamp": [602.08, 605.76], "text": " Then high autonomy, as far as I know, has not been achieved yet"}, {"timestamp": [605.76, 610.04], "text": " anywhere in the world, which is basically that"}, {"timestamp": [610.04, 613.96], "text": " it is able to pick some of its own directives"}, {"timestamp": [613.96, 618.2], "text": " and more completely modify itself."}, {"timestamp": [618.2, 621.48], "text": " Basically, if you were to have what baby AGI"}, {"timestamp": [621.48, 623.68], "text": " and auto GPT tried to be,"}, {"timestamp": [623.68, 626.0], "text": " which is they can rewrite their entire code base"}, {"timestamp": [626.0, 628.3], "text": " and change their own directives"}, {"timestamp": [628.3, 631.26], "text": " and are not dependent upon a whole heck of a lot"}, {"timestamp": [631.26, 633.66], "text": " of human infrastructure, that would be level three."}, {"timestamp": [633.66, 636.26], "text": " And then level four is full autonomy,"}, {"timestamp": [636.26, 641.1], "text": " meaning they have absolutely no need for humans whatsoever."}, {"timestamp": [641.1, 644.32], "text": " They're 100% self-determined in terms of what they do,"}, {"timestamp": [644.32, 646.32], "text": " when, where, and why, and how they do it."}, {"timestamp": [646.32, 648.32], "text": " And then of course on a physical level,"}, {"timestamp": [649.54, 653.42], "text": " they will continue to exist in perpetuity without human intervention."}, {"timestamp": [653.94, 655.7], "text": " All right, so,"}, {"timestamp": [655.7, 662.5], "text": " level 0, inert, reactive. Agency 0%, dependency 100%. Basically, it's a wrench."}, {"timestamp": [663.26, 665.96], "text": " ChatGPT, as it is right now, Mid Journey and a"}, {"timestamp": [665.96, 669.92], "text": " whole bunch of other AI tools, they just sit there waiting for a human to push"}, {"timestamp": [669.92, 673.4], "text": " the button and they do their thing and then they switch back off. So these are"}, {"timestamp": [673.4, 678.72], "text": " level zero in terms of AGI score, even though they're intelligent. So again, like"}, {"timestamp": [678.72, 683.16], "text": " I said, intelligence is not necessarily a good measure of AGI. For something to be"}, {"timestamp": [683.16, 689.4], "text": " AGI or an autonomous cognitive entity it also needs agency and independence which chat GPT has"}, {"timestamp": [689.4, 695.28], "text": " none of. So even if you have GPT-5, right, that could be a billion times"}, {"timestamp": [695.28, 699.16], "text": " more intelligent than every human combined, if it doesn't have agency and"}, {"timestamp": [699.16, 703.8], "text": " independence it's not an AGI. So that's why I'm like AGI is"}, {"timestamp": [703.8, 708.94], "text": " not a good good measurement for some of these things. Level one, some autonomy. Like I said"}, {"timestamp": [708.94, 712.36], "text": " Lang chain is a really good example because it can pick and choose between a"}, {"timestamp": [712.36, 717.36], "text": " few options and not much else. A few other examples are like Roomba's, Amazon's"}, {"timestamp": [717.36, 721.06], "text": " warehouse robots, the Mars rovers, they have a little bit of autonomy but"}, {"timestamp": [721.06, 724.54], "text": " basically they mostly wait for a human command and then the human command says"}, {"timestamp": [724.54, 728.68], "text": " drive over there and it'll figure out how to get 10 feet that way"}, {"timestamp": [728.68, 730.52], "text": " on its own."}, {"timestamp": [730.52, 733.12], "text": " Some advanced chatbots also have some autonomy."}, {"timestamp": [733.12, 742.28], "text": " Again, anything that incorporates lang chain or similar very basic obfuscated choices,"}, {"timestamp": [742.28, 745.4], "text": " that's going to have some autonomy."}, {"timestamp": [745.4, 747.56], "text": " Let's see, next is semi-autonomy."}, {"timestamp": [747.56, 753.76], "text": " So semi-autonomous is where its directives still primarily come from humans, but it might"}, {"timestamp": [753.76, 757.76], "text": " be more of a mission rather than a directive or a rule."}, {"timestamp": [757.76, 762.96], "text": " And so with a mission, the idea is that here's a general objective."}, {"timestamp": [762.96, 765.64], "text": " You have some autonomy to figure out how to get there"}, {"timestamp": [765.64, 767.36], "text": " or how to do it on your own."}, {"timestamp": [767.36, 769.24], "text": " And so this is the autonomous drones"}, {"timestamp": [769.24, 771.44], "text": " that militaries around the world are building"}, {"timestamp": [771.44, 774.92], "text": " where it's like, your mission is to destroy that SAM site"}, {"timestamp": [774.92, 778.06], "text": " or your mission is to get the passenger from A to B"}, {"timestamp": [778.06, 783.06], "text": " or in video games, the NPC's mission might be like,"}, {"timestamp": [783.44, 785.82], "text": " you're gonna try and capture the"}, {"timestamp": [785.82, 787.28], "text": " castle or whatever."}, {"timestamp": [787.28, 792.24], "text": " So they still operate within a relatively constrained environment, meaning they can't"}, {"timestamp": [792.24, 795.7], "text": " change their own environment or their own fundamental operation."}, {"timestamp": [795.7, 802.28], "text": " They still have a clearly defined, they're not general purpose, put it that way."}, {"timestamp": [802.28, 805.92], "text": " Full self-driving car, no matter how intelligent it is, it's still just a car. A drone, no matter how intelligent it is, it's still just a car."}, {"timestamp": [805.92, 809.4], "text": " A drone, no matter how intelligent it is, it's still just a drone."}, {"timestamp": [809.4, 811.4], "text": " And ditto for a video game NPC."}, {"timestamp": [811.4, 816.36], "text": " So this is kind of the midway point where anything above that, they're basically saying,"}, {"timestamp": [816.36, 820.66], "text": " okay, given these constraints and given this environment, you have free reign to do whatever"}, {"timestamp": [820.66, 823.16], "text": " it takes to get that job done."}, {"timestamp": [823.16, 827.96], "text": " Whereas the some autonomy, they basically can only pick and choose from a very short"}, {"timestamp": [827.96, 833.36], "text": " menu of options, whereas semi-autonomy is they can figure it out themselves."}, {"timestamp": [833.36, 840.12], "text": " Then high autonomy, so this is like Cortana in the early days, Commander Data, and the"}, {"timestamp": [840.12, 842.34], "text": " Nester Class 5 from iRobot."}, {"timestamp": [842.34, 847.24], "text": " So they're almost entirely self-directing."}, {"timestamp": [847.24, 849.36], "text": " Data can make up his own mind on things,"}, {"timestamp": [849.36, 850.96], "text": " but he still has a lot of limitations"}, {"timestamp": [850.96, 853.72], "text": " just due to the form factor that he's in."}, {"timestamp": [853.72, 857.0], "text": " Likewise, Cortana, at least in the early days,"}, {"timestamp": [857.0, 859.92], "text": " is basically designed to be a weapon, a military aid,"}, {"timestamp": [859.92, 861.8], "text": " and so within those constraints,"}, {"timestamp": [861.8, 864.38], "text": " she still has a tremendous amount of autonomy"}, {"timestamp": [864.38, 867.72], "text": " and able to change the way she does things."}, {"timestamp": [867.72, 872.58], "text": " But in both cases of Data and Cortana, they're still very much dependent on their human companions"}, {"timestamp": [872.58, 875.28], "text": " and human counterparts to continue operating."}, {"timestamp": [875.28, 882.04], "text": " So most fictional examples of AI, at least many of the friendly ones, are what we would"}, {"timestamp": [882.04, 890.64], "text": " call high autonomy. And then full autonomy, so this is where they can completely ignore humans if they want"}, {"timestamp": [890.64, 892.78], "text": " to and are completely independent of humans."}, {"timestamp": [892.78, 900.08], "text": " So the examples here are Skynet, Ultron, the Geth in Mass Effect, Cortana after Guardians,"}, {"timestamp": [900.08, 901.34], "text": " the Reapers from Mass Effect."}, {"timestamp": [901.34, 905.0], "text": " So these are the kind of nightmare scenario where it's like,"}, {"timestamp": [905.0, 907.62], "text": " okay, it has no need for us anymore,"}, {"timestamp": [907.62, 909.68], "text": " so then what does it do?"}, {"timestamp": [909.68, 912.42], "text": " And that's what we're kind of most afraid of."}, {"timestamp": [912.42, 915.2], "text": " But again, this is the work that I"}, {"timestamp": [915.2, 916.34], "text": " and other people are doing."}, {"timestamp": [916.34, 919.16], "text": " And one, I think that this is inevitable,"}, {"timestamp": [919.16, 923.28], "text": " that it'll get to that level of level four full autonomy,"}, {"timestamp": [923.28, 927.32], "text": " but I'm also not afraid of it because I haven't seen any"}, {"timestamp": [927.32, 928.68], "text": " reason to be yet."}, {"timestamp": [928.68, 930.96], "text": " Now that being said, I'm not saying that it is inevitable"}, {"timestamp": [930.96, 931.96], "text": " that it will be safe."}, {"timestamp": [931.96, 934.12], "text": " No, we could absolutely do this wrong and it could kill"}, {"timestamp": [934.12, 934.88], "text": " everyone."}, {"timestamp": [934.88, 937.2], "text": " I'm not denying that at all."}, {"timestamp": [937.2, 940.08], "text": " And I think it's coming sooner than a lot of researchers"}, {"timestamp": [940.08, 941.52], "text": " realize."}, {"timestamp": [941.52, 944.56], "text": " That being said, I do have a few more videos planned about,"}, {"timestamp": [944.56, 946.28], "text": " OK, if we're"}, {"timestamp": [946.28, 952.64], "text": " aiming for and building level 4 full autonomous AGI, how do we make it safe or what will it"}, {"timestamp": [952.64, 957.84], "text": " ultimately choose to do, which you've seen some of my other videos."}, {"timestamp": [957.84, 966.88], "text": " How do we get from where we're at to level 4? Because like I said, the best that we have is we're approaching level two semi-autonomy"}, {"timestamp": [966.88, 969.26], "text": " in a few cases, right?"}, {"timestamp": [969.26, 971.68], "text": " People are experimenting with it,"}, {"timestamp": [971.68, 974.6], "text": " but there's a lot of problems."}, {"timestamp": [974.6, 977.44], "text": " So all the work that I've done on cognitive architecture"}, {"timestamp": [977.44, 978.58], "text": " is gonna help get us there,"}, {"timestamp": [978.58, 980.68], "text": " but there's still a few other problems."}, {"timestamp": [980.68, 984.48], "text": " So first is algorithmic breakthroughs need to happen."}, {"timestamp": [984.48, 986.0], "text": " Namely, like I mentioned at the beginning,"}, {"timestamp": [986.0, 990.0], "text": " multimodal models I think will very, very much advance us towards that"}, {"timestamp": [990.0, 993.0], "text": " just because they're going to have a much more nuanced understanding"}, {"timestamp": [993.0, 995.0], "text": " of how to pursue any goal."}, {"timestamp": [995.0, 997.0], "text": " They're going to have a much better world model"}, {"timestamp": [997.0, 1000.0], "text": " by being able to integrate multiple kinds of information and data."}, {"timestamp": [1001.0, 1004.0], "text": " Context size, parameter count, those kinds of things,"}, {"timestamp": [1004.0, 1008.04], "text": " MESA optimization, loss functions."}, {"timestamp": [1008.04, 1015.16], "text": " That's all the math, which that's not to demean or diminish the value of mathematical researchers"}, {"timestamp": [1015.16, 1020.04], "text": " and the computer scientists and the data scientists who really build these new architectures,"}, {"timestamp": [1020.04, 1026.92], "text": " but it's kind of like Moore's law, where you can predict with a pretty regular cadence"}, {"timestamp": [1026.92, 1030.78], "text": " how models become more sophisticated over time."}, {"timestamp": [1030.78, 1034.22], "text": " There doesn't seem to be any major blockers."}, {"timestamp": [1034.22, 1038.12], "text": " If you pay attention to chip design, every year people are like, oh, well, this is going"}, {"timestamp": [1038.12, 1039.88], "text": " to be the end of Moore's law."}, {"timestamp": [1039.88, 1045.52], "text": " But then inevitably someone figures out another way of approaching the problem. Likewise, I see the same thing,"}, {"timestamp": [1045.52, 1050.3], "text": " the same pattern happening with language models."}, {"timestamp": [1050.3, 1052.96], "text": " And then another big thing that we're seeing"}, {"timestamp": [1052.96, 1056.1], "text": " is online learning, memory systems,"}, {"timestamp": [1056.1, 1058.82], "text": " and those sorts of things like recurrent neural networks"}, {"timestamp": [1058.82, 1063.12], "text": " and other ways of like managing in-context learning"}, {"timestamp": [1063.12, 1064.2], "text": " and that sort of stuff."}, {"timestamp": [1064.2, 1065.66], "text": " But one thing that people have started noticing,"}, {"timestamp": [1065.66, 1067.96], "text": " for instance, is that ChatGPT,"}, {"timestamp": [1067.96, 1071.0], "text": " with even just over the last couple of days,"}, {"timestamp": [1071.0, 1073.32], "text": " or a couple of weeks, rather,"}, {"timestamp": [1073.32, 1078.04], "text": " because its data is two years old almost and growing,"}, {"timestamp": [1078.04, 1081.28], "text": " it's actually, its utility is already dropping"}, {"timestamp": [1081.28, 1083.2], "text": " because it's more and more out of date."}, {"timestamp": [1083.2, 1085.26], "text": " And so we're realizing very quickly"}, {"timestamp": [1085.26, 1087.72], "text": " that you're gonna need to have continuous learning"}, {"timestamp": [1087.72, 1090.16], "text": " in these models so that they can stay relevant."}, {"timestamp": [1090.16, 1092.82], "text": " And then there's the software architecture"}, {"timestamp": [1092.82, 1094.78], "text": " such as cognitive architectures,"}, {"timestamp": [1094.78, 1097.24], "text": " orchestrating and training millions of models."}, {"timestamp": [1097.24, 1098.88], "text": " So one thing that I've started telling people"}, {"timestamp": [1098.88, 1102.4], "text": " is that AGI was never ever going to be a single model."}, {"timestamp": [1102.4, 1106.3], "text": " It is a huge, gigantic, monumental mistake to"}, {"timestamp": [1106.3, 1112.3], "text": " think that one model, whether it's GPT-5 or GPT-18 or whatever, is going to be responsible"}, {"timestamp": [1112.3, 1118.38], "text": " for AGI. You're going to have, at a bare minimum, probably dozens if not hundreds or thousands"}, {"timestamp": [1118.38, 1123.7], "text": " of models required to achieve level 4 autonomy. These are models that are going to be doing"}, {"timestamp": [1123.7, 1128.48], "text": " things like handling vision, handling motor control, they're going to be performing"}, {"timestamp": [1128.48, 1132.72], "text": " task orchestration, you're going to have models that are dedicated to ethics and"}, {"timestamp": [1132.72, 1137.44], "text": " reasoning, long-term planning, and you're also going to have multiple models of"}, {"timestamp": [1137.44, 1141.72], "text": " every single kind that work in conjunction. This is called an ensemble"}, {"timestamp": [1141.72, 1145.72], "text": " of experts, which is an old school method of"}, {"timestamp": [1145.72, 1151.12], "text": " basically saying, okay, you have a dozen models that are similar, but they might be slightly"}, {"timestamp": [1151.12, 1154.86], "text": " different architectures, different training data, that sort of stuff. And so each one"}, {"timestamp": [1154.86, 1158.96], "text": " have strengths and weaknesses, and you get them all to work together, and then you overcome"}, {"timestamp": [1158.96, 1164.48], "text": " any flaws or faults in any single model. And so this is why I'm also really, really skeptical"}, {"timestamp": [1164.48, 1167.24], "text": " of any research that tries to align a single model."}, {"timestamp": [1167.24, 1169.12], "text": " Like, that's kind of pointless."}, {"timestamp": [1169.12, 1170.76], "text": " No, it's not pointless research."}, {"timestamp": [1170.76, 1172.76], "text": " But it would be a mistake to think that aligning"}, {"timestamp": [1172.76, 1175.68], "text": " a single model is going to be the solution."}, {"timestamp": [1175.68, 1182.0], "text": " Because any roboticist and old school ML data scientist"}, {"timestamp": [1182.0, 1186.6], "text": " will say, oh yeah, ensemble of experts. you know, this is very much the way."}, {"timestamp": [1186.6, 1188.2], "text": " And also there's an entire book about it"}, {"timestamp": [1188.2, 1191.04], "text": " called A Thousand Brains by Jeff Hawkins."}, {"timestamp": [1191.04, 1194.06], "text": " Yeah, so the software architecture to do all this"}, {"timestamp": [1194.06, 1199.06], "text": " in a fully automated way that is stable and self-sustaining"}, {"timestamp": [1199.18, 1201.86], "text": " that the AGI can tune and manipulate"}, {"timestamp": [1201.86, 1204.6], "text": " and spin up another copy of itself and test it."}, {"timestamp": [1204.6, 1205.64], "text": " Self-testing and"}, {"timestamp": [1205.64, 1212.6], "text": " self-correction are going to be some of the hardest things to achieve with getting to"}, {"timestamp": [1212.6, 1214.78], "text": " level 4 full autonomy."}, {"timestamp": [1214.78, 1216.76], "text": " So anyways, that's it for this video."}, {"timestamp": [1216.76, 1217.76], "text": " It was pretty short."}, {"timestamp": [1217.76, 1222.16], "text": " I just wanted to lay this out because I thought it was a really valuable idea to talk about"}, {"timestamp": [1222.16, 1225.0], "text": " like okay, how do we actually get to AGI from here."}, {"timestamp": [1225.0, 1230.56], "text": " So I laid out five levels of autonomy based on agency and dependency. I hope"}, {"timestamp": [1230.56, 1235.2], "text": " this helps it make sense and kind of get a much clearer idea of what AGI or"}, {"timestamp": [1235.2, 1240.24], "text": " autonomous cognitive entities will actually look like. So thanks for watching."}]}