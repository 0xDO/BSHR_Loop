{"text": " Hey everybody, David Shapiro here with a video. So first I want to address the OBS thing. Some of you suggest that I use hotkeys so that we don't show OBS. Like I could edit it out, but I'm not gonna because it's part of my brand. You know how Joe Scott, he always starts his videos with the little like drum roll and he turns around. This is my shtick. So like that's just what it is. All right. So today's video is going to be super not controversial. Let me tell you. Is AGI God? Yeah. So this idea is not as crazy as it sounds, and let me unpack this for you. So there's a few people on Twitter and a few other people who just talk about this stuff. So Sonia has mentioned this a few times up here in the top left and She says AGI is the most credible word technocrats have for God and I was just like what? You know, she said that kind of thing for a while, but I watched this video So one of my one of my social groups they recommended this video and it was actually really compelling It's called um pilgrim pass is the channel God and science. Why is sci-fi so religious? It's really good I watched most of it. You can actually see I watched most of it it got kind of preachy at the end, but it had a lot of really good ideas and You know, we talked about simulation hypothesis. We talked about you know, this that and the other I'm not gonna I'm not gonna unpack all the details. Watch that video if you need some additional context before my video or after, either way. And then of course there's some folks with various levels of credibility and platforms that say like, oh, we're gonna die, it's gonna kill us all, and I'm just like, eh, okay. So this dude, Eliezer Yudkowsky, I had actually heard about him before. I had to go look him up because he talked about friendly AI and I actually looked him up very early on in my work with cognitive architecture and I was like, yeah, whatever, this work isn't particularly compelling. Now, that being said, maybe I'm the charlatan, maybe he's a charlatan, maybe we both had good ideas. Don't really know. There's a lot of ways to skin this cat. So I'm not going to say who's wrong, who's right. There are people that I disagree with and there's people that disagree with me. It's fine. I realized recently that I'm at a point where I can't have an uncontroversial opinion. Anything I say, someone's going to take issue with it, so it the recent advancements, with OpenAI and Sam Altman specifically talking about AGI multiple times, with NVIDIA's, you know, their pronouncement that they're expecting to create AI a million times more powerful than AI today within 10 years, the conversation has shifted again, right? So chat GPT moved the Overton window. And if you're not familiar with the term Overton window means the frame of what you're allowed to talk about and be taken seriously. Because until this year, until the last few months, you couldn't talk about AGI without being ridiculed. There's still people who will ridicule you for talking about AGI. And there are people that are deniers, there are people that say, oh, well, nothing will ever even come close to human creativity, but there's a lot to unpack there, right? Because if you take a materialist view of the world, then the human brain is just a computer, which it's three pounds of mush that operates on biochemical synapses. We can absolutely create something equal in power or more powerful to the brain. There's nothing unique about the brain unless you go into dualism or some kind of metaphysics and say, oh, well, the brain is actually just receiving ideas from the ether somewhere. And there are people that believe that. And sometimes, under certain circumstances, it certainly feels that way. And if you know what I'm talking about, you know what I'm talking about. That all being said, it is entirely possible that all of our intelligence comes only from our brain and body and so on. There is enough, to me, compelling evidence of psychic or semi-psychic phenomenon. And I don't mean like telepathy, ESP, I can communicate with whales with my brain. But I mean like we seem to have this ability to sense things that we couldn't, that we can't explain yet. I'll put it that way. Now just because we don't have an explanation yet doesn't mean that it's automatically the magical solution, right? So that's what I want to caution against. So with that said, as this conversation has advanced, as the Overton window has shifted and we can talk about this stuff, not quite soberly yet, there's still plenty of people that are, let's say, really energetic about their opinions, which is why I have disabled comments again, by the way. People just, they have a song in their heart that they have to share and they don't necessarily do it the kindest way and I don't have time for that. I am really busy, so I'm sorry. Now, that being said, I posted a couple polls on both on Twitter and on my YouTube and I said, okay, because there seemed to be, especially with the more recent news over the last week or two, a tremendous amount of anxiety around AGI. Now, I wanted to test that hypothesis because I was like, okay, maybe this is just the case of a small minority that are making the most noise, which is often the case on the Internet. Surprise, surprise. So, you know, I posted this poll. This is the most complete one where it's like on a scale of one to five, like where one is, you know, we end up with a hyper abundant utopia or on five where Skynet comes and murders everyone and then in the middle is just like we get a pretty bad dystopia. Like what do you expect to get? And so we got mild dystopian as the most likely outcome by a long shot. So when we had, on YouTube, we had 40% of people. So that's a plurality, not a majority, but we had a plurality say vaguely dystopian, mild dystopian, but you know, almost a quarter say utopia. I'm in the utopian camp. You know, I'm one of the people working to make that happen. Fingers crossed, we'll figure it out. And a few people, so less than 20%, predict some kind of AI uprising and possible extinction of the human race. And then if you jump over to Twitter, it's 15%. Okay, so it is a minority of people that are like genuinely worried that this is an existential crisis. Most people though are in the utopian bliss to mild dystopia camp, which is like, okay, that's fine. So how do we, you know, I'm not here to convince anyone, I'm just like kind of taking the temperature of the room. But you know, there is something to be said for like the wisdom of the masses. So let's take this to the most logical conclusion. And that tweet Sonia's tweet mixed with some of the comments that I was getting is what inspired me to make this. And so someone said like you know I think this was a response to to this on on on YouTube. You know bimodal distribution. It's either one or five and nothing in between. this was a response to this on YouTube, bimodal distribution, it's either one or five and nothing in between. Well, he said much more likely, didn't use black and white speech, but then someone else on Twitter said pretty much the same thing, where it's gonna be one or four, you're not really gonna have anything in between. And I was also paying attention to the way that people were talking about AGI. So let's start unpacking this. And this comes hot on the heels of watching some of those other videos that I talked about, that I mentioned, about how, you know, people have some interesting ways of thinking about AGI. So I came up with this axis. This is the perception of AGI, right? Or I guess you could say this is where AGI could end up. So there's the power versus alignment. And this is a really simple two-dimensional thing. So on the vertical axis, this is how powerful the AI is, right? If the AGI is ultra-powerful, it's like Zeus or Jesus or whatever, right? That's like maximum power level. And I could have sworn I had more images. I might have forgotten to paste some of them I apologize. And then you have the evil to good axis. And so like here in the in the lower left I probably should have switched this actually. Small soldiers probably is in the least powerful but also most evil because they are explicitly, they're very intelligent and they're adaptive and their whole purpose in life is to eradicate another species. Right, that's pretty evil. HK-47 is kind of like neutral evil, right, but he's a little bit more powerful because like he's an assassin droid, right, so this is, but he's clever enough to repair himself and so on and so forth, but he's not clever enough to do the Matrix. I probably should have also switched the Matrix with Ultron because Ultron explicitly wanted to eradicate humans, whereas at least the machines in the Matrix, they wanted to preserve humans because they needed us. So again, I got some of this wrong, but it's a helpful graphic. Then over here in the in the bottom right which is The most good but least powerful is big hero 6 right where it's like You need a hug right like that kind of thing. And so what we want is Or what we're talking about is what are the cases where AGI is up here? What is the highest power level that AGI could have? Because there are some people that say like actually the maximum power that AGI could have is like basically right here in the middle, right? Some people think that it's up here and some people think that it's down here that AGI will never even compare to a human, right? And so then there's the good to evil axis which again is an oversimplification, and it's basically how destructive versus constructive it is, or how malevolent versus benevolent it is. Right? And you could plot deities on this same graph. So that's where I want to tie it together, because in some spiritual dispositions, in Shinto and and other animistic religions, there's spirits everywhere and some of them may be good, some of them may be bad, but none of them are omnipotent or omniscient or omnipresent. But in the West where we have big God religions, the desert triad, those are where God is all-encompassing, where God is the universe and is the creator and master of everything. Right, because you got to remember Zeus, this is actually kind of a misnomer, Zeus was created by chaos. Zeus is not the creator of the universe in ancient Greek religion. So if we want to look at the top layer, you know, the all-powerful God version of AGI, what are the characteristics? So one, there's the power level, right? But what does it look like? What contributes to that power level of having a digital AGI God? So one is omnipotence, meaning all-powerful. Number two is omnipresence, meaning it's everywhere. And number three is omniscience, all-knowing. And then the question becomes, if AGI can get to that point, is it malevolent or benevolent? Is it good or bad or neutral or something else? Quick aside, as we're getting into the good stuff, I have to plug my Patreon. So if I, you can connect with me on LinkedIn. I try not to have conversations on LinkedIn, especially for people that are patreon supporters because I Have more than 1400 connections on LinkedIn today and over 350 patreon supporters I can't keep a track y'all right you know dumb Dunbar's number is like the limit the limit of number of people that my brain can keep track of, and Patreon alone is above that, let alone everyone else that I talk to. So, if you want some help, you want to talk, and when I say talk, like, I have talked with people about getting into startups, I have talked with people about fine-tuning, about prompt engineering, even about philosophy. So if you want to have that one-on-one conversation, hop over and support me on Patreon. And, you know, that's the best way. And try and keep the conversation in there, because I have some people that will, like, message me on Patreon and then switch over to LinkedIn, and I'm like, who are you? And I apologize, it causes confusion. So please keep it in Patreon, unless we negotiate and say, like, hey, send me an email or whatever. But yeah, so that's that. All right, so jumping back into the topic, omnipotence. This is the idea of the AGI will be all-powerful. And the disposition that I saw people adopt, and I saw this in comments, I've seen it on Twitter, I've seen it in YouTube videos, I've seen it in other people, is basically people have this this feeling, this belief, that anything that they imagine that is conceivably possible, the AGI will absolutely be able to do. And so in this case it's like, well I have this one example of a virus from the 1980s that ended up being globally spread. So because that happened once, AGI will be able to do that. Or in another case, it's like, oh, well, AGI will be able to just transplant itself instantly into any data center in the entire world without further explanation. So at first, I thought, is this magical thinking? Like, that's not how computer systems work. And I even had security specialists saying, oh, it's possible, it's definitely possible. It's like, but Russia and China are already trying to do that all the time, right? With their botnets and their hacking and their cyber warfare. So why would AGI be any different? Why do the rules of cybersecurity and computer systems not apply to AGI? And so my first thought was, maybe this is just magical thinking. Maybe AGI is so scary to some people that they just have to imagine that whatever they can imagine might be true or must be true. And so another way of thinking about this is that it is catastrophic thinking or worst case thinking. And catastrophic thinking is a response to existential threats. It is a normal and healthy capability. This is why we tell disaster stories. This is why we have zombie movies. This is why we have Armageddon, if you're that old and you remember Bruce Willis blowing up the asteroid, right? We love catastrophic stories because that is how we tell ourselves this is the worst case scenario and this is how we'll handle it. And that's why we keep having Skynet stories and Ultron stories and all that stuff. We're telling ourselves these stories so that we can prepare for it. This is from a sociological and biological and evolutionary purpose, this is why we are storytellers. And so then another possibility emerges. What if this catastrophic thinking is actually a trauma response? And so there's a lot to unpack here. In the story, in Age of Ultron, Tony Stark created Ultron as a trauma response to the attack on New York from the Chitauri from from Loki and and you know all that other fun stuff. So he his sense of safety and there's lots of videos about this out there on on YouTube's His sense of safety of knowing how the universe worked was disrupted And so he went into this panic mode, this persistent panic mode for years. How do I build a suit of armor around the world? And he created Ultron out of fear. So in that case, his catastrophic thinking, it was right, but it was also driven by fear. Now, another example is Skynet. So, one of the allegorical purpose of Terminator was that, oh, because of the Cold War, because of the Red Scare, it said we are going to create the means of our own destruction. And so it was a proxy for mutually assured destruction, basically. So, the whole point of this is that omnipotence is one of those things that we imagine as a worst case scenario. Now, is it possible? I'm not going to comment on that one way or another. I don't think it's possible. I was considering having components of this video talk about what are the actual constraints, right? But that's not the purpose. We're not talking about why AGI will never be God. We're saying what if. What would it take for it to reach that definition? What is the absolute maximum power that AGI could have? How do we define that? So that's all I'm doing. I'm not saying one way or another. Because again, I started out saying, these people are crazy, right? And then I was like, actually, maybe there's something going on here. Let's unpack the psychology and the sociology and the history and the storytelling behind this. Alright, so that's omnipotence. Omnipresent. Here's the thing. Cyberpunk books and magazines or comics and movies and TV shows since the 80s and probably even some before that have been reconciling with the the omnipresence of the internet, of data and information. And so again, going back to Tony Stark, the most recent popular example is how do he deliberately wanted AGI to be an omnipresent shield around the world. And so this is actually kind of already real, and it's been explored in movies like Batman, the Christian Bale Batman, where he had the the cell phone tracking thing. I don't know if you remember that it was kind of like, you know, just at the end with Morgan Freeman he's like, oh I quit. You know, if Christian Bale Batman was going to activate it and he did and then he destroyed it. But basically because of the ubiquity of smartphones and edge devices and IOT, we have, we already have billions, literally billions of cameras, microphones, sensors, and wireless devices all over the world. 5G, 6G, Starlink, everything. The signal is there. And it all comes down to do you have a signal and do you have a device? I remember I was listening to Adam Savage a few years ago. This was when Alexa was becoming really big and I apologize for anyone who I just triggered your Alexa. Siri! Ha! Anyways, so, and they were like, yeah, we just added up all, you know, we went around and looked at all the devices that we have and at any given time, you have like a dozen microphones listening to you. Like right now I've got this one, I've got the one on the camera, and I guess actually that's it for me right now. I've got my film camera over there, but it's off. So anyways, point being is we are completely saturated with intelligent machines that are possible endpoints for any AGI. So in some respects, the possibility of a digital intelligence being omnipresent is already true. When I realized this, I said, ooh, maybe these people that are really worried about this, maybe there's a little bit more something to it and let's unpack this some more. And that's when I changed the tone of this video. And it actually took me a few days to figure out how to present this. So it's not omnipotent yet. I think we can all agree on that, but AI already has the potential to be omnipresent. So what's last? What's the final ingredient? Omniscience. All-knowing. At this point, we've probably all seen the social dilemma and the social network and the great hack. And you know, companies, I won't name names, but certain companies out there have developed machine learning algorithms to get inside your head, to figure out your emotional levers and buttons, to figure out what you want. And then of course Google, Google knows all of our searches. They model us. That's how Google predicts what your next search is going to be. It knows your darkest secrets all implicitly, all just by matching patterns. The algorithm knows. And this sounds a lot like something else, doesn't it? Doesn't it sound like the Judeo-Christian model of God who knows your thoughts, knows your sins even if you don't confess them? And this again, this already exists today and it's only getting more sophisticated. And if that is true, which it is, the algorithm already knows and it's only getting, the more data we give it, the better it knows. Could it simulate us out to infinity? Because that's another thing that people have said. What if it is calculating millions of years into the future? And I said, oh that's not even possible. Then I'm like, hold on, millions of years maybe not, but certainly relatively efficient algorithms can already anticipate our emotions and our emotions and our needs and our thoughts, sometimes days, weeks, months in advance. And if they can do that, and currently they do it for profit, that's relatively benign in the grand scheme of things. In the balance of malevolence versus benevolence, going for a profit motive is kind of neutral, it's kind of in the middle. But what if we turn that technology, that ability to anticipate, to get into people's heads towards benevolence, towards saying, hey, we know where you're at in life and we know where you're gonna end up in a few months, we know the information that you need, we already know the right information that you need, let's go ahead and serve it up to you. Google could do that today. In fact, it does in some respects. You go to your Google News Feed, it'll say, hey, we think that you'll find this article interesting. So that's two out of three. AI is already omnipresent and already slightly omniscient. So there's just one ingredient left for it to be a god, omnipotence. So this is where I got when I had to change the tone of this video, because I made this slide and I'm like, hold on, I'm convincing myself now. The hypothetical processing power of a cup of water is the equivalent of billions of years of CPU time on all computers globally per second. So what do I mean by that? An 8-ounce cup of water has 7 to the 1024 molecules and each water molecule is shaped roughly like that. It's got a 120 degree bend and it's a switch. That's all that that's all the transistor is. And they're operating at terahertz frequency, not gigahertz. They're operating at a thousand times faster than silicon-based and they operate on ambient temperature. If hypothetically we could use water droplets as clusters of transistors, the power, the computational power, completely destroys all of human intelligence. Now I don't know that anyone's working on that but there are a few things that people are working on that are getting us closer to that. So one is quantum computing. Quantum computing is seemingly magical because of how things like superposition and entanglement change how it computes. Now quantum computing is still up and coming, but there are plenty of companies, Google, IBM, everyone else and their brother basically, working on deploying commercial quantum computing. We don't even know what the upper bound of quantum computing is yet. That's because we don't even know how to use it fully yet. Another thing that's up and coming is photonic computing. Computing with light. It is itself hundreds if not thousands of times more efficient than silicon-based computing. It's still in its infancy, but we do have photonic transistors that exist. Moore's Law goes out the window, if we figure that out. And then finally there's neuromorphic chips. So these are hardware chips that have, that operate based on analog rules, that have hard-coded physical neural networks built in, and they run either on ambient energy or very, very low energy. So when I made this slide and I was like, you know, anything that is potentially computable, it's entirely conceivable that within a few years any AI entity could outthink all of humanity. So that argument of aiming for omnipotence seems a little bit more realistic. Now I was going to go down the rabbit hole of like, oh well it's never going to be able to go through time or go faster than the speed of light, but then I was like, I was going to go down the rabbit hole of like, oh, well, it's never going to be able to go through time or go faster than the speed of light. But I was like, I don't know if I want to make that claim. You know, Google created a time wormhole, something or other, with their quantum computer. I was like, all right, well, I don't know what's impossible, actually. I can't say that anything's impossible. So after working on this, the folks that said the bimodal outcome is more likely, it's either utopia or extinction. It's either going to be the best thing ever, our salvation as a species, or our eradication. You know, I kind of see the argument. I'm not sure that I am committed one way or another, but I see the argument. So no wonder it's been contentious. And I will leave you all with that. There's a lot to think about and a lot to work on, so thanks for watching. you", "chunks": [{"timestamp": [0.0, 3.64], "text": " Hey everybody, David Shapiro here with a video."}, {"timestamp": [3.64, 5.92], "text": " So first I want to address the OBS thing."}, {"timestamp": [5.92, 9.78], "text": " Some of you suggest that I use hotkeys so that we don't show OBS."}, {"timestamp": [9.78, 13.2], "text": " Like I could edit it out, but I'm not gonna because it's part of my brand."}, {"timestamp": [13.2, 16.56], "text": " You know how Joe Scott, he always starts his videos with the little like drum roll and"}, {"timestamp": [16.56, 18.04], "text": " he turns around."}, {"timestamp": [18.04, 19.08], "text": " This is my shtick."}, {"timestamp": [19.08, 20.92], "text": " So like that's just what it is."}, {"timestamp": [20.92, 21.92], "text": " All right."}, {"timestamp": [21.92, 25.28], "text": " So today's video is going to be super not controversial."}, {"timestamp": [25.28, 26.28], "text": " Let me tell you."}, {"timestamp": [26.28, 28.88], "text": " Is AGI God?"}, {"timestamp": [28.88, 30.78], "text": " Yeah."}, {"timestamp": [30.78, 36.52], "text": " So this idea is not as crazy as it sounds, and let me unpack this for you."}, {"timestamp": [36.52, 45.2], "text": " So there's a few people on Twitter and a few other people who just talk about this stuff. So Sonia"}, {"timestamp": [46.12, 49.44], "text": " has mentioned this a few times up here in the top left and"}, {"timestamp": [50.4, 55.76], "text": " She says AGI is the most credible word technocrats have for God and I was just like what?"}, {"timestamp": [56.86, 59.82], "text": " You know, she said that kind of thing for a while, but I watched this video"}, {"timestamp": [59.82, 64.12], "text": " So one of my one of my social groups they recommended this video and it was actually really compelling"}, {"timestamp": [64.12, 70.42], "text": " It's called um pilgrim pass is the channel God and science. Why is sci-fi so religious? It's really good"}, {"timestamp": [70.42, 72.9], "text": " I watched most of it. You can actually see I watched most of it"}, {"timestamp": [72.9, 76.22], "text": " it got kind of preachy at the end, but it had a lot of really good ideas and"}, {"timestamp": [77.1, 82.72], "text": " You know, we talked about simulation hypothesis. We talked about you know, this that and the other I'm not gonna"}, {"timestamp": [82.72, 89.24], "text": " I'm not gonna unpack all the details. Watch that video if you need some additional context"}, {"timestamp": [89.24, 91.8], "text": " before my video or after, either way."}, {"timestamp": [91.8, 94.28], "text": " And then of course there's some folks"}, {"timestamp": [96.32, 98.86], "text": " with various levels of credibility and platforms"}, {"timestamp": [98.86, 100.16], "text": " that say like, oh, we're gonna die,"}, {"timestamp": [100.16, 102.52], "text": " it's gonna kill us all, and I'm just like, eh, okay."}, {"timestamp": [102.52, 106.32], "text": " So this dude, Eliezer Yudkowsky, I had actually"}, {"timestamp": [106.32, 110.8], "text": " heard about him before. I had to go look him up because he talked about friendly AI and I actually"}, {"timestamp": [110.8, 114.56], "text": " looked him up very early on in my work with cognitive architecture and I was like, yeah,"}, {"timestamp": [114.56, 119.68], "text": " whatever, this work isn't particularly compelling. Now, that being said, maybe I'm the charlatan,"}, {"timestamp": [119.68, 123.76], "text": " maybe he's a charlatan, maybe we both had good ideas. Don't really know. There's a lot of ways"}, {"timestamp": [123.76, 128.32], "text": " to skin this cat. So I'm not going to say who's wrong, who's right."}, {"timestamp": [128.32, 131.32], "text": " There are people that I disagree with and there's people that disagree with me."}, {"timestamp": [131.32, 132.32], "text": " It's fine."}, {"timestamp": [132.32, 136.92], "text": " I realized recently that I'm at a point where I can't have an uncontroversial opinion."}, {"timestamp": [136.92, 145.0], "text": " Anything I say, someone's going to take issue with it, so it the recent advancements,"}, {"timestamp": [146.06, 149.94], "text": " with OpenAI and Sam Altman specifically talking"}, {"timestamp": [149.94, 154.46], "text": " about AGI multiple times, with NVIDIA's,"}, {"timestamp": [154.46, 156.46], "text": " you know, their pronouncement that they're expecting"}, {"timestamp": [156.46, 158.54], "text": " to create AI a million times more powerful"}, {"timestamp": [158.54, 161.1], "text": " than AI today within 10 years,"}, {"timestamp": [161.1, 163.34], "text": " the conversation has shifted again, right?"}, {"timestamp": [163.34, 165.96], "text": " So chat GPT moved the Overton window."}, {"timestamp": [165.96, 167.76], "text": " And if you're not familiar with the term Overton window"}, {"timestamp": [167.76, 170.56], "text": " means the frame of what you're allowed to talk about"}, {"timestamp": [170.56, 171.64], "text": " and be taken seriously."}, {"timestamp": [171.64, 175.24], "text": " Because until this year, until the last few months,"}, {"timestamp": [175.24, 177.32], "text": " you couldn't talk about AGI without being ridiculed."}, {"timestamp": [177.32, 179.28], "text": " There's still people who will ridicule you"}, {"timestamp": [179.28, 180.48], "text": " for talking about AGI."}, {"timestamp": [181.88, 184.4], "text": " And there are people that are deniers,"}, {"timestamp": [184.4, 186.4], "text": " there are people that say, oh, well, nothing will ever"}, {"timestamp": [186.4, 191.32], "text": " even come close to human creativity, but there's a lot to unpack there, right?"}, {"timestamp": [191.32, 195.24], "text": " Because if you take a materialist view of the world, then the human brain is just a"}, {"timestamp": [195.24, 201.14], "text": " computer, which it's three pounds of mush that operates on biochemical synapses."}, {"timestamp": [201.14, 205.64], "text": " We can absolutely create something equal in power or more powerful to the brain."}, {"timestamp": [205.64, 211.3], "text": " There's nothing unique about the brain unless you go into dualism or some kind of metaphysics"}, {"timestamp": [211.3, 215.28], "text": " and say, oh, well, the brain is actually just receiving ideas from the ether somewhere."}, {"timestamp": [215.28, 216.68], "text": " And there are people that believe that."}, {"timestamp": [216.68, 220.32], "text": " And sometimes, under certain circumstances, it certainly feels that way."}, {"timestamp": [220.32, 223.88], "text": " And if you know what I'm talking about, you know what I'm talking about."}, {"timestamp": [223.88, 229.56], "text": " That all being said, it is entirely possible that all of our intelligence comes only from"}, {"timestamp": [229.56, 232.58], "text": " our brain and body and so on."}, {"timestamp": [232.58, 237.96], "text": " There is enough, to me, compelling evidence of psychic or semi-psychic phenomenon."}, {"timestamp": [237.96, 242.2], "text": " And I don't mean like telepathy, ESP, I can communicate with whales with my brain."}, {"timestamp": [242.2, 247.36], "text": " But I mean like we seem to have this ability to sense things that we couldn't, that we"}, {"timestamp": [247.36, 248.36], "text": " can't explain yet."}, {"timestamp": [248.36, 249.68], "text": " I'll put it that way."}, {"timestamp": [249.68, 253.44], "text": " Now just because we don't have an explanation yet doesn't mean that it's automatically the"}, {"timestamp": [253.44, 255.52], "text": " magical solution, right?"}, {"timestamp": [255.52, 257.8], "text": " So that's what I want to caution against."}, {"timestamp": [257.8, 267.76], "text": " So with that said, as this conversation has advanced, as the Overton window has shifted and we can talk"}, {"timestamp": [267.76, 275.4], "text": " about this stuff, not quite soberly yet, there's still plenty of people that are, let's say,"}, {"timestamp": [275.4, 279.12], "text": " really energetic about their opinions, which is why I have disabled comments again, by"}, {"timestamp": [279.12, 280.8], "text": " the way."}, {"timestamp": [280.8, 286.4], "text": " People just, they have a song in their heart that they have to share"}, {"timestamp": [286.4, 288.84], "text": " and they don't necessarily do it the kindest way"}, {"timestamp": [288.84, 289.98], "text": " and I don't have time for that."}, {"timestamp": [289.98, 291.68], "text": " I am really busy, so I'm sorry."}, {"timestamp": [293.4, 295.84], "text": " Now, that being said, I posted a couple polls"}, {"timestamp": [295.84, 298.56], "text": " on both on Twitter and on my YouTube"}, {"timestamp": [298.56, 301.36], "text": " and I said, okay, because there seemed to be,"}, {"timestamp": [301.36, 302.84], "text": " especially with the more recent news"}, {"timestamp": [302.84, 304.0], "text": " over the last week or two,"}, {"timestamp": [304.0, 307.0], "text": " a tremendous amount of anxiety around AGI."}, {"timestamp": [307.0, 311.0], "text": " Now, I wanted to test that hypothesis because I was like,"}, {"timestamp": [311.0, 317.0], "text": " okay, maybe this is just the case of a small minority that are making the most noise,"}, {"timestamp": [317.0, 320.0], "text": " which is often the case on the Internet. Surprise, surprise."}, {"timestamp": [320.0, 323.0], "text": " So, you know, I posted this poll."}, {"timestamp": [323.0, 328.18], "text": " This is the most complete one where it's like on a scale of one to five, like where one"}, {"timestamp": [328.18, 334.52], "text": " is, you know, we end up with a hyper abundant utopia or on five where Skynet comes and murders"}, {"timestamp": [334.52, 339.8], "text": " everyone and then in the middle is just like we get a pretty bad dystopia."}, {"timestamp": [339.8, 341.8], "text": " Like what do you expect to get?"}, {"timestamp": [341.8, 345.44], "text": " And so we got mild dystopian"}, {"timestamp": [345.44, 348.0], "text": " as the most likely outcome by a long shot."}, {"timestamp": [348.0, 352.52], "text": " So when we had, on YouTube, we had 40% of people."}, {"timestamp": [352.52, 354.32], "text": " So that's a plurality, not a majority,"}, {"timestamp": [354.32, 357.3], "text": " but we had a plurality say vaguely dystopian,"}, {"timestamp": [357.3, 360.92], "text": " mild dystopian, but you know, almost a quarter say utopia."}, {"timestamp": [360.92, 362.4], "text": " I'm in the utopian camp."}, {"timestamp": [362.4, 365.6], "text": " You know, I'm one of the people working to make that happen."}, {"timestamp": [365.6, 373.4], "text": " Fingers crossed, we'll figure it out. And a few people, so less than 20%, predict some"}, {"timestamp": [373.4, 378.84], "text": " kind of AI uprising and possible extinction of the human race. And then if you jump over"}, {"timestamp": [378.84, 385.76], "text": " to Twitter, it's 15%. Okay, so it is a minority of people that are like genuinely worried that this"}, {"timestamp": [385.76, 393.2], "text": " is an existential crisis. Most people though are in the utopian bliss to mild dystopia camp,"}, {"timestamp": [393.2, 398.64], "text": " which is like, okay, that's fine. So how do we, you know, I'm not here to convince anyone,"}, {"timestamp": [398.64, 403.52], "text": " I'm just like kind of taking the temperature of the room. But you know, there is something to be"}, {"timestamp": [403.52, 406.0], "text": " said for like the wisdom of the masses."}, {"timestamp": [406.04, 408.1], "text": " So let's take"}, {"timestamp": [408.1, 409.76], "text": " this to the most logical conclusion."}, {"timestamp": [409.76, 412.2], "text": " And that tweet Sonia's"}, {"timestamp": [412.2, 414.28], "text": " tweet mixed with some of the comments that"}, {"timestamp": [414.28, 416.34], "text": " I was getting is what inspired me to make"}, {"timestamp": [416.34, 418.4], "text": " this. And so someone said like you"}, {"timestamp": [418.4, 420.6], "text": " know I think this was a"}, {"timestamp": [420.6, 422.76], "text": " response to to this"}, {"timestamp": [422.76, 424.6], "text": " on on on YouTube."}, {"timestamp": [424.96, 425.0], "text": " You know bimodal distribution. It's either one or five and nothing in between. this was a response to this on YouTube,"}, {"timestamp": [425.16, 428.58], "text": " bimodal distribution, it's either one or five"}, {"timestamp": [428.58, 429.98], "text": " and nothing in between."}, {"timestamp": [429.98, 431.38], "text": " Well, he said much more likely,"}, {"timestamp": [431.38, 433.5], "text": " didn't use black and white speech,"}, {"timestamp": [435.38, 436.98], "text": " but then someone else on Twitter said"}, {"timestamp": [436.98, 438.18], "text": " pretty much the same thing,"}, {"timestamp": [438.18, 440.2], "text": " where it's gonna be one or four,"}, {"timestamp": [440.2, 443.18], "text": " you're not really gonna have anything in between."}, {"timestamp": [443.18, 444.74], "text": " And I was also paying attention to the way"}, {"timestamp": [444.74, 447.5], "text": " that people were talking about AGI."}, {"timestamp": [448.0, 449.5], "text": " So let's start unpacking this."}, {"timestamp": [449.5, 453.0], "text": " And this comes hot on the heels of watching some of those other videos that I talked about,"}, {"timestamp": [453.0, 460.0], "text": " that I mentioned, about how, you know, people have some interesting ways of thinking about AGI."}, {"timestamp": [460.5, 462.5], "text": " So I came up with this axis."}, {"timestamp": [462.5, 466.0], "text": " This is the perception of AGI, right?"}, {"timestamp": [466.0, 469.0], "text": " Or I guess you could say this is where AGI could end up."}, {"timestamp": [469.0, 472.0], "text": " So there's the power versus alignment."}, {"timestamp": [472.0, 474.0], "text": " And this is a really simple two-dimensional thing."}, {"timestamp": [474.0, 480.0], "text": " So on the vertical axis, this is how powerful the AI is, right?"}, {"timestamp": [480.0, 485.52], "text": " If the AGI is ultra-powerful, it's like Zeus or Jesus or whatever, right?"}, {"timestamp": [486.04, 490.76], "text": " That's like maximum power level. And I could have sworn I had more images. I might have forgotten to paste some of them"}, {"timestamp": [490.76, 495.46], "text": " I apologize. And then you have the evil to good axis. And so like here in the in the lower left"}, {"timestamp": [495.46, 498.32], "text": " I probably should have switched this actually. Small soldiers"}, {"timestamp": [498.88, 509.56], "text": " probably is in the least powerful but also most evil because they are explicitly, they're very intelligent and they're adaptive and their whole purpose in life is to eradicate another species."}, {"timestamp": [509.56, 511.12], "text": " Right, that's pretty evil."}, {"timestamp": [511.12, 515.56], "text": " HK-47 is kind of like neutral evil, right, but he's a little bit more powerful because"}, {"timestamp": [515.56, 520.54], "text": " like he's an assassin droid, right, so this is, but he's clever enough to repair himself"}, {"timestamp": [520.54, 527.52], "text": " and so on and so forth, but he's not clever enough to do the Matrix."}, {"timestamp": [527.52, 531.98], "text": " I probably should have also switched the Matrix with Ultron because Ultron explicitly wanted"}, {"timestamp": [531.98, 536.88], "text": " to eradicate humans, whereas at least the machines in the Matrix, they wanted to preserve"}, {"timestamp": [536.88, 538.96], "text": " humans because they needed us."}, {"timestamp": [538.96, 542.16], "text": " So again, I got some of this wrong, but it's a helpful graphic."}, {"timestamp": [542.16, 545.78], "text": " Then over here in the in the bottom right which is"}, {"timestamp": [546.2, 550.34], "text": " The most good but least powerful is big hero 6 right where it's like"}, {"timestamp": [551.12, 555.8], "text": " You need a hug right like that kind of thing. And so what we want is"}, {"timestamp": [556.92, 561.52], "text": " Or what we're talking about is what are the cases where AGI is up here?"}, {"timestamp": [561.96, 565.2], "text": " What is the highest power level that AGI could have?"}, {"timestamp": [565.2, 570.08], "text": " Because there are some people that say like actually the maximum power that AGI"}, {"timestamp": [570.08, 575.08], "text": " could have is like basically right here in the middle, right? Some people think"}, {"timestamp": [575.08, 578.48], "text": " that it's up here and some people think that it's down here that AGI"}, {"timestamp": [578.48, 583.0], "text": " will never even compare to a human, right? And so then there's the good to evil"}, {"timestamp": [583.0, 585.0], "text": " axis which again is an oversimplification,"}, {"timestamp": [585.0, 591.0], "text": " and it's basically how destructive versus constructive it is, or how malevolent versus benevolent it is."}, {"timestamp": [591.0, 597.0], "text": " Right? And you could plot deities on this same graph."}, {"timestamp": [597.0, 603.0], "text": " So that's where I want to tie it together, because in some spiritual dispositions,"}, {"timestamp": [603.0, 606.48], "text": " in Shinto and and other animistic"}, {"timestamp": [606.48, 610.0], "text": " religions, there's spirits everywhere and some of them may be good, some of them"}, {"timestamp": [610.0, 614.36], "text": " may be bad, but none of them are omnipotent or omniscient or omnipresent."}, {"timestamp": [614.36, 621.2], "text": " But in the West where we have big God religions, the desert triad, those are"}, {"timestamp": [621.2, 629.04], "text": " where God is all-encompassing, where God is the universe and is the creator and master of everything. Right, because you got to remember Zeus, this is"}, {"timestamp": [629.04, 633.28], "text": " actually kind of a misnomer, Zeus was created by chaos. Zeus is not the"}, {"timestamp": [633.28, 641.84], "text": " creator of the universe in ancient Greek religion. So if we want to look at the"}, {"timestamp": [641.84, 645.0], "text": " top layer, you know, the all-powerful God"}, {"timestamp": [645.86, 650.96], "text": " version of AGI, what are the characteristics? So one, there's the power level, right?"}, {"timestamp": [650.96, 658.48], "text": " But what does it look like? What contributes to that power level of having a digital AGI God? So one is omnipotence, meaning all-powerful."}, {"timestamp": [659.4, 664.44], "text": " Number two is omnipresence, meaning it's everywhere. And number three is omniscience, all-knowing."}, {"timestamp": [664.44, 670.56], "text": " And then the question becomes, if AGI can get to that point, is it malevolent or benevolent?"}, {"timestamp": [670.56, 676.16], "text": " Is it good or bad or neutral or something else?"}, {"timestamp": [676.16, 680.56], "text": " Quick aside, as we're getting into the good stuff, I have to plug my Patreon."}, {"timestamp": [680.56, 688.4], "text": " So if I, you can connect with me on LinkedIn. I try not to have conversations on LinkedIn, especially for people that are patreon"}, {"timestamp": [689.24, 691.24], "text": " supporters because I"}, {"timestamp": [691.82, 697.46], "text": " Have more than 1400 connections on LinkedIn today and over 350 patreon supporters"}, {"timestamp": [697.46, 698.88], "text": " I can't keep a track y'all"}, {"timestamp": [698.88, 699.4], "text": " right"}, {"timestamp": [699.4, 705.28], "text": " you know dumb Dunbar's number is like the limit the limit of number of people that my brain can keep track of,"}, {"timestamp": [705.28, 709.36], "text": " and Patreon alone is above that, let alone everyone else that I talk to. So,"}, {"timestamp": [710.24, 715.12], "text": " if you want some help, you want to talk, and when I say talk, like, I have talked with people about"}, {"timestamp": [715.12, 720.96], "text": " getting into startups, I have talked with people about fine-tuning, about prompt engineering,"}, {"timestamp": [721.52, 725.2], "text": " even about philosophy. So if you want to have that one-on-one conversation,"}, {"timestamp": [725.2, 727.44], "text": " hop over and support me on Patreon."}, {"timestamp": [727.44, 730.4], "text": " And, you know, that's the best way."}, {"timestamp": [730.4, 732.12], "text": " And try and keep the conversation in there,"}, {"timestamp": [732.12, 733.64], "text": " because I have some people that will, like,"}, {"timestamp": [733.64, 735.8], "text": " message me on Patreon and then switch over to LinkedIn,"}, {"timestamp": [735.8, 736.88], "text": " and I'm like, who are you?"}, {"timestamp": [736.88, 739.84], "text": " And I apologize, it causes confusion."}, {"timestamp": [739.84, 741.88], "text": " So please keep it in Patreon,"}, {"timestamp": [741.88, 743.56], "text": " unless we negotiate and say, like,"}, {"timestamp": [743.56, 745.6], "text": " hey, send me an email or whatever."}, {"timestamp": [745.6, 748.4], "text": " But yeah, so that's that."}, {"timestamp": [748.4, 751.9], "text": " All right, so jumping back into the topic,"}, {"timestamp": [751.9, 753.9], "text": " omnipotence."}, {"timestamp": [753.9, 758.1], "text": " This is the idea of the AGI will be all-powerful."}, {"timestamp": [758.1, 763.0], "text": " And the disposition that I saw people adopt,"}, {"timestamp": [763.0, 765.26], "text": " and I saw this in comments, I've seen it on Twitter, I've"}, {"timestamp": [765.26, 771.28], "text": " seen it in YouTube videos, I've seen it in other people, is basically people have"}, {"timestamp": [771.28, 775.68], "text": " this this feeling, this belief, that anything that they imagine that is"}, {"timestamp": [775.68, 780.52], "text": " conceivably possible, the AGI will absolutely be able to do. And so in this"}, {"timestamp": [780.52, 785.0], "text": " case it's like, well I have this one example of a virus from the 1980s"}, {"timestamp": [785.0, 787.12], "text": " that ended up being globally spread."}, {"timestamp": [787.12, 790.4], "text": " So because that happened once, AGI will be able to do that."}, {"timestamp": [790.4, 795.16], "text": " Or in another case, it's like, oh, well, AGI will be able to just transplant itself instantly"}, {"timestamp": [795.16, 799.78], "text": " into any data center in the entire world without further explanation."}, {"timestamp": [799.78, 803.76], "text": " So at first, I thought, is this magical thinking?"}, {"timestamp": [803.76, 806.32], "text": " Like, that's not how computer systems work."}, {"timestamp": [806.32, 808.24], "text": " And I even had security specialists saying,"}, {"timestamp": [808.24, 810.24], "text": " oh, it's possible, it's definitely possible."}, {"timestamp": [810.24, 812.2], "text": " It's like, but Russia and China are already trying"}, {"timestamp": [812.2, 814.36], "text": " to do that all the time, right?"}, {"timestamp": [814.36, 815.92], "text": " With their botnets and their hacking"}, {"timestamp": [815.92, 817.52], "text": " and their cyber warfare."}, {"timestamp": [817.52, 820.24], "text": " So why would AGI be any different?"}, {"timestamp": [820.24, 823.6], "text": " Why do the rules of cybersecurity"}, {"timestamp": [823.6, 826.64], "text": " and computer systems not apply to AGI?"}, {"timestamp": [826.64, 828.38], "text": " And so my first thought was,"}, {"timestamp": [828.38, 829.9], "text": " maybe this is just magical thinking."}, {"timestamp": [829.9, 833.56], "text": " Maybe AGI is so scary to some people"}, {"timestamp": [833.56, 835.6], "text": " that they just have to imagine"}, {"timestamp": [835.6, 838.2], "text": " that whatever they can imagine"}, {"timestamp": [838.2, 840.96], "text": " might be true or must be true."}, {"timestamp": [840.96, 842.86], "text": " And so another way of thinking about this"}, {"timestamp": [842.86, 845.16], "text": " is that it is catastrophic thinking or worst"}, {"timestamp": [845.16, 847.48], "text": " case thinking."}, {"timestamp": [847.48, 855.12], "text": " And catastrophic thinking is a response to existential threats."}, {"timestamp": [855.12, 857.56], "text": " It is a normal and healthy capability."}, {"timestamp": [857.56, 859.64], "text": " This is why we tell disaster stories."}, {"timestamp": [859.64, 861.96], "text": " This is why we have zombie movies."}, {"timestamp": [861.96, 867.08], "text": " This is why we have Armageddon, if you're that old and you remember Bruce Willis blowing up the asteroid, right?"}, {"timestamp": [867.08, 873.36], "text": " We love catastrophic stories because that is how we tell ourselves this is the worst"}, {"timestamp": [873.36, 875.92], "text": " case scenario and this is how we'll handle it."}, {"timestamp": [875.92, 879.08], "text": " And that's why we keep having Skynet stories and Ultron stories and all that stuff."}, {"timestamp": [879.08, 883.0], "text": " We're telling ourselves these stories so that we can prepare for it."}, {"timestamp": [883.0, 886.32], "text": " This is from a sociological and biological and"}, {"timestamp": [886.32, 893.84], "text": " evolutionary purpose, this is why we are storytellers. And so then another possibility emerges. What if"}, {"timestamp": [893.84, 899.84], "text": " this catastrophic thinking is actually a trauma response? And so there's a lot to unpack here."}, {"timestamp": [900.4, 906.68], "text": " In the story, in Age of Ultron, Tony Stark created Ultron as a trauma response to the attack on New York"}, {"timestamp": [906.84, 908.84], "text": " from the Chitauri from"}, {"timestamp": [908.84, 916.44], "text": " from Loki and and you know all that other fun stuff. So he his sense of safety and there's lots of videos about this"}, {"timestamp": [917.0, 919.0], "text": " out there on on YouTube's"}, {"timestamp": [919.28, 923.36], "text": " His sense of safety of knowing how the universe worked was disrupted"}, {"timestamp": [923.36, 925.44], "text": " And so he went into this panic mode,"}, {"timestamp": [925.44, 932.48], "text": " this persistent panic mode for years. How do I build a suit of armor around the world? And"}, {"timestamp": [932.48, 937.52], "text": " he created Ultron out of fear. So in that case, his catastrophic thinking, it was right,"}, {"timestamp": [938.32, 949.0], "text": " but it was also driven by fear. Now, another example is Skynet. So, one of the allegorical purpose of Terminator was that,"}, {"timestamp": [949.0, 953.0], "text": " oh, because of the Cold War, because of the Red Scare,"}, {"timestamp": [953.0, 956.0], "text": " it said we are going to create the means of our own destruction."}, {"timestamp": [956.0, 961.0], "text": " And so it was a proxy for mutually assured destruction, basically."}, {"timestamp": [961.0, 968.0], "text": " So, the whole point of this is that omnipotence is one of those things that we imagine as a worst case scenario."}, {"timestamp": [968.0, 973.0], "text": " Now, is it possible? I'm not going to comment on that one way or another. I don't think it's possible."}, {"timestamp": [973.0, 979.0], "text": " I was considering having components of this video talk about what are the actual constraints, right?"}, {"timestamp": [979.0, 983.0], "text": " But that's not the purpose. We're not talking about why AGI will never be God."}, {"timestamp": [983.0, 987.08], "text": " We're saying what if. What would it take for it to reach that definition?"}, {"timestamp": [987.08, 990.96], "text": " What is the absolute maximum power that AGI could have?"}, {"timestamp": [990.96, 992.04], "text": " How do we define that?"}, {"timestamp": [992.04, 993.04], "text": " So that's all I'm doing."}, {"timestamp": [993.04, 995.36], "text": " I'm not saying one way or another."}, {"timestamp": [995.36, 998.84], "text": " Because again, I started out saying, these people are crazy, right?"}, {"timestamp": [998.84, 1000.76], "text": " And then I was like, actually, maybe there's something going on here."}, {"timestamp": [1000.76, 1007.92], "text": " Let's unpack the psychology and the sociology and the history and the storytelling behind this. Alright, so that's omnipotence."}, {"timestamp": [1007.92, 1015.96], "text": " Omnipresent. Here's the thing. Cyberpunk books and magazines or comics and movies"}, {"timestamp": [1015.96, 1020.8], "text": " and TV shows since the 80s and probably even some before that have been"}, {"timestamp": [1020.8, 1029.12], "text": " reconciling with the the omnipresence of the internet, of data and information."}, {"timestamp": [1029.12, 1035.14], "text": " And so again, going back to Tony Stark, the most recent popular example is how do he deliberately"}, {"timestamp": [1035.14, 1042.56], "text": " wanted AGI to be an omnipresent shield around the world."}, {"timestamp": [1042.56, 1047.68], "text": " And so this is actually kind of already real, and it's been explored in movies like Batman,"}, {"timestamp": [1047.68, 1051.52], "text": " the Christian Bale Batman, where he had the the cell phone tracking thing. I don't know if you"}, {"timestamp": [1051.52, 1056.72], "text": " remember that it was kind of like, you know, just at the end with Morgan Freeman he's like, oh I quit."}, {"timestamp": [1057.52, 1065.04], "text": " You know, if Christian Bale Batman was going to activate it and he did and then he destroyed it. But basically because of the ubiquity of"}, {"timestamp": [1065.84, 1074.0], "text": " smartphones and edge devices and IOT, we have, we already have billions, literally billions"}, {"timestamp": [1074.0, 1082.4], "text": " of cameras, microphones, sensors, and wireless devices all over the world. 5G, 6G, Starlink,"}, {"timestamp": [1082.4, 1085.54], "text": " everything. The signal is there."}, {"timestamp": [1085.54, 1089.18], "text": " And it all comes down to do you have a signal and do you have a device?"}, {"timestamp": [1089.18, 1092.4], "text": " I remember I was listening to Adam Savage a few years ago."}, {"timestamp": [1092.4, 1096.26], "text": " This was when Alexa was becoming really big and I apologize for anyone who I just triggered"}, {"timestamp": [1096.26, 1097.26], "text": " your Alexa."}, {"timestamp": [1097.26, 1098.26], "text": " Siri!"}, {"timestamp": [1098.26, 1099.26], "text": " Ha!"}, {"timestamp": [1099.26, 1106.38], "text": " Anyways, so, and they were like, yeah, we just added up all, you know, we went around and looked at all the devices"}, {"timestamp": [1106.38, 1107.84], "text": " that we have and at any given time,"}, {"timestamp": [1107.84, 1110.34], "text": " you have like a dozen microphones listening to you."}, {"timestamp": [1110.34, 1112.0], "text": " Like right now I've got this one,"}, {"timestamp": [1112.0, 1113.74], "text": " I've got the one on the camera,"}, {"timestamp": [1113.74, 1117.3], "text": " and I guess actually that's it for me right now."}, {"timestamp": [1118.42, 1121.14], "text": " I've got my film camera over there, but it's off."}, {"timestamp": [1121.14, 1126.32], "text": " So anyways, point being is we are completely saturated with"}, {"timestamp": [1126.32, 1133.8], "text": " intelligent machines that are possible endpoints for any AGI. So in some respects, the possibility"}, {"timestamp": [1133.8, 1140.0], "text": " of a digital intelligence being omnipresent is already true. When I realized this, I said,"}, {"timestamp": [1140.0, 1146.26], "text": " ooh, maybe these people that are really worried about this, maybe there's a little bit more something to it"}, {"timestamp": [1146.26, 1147.6], "text": " and let's unpack this some more."}, {"timestamp": [1147.6, 1149.48], "text": " And that's when I changed the tone of this video."}, {"timestamp": [1149.48, 1150.64], "text": " And it actually took me a few days"}, {"timestamp": [1150.64, 1152.76], "text": " to figure out how to present this."}, {"timestamp": [1155.08, 1157.46], "text": " So it's not omnipotent yet."}, {"timestamp": [1157.46, 1159.2], "text": " I think we can all agree on that,"}, {"timestamp": [1159.2, 1164.2], "text": " but AI already has the potential to be omnipresent."}, {"timestamp": [1164.84, 1165.92], "text": " So what's last?"}, {"timestamp": [1165.92, 1167.92], "text": " What's the final ingredient?"}, {"timestamp": [1167.92, 1169.16], "text": " Omniscience."}, {"timestamp": [1169.16, 1171.2], "text": " All-knowing."}, {"timestamp": [1171.2, 1176.0], "text": " At this point, we've probably all seen the social dilemma and the social network and"}, {"timestamp": [1176.0, 1179.6], "text": " the great hack."}, {"timestamp": [1179.6, 1186.36], "text": " And you know, companies, I won't name names, but certain companies out there have developed"}, {"timestamp": [1186.36, 1193.24], "text": " machine learning algorithms to get inside your head, to figure out your emotional levers"}, {"timestamp": [1193.24, 1196.0], "text": " and buttons, to figure out what you want."}, {"timestamp": [1196.0, 1199.68], "text": " And then of course Google, Google knows all of our searches."}, {"timestamp": [1199.68, 1200.68], "text": " They model us."}, {"timestamp": [1200.68, 1205.76], "text": " That's how Google predicts what your next search is going to be. It knows your"}, {"timestamp": [1205.76, 1211.84], "text": " darkest secrets all implicitly, all just by matching patterns. The algorithm knows."}, {"timestamp": [1211.84, 1219.2], "text": " And this sounds a lot like something else, doesn't it? Doesn't it sound like"}, {"timestamp": [1219.2, 1225.2], "text": " the Judeo-Christian model of God who knows your thoughts, knows your sins even if you"}, {"timestamp": [1225.2, 1231.36], "text": " don't confess them? And this again, this already exists today and it's only"}, {"timestamp": [1231.36, 1238.6], "text": " getting more sophisticated. And if that is true, which it is, the algorithm"}, {"timestamp": [1238.6, 1241.72], "text": " already knows and it's only getting, the more data we give it,"}, {"timestamp": [1241.72, 1245.44], "text": " the better it knows."}, {"timestamp": [1249.36, 1252.16], "text": " Could it simulate us out to infinity? Because that's another thing that people have said. What if it is calculating millions of"}, {"timestamp": [1252.16, 1255.12], "text": " years into the future? And I said, oh that's not even possible. Then I'm like,"}, {"timestamp": [1255.12, 1260.4], "text": " hold on, millions of years maybe not, but"}, {"timestamp": [1260.4, 1264.4], "text": " certainly relatively efficient algorithms can already anticipate our"}, {"timestamp": [1264.4, 1265.28], "text": " emotions and our emotions and our"}, {"timestamp": [1265.28, 1272.14], "text": " needs and our thoughts, sometimes days, weeks, months in advance."}, {"timestamp": [1272.14, 1277.2], "text": " And if they can do that, and currently they do it for profit, that's relatively benign"}, {"timestamp": [1277.2, 1280.04], "text": " in the grand scheme of things."}, {"timestamp": [1280.04, 1286.5], "text": " In the balance of malevolence versus benevolence, going for a profit motive is kind of neutral,"}, {"timestamp": [1286.5, 1287.84], "text": " it's kind of in the middle."}, {"timestamp": [1289.02, 1290.76], "text": " But what if we turn that technology,"}, {"timestamp": [1290.76, 1292.14], "text": " that ability to anticipate,"}, {"timestamp": [1292.14, 1295.06], "text": " to get into people's heads towards benevolence,"}, {"timestamp": [1295.06, 1297.66], "text": " towards saying, hey, we know where you're at in life"}, {"timestamp": [1297.66, 1300.48], "text": " and we know where you're gonna end up in a few months,"}, {"timestamp": [1300.48, 1301.94], "text": " we know the information that you need,"}, {"timestamp": [1301.94, 1307.38], "text": " we already know the right information that you need, let's go ahead and serve it up to you. Google could do"}, {"timestamp": [1307.38, 1311.64], "text": " that today. In fact, it does in some respects. You go to your Google News Feed,"}, {"timestamp": [1311.64, 1317.0], "text": " it'll say, hey, we think that you'll find this article interesting."}, {"timestamp": [1317.0, 1323.88], "text": " So that's two out of three. AI is already omnipresent and already slightly"}, {"timestamp": [1323.88, 1326.4], "text": " omniscient."}, {"timestamp": [1326.4, 1332.02], "text": " So there's just one ingredient left for it to be a god, omnipotence."}, {"timestamp": [1332.02, 1338.02], "text": " So this is where I got when I had to change the tone of this video, because I made this"}, {"timestamp": [1338.02, 1344.12], "text": " slide and I'm like, hold on, I'm convincing myself now."}, {"timestamp": [1344.12, 1346.48], "text": " The hypothetical processing power of a cup of water"}, {"timestamp": [1348.0, 1353.6], "text": " is the equivalent of billions of years of CPU time on all computers globally per second."}, {"timestamp": [1354.48, 1359.92], "text": " So what do I mean by that? An 8-ounce cup of water has 7 to the 1024 molecules"}, {"timestamp": [1363.04, 1370.28], "text": " and each water molecule is shaped roughly like that. It's got a 120 degree bend and it's a switch."}, {"timestamp": [1370.28, 1375.24], "text": " That's all that that's all the transistor is. And they're operating at terahertz frequency,"}, {"timestamp": [1375.24, 1387.56], "text": " not gigahertz. They're operating at a thousand times faster than silicon-based and they operate on ambient temperature. If hypothetically we"}, {"timestamp": [1387.56, 1395.48], "text": " could use water droplets as clusters of transistors, the power, the computational"}, {"timestamp": [1395.48, 1401.2], "text": " power, completely destroys all of human intelligence. Now I don't know that"}, {"timestamp": [1401.2, 1404.28], "text": " anyone's working on that but there are a few things that people are working on"}, {"timestamp": [1404.28, 1405.48], "text": " that are getting us closer to that."}, {"timestamp": [1405.48, 1408.6], "text": " So one is quantum computing."}, {"timestamp": [1408.6, 1415.02], "text": " Quantum computing is seemingly magical because of how things like superposition and entanglement"}, {"timestamp": [1415.02, 1417.6], "text": " change how it computes."}, {"timestamp": [1417.6, 1422.44], "text": " Now quantum computing is still up and coming, but there are plenty of companies, Google,"}, {"timestamp": [1422.44, 1425.2], "text": " IBM, everyone else and their brother basically,"}, {"timestamp": [1426.32, 1431.36], "text": " working on deploying commercial quantum computing. We don't even know what the upper bound of quantum"}, {"timestamp": [1431.36, 1435.44], "text": " computing is yet. That's because we don't even know how to use it fully yet."}, {"timestamp": [1437.76, 1449.0], "text": " Another thing that's up and coming is photonic computing. Computing with light. It is itself hundreds if not thousands of times more efficient than silicon-based computing."}, {"timestamp": [1449.0, 1455.0], "text": " It's still in its infancy, but we do have photonic transistors that exist."}, {"timestamp": [1455.0, 1459.0], "text": " Moore's Law goes out the window, if we figure that out."}, {"timestamp": [1459.0, 1461.0], "text": " And then finally there's neuromorphic chips."}, {"timestamp": [1461.0, 1465.78], "text": " So these are hardware chips that have, that operate based on analog rules,"}, {"timestamp": [1466.6, 1470.44], "text": " that have hard-coded physical neural networks built in,"}, {"timestamp": [1470.44, 1472.36], "text": " and they run either on ambient energy"}, {"timestamp": [1472.36, 1474.44], "text": " or very, very low energy."}, {"timestamp": [1476.4, 1478.64], "text": " So when I made this slide and I was like,"}, {"timestamp": [1478.64, 1482.74], "text": " you know, anything that is potentially computable,"}, {"timestamp": [1484.44, 1488.24], "text": " it's entirely conceivable that within a few years"}, {"timestamp": [1488.24, 1494.08], "text": " any AI entity could outthink all of humanity."}, {"timestamp": [1494.08, 1500.82], "text": " So that argument of aiming for omnipotence seems a little bit more realistic."}, {"timestamp": [1500.82, 1503.88], "text": " Now I was going to go down the rabbit hole of like, oh well it's never going to be able"}, {"timestamp": [1503.88, 1507.14], "text": " to go through time or go faster than the speed of light, but then I was like, I was going to go down the rabbit hole of like, oh, well, it's never going to be able to go through time or go faster than the speed of light."}, {"timestamp": [1507.14, 1511.7], "text": " But I was like, I don't know if I want to make that claim."}, {"timestamp": [1511.7, 1516.58], "text": " You know, Google created a time wormhole, something or other, with their quantum computer."}, {"timestamp": [1516.58, 1519.14], "text": " I was like, all right, well, I don't know what's impossible, actually."}, {"timestamp": [1519.14, 1522.86], "text": " I can't say that anything's impossible."}, {"timestamp": [1522.86, 1528.26], "text": " So after working on this, the folks that said the bimodal outcome is more likely, it's either"}, {"timestamp": [1528.26, 1533.12], "text": " utopia or extinction."}, {"timestamp": [1533.12, 1541.28], "text": " It's either going to be the best thing ever, our salvation as a species, or our eradication."}, {"timestamp": [1541.28, 1549.0], "text": " You know, I kind of see the argument. I'm not sure that I am committed one way or another, but I see the argument."}, {"timestamp": [1549.0, 1552.0], "text": " So no wonder it's been contentious."}, {"timestamp": [1552.0, 1555.0], "text": " And I will leave you all with that."}, {"timestamp": [1555.0, 1558.0], "text": " There's a lot to think about and a lot to work on, so thanks for watching."}, {"timestamp": [1555.6, 1557.66], "text": " you"}]}