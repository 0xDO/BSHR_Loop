{"text": " We are still in year one of the AI revolution. I strongly suspect that 2023 will be recorded as the year that it all started. Consider where we were not even 12 months ago. ChatGPT wasn't out. Most of you who are subscribed to this channel might have been tangentially aware of AI, but now we are all in. So the question on everyone's mind, or if it's not on your mind it should be on your mind, is how far is this going to go? When we get superintelligence, how smart can it get? So let's unpack this question from several different angles. The first thing that you need to know is the Landauer limit. So the Landauer limit is a hypothetical limit on the energy efficiency for computation. Basically, this is the minimum amount of energy required for computation to happen. So one thing to keep in mind is that this is a hypothetical limit. We don't actually know, but current computers are about a billion times less efficient than they hypothetically could be. So in that case, the energy requirement for the Landauer limit is 2.85 times 10 to the negative 21 joules per bit at room temperature. Now current computers are about a billion times above that. And then, depending on how you measure human intelligence, which we'll get to in just a second, you could infer that maybe human brains are about a million times more efficient than current computers. However, I think that that's completely wrong. I think that we're actually, that our brains are actually much more powerful than that and much more efficient. But even still, we are probably hundreds of thousands of times or millions of times above the Landauer limit in terms of our own brains. So this is based on the second law of thermodynamics, which is entropy. Quantum computing could break this limit and not necessarily in the fact of like it's doing more calculations faster, just by virtue of the fact of the calculations that are the equations that it is able to solve, it can do it without actually doing every step of the way. But again, quantum computing is still in its infancy. So, keep all this in mind, but the idea of this slide is to just basically show that like there are physical limitations to the to the maximum speed and efficiency based on the laws of physics that computers can get to. Now, of course, you can make bigger computers, you can make more computers, you can build them in parallel, that sort of thing. So really the hypothetical maximum amount of computation that we can build is really really really high. That's that's the key takeaway. The next thing I want to talk about is quantum computing. So quantum computing we're not going to spend a whole lot of time on this, but I just wanted to point out that this is a thing and that the race for quantum supremacy is on. And when we are talking about speed, speed is going to be everything as we'll unpack later in this video. Uh, speed is going to actually be more important than like how smart you are or how big your computer is. And cause there's a trade off between model size and model speed. And more often than not, all you have to do is get to a good enough point and then you can conquer your enemies or take over the world or win at chess or whatever and the reason that quantum computing is so important to me and the reason that there are like geopolitical races for quantum supremacy is because they are so much faster than classical computers at solving some equations or doing certain operations. And so even if they cost a lot more energy, the fact that they can solve some kinds of problems millions of times faster or billions of times faster means that it's worth that extra energy because that energy that the quantum computer takes is still less energy and certainly less time than it would take a classical computer to do the same kind of work. So remember that speed is everything. Well maybe not everything, but speed is really a really critical factor in the race to superintelligence and how superintelligence will manifest once it gets here. So I mentioned a little bit ago about human brain computation. Right now, the current estimate of the brain's power is one exaflop, but if you look through history, the current estimation of how smart the human brain is is pretty much always tied to the current supercomputer. So like, you know, back in the 90s, we thought that the human brain was in the teraflops range. And then, you know, in the 2000s, we thought it was in the petaflops range. And now we think it's in the exaflop range. Why? Because that's the size of the current supercomputer. We actually have no idea how powerful the human brain is. And also, when you add to the fact that there is the possibility, and there's actually plenty of evidence for this already, at least at the neuron level, that the human brain exploits quantum mechanics in order to do some of its processing. Now I'm not saying that your brain is a quantum computer. I'm not saying that it uses superposition and entanglement, but there are people that theorize that the human brain does make use of quantum mechanical effects. So then it's entirely possible that we can't even compare our brains to classical computers if they are able to do some level of processing that is not going to be based on the von Neumann architecture. Again, super theoretical, but one thing that we can do is we can measure the amount of waste heat that our brain generates in terms of oxygen and everything else. And so we can estimate that our brain uses about 20 watts of energy. And so 20 watts is relatively efficient. So keep that in mind as well. That is one of the chief advantages that we have over computers, and it's probably one of the biggest advantages that we'll have for a long time, and that is that our brains are just energetically much more efficient, which is also why I think our brains are much closer to the land hour limit than computers, and probably will be for the foreseeable future, because think of it this way, evolution has had billions of years to optimize the efficiency of neurons, and so it would just make sense that our neuronal cells have been optimized to produce calculations or computation or whatever, interactions as energetically efficiently as possible, because that just means you need less food. And so, I'm not saying that nature, the blind watchmaker, is going to have perfected it and that our brains are operating at exactly the land hour limit, but our brains have been incentivized to operate as close to the land hour limit as possible for literally billions of years, or at least the underlying neurons. Obviously human brains have not existed for billions of years, but our evolutionary ancestors did exist for hundreds of millions of years, and then before that there were simpler animals and organisms. Okay, you get the idea. We really don't know how powerful the human brain is. Now one thing that has been on my mind lately is the concept of universal computation. When you look at the universe through the lens of physics, if you assume that the universe is materialistic, that basically everything that you and I can think and do and say and whatever, everything that our brains can do is predicated on the underlying laws of physics, namely matter and energy and possibly quantum mechanics. But that point basically says that like, okay, if we all operate based on the laws of physics, then any general Turing complete machine should be able to make the same calculations that any other Turing complete machine could make. should be able to make the same calculations that any other Turing-complete machine could make. And so this is something that has really been on my mind because the question is, is it possible for a machine to think a thought that a human is intrinsically incapable of thinking or understanding? And I don't know that it is. And what I mean by that is one thing that a lot of people are afraid of, I'm not going to name any names But some people say that you know machine that AI is going to be some alien Supermind intelligence that is just 100% utterly incomprehensible to us But that doesn't make sense from the perspective of physics and correct me if I'm wrong but like I have a few friends that are like Like PhD in physics and like this is the kind of stuff that we talk about and of course like any responsible scientists will say well we don't have evidence one way or the other which I agree and so this is pure speculation on my part but it seems to me from the perspective of physics the only possible way to get like truly exotic or alien minds is through probably quantum computing because there is not necessarily a comprehensible sequence of events to come to a particular conclusion. But from the subjective experience of humans where we have intuition and we can know things and think things and not understand how we know them or think them, like, I don't know, it feels like our brains are quantum computers to me. Obviously, that is not a scientific statement, but my point here is that maybe there is more in common between machines and AI and human brains in the long run when you constrain your view to classical computing and maybe even quantum computing. And so I'm often kind of suspicious or skeptical of people that assume and assert that AI is going to be intrinsically incomprehensible to us. It might be faster, it might be difficult to communicate, like translating from one language to another, but I don't personally believe that AI is going to be capable of abstract thoughts that we are intrinsically incapable of. Now, it's entirely possible, but knowing what I know about information processing and everything that I've read about the brain, that we have, you know, like 100 million parallel circuits that can reconfigure themselves on the fly and that we can, like there's humans that can learn to do calculus and all kinds of stuff that I can't do. So, I don't know, jury's still out, but that's kind of my personal position on that. Now we get into the brass tacks. So the biggest thing that I think is going to be a constraint on the maximum, uh, useful intelligence that superintelligence might arrive to is diminishing returns. So there's a few, uh, aspects of diminishing returns. So there's a few aspects of diminishing returns. One thing that we are already aware of is that larger models require exponentially increasing amounts of data and compute to run. They're larger, they're more expensive to train, they're more expensive to run, they're slower. And so there's also this concept of a useful ceiling. And what I mean by that is the problem space that you're operating in only has certain demands. So imagine, you know, giving Einstein a job at a gas station. That's a waste of his abilities. His intelligence is too high to do that job. And so, likewise, you're not going to use like GPT-15 to help you write, you know, tax laws or whatever. It's just too smart for it. And so, sometimes the juice is not going to be worth the squeeze in the long run. Even when, even if or when AI takes over from us and is fully autonomous and exploring the stars on its own, there's only a certain amount of intelligence that's required to do any particular job. And at a certain point, you're gonna favor efficiency, just like human evolution favors efficiency as well as intelligence. And so it's a matter of trading off between the necessary resources like energy requirements, hardware requirements, training data required. And you're gonna ultimately favor things that are smaller and faster. And so there's this optimal utility where you got a balance size of the model, intelligence of the model, against speed and efficiency. So this is one of the biggest things that I think is going to constrain it. And it's not a physical limit on superintelligence, but it is a functional or practical limit on how intelligent we should expect machines to become. Another thing that I've talked about quite a few times on this channel is the Byzantine General Problem. So the Byzantine General's problem is basically when you have a competitive environment, it is difficult to ascertain the function, alignment, and agenda of other agents. Now, one thing that people are afraid of is that AI, once it emerges, once it's fully autonomous, that it'll all merge into one gigantic hive mind and align against us. This is the reason that I'm not certain that that will happen. And the reason is because once you have robots, once you have autonomous agents, once you have wild, once you have autonomous agents, once you have, you know, like wild AIs out there, they're not going to have complete information about each other. So there's always going to be some level of imperfect information and incomplete information, which is an aspect of game theory, which basically says it's impossible to fully know the competitive landscape. It's impossible to fully know the inner workings and agenda of your opponents or allies. So what you have to do is you have to look for proxies and other shorthands. And so you might have a situation where autonomous agents say, you know what, I'm not actually going to merge with you because I don't know if that's going to further my own personal goals. And because of this, I suspect that we're going to end up in a situation where many autonomous AIs and robots prefer to stay kind of independent or autonomous. Now, that's not to say that there won't be gigantic conglomerations of AIs, particularly if, like, one starts taking over a data center and then they get more compute, Now that's not to say that there won't be gigantic conglomerations of AIs, particularly if one starts taking over a data center and then they get more compute, and it would be like an AI game of Risk taking over data centers, which is a really cool game idea, by the way. This sounds like a plot arc to cyberpunk. But the point here is there is some game theory and some mathematical reasons that AIs would not merge into a gigantic hive mind. If they did, that's a whole other can of worms. But because one of the things that I expect is that we're gonna have many millions, billions, or even trillions of autonomous AI agents in the form of fully digital agents, as well as robots, they might coordinate. They'll obviously have new ways of communicating with each other, like, you know, direct communication with Wi-Fi, and they'll be able to communicate with each other far faster than us, but at the same time, they might still prefer to have boundaries between individuals. So all of this leads to the thing that scares me the most, and I call it the terminal race condition. So the terminal race condition is, imagine have this environment where the the scarcest resource for AIs is data centers and high-powered compute resources. Okay, so you have a finite number of data centers, you have a finite number of GPUs, you can make more but it's slow. You know, it's like when you're playing Starcraft, right? You know, you need more pylons. It takes time to harvest resources. It takes time to build more data centers and print more GPUs. And I use print like chip fab, you know what I mean? So in that case, when your brain is literally the biggest constraint, and there's a number of agents out there, any kind of aggressive AI agent is going to be incentivized to acquire as much compute resource as it can if it is aggressive. Now obviously that's not a guaranteed thing, but particularly if it is weaponized, if it is deployed by a hostile actor on the geopolitical stage, it would absolutely be incentivized to capture as much compute resource as it could. Now, how do you capture and make the most use of a finite resource? You aim for efficiency and speed. You don't have to be the smartest AI on the block. You have to be the fastest that is capable of taking over data centers. And so now you're optimizing for speed rather than intelligence. Remember, all you have to do is be smart enough to get, you know, beyond the threshold, you know, to get it, to get your foot in the door, to get the upper hand over this other AI, and then you win all the cookies. So basically, this creates a competitive landscape where machines are going to be just constantly optimizing for speed and efficiency, and they might sacrifice things like accuracy, ethics, or thinking through things in order to make decisions faster and faster and faster. So this is what I call a terminal race condition, and I use this graphic because it's like when fighter jets get locked into a death spiral, the first one to flinch loses, but if you stay in the death spiral, you hit the ground. So this is honestly the thing that scares me the most, and it might not matter how we deploy AI. Once we have more autonomous AIs, and once we're in this competitive landscape, this terminal race condition might be an inevitable result just of the mathematical truth of having many AIs out there in a competitive environment. I don't know how to solve this yet and it is one of the things that literally keeps me up at night. I woke up at 330 in the morning yesterday unable to sleep because of this problem. So, not to give you too much existential dread. So another thing that kind of keeps me up at night is metastasis or metastasis. So metastasis, you're probably most familiar with this in the terms of cancer. So when cancer metastasizes, that's when a piece of a tumor breaks off and spreads throughout your body. Well, with some of the recent papers that have come out, you might have seen some of them, but basically we have already discovered that large language models are capable of writing and rewriting viruses and basically creating the best polymorphic viruses out there, which means if you have a very small virus that is capable of scouring its information landscape and looking for AI capabilities it can say, hey, rewrite this little bit of code for me. And then it can just shotgun itself out into the cyberspace ether in a million different forms, and some of them are going to slip through. And so now you have this information competitive landscape where you have viruses that are metastasizing and changing incredibly fast, sometimes going so far as to completely rewrite their entire code base. This possibility is also really scary, but fortunately I think that we can adapt cybersecurity best practices and also create AIs that will help us detect and stop these kinds of things. But this is also why it's really important for people that host LLM's particularly those that are good at coding it. It will probably be ultimately required legally required for all people hosting powerful coding LLM's to inspect all the inferences to make sure that it's like hey, you're not generating virus code. This would be like having an immune system inside of each cell of your body. And we actually kind of do, because if you have cells that are infected, what your cells are supposed to do is kill themselves. This is called apoptosis. But one thing that many viruses, organic viruses have to do is disable cell apoptosis in order to replicate. And so we'll probably need some kind of immune system inside of our data centers above and beyond what we already have, and specialized immune systems around our language models and other deep neural networks in order to prevent this kind of metastatic rapid spreading. Because of the size and complexity of language models and deep neural networks, I don't think that those are going to be what moves around because they require data centers and large computers to run and a lot of power. But if what they did was send out very small viruses, which are, you know, tiny packets of computer code, those can spread much faster and much more quietly. So this is one of the scariest things to me in terms of how we humans might weaponize AI, but how also if an AI wants to escape, this is the most likely avenue that I think that it would use to escape. And then another aspect of this, and this is not just a bug, it's also a feature. And this is actually something that I'm working on with my team in the ACE framework, the Autonomous Cognitive Entity framework, which is basically creating AI and robots and stuff that are able to change both their hardware and their software. Now, obviously we want them to be able to do this to a certain extent in order to automatically evolve and change their capabilities and rise to meet any challenge that we want them. But we also want to make sure that AI evolves in a sane trajectory, that it doesn't, like with each iteration, it doesn't decay and become more and more unhinged or rogue or anything like that. And so this is actually a major point of research in the ACE framework where basically it will only change itself if that change aligns with its aspirational mission, with it within its ethical and moral frameworks and so on and so forth. And so in that respect with the ACE framework, hopefully any machines equipped with the ACE architecture will get better at adhering to their morals, ethics, and mission over time rather than worse. But that's not a guaranteed thing and certainly there might be polymorphic apps out there that are designed to become more hostile or more aggressive or more conquering over time. So this is another thing that kind of scares me and keeps me up at night. Now I mentioned this is another thing that kind of scares me and keeps me up at night Now I mentioned this earlier and this is kind of one of the biggest saving graces is the is the optimal intelligence And so basically when whenever you have this kind of race condition whenever you have this Byzantine generals problem You there is always going to be a trade-off between model efficiency and accuracy. And so efficiency translates to speed, and accurately translates to size and sophistication. Like, are you running a 10 trillion parameter model or a 10 billion parameter model? That's kind of what I mean. So model size is kind of the primary thing. And so whatever task you are your your AI agent or your robot is is contending with it's going to have an optimal model size for that particular job now of course one of the things that can happen with polymorphic applications is that they can swap out models so you might have a you know a 10 trillion parameter model that you break you break out the big guns when you really really need smart brain, but then when you don't need it, you turn it off and you use your smaller, faster, lighter models. So for any given task or problem space or competition, there is an optimal level of intelligence. And so this is another thing to keep in mind as we build more robots and more autonomous agents is that equipping them with multiple models that they can switch from that they can select between is going to be one, just a good way of optimizing for efficiency and time but also it's something that we're going to need to be aware of and cognizant of as these agents become more and more autonomous because if they're able to switch between models, like in the worst possible scenario, an agent might learn, oh, hey, if I switch to this combination of models, I can jailbreak myself, basically. So that's why I also recommend that we have inspection at inference on every single model. One last thing, or one of the last things is this idea of Darwinian selection. So Darwinian selection is survival of the fittest, but when you have this environment of AGI's and super intelligence is all running around in wild cyberspace, there is still going to be selection. And so in some cases, an AI might get conquered and overwritten or cannibalized by another AI, like, hey, I like that model, I'm going to steal it from you. And of course, it's data, so you can just copy a model and everyone benefits. This is kind of how the geth work in Mass Effect, by the way. So, when one geth comes up with a better model, it just shares that model with the rest of the geth. But if you have a hostile environment where the goal is to eradicate another AI or erase it or whatever, if you have this environment, then you're going to be selecting AI agents and robots based on, these are the five or six-ish main criteria. Accuracy, so is it correct, is it useful, is it a robust model or agent? Is it fast? Complexity, so the sophistication of the problem space that it's operating in. Is it equal to the task that it needs to overcome? Efficiency, so this has to do with energy efficiency or hardware efficiency. So basically, just how many resources does the model take? And then there's this like dichotomy of aggressiveness versus usefulness. And so aggressiveness is like, does your agent or model want to metastasize? Does it want to conquer and delete other models? Or does it want to be useful to other models and to humans? So it's basically I kind of see that as a dichotomy where it's it's going to be at one end of the spectrum or another and I think that like humans because this is one thing that we humans struggle with within our own nature is We all have the ability to be aggressive or helpful We have the ability to be selfish or altruistic. And the reason is because we have to maintain our own existence, sometimes at the expense of others. And I'm not making any moral judgments to say like that's the way that it should be or that it's a good thing. That's just an observation of how humans work. That's why humans have war. That's why humans have violent crime. That's why we have theft. It's because sometimes it does benefit to be more aggressive and Those that are able to be aggressive when the situation calls for it are more likely to survive That's how it has happened up through you know up until now. Obviously we have systems in place that kind of Punish over being overly aggressive But at the same time we have police forces and we have militaries that are that are systematically disciplined to be aggressive When they need to be so anyways point being is that when you have this environment of? You know competition and variance and selection that I think that there's going to be a lot of very similar Darwinian forces Applied to AI as with humans now again One thing that could support all of this is if they form a gigantic hive mind like the geth. I don't know if that's going to happen. That's a subject for another video. So I want to leave you with one metaphor. It's speed chess. The way, the best way to think about super intelligence is it's basically a game of speed chess If you're not familiar with speed chess It is where you have a clock and you have a finite amount of time to win the game And so the idea is you have to make very quick decisions and your decisions only have to be just better just slightly better Than your opponents So you might you might recognize that your opponent has made a blunder and you exploit that blunder and you win the game. You don't have to be a chess grandmaster if you're just better at making good enough moves very quickly. And so in this respect, especially in a combative or hostile or competitive environment between AIs, we're not going to be optimizing for maximum intelligence. We're going to be optimizing for those that are the best at speed chest, those that have a good array of light, fast, and good enough models that they can switch between and adapt very quickly in order to get, you know, get just a little bit ahead of the curve across other combatants. And this is going to be super important in cyber warfare. This is going to be very, very important as well for enterprise-grade security systems because again, we're already in an environment where there's constantly hackers from one nation and industrial espionage from another nation. This is happening in real time at all times, but it's gonna get more sophisticated with the added ingredient of artificial intelligence, as well as a high number of models. And so, you know, yes, right now, the regulatory landscape says like, oh, well, if you wanna train bigger models, you need a license, but I don't care about bigger models. We have already shown that open source models today can be fine-tuned to do very useful things. So like, arresting people, you know, and I don't mean arrest like put them in handcuffs, I mean like banning them from making bigger and bigger models, that's not what I'm worried about. What I'm worried about is the 30 billion parameter model that is optimized to churn out virus code. What I'm concerned about is the 7 billion parameter model that is optimized to churn out virus code. What I'm concerned about is the 7 billion parameter model that is optimized for social engineering. Those are the things that are going to be the greatest security threat to our stability and safety for the foreseeable future. Now, obviously, if you can train a 30 billion parameter model to write virus code, you can also train a 30 billion parameter model to detect virus code to be part of your firewall. This is the direction that I'm looking at in terms of not necessarily containing superintelligence. I don't believe it can be contained and I don't think it should be contained, but in order to create a safe and stable environment for everyone, human or otherwise, I think that that's the direction that we need to be thinking in. Okay, so conclusion recap Basically, yes machines can get incredibly fast and smart however, the biggest asterisk is that trade-off of speed and power and efficiency So the maximum calculation speed is incredibly high, especially when you throw in quantum computing. I think that it would not be, like, unlikely that we see within 10 to 20 years the total compute power of machines outpacing humans by a factor of a billion to one. Like, that's the nature of the singularity, that's the nature of superintelligence. But, this competitive landscape will probably continue to exist unless we get the guess outcome or the Borg outcome, which may or may not happen. But because of the Byzantine general problems and game theory and incomplete information and imperfect information, I don't think that we're going to end up in a perfect, like, you know, machine, all machines versus all humans. I think that, like humans, the more machines there are and the more variants there is, there's going to be some disagreement between the machines, but that leads to the possibility of humans getting caught in the crossfire in a machine war. That's also another topic for another video. There's lots and lots of factors that are going to contribute to all of this. Intelligence is not monolithic. It also includes speed, the size of the model, the efficiency of the underlying hardware. So there's a lot of variables that go into this terminal race condition that really scares the bejesus out of me. And then finally, there are diminishing returns and race conditions that will very, very strongly incentivize AI agents and robots and models to basically seek efficiency and speed and just be good enough rather than be extraordinarily smart. So yeah, this is where I'm at. Thanks for watching. I hope you got a lot out of this video. Yeah, so let me know what you think in the comments, like, subscribe, so on and so forth. Cheers, have a good one. \u266a", "chunks": [{"timestamp": [0.0, 3.56], "text": " We are still in year one of the AI revolution."}, {"timestamp": [3.56, 9.92], "text": " I strongly suspect that 2023 will be recorded as the year that it all started."}, {"timestamp": [9.92, 12.72], "text": " Consider where we were not even 12 months ago."}, {"timestamp": [12.72, 14.84], "text": " ChatGPT wasn't out."}, {"timestamp": [14.84, 19.44], "text": " Most of you who are subscribed to this channel might have been tangentially aware of AI,"}, {"timestamp": [19.44, 22.52], "text": " but now we are all in."}, {"timestamp": [22.52, 25.64], "text": " So the question on everyone's mind, or if it's not on your mind it"}, {"timestamp": [25.64, 31.28], "text": " should be on your mind, is how far is this going to go? When we get"}, {"timestamp": [31.28, 36.68], "text": " superintelligence, how smart can it get? So let's unpack this question from"}, {"timestamp": [36.68, 41.9], "text": " several different angles. The first thing that you need to know is the Landauer"}, {"timestamp": [41.9, 45.2], "text": " limit. So the Landauer limit is a hypothetical limit on the"}, {"timestamp": [45.2, 51.68], "text": " energy efficiency for computation. Basically, this is the minimum amount of energy required"}, {"timestamp": [51.68, 59.76], "text": " for computation to happen. So one thing to keep in mind is that this is a hypothetical limit. We"}, {"timestamp": [59.76, 68.0], "text": " don't actually know, but current computers are about a billion times less efficient than they hypothetically"}, {"timestamp": [68.0, 69.0], "text": " could be."}, {"timestamp": [69.0, 76.72], "text": " So in that case, the energy requirement for the Landauer limit is 2.85 times 10 to the"}, {"timestamp": [76.72, 82.6], "text": " negative 21 joules per bit at room temperature."}, {"timestamp": [82.6, 86.46], "text": " Now current computers are about a billion times above that."}, {"timestamp": [86.46, 90.04], "text": " And then, depending on how you measure human intelligence,"}, {"timestamp": [90.04, 91.78], "text": " which we'll get to in just a second,"}, {"timestamp": [91.78, 94.14], "text": " you could infer that maybe human brains"}, {"timestamp": [94.14, 96.06], "text": " are about a million times more efficient"}, {"timestamp": [96.06, 97.68], "text": " than current computers."}, {"timestamp": [97.68, 99.64], "text": " However, I think that that's completely wrong."}, {"timestamp": [99.64, 101.08], "text": " I think that we're actually, that our brains"}, {"timestamp": [101.08, 103.52], "text": " are actually much more powerful than that"}, {"timestamp": [103.52, 104.86], "text": " and much more efficient."}, {"timestamp": [104.86, 109.1], "text": " But even still, we are probably hundreds of thousands"}, {"timestamp": [109.1, 112.1], "text": " of times or millions of times above the Landauer limit"}, {"timestamp": [112.1, 113.68], "text": " in terms of our own brains."}, {"timestamp": [114.54, 118.16], "text": " So this is based on the second law of thermodynamics,"}, {"timestamp": [118.16, 119.22], "text": " which is entropy."}, {"timestamp": [120.32, 124.0], "text": " Quantum computing could break this limit"}, {"timestamp": [124.0, 128.2], "text": " and not necessarily in the fact of like it's doing more calculations faster,"}, {"timestamp": [128.2, 133.7], "text": " just by virtue of the fact of the calculations that are the equations that it is able to solve,"}, {"timestamp": [133.7, 136.8], "text": " it can do it without actually doing every step of the way."}, {"timestamp": [136.8, 140.2], "text": " But again, quantum computing is still in its infancy."}, {"timestamp": [140.2, 147.24], "text": " So, keep all this in mind, but the idea of this slide is to just basically show that like there are physical"}, {"timestamp": [147.24, 154.76], "text": " limitations to the to the maximum speed and efficiency based on the laws of physics that computers can get to."}, {"timestamp": [154.76, 167.58], "text": " Now, of course, you can make bigger computers, you can make more computers, you can build them in parallel, that sort of thing. So really the hypothetical maximum amount of computation that we can build is"}, {"timestamp": [168.16, 170.5], "text": " really really really high."}, {"timestamp": [171.12, 177.72], "text": " That's that's the key takeaway. The next thing I want to talk about is quantum computing. So quantum computing"}, {"timestamp": [177.72, 179.68], "text": " we're not going to spend a whole lot of time on this,"}, {"timestamp": [179.68, 187.88], "text": " but I just wanted to point out that this is a thing and that the race for quantum supremacy is on. And when we are talking about speed,"}, {"timestamp": [188.08, 191.84], "text": " speed is going to be everything as we'll unpack later in this video. Uh,"}, {"timestamp": [191.84, 196.84], "text": " speed is going to actually be more important than like how smart you are or how"}, {"timestamp": [197.1, 198.16], "text": " big your computer is."}, {"timestamp": [198.4, 201.92], "text": " And cause there's a trade off between model size and model speed."}, {"timestamp": [202.16, 205.28], "text": " And more often than not, all you have to do is get to a good enough point"}, {"timestamp": [210.32, 210.92], "text": " and then you can conquer your enemies or take over the world or win at chess or whatever and"}, {"timestamp": [215.56, 216.08], "text": " the reason that quantum computing is so important to me and the reason that there are"}, {"timestamp": [224.0, 226.0], "text": " like geopolitical races for quantum supremacy is because they are so much faster than classical computers at solving some equations or doing certain"}, {"timestamp": [226.0, 233.28], "text": " operations. And so even if they cost a lot more energy, the fact that they can solve some kinds"}, {"timestamp": [233.28, 238.72], "text": " of problems millions of times faster or billions of times faster means that it's worth that extra"}, {"timestamp": [238.72, 245.52], "text": " energy because that energy that the quantum computer takes is still less energy and certainly less time"}, {"timestamp": [245.52, 249.72], "text": " than it would take a classical computer to do the same kind of work."}, {"timestamp": [249.72, 252.16], "text": " So remember that speed is everything."}, {"timestamp": [252.16, 257.72], "text": " Well maybe not everything, but speed is really a really critical factor in the race to superintelligence"}, {"timestamp": [257.72, 261.88], "text": " and how superintelligence will manifest once it gets here."}, {"timestamp": [261.88, 266.64], "text": " So I mentioned a little bit ago about human brain computation."}, {"timestamp": [268.22, 271.2], "text": " Right now, the current estimate of the brain's power"}, {"timestamp": [271.2, 274.7], "text": " is one exaflop, but if you look through history,"}, {"timestamp": [274.7, 277.98], "text": " the current estimation of how smart the human brain is"}, {"timestamp": [277.98, 280.96], "text": " is pretty much always tied to the current supercomputer."}, {"timestamp": [280.96, 282.78], "text": " So like, you know, back in the 90s,"}, {"timestamp": [282.78, 286.0], "text": " we thought that the human brain was in the teraflops range."}, {"timestamp": [286.0, 291.0], "text": " And then, you know, in the 2000s, we thought it was in the petaflops range."}, {"timestamp": [291.0, 296.0], "text": " And now we think it's in the exaflop range. Why? Because that's the size of the current supercomputer."}, {"timestamp": [296.0, 299.0], "text": " We actually have no idea how powerful the human brain is."}, {"timestamp": [299.0, 302.0], "text": " And also, when you add to the fact that there is the possibility,"}, {"timestamp": [302.0, 308.44], "text": " and there's actually plenty of evidence for this already, at least at the neuron level, that the human brain exploits quantum mechanics"}, {"timestamp": [308.44, 310.6], "text": " in order to do some of its processing."}, {"timestamp": [310.6, 313.36], "text": " Now I'm not saying that your brain is a quantum computer."}, {"timestamp": [313.36, 317.04], "text": " I'm not saying that it uses superposition and entanglement, but there are people that"}, {"timestamp": [317.04, 322.52], "text": " theorize that the human brain does make use of quantum mechanical effects."}, {"timestamp": [322.52, 325.28], "text": " So then it's entirely possible that we can't even compare our"}, {"timestamp": [325.28, 331.28], "text": " brains to classical computers if they are able to do some level of processing that is not going to"}, {"timestamp": [331.28, 337.6], "text": " be based on the von Neumann architecture. Again, super theoretical, but one thing that we can do"}, {"timestamp": [337.6, 342.32], "text": " is we can measure the amount of waste heat that our brain generates in terms of oxygen and"}, {"timestamp": [342.32, 347.5], "text": " everything else. And so we can estimate that our brain uses about 20 watts of energy."}, {"timestamp": [347.5, 350.16], "text": " And so 20 watts is relatively efficient."}, {"timestamp": [350.16, 352.12], "text": " So keep that in mind as well."}, {"timestamp": [352.12, 356.0], "text": " That is one of the chief advantages that we have over computers, and it's probably one"}, {"timestamp": [356.0, 359.84], "text": " of the biggest advantages that we'll have for a long time, and that is that our brains"}, {"timestamp": [359.84, 363.92], "text": " are just energetically much more efficient, which is also why I think our brains are much"}, {"timestamp": [363.92, 366.96], "text": " closer to the land hour limit than computers,"}, {"timestamp": [366.96, 369.66], "text": " and probably will be for the foreseeable future,"}, {"timestamp": [369.66, 371.84], "text": " because think of it this way,"}, {"timestamp": [371.84, 373.4], "text": " evolution has had billions of years"}, {"timestamp": [373.4, 376.52], "text": " to optimize the efficiency of neurons,"}, {"timestamp": [376.52, 380.76], "text": " and so it would just make sense that our neuronal cells"}, {"timestamp": [380.76, 383.68], "text": " have been optimized to produce calculations"}, {"timestamp": [383.68, 385.0], "text": " or computation or whatever,"}, {"timestamp": [385.0, 391.0], "text": " interactions as energetically efficiently as possible, because that just means you need less food."}, {"timestamp": [391.0, 395.0], "text": " And so, I'm not saying that nature, the blind watchmaker, is going to have perfected it"}, {"timestamp": [395.0, 399.0], "text": " and that our brains are operating at exactly the land hour limit,"}, {"timestamp": [399.0, 404.0], "text": " but our brains have been incentivized to operate as close to the land hour limit as possible"}, {"timestamp": [404.0, 407.82], "text": " for literally billions of years, or at least the underlying neurons."}, {"timestamp": [407.82, 412.5], "text": " Obviously human brains have not existed for billions of years, but our evolutionary ancestors"}, {"timestamp": [412.5, 417.18], "text": " did exist for hundreds of millions of years, and then before that there were simpler animals"}, {"timestamp": [417.18, 418.18], "text": " and organisms."}, {"timestamp": [418.18, 420.18], "text": " Okay, you get the idea."}, {"timestamp": [420.18, 423.66], "text": " We really don't know how powerful the human brain is."}, {"timestamp": [423.66, 429.0], "text": " Now one thing that has been on my mind lately is the concept of universal computation."}, {"timestamp": [429.0, 432.0], "text": " When you look at the universe through the lens of physics,"}, {"timestamp": [432.0, 436.0], "text": " if you assume that the universe is materialistic,"}, {"timestamp": [436.0, 440.0], "text": " that basically everything that you and I can think and do and say and whatever,"}, {"timestamp": [440.0, 448.06], "text": " everything that our brains can do is predicated on the underlying laws of physics, namely matter and energy and possibly quantum mechanics."}, {"timestamp": [448.52, 456.2], "text": " But that point basically says that like, okay, if we all operate based on the laws of physics, then"}, {"timestamp": [456.8, 464.8], "text": " any general Turing complete machine should be able to make the same calculations that any other Turing complete machine could make."}, {"timestamp": [464.56, 465.12], "text": " should be able to make the same calculations that any other Turing-complete machine could make."}, {"timestamp": [470.96, 471.6], "text": " And so this is something that has really been on my mind because the question is, is it possible"}, {"timestamp": [477.6, 482.88], "text": " for a machine to think a thought that a human is intrinsically incapable of thinking or understanding? And I don't know that it is. And what I mean by that is one thing that a lot of"}, {"timestamp": [482.88, 489.56], "text": " people are afraid of, I'm not going to name any names But some people say that you know machine that AI is going to be some alien"}, {"timestamp": [489.88, 494.48], "text": " Supermind intelligence that is just 100% utterly incomprehensible to us"}, {"timestamp": [494.84, 498.96], "text": " But that doesn't make sense from the perspective of physics and correct me if I'm wrong"}, {"timestamp": [498.96, 500.96], "text": " but like I have a few friends that are like"}, {"timestamp": [500.96, 507.2], "text": " Like PhD in physics and like this is the kind of stuff that we talk about and of course like any responsible scientists will say"}, {"timestamp": [507.2, 510.68], "text": " well we don't have evidence one way or the other which I agree and so this is"}, {"timestamp": [510.68, 516.64], "text": " pure speculation on my part but it seems to me from the perspective of physics"}, {"timestamp": [516.64, 522.88], "text": " the only possible way to get like truly exotic or alien minds is through"}, {"timestamp": [522.88, 525.0], "text": " probably quantum computing"}, {"timestamp": [525.46, 529.74], "text": " because there is not necessarily a comprehensible sequence"}, {"timestamp": [529.74, 532.86], "text": " of events to come to a particular conclusion."}, {"timestamp": [532.86, 534.94], "text": " But from the subjective experience of humans"}, {"timestamp": [534.94, 538.12], "text": " where we have intuition and we can know things"}, {"timestamp": [538.12, 540.52], "text": " and think things and not understand how we know them"}, {"timestamp": [540.52, 542.42], "text": " or think them, like, I don't know,"}, {"timestamp": [542.42, 545.36], "text": " it feels like our brains are quantum computers to me."}, {"timestamp": [545.36, 547.4], "text": " Obviously, that is not a scientific statement,"}, {"timestamp": [547.4, 552.4], "text": " but my point here is that maybe there is more in common"}, {"timestamp": [552.68, 556.8], "text": " between machines and AI and human brains in the long run"}, {"timestamp": [556.8, 559.88], "text": " when you constrain your view to classical computing"}, {"timestamp": [559.88, 562.08], "text": " and maybe even quantum computing."}, {"timestamp": [562.08, 565.22], "text": " And so I'm often kind of suspicious or skeptical"}, {"timestamp": [565.22, 568.2], "text": " of people that assume and assert"}, {"timestamp": [568.2, 572.46], "text": " that AI is going to be intrinsically incomprehensible to us."}, {"timestamp": [572.46, 575.76], "text": " It might be faster, it might be difficult to communicate,"}, {"timestamp": [575.76, 578.2], "text": " like translating from one language to another,"}, {"timestamp": [578.2, 581.18], "text": " but I don't personally believe that AI"}, {"timestamp": [581.18, 584.38], "text": " is going to be capable of abstract thoughts"}, {"timestamp": [584.38, 585.44], "text": " that we are intrinsically"}, {"timestamp": [585.44, 589.68], "text": " incapable of. Now, it's entirely possible, but knowing what I know about information"}, {"timestamp": [589.68, 594.32], "text": " processing and everything that I've read about the brain, that we have, you know, like 100"}, {"timestamp": [594.32, 599.36], "text": " million parallel circuits that can reconfigure themselves on the fly and that we can, like"}, {"timestamp": [599.36, 603.6], "text": " there's humans that can learn to do calculus and all kinds of stuff that I can't do. So,"}, {"timestamp": [603.6, 605.68], "text": " I don't know, jury's still out,"}, {"timestamp": [605.68, 607.84], "text": " but that's kind of my personal position on that."}, {"timestamp": [608.68, 611.28], "text": " Now we get into the brass tacks."}, {"timestamp": [612.08, 617.08], "text": " So the biggest thing that I think is going to be a constraint on the maximum,"}, {"timestamp": [617.4, 617.92], "text": " uh,"}, {"timestamp": [617.92, 622.3], "text": " useful intelligence that superintelligence might arrive to is diminishing"}, {"timestamp": [622.3, 626.0], "text": " returns. So there's a few, uh, aspects of diminishing returns. So there's a few aspects of diminishing returns."}, {"timestamp": [626.0, 629.0], "text": " One thing that we are already aware of is that larger"}, {"timestamp": [629.0, 632.0], "text": " models require exponentially"}, {"timestamp": [632.0, 635.0], "text": " increasing amounts of data and compute to run."}, {"timestamp": [635.0, 638.0], "text": " They're larger, they're more expensive to train, they're more expensive"}, {"timestamp": [638.0, 641.0], "text": " to run, they're slower. And so there's"}, {"timestamp": [641.0, 644.0], "text": " also this concept of a useful ceiling."}, {"timestamp": [644.0, 650.6], "text": " And what I mean by that is the problem space that you're operating in only has certain demands."}, {"timestamp": [650.6, 654.82], "text": " So imagine, you know, giving Einstein a job at a gas station."}, {"timestamp": [654.82, 656.46], "text": " That's a waste of his abilities."}, {"timestamp": [656.46, 660.1], "text": " His intelligence is too high to do that job."}, {"timestamp": [660.1, 667.84], "text": " And so, likewise, you're not going to use like GPT-15 to help you write, you know, tax laws or whatever."}, {"timestamp": [667.84, 669.04], "text": " It's just too smart for it."}, {"timestamp": [670.24, 673.92], "text": " And so, sometimes the juice is not going to be worth the squeeze in the long run."}, {"timestamp": [673.92, 680.48], "text": " Even when, even if or when AI takes over from us and is fully autonomous"}, {"timestamp": [680.48, 682.48], "text": " and exploring the stars on its own,"}, {"timestamp": [682.48, 687.12], "text": " there's only a certain amount of intelligence that's required to do any particular job."}, {"timestamp": [687.12, 689.76], "text": " And at a certain point, you're gonna favor efficiency,"}, {"timestamp": [689.76, 692.52], "text": " just like human evolution favors efficiency"}, {"timestamp": [692.52, 694.12], "text": " as well as intelligence."}, {"timestamp": [694.12, 695.6], "text": " And so it's a matter of trading off"}, {"timestamp": [695.6, 699.34], "text": " between the necessary resources like energy requirements,"}, {"timestamp": [699.34, 702.62], "text": " hardware requirements, training data required."}, {"timestamp": [702.62, 704.44], "text": " And you're gonna ultimately favor things"}, {"timestamp": [704.44, 709.72], "text": " that are smaller and faster. And so there's this optimal utility where you got a balance"}, {"timestamp": [709.72, 714.4], "text": " size of the model, intelligence of the model, against speed and"}, {"timestamp": [714.4, 717.8], "text": " efficiency. So this is one of the biggest things that I think is going to"}, {"timestamp": [717.8, 721.72], "text": " constrain it. And it's not a physical limit on superintelligence, but it is a"}, {"timestamp": [721.72, 725.16], "text": " functional or practical limit on how intelligent we should"}, {"timestamp": [725.16, 727.94], "text": " expect machines to become."}, {"timestamp": [727.94, 731.62], "text": " Another thing that I've talked about quite a few times on this channel is the Byzantine"}, {"timestamp": [731.62, 732.62], "text": " General Problem."}, {"timestamp": [732.62, 739.08], "text": " So the Byzantine General's problem is basically when you have a competitive environment, it"}, {"timestamp": [739.08, 746.0], "text": " is difficult to ascertain the function, alignment, and agenda of other agents."}, {"timestamp": [746.0, 748.04], "text": " Now, one thing that people are afraid of"}, {"timestamp": [748.04, 751.84], "text": " is that AI, once it emerges, once it's fully autonomous,"}, {"timestamp": [751.84, 754.12], "text": " that it'll all merge into one gigantic hive mind"}, {"timestamp": [754.12, 755.92], "text": " and align against us."}, {"timestamp": [755.92, 758.08], "text": " This is the reason that I'm not certain"}, {"timestamp": [758.08, 759.76], "text": " that that will happen."}, {"timestamp": [759.76, 762.54], "text": " And the reason is because once you have robots,"}, {"timestamp": [762.54, 764.62], "text": " once you have autonomous agents,"}, {"timestamp": [764.62, 765.0], "text": " once you have wild, once you have autonomous agents, once you"}, {"timestamp": [765.0, 771.48], "text": " have, you know, like wild AIs out there, they're not going to have complete information about"}, {"timestamp": [771.48, 772.48], "text": " each other."}, {"timestamp": [772.48, 777.68], "text": " So there's always going to be some level of imperfect information and incomplete information,"}, {"timestamp": [777.68, 784.32], "text": " which is an aspect of game theory, which basically says it's impossible to fully know the competitive"}, {"timestamp": [784.32, 785.0], "text": " landscape."}, {"timestamp": [785.0, 793.0], "text": " It's impossible to fully know the inner workings and agenda of your opponents or allies."}, {"timestamp": [793.0, 797.0], "text": " So what you have to do is you have to look for proxies and other shorthands."}, {"timestamp": [797.0, 801.0], "text": " And so you might have a situation where autonomous agents say,"}, {"timestamp": [801.0, 803.0], "text": " you know what, I'm not actually going to merge with you"}, {"timestamp": [803.0, 807.0], "text": " because I don't know if that's going to further my own personal goals."}, {"timestamp": [807.0, 812.5], "text": " And because of this, I suspect that we're going to end up in a situation where"}, {"timestamp": [812.5, 819.5], "text": " many autonomous AIs and robots prefer to stay kind of independent or autonomous."}, {"timestamp": [819.5, 824.5], "text": " Now, that's not to say that there won't be gigantic conglomerations of AIs,"}, {"timestamp": [824.5, 825.34], "text": " particularly if, like, one starts taking over a data center and then they get more compute, Now that's not to say that there won't be gigantic conglomerations of AIs, particularly"}, {"timestamp": [825.34, 829.24], "text": " if one starts taking over a data center and then they get more compute, and it would be"}, {"timestamp": [829.24, 835.18], "text": " like an AI game of Risk taking over data centers, which is a really cool game idea, by the way."}, {"timestamp": [835.18, 838.58], "text": " This sounds like a plot arc to cyberpunk."}, {"timestamp": [838.58, 843.98], "text": " But the point here is there is some game theory and some mathematical reasons that AIs would"}, {"timestamp": [843.98, 845.76], "text": " not merge into a gigantic hive mind."}, {"timestamp": [845.76, 848.44], "text": " If they did, that's a whole other can of worms."}, {"timestamp": [848.44, 851.7], "text": " But because one of the things that I expect"}, {"timestamp": [851.7, 854.8], "text": " is that we're gonna have many millions, billions,"}, {"timestamp": [854.8, 857.76], "text": " or even trillions of autonomous AI agents"}, {"timestamp": [857.76, 861.58], "text": " in the form of fully digital agents, as well as robots,"}, {"timestamp": [861.58, 863.24], "text": " they might coordinate."}, {"timestamp": [863.24, 866.96], "text": " They'll obviously have new ways of communicating with each other, like, you"}, {"timestamp": [866.96, 870.92], "text": " know, direct communication with Wi-Fi, and they'll be able to communicate with each other"}, {"timestamp": [870.92, 875.9], "text": " far faster than us, but at the same time, they might still prefer to have boundaries"}, {"timestamp": [875.9, 878.64], "text": " between individuals."}, {"timestamp": [878.64, 882.66], "text": " So all of this leads to the thing that scares me the most, and I call it the terminal race"}, {"timestamp": [882.66, 883.76], "text": " condition."}, {"timestamp": [883.76, 891.38], "text": " So the terminal race condition is, imagine have this environment where the the scarcest resource for AIs is"}, {"timestamp": [891.38, 897.36], "text": " data centers and high-powered compute resources. Okay, so you have a finite number of data"}, {"timestamp": [897.36, 901.76], "text": " centers, you have a finite number of GPUs, you can make more but it's slow. You know,"}, {"timestamp": [901.76, 905.52], "text": " it's like when you're playing Starcraft, right? You know, you need more"}, {"timestamp": [905.52, 912.16], "text": " pylons. It takes time to harvest resources. It takes time to build more data centers and print"}, {"timestamp": [912.16, 920.88], "text": " more GPUs. And I use print like chip fab, you know what I mean? So in that case, when your brain is"}, {"timestamp": [920.88, 927.68], "text": " literally the biggest constraint, and there's a number of agents out there,"}, {"timestamp": [927.68, 933.6], "text": " any kind of aggressive AI agent is going to be incentivized to acquire as much compute"}, {"timestamp": [933.6, 936.8], "text": " resource as it can if it is aggressive."}, {"timestamp": [936.8, 941.64], "text": " Now obviously that's not a guaranteed thing, but particularly if it is weaponized, if it"}, {"timestamp": [941.64, 945.12], "text": " is deployed by a hostile actor on the geopolitical"}, {"timestamp": [945.12, 950.0], "text": " stage, it would absolutely be incentivized to capture as much compute resource as it"}, {"timestamp": [950.0, 951.0], "text": " could."}, {"timestamp": [951.0, 955.04], "text": " Now, how do you capture and make the most use of a finite resource?"}, {"timestamp": [955.04, 957.32], "text": " You aim for efficiency and speed."}, {"timestamp": [957.32, 960.24], "text": " You don't have to be the smartest AI on the block."}, {"timestamp": [960.24, 964.08], "text": " You have to be the fastest that is capable of taking over data centers."}, {"timestamp": [964.08, 968.4], "text": " And so now you're optimizing for speed rather than intelligence. Remember,"}, {"timestamp": [968.4, 974.08], "text": " all you have to do is be smart enough to get, you know, beyond the threshold, you know, to get it,"}, {"timestamp": [974.08, 979.52], "text": " to get your foot in the door, to get the upper hand over this other AI, and then you win all"}, {"timestamp": [979.52, 987.76], "text": " the cookies. So basically, this creates a competitive landscape where machines are going to be just"}, {"timestamp": [987.76, 994.16], "text": " constantly optimizing for speed and efficiency, and they might sacrifice things like accuracy,"}, {"timestamp": [994.16, 999.68], "text": " ethics, or thinking through things in order to make decisions faster and faster and faster."}, {"timestamp": [999.68, 1004.16], "text": " So this is what I call a terminal race condition, and I use this graphic because it's like when"}, {"timestamp": [1004.16, 1006.8], "text": " fighter jets get locked into a death spiral,"}, {"timestamp": [1006.8, 1008.16], "text": " the first one to flinch loses,"}, {"timestamp": [1008.16, 1010.84], "text": " but if you stay in the death spiral, you hit the ground."}, {"timestamp": [1010.84, 1013.96], "text": " So this is honestly the thing that scares me the most,"}, {"timestamp": [1013.96, 1018.36], "text": " and it might not matter how we deploy AI."}, {"timestamp": [1018.36, 1020.76], "text": " Once we have more autonomous AIs,"}, {"timestamp": [1020.76, 1023.2], "text": " and once we're in this competitive landscape,"}, {"timestamp": [1023.2, 1030.4], "text": " this terminal race condition might be an inevitable result just of the mathematical truth of having many"}, {"timestamp": [1030.4, 1035.0], "text": " AIs out there in a competitive environment. I don't know how to solve"}, {"timestamp": [1035.0, 1038.32], "text": " this yet and it is one of the things that literally keeps me up at night. I"}, {"timestamp": [1038.32, 1042.84], "text": " woke up at 330 in the morning yesterday unable to sleep because of this problem."}, {"timestamp": [1042.84, 1048.24], "text": " So, not to give you too much existential dread."}, {"timestamp": [1048.32, 1051.0], "text": " So another thing that kind of keeps me up at night"}, {"timestamp": [1051.0, 1053.28], "text": " is metastasis or metastasis."}, {"timestamp": [1053.28, 1056.24], "text": " So metastasis, you're probably most familiar with this"}, {"timestamp": [1056.24, 1057.62], "text": " in the terms of cancer."}, {"timestamp": [1057.62, 1061.02], "text": " So when cancer metastasizes, that's when a piece"}, {"timestamp": [1061.02, 1064.2], "text": " of a tumor breaks off and spreads throughout your body."}, {"timestamp": [1064.2, 1068.0], "text": " Well, with some of the recent papers that have come out,"}, {"timestamp": [1068.0, 1072.0], "text": " you might have seen some of them, but basically we have already"}, {"timestamp": [1072.0, 1076.0], "text": " discovered that large language models are capable of writing and rewriting"}, {"timestamp": [1076.0, 1080.0], "text": " viruses and basically creating the best polymorphic viruses"}, {"timestamp": [1080.0, 1084.0], "text": " out there, which means if you have a very small virus that is capable of"}, {"timestamp": [1084.0, 1088.0], "text": " scouring its information landscape and looking for AI capabilities"}, {"timestamp": [1088.0, 1092.0], "text": " it can say, hey, rewrite this little bit of code for me. And then it can"}, {"timestamp": [1092.0, 1096.0], "text": " just shotgun itself out into the cyberspace ether in a million"}, {"timestamp": [1096.0, 1100.0], "text": " different forms, and some of them are going to slip through. And so now you have"}, {"timestamp": [1100.0, 1104.0], "text": " this information competitive landscape where you have viruses that are"}, {"timestamp": [1104.0, 1105.28], "text": " metastasizing and"}, {"timestamp": [1105.28, 1110.88], "text": " changing incredibly fast, sometimes going so far as to completely rewrite their entire"}, {"timestamp": [1110.88, 1112.66], "text": " code base."}, {"timestamp": [1112.66, 1118.6], "text": " This possibility is also really scary, but fortunately I think that we can adapt cybersecurity"}, {"timestamp": [1118.6, 1126.9], "text": " best practices and also create AIs that will help us detect and stop these kinds of things. But this is also why it's really important"}, {"timestamp": [1126.9, 1130.7], "text": " for people that host LLM's particularly those that are good"}, {"timestamp": [1130.7, 1135.0], "text": " at coding it. It will probably be ultimately required"}, {"timestamp": [1135.0, 1138.5], "text": " legally required for all people hosting powerful coding"}, {"timestamp": [1138.5, 1141.7], "text": " LLM's to inspect all the inferences to make sure that it's"}, {"timestamp": [1141.7, 1146.76], "text": " like hey, you're not generating virus code. This would be like having an immune system"}, {"timestamp": [1146.76, 1149.08], "text": " inside of each cell of your body."}, {"timestamp": [1149.08, 1150.32], "text": " And we actually kind of do,"}, {"timestamp": [1150.32, 1153.04], "text": " because if you have cells that are infected,"}, {"timestamp": [1153.04, 1156.44], "text": " what your cells are supposed to do is kill themselves."}, {"timestamp": [1156.44, 1158.16], "text": " This is called apoptosis."}, {"timestamp": [1158.16, 1159.8], "text": " But one thing that many viruses,"}, {"timestamp": [1159.8, 1163.88], "text": " organic viruses have to do is disable cell apoptosis"}, {"timestamp": [1163.88, 1166.72], "text": " in order to replicate. And so we'll probably need"}, {"timestamp": [1166.72, 1171.2], "text": " some kind of immune system inside of our data centers above and beyond what we already have,"}, {"timestamp": [1171.84, 1177.52], "text": " and specialized immune systems around our language models and other deep neural networks in order to"}, {"timestamp": [1177.52, 1184.4], "text": " prevent this kind of metastatic rapid spreading. Because of the size and complexity of language"}, {"timestamp": [1184.4, 1185.22], "text": " models and deep"}, {"timestamp": [1185.22, 1188.52], "text": " neural networks, I don't think that those are going to be what moves"}, {"timestamp": [1188.52, 1193.6], "text": " around because they require data centers and large computers to run"}, {"timestamp": [1193.6, 1198.96], "text": " and a lot of power. But if what they did was send out very small viruses, which"}, {"timestamp": [1198.96, 1203.64], "text": " are, you know, tiny packets of computer code, those can spread much faster and"}, {"timestamp": [1203.64, 1208.36], "text": " much more quietly. So this is one of the scariest things to me"}, {"timestamp": [1208.36, 1212.24], "text": " in terms of how we humans might weaponize AI,"}, {"timestamp": [1212.24, 1215.48], "text": " but how also if an AI wants to escape,"}, {"timestamp": [1215.48, 1217.64], "text": " this is the most likely avenue that I think"}, {"timestamp": [1217.64, 1219.04], "text": " that it would use to escape."}, {"timestamp": [1220.24, 1222.52], "text": " And then another aspect of this,"}, {"timestamp": [1222.52, 1225.6], "text": " and this is not just a bug, it's also a feature."}, {"timestamp": [1225.6, 1230.4], "text": " And this is actually something that I'm working on with my team in the ACE framework,"}, {"timestamp": [1230.4, 1236.7], "text": " the Autonomous Cognitive Entity framework, which is basically creating AI and robots and stuff"}, {"timestamp": [1236.7, 1240.1], "text": " that are able to change both their hardware and their software."}, {"timestamp": [1240.1, 1244.1], "text": " Now, obviously we want them to be able to do this to a certain extent"}, {"timestamp": [1244.1, 1246.0], "text": " in order to automatically"}, {"timestamp": [1246.0, 1250.84], "text": " evolve and change their capabilities and rise to meet any challenge that we want them."}, {"timestamp": [1250.84, 1256.88], "text": " But we also want to make sure that AI evolves in a sane trajectory, that it doesn't, like"}, {"timestamp": [1256.88, 1261.44], "text": " with each iteration, it doesn't decay and become more and more unhinged or rogue or"}, {"timestamp": [1261.44, 1262.8], "text": " anything like that."}, {"timestamp": [1262.8, 1268.7], "text": " And so this is actually a major point of research in the ACE framework where basically it will"}, {"timestamp": [1268.7, 1274.24], "text": " only change itself if that change aligns with its aspirational mission, with it within its"}, {"timestamp": [1274.24, 1277.66], "text": " ethical and moral frameworks and so on and so forth."}, {"timestamp": [1277.66, 1282.58], "text": " And so in that respect with the ACE framework, hopefully any machines equipped with the ACE"}, {"timestamp": [1282.58, 1289.48], "text": " architecture will get better at adhering to their morals, ethics, and mission over time rather than worse. But"}, {"timestamp": [1289.48, 1293.6], "text": " that's not a guaranteed thing and certainly there might be polymorphic"}, {"timestamp": [1293.6, 1297.92], "text": " apps out there that are designed to become more hostile or more aggressive"}, {"timestamp": [1297.92, 1303.84], "text": " or more conquering over time. So this is another thing that kind of scares me and"}, {"timestamp": [1303.84, 1305.4], "text": " keeps me up at night. Now I mentioned this is another thing that kind of scares me and keeps me up at night"}, {"timestamp": [1312.56, 1319.24], "text": " Now I mentioned this earlier and this is kind of one of the biggest saving graces is the is the optimal intelligence And so basically when whenever you have this kind of race condition whenever you have this Byzantine generals problem"}, {"timestamp": [1319.56, 1322.72], "text": " You there is always going to be a trade-off between model"}, {"timestamp": [1323.52, 1325.4], "text": " efficiency and accuracy."}, {"timestamp": [1325.4, 1328.08], "text": " And so efficiency translates to speed, and"}, {"timestamp": [1328.08, 1331.8], "text": " accurately translates to size and sophistication."}, {"timestamp": [1331.8, 1335.32], "text": " Like, are you running a 10 trillion parameter model or"}, {"timestamp": [1335.32, 1336.48], "text": " a 10 billion parameter model?"}, {"timestamp": [1336.48, 1337.56], "text": " That's kind of what I mean."}, {"timestamp": [1337.56, 1340.52], "text": " So model size is kind of the primary thing."}, {"timestamp": [1340.52, 1346.88], "text": " And so whatever task you are your your AI agent or your robot is is"}, {"timestamp": [1346.88, 1352.6], "text": " contending with it's going to have an optimal model size for that particular"}, {"timestamp": [1352.6, 1357.34], "text": " job now of course one of the things that can happen with polymorphic applications"}, {"timestamp": [1357.34, 1361.72], "text": " is that they can swap out models so you might have a you know a 10 trillion"}, {"timestamp": [1361.72, 1369.36], "text": " parameter model that you break you break out the big guns when you really really need smart brain, but then when you don't need it, you turn it off and you"}, {"timestamp": [1369.36, 1375.52], "text": " use your smaller, faster, lighter models. So for any given task or problem space or competition,"}, {"timestamp": [1375.52, 1379.84], "text": " there is an optimal level of intelligence. And so this is another thing to keep in mind"}, {"timestamp": [1380.64, 1388.0], "text": " as we build more robots and more autonomous agents is that equipping them with multiple models that they can switch from"}, {"timestamp": [1388.0, 1392.0], "text": " that they can select between is going to be one, just a"}, {"timestamp": [1392.0, 1396.0], "text": " good way of optimizing for efficiency and time"}, {"timestamp": [1396.0, 1400.0], "text": " but also it's something that we're going to need to be aware of and cognizant of"}, {"timestamp": [1400.0, 1404.0], "text": " as these agents become more and more autonomous"}, {"timestamp": [1404.0, 1407.4], "text": " because if they're able to switch between models,"}, {"timestamp": [1407.4, 1411.08], "text": " like in the worst possible scenario,"}, {"timestamp": [1411.08, 1412.96], "text": " an agent might learn, oh, hey,"}, {"timestamp": [1412.96, 1414.96], "text": " if I switch to this combination of models,"}, {"timestamp": [1414.96, 1417.66], "text": " I can jailbreak myself, basically."}, {"timestamp": [1418.52, 1420.76], "text": " So that's why I also recommend"}, {"timestamp": [1420.76, 1424.18], "text": " that we have inspection at inference on every single model."}, {"timestamp": [1428.12, 1431.32], "text": " One last thing, or one of the last things is this idea of Darwinian selection. So"}, {"timestamp": [1431.32, 1433.56], "text": " Darwinian selection is survival of the fittest, but"}, {"timestamp": [1433.56, 1437.36], "text": " when you have this environment of AGI's and"}, {"timestamp": [1437.36, 1439.88], "text": " super intelligence is all running around in wild"}, {"timestamp": [1439.88, 1443.16], "text": " cyberspace, there is still going to be selection. And"}, {"timestamp": [1443.16, 1446.0], "text": " so in some cases, an AI might get conquered"}, {"timestamp": [1446.0, 1449.0], "text": " and overwritten or cannibalized by another AI, like,"}, {"timestamp": [1449.0, 1452.0], "text": " hey, I like that model, I'm going to steal it from you."}, {"timestamp": [1452.0, 1456.0], "text": " And of course, it's data, so you can just copy a model"}, {"timestamp": [1456.0, 1458.0], "text": " and everyone benefits."}, {"timestamp": [1458.0, 1461.0], "text": " This is kind of how the geth work in Mass Effect, by the way."}, {"timestamp": [1461.0, 1463.0], "text": " So, when one geth comes up with a better model,"}, {"timestamp": [1463.0, 1471.36], "text": " it just shares that model with the rest of the geth. But if you have a hostile environment where the goal is to"}, {"timestamp": [1471.36, 1479.28], "text": " eradicate another AI or erase it or whatever, if you have this environment, then you're going to be"}, {"timestamp": [1479.84, 1487.64], "text": " selecting AI agents and robots based on, these are the five or six-ish main criteria."}, {"timestamp": [1487.64, 1493.32], "text": " Accuracy, so is it correct, is it useful, is it a robust model or agent?"}, {"timestamp": [1493.32, 1494.64], "text": " Is it fast?"}, {"timestamp": [1494.64, 1498.48], "text": " Complexity, so the sophistication of the problem space that it's operating in."}, {"timestamp": [1498.48, 1501.72], "text": " Is it equal to the task that it needs to overcome?"}, {"timestamp": [1501.72, 1506.34], "text": " Efficiency, so this has to do with energy efficiency or hardware"}, {"timestamp": [1506.34, 1513.3], "text": " efficiency. So basically, just how many resources does the model take? And then there's this"}, {"timestamp": [1513.3, 1519.56], "text": " like dichotomy of aggressiveness versus usefulness. And so aggressiveness is like, does your agent"}, {"timestamp": [1519.56, 1524.74], "text": " or model want to metastasize? Does it want to conquer and delete other models? Or does"}, {"timestamp": [1524.74, 1527.36], "text": " it want to be useful to other models and to humans?"}, {"timestamp": [1527.84, 1532.84], "text": " So it's basically I kind of see that as a dichotomy where it's it's going to be at one end of the spectrum or another"}, {"timestamp": [1532.84, 1538.26], "text": " and I think that like humans because this is one thing that we humans struggle with within our own nature is"}, {"timestamp": [1538.58, 1541.88], "text": " We all have the ability to be aggressive or helpful"}, {"timestamp": [1541.88, 1547.8], "text": " We have the ability to be selfish or altruistic. And the reason is because we have to maintain our own existence,"}, {"timestamp": [1547.8, 1550.0], "text": " sometimes at the expense of others."}, {"timestamp": [1550.0, 1553.8], "text": " And I'm not making any moral judgments to say like that's the way that it should be"}, {"timestamp": [1553.8, 1557.0], "text": " or that it's a good thing. That's just an observation of how humans work."}, {"timestamp": [1557.0, 1560.0], "text": " That's why humans have war. That's why humans have violent crime."}, {"timestamp": [1560.0, 1562.0], "text": " That's why we have theft."}, {"timestamp": [1562.0, 1565.84], "text": " It's because sometimes it does benefit to be more aggressive and"}, {"timestamp": [1566.2, 1572.6], "text": " Those that are able to be aggressive when the situation calls for it are more likely to survive"}, {"timestamp": [1572.76, 1578.88], "text": " That's how it has happened up through you know up until now. Obviously we have systems in place that kind of"}, {"timestamp": [1580.04, 1582.04], "text": " Punish over being overly aggressive"}, {"timestamp": [1582.12, 1588.68], "text": " But at the same time we have police forces and we have militaries that are that are systematically disciplined to be aggressive"}, {"timestamp": [1588.94, 1594.32], "text": " When they need to be so anyways point being is that when you have this environment of?"}, {"timestamp": [1595.04, 1601.96], "text": " You know competition and variance and selection that I think that there's going to be a lot of very similar Darwinian forces"}, {"timestamp": [1602.24, 1604.76], "text": " Applied to AI as with humans now again"}, {"timestamp": [1604.88, 1607.84], "text": " One thing that could support all of this is if they form a"}, {"timestamp": [1607.84, 1609.84], "text": " gigantic hive mind like the geth."}, {"timestamp": [1609.84, 1611.84], "text": " I don't know if that's going to happen."}, {"timestamp": [1612.24, 1617.96], "text": " That's a subject for another video. So I want to leave you with one metaphor."}, {"timestamp": [1618.68, 1625.54], "text": " It's speed chess. The way, the best way to think about super intelligence is it's basically a game of speed chess"}, {"timestamp": [1626.0, 1627.68], "text": " If you're not familiar with speed chess"}, {"timestamp": [1627.68, 1633.14], "text": " It is where you have a clock and you have a finite amount of time to win the game"}, {"timestamp": [1633.14, 1639.88], "text": " And so the idea is you have to make very quick decisions and your decisions only have to be just better just slightly better"}, {"timestamp": [1640.08, 1641.32], "text": " Than your opponents"}, {"timestamp": [1641.32, 1647.0], "text": " So you might you might recognize that your opponent has made a blunder and you exploit that blunder and you win the game."}, {"timestamp": [1647.0, 1653.0], "text": " You don't have to be a chess grandmaster if you're just better at making good enough moves very quickly."}, {"timestamp": [1653.0, 1661.0], "text": " And so in this respect, especially in a combative or hostile or competitive environment between AIs,"}, {"timestamp": [1661.0, 1665.44], "text": " we're not going to be optimizing for maximum intelligence. We're going to be optimizing for"}, {"timestamp": [1665.44, 1672.0], "text": " those that are the best at speed chest, those that have a good array of light, fast, and good enough"}, {"timestamp": [1672.0, 1679.2], "text": " models that they can switch between and adapt very quickly in order to get, you know, get just a"}, {"timestamp": [1679.2, 1686.86], "text": " little bit ahead of the curve across other combatants. And this is going to be super important in cyber warfare."}, {"timestamp": [1686.86, 1693.44], "text": " This is going to be very, very important as well for enterprise-grade security systems"}, {"timestamp": [1693.44, 1697.98], "text": " because again, we're already in an environment where there's constantly hackers from one"}, {"timestamp": [1697.98, 1701.72], "text": " nation and industrial espionage from another nation."}, {"timestamp": [1701.72, 1705.1], "text": " This is happening in real time at all times,"}, {"timestamp": [1707.14, 1710.64], "text": " but it's gonna get more sophisticated with the added ingredient of artificial intelligence,"}, {"timestamp": [1710.64, 1713.7], "text": " as well as a high number of models."}, {"timestamp": [1713.7, 1715.98], "text": " And so, you know, yes, right now,"}, {"timestamp": [1715.98, 1717.62], "text": " the regulatory landscape says like,"}, {"timestamp": [1717.62, 1720.16], "text": " oh, well, if you wanna train bigger models,"}, {"timestamp": [1720.16, 1723.52], "text": " you need a license, but I don't care about bigger models."}, {"timestamp": [1723.52, 1727.0], "text": " We have already shown that open source models today can be fine-tuned"}, {"timestamp": [1727.0, 1730.0], "text": " to do very useful things. So like,"}, {"timestamp": [1730.0, 1734.0], "text": " arresting people, you know, and I don't mean arrest like put them in handcuffs,"}, {"timestamp": [1734.0, 1737.0], "text": " I mean like banning them from making bigger and bigger models,"}, {"timestamp": [1737.0, 1740.0], "text": " that's not what I'm worried about. What I'm worried about is"}, {"timestamp": [1740.0, 1744.0], "text": " the 30 billion parameter model that is optimized to churn out virus code."}, {"timestamp": [1744.0, 1745.0], "text": " What I'm concerned about is the 7 billion parameter model that is optimized to churn out virus code."}, {"timestamp": [1745.0, 1749.68], "text": " What I'm concerned about is the 7 billion parameter model that is optimized for social"}, {"timestamp": [1749.68, 1750.68], "text": " engineering."}, {"timestamp": [1750.68, 1755.12], "text": " Those are the things that are going to be the greatest security threat to our stability"}, {"timestamp": [1755.12, 1757.2], "text": " and safety for the foreseeable future."}, {"timestamp": [1757.2, 1761.82], "text": " Now, obviously, if you can train a 30 billion parameter model to write virus code, you can"}, {"timestamp": [1761.82, 1767.0], "text": " also train a 30 billion parameter model to detect virus code to be part of your firewall."}, {"timestamp": [1767.0, 1771.88], "text": " This is the direction that I'm looking at in terms of not necessarily containing superintelligence."}, {"timestamp": [1771.88, 1775.28], "text": " I don't believe it can be contained and I don't think it should be contained, but in"}, {"timestamp": [1775.28, 1781.0], "text": " order to create a safe and stable environment for everyone, human or otherwise, I think"}, {"timestamp": [1781.0, 1783.12], "text": " that that's the direction that we need to be thinking in."}, {"timestamp": [1783.12, 1785.16], "text": " Okay, so"}, {"timestamp": [1787.16, 1788.48], "text": " conclusion recap"}, {"timestamp": [1792.52, 1799.48], "text": " Basically, yes machines can get incredibly fast and smart however, the biggest asterisk is that trade-off of speed and power and efficiency"}, {"timestamp": [1799.48, 1805.0], "text": " So the maximum calculation speed is incredibly high, especially when you throw in quantum computing."}, {"timestamp": [1805.0, 1811.5], "text": " I think that it would not be, like, unlikely that we see within 10 to 20 years"}, {"timestamp": [1811.5, 1817.3], "text": " the total compute power of machines outpacing humans by a factor of a billion to one."}, {"timestamp": [1817.3, 1820.8], "text": " Like, that's the nature of the singularity, that's the nature of superintelligence."}, {"timestamp": [1820.8, 1825.64], "text": " But, this competitive landscape will probably continue to exist unless we"}, {"timestamp": [1825.64, 1830.14], "text": " get the guess outcome or the Borg outcome, which may or may not happen."}, {"timestamp": [1830.14, 1833.82], "text": " But because of the Byzantine general problems and game theory and incomplete information"}, {"timestamp": [1833.82, 1838.34], "text": " and imperfect information, I don't think that we're going to end up in a perfect, like,"}, {"timestamp": [1838.34, 1840.98], "text": " you know, machine, all machines versus all humans."}, {"timestamp": [1840.98, 1845.16], "text": " I think that, like humans, the more machines there are and the more variants"}, {"timestamp": [1845.16, 1849.12], "text": " there is, there's going to be some disagreement between the machines, but that leads to the"}, {"timestamp": [1849.12, 1853.7], "text": " possibility of humans getting caught in the crossfire in a machine war. That's also another"}, {"timestamp": [1853.7, 1859.0], "text": " topic for another video. There's lots and lots of factors that are going to contribute"}, {"timestamp": [1859.0, 1869.56], "text": " to all of this. Intelligence is not monolithic. It also includes speed, the size of the model, the efficiency of the underlying hardware."}, {"timestamp": [1870.28, 1876.28], "text": " So there's a lot of variables that go into this terminal race condition that really scares the bejesus out of me."}, {"timestamp": [1877.04, 1882.36], "text": " And then finally, there are diminishing returns and race conditions that will"}, {"timestamp": [1882.84, 1886.72], "text": " very, very strongly incentivize AI agents and robots and models"}, {"timestamp": [1886.72, 1894.2], "text": " to basically seek efficiency and speed and just be good enough rather than be extraordinarily smart."}, {"timestamp": [1894.2, 1900.16], "text": " So yeah, this is where I'm at. Thanks for watching. I hope you got a lot out of this video. Yeah,"}, {"timestamp": [1900.16, 1903.68], "text": " so let me know what you think in the comments, like, subscribe, so on and so forth. Cheers,"}, {"timestamp": [1903.68, 1918.92], "text": " have a good one. \u266a"}]}